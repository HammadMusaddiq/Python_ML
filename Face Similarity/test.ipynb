{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "license_image = cv2.imread(\"/home/hammad/Downloads/WhatsApp Image 2022-10-04 at 12.01.39 AM.jpeg\")\n",
    "test_image = cv2.imread(\"/home/hammad/Downloads/WhatsApp Image 2022-10-04 at 12.01.59 AM.jpeg\")\n",
    "\n",
    "_, encoded_license_image = cv2.imencode('.png', license_image)\n",
    "_, encoded_test_image = cv2.imencode('.png', test_image)\n",
    "\n",
    "\n",
    "bytes_license_image = encoded_license_image.tobytes()\n",
    "bytes_test_image = encoded_test_image.tobytes()\n",
    "\n",
    "print(type(bytes_license_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('An Exception Occured: embedded null byte', 500)\n"
     ]
    }
   ],
   "source": [
    "!python3 face_similarity.py \"/home/hammad/Downloads/WhatsApp Image 2022-10-04 at 12.01.39 AM.jpeg\" \"/home/hammad/Downloads/WhatsApp Image 2022-10-04 at 12.01.59 AM.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 17:55:55.665075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-05 17:55:55.854804: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-05 17:55:55.854832: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-05 17:55:55.893147: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-05 17:55:57.140662: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-05 17:55:57.140903: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-05 17:55:57.140922: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from deepface import DeepFace # pip install deepface\n",
    "from retinaface import RetinaFace # pip install retina-face\n",
    "# from mtcnn.mtcnn import MTCNN # pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    # return load_model('facenet_keras.h5')\n",
    "\treturn DeepFace.build_model(\"Facenet\")\n",
    "\n",
    "def detect_face(image_array):\n",
    "\tface_results = RetinaFace.detect_faces(image_array)\n",
    "\tprint(\"Number of faces found from Image: \" +str(len(face_results)))\n",
    "\treturn face_results\n",
    "\n",
    "def get_embedding(face_pixels):\n",
    "    # scale pixel values\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    # standardize pixel values across channels (global)\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean) / std\n",
    "    # transform face into one sample\n",
    "    samples = expand_dims(face_pixels, axis=0)\n",
    "    # make prediction to get embedding\n",
    "    yhat = getModel().predict(samples)\n",
    "    return yhat[0]\n",
    "\n",
    "# extract a single face from a given photograph\n",
    "def extract_face(image_array, face_box, required_size=(160, 160)):\n",
    "\t# extract the bounding box from the first face\n",
    "\tx1, y1, width, height = face_box\n",
    "\t# bug fix\n",
    "\tx1, y1 = abs(x1), abs(y1)\n",
    "\tx2, y2 = x1 + width, y1 + height\n",
    "\t# extract the face\n",
    "\tface = image_array[y1:y2, x1:x2]\n",
    "\t# resize pixels to the model size\n",
    "\timage = Image.fromarray(face)\n",
    "\timage = image.resize(required_size)\n",
    "\tface_array = asarray(image)\n",
    "\n",
    "\ttarget_embedding = get_embedding(face_array)\n",
    "\treturn target_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_act = '/home/hammad/Downloads/Hammad.jpg'\n",
    "ham_lic = '/home/hammad/Downloads/license3.jpg'\n",
    "\n",
    "_lic = \"/home/hammad/Downloads/WhatsApp Image 2022-10-04 at 12.01.39 AM.jpeg\"\n",
    "_act1 = \"/home/hammad/Downloads/WhatsApp Image 2022-10-04 at 12.01.59 AM.jpeg\"\n",
    "_act2 = \"/home/hammad/Downloads/WhatsApp Image 2022-10-04 at 12.03.23 AM.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 1200, 3)\n",
      "(406, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "actual_image_array = np.asarray(Image.open(_act1))\n",
    "license_image_array = np.asarray(Image.open(ham_lic))\n",
    "print(actual_image_array.shape)\n",
    "print(license_image_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces found from Image: 1\n",
      "{'face_1': {'score': 0.9993330836296082, 'facial_area': [1155, 1151, 1860, 2080], 'landmarks': {'right_eye': [1343.0, 1527.943], 'left_eye': [1680.0652, 1504.1787], 'nose': [1527.2783, 1691.588], 'mouth_right': [1391.7368, 1865.6737], 'mouth_left': [1662.9031, 1847.1886]}}}\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "No. of Embeddings in actual Image: 1\n",
      "#########\n",
      "Number of faces found from Image: 3\n",
      "{'face_1': {'score': 0.9995424151420593, 'facial_area': [451, 244, 505, 317], 'landmarks': {'right_eye': [466.29004, 273.76425], 'left_eye': [492.0351, 274.3157], 'nose': [478.9465, 287.6408], 'mouth_right': [466.97583, 297.65808], 'mouth_left': [490.81198, 297.99496]}}, 'face_2': {'score': 0.9993719458580017, 'facial_area': [62, 128, 208, 323], 'landmarks': {'right_eye': [98.805, 200.3082], 'left_eye': [168.17332, 200.96437], 'nose': [131.01694, 234.30092], 'mouth_right': [107.446396, 275.74515], 'mouth_left': [158.6073, 275.91513]}}, 'face_3': {'score': 0.993498682975769, 'facial_area': [588, 91, 618, 125], 'landmarks': {'right_eye': [593.6529, 103.76423], 'left_eye': [604.93414, 102.425], 'nose': [598.0696, 112.06852], 'mouth_right': [599.23395, 118.15748], 'mouth_left': [607.3763, 116.84094]}}}\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "No. of Embeddings in license Image: 3\n"
     ]
    }
   ],
   "source": [
    "actual_embedding_list = []\n",
    "actual_recognized_boxes = []\n",
    "\n",
    "actual_face_conf = None\n",
    "acutal_face_box = None\n",
    "\n",
    "actual_face_results = detect_face(actual_image_array)\n",
    "print(actual_face_results)\n",
    "\n",
    "for actual_face in actual_face_results:\n",
    "    try:\n",
    "        actual_face = actual_face_results.get(actual_face)  # Retina Face\n",
    "        actual_face_conf = actual_face['score']\n",
    "        actual_face_box = actual_face['facial_area']\n",
    "        actual_face_box = [actual_face_box[0],actual_face_box[1],actual_face_box[2]-actual_face_box[0], actual_face_box[3]-actual_face_box[1]] \n",
    "    except:\n",
    "        print(\"Extracted face has no data.\")    \n",
    "        continue\n",
    "    \n",
    "    if actual_face_conf > 0.95:\n",
    "        actual_target_embedding = extract_face(actual_image_array, actual_face_box)\n",
    "        actual_embedding_list.append(actual_target_embedding)    \n",
    "        actual_recognized_boxes.append(actual_face_box)\n",
    "\n",
    "print(\"No. of Embeddings in actual Image: \" + str(len(actual_embedding_list)))\n",
    "print(\"#########\")\n",
    "\n",
    "\n",
    "license_embedding_list = []\n",
    "license_recognized_boxes = []\n",
    "\n",
    "license_face_conf = None\n",
    "license_face_box = None\n",
    "\n",
    "license_face_results = detect_face(license_image_array)\n",
    "print(license_face_results)\n",
    "\n",
    "for license_face in license_face_results:\n",
    "    try:\n",
    "        license_face = license_face_results.get(license_face)  # Retina Face\n",
    "        license_face_conf = license_face['score']\n",
    "        license_face_box = license_face['facial_area']\n",
    "        license_face_box = [license_face_box[0],license_face_box[1],license_face_box[2]-license_face_box[0], license_face_box[3]-license_face_box[1]] \n",
    "    except:\n",
    "        print(\"Extracted face has no data.\")    \n",
    "        continue\n",
    "    \n",
    "    if license_face_conf > 0.95:\n",
    "        license_target_embedding = extract_face(license_image_array, license_face_box)\n",
    "        license_embedding_list.append(license_target_embedding)    \n",
    "        license_recognized_boxes.append(license_face_box)\n",
    "\n",
    "\n",
    "print(\"No. of Embeddings in license Image: \" + str(len(license_embedding_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(list1, list2):\n",
    "    \"\"\"Distance between two vectors.\"\"\"\n",
    "    squares = [(p-q) ** 2 for p, q in zip(list1, list2)]\n",
    "    return sum(squares) ** .5\n",
    "\n",
    "\n",
    "distances = []\n",
    "for ac_emb in actual_embedding_list:\n",
    "    for li_emb in license_embedding_list:  \n",
    "        distances.append(distance(ac_emb, li_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.369561071261966, 6.049696774019564, 13.100216410626743]\n"
     ]
    }
   ],
   "source": [
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def plotAnnotation(_boxes, image):\n",
    "    # plot annotation\n",
    "    annotated_image = image\n",
    "    for box in _boxes:\n",
    "        # annotated_image = cv2.rectangle(annotated_image, (box[0], box[1]), (box[2]+box[0], box[3]+box[1]), (255,0,0), 3) \n",
    "        annotated_image = cv2.rectangle(annotated_image, (box[0], box[1]), (box[2], box[3]), (255,0,0), 3) \n",
    "        # annotated_image = cv2.putText(annotated_image, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 2)\n",
    "    return annotated_image\n",
    "\n",
    "# ann_image = plotAnnotation([[62, 128, 208, 323]], license_image_array)\n",
    "ann_image = plotAnnotation([[1155, 1151, 1860, 2080]], actual_image_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"\", ann_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California\n",
      "DRIVER LICENSE\n",
      "u 11234568\n",
      "08/31/201\n",
      "END NONE\n",
      "LNCARDHOLDER\n",
      "FNIMA\n",
      "257024TH TREET\n",
      "ANYTOWN, CA 95818\n",
      "31977\n",
      "lb Iks\n",
      "DD OOIOO/OOOONNNAN/ANFD/YY\n",
      "08/31/2009\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import json\n",
    "ocr_api_key = 'K83433112388957'\n",
    "ocr_lang = 'eng'\n",
    "ocr_overlay = 'true'\n",
    "\n",
    "\n",
    "def plate_preproces(self, plate):\n",
    "    plate = ''.join(e for e in plate if e.isalnum()) # remove special characters\n",
    "    \n",
    "    plate = re.sub('(\\d+(\\.\\d+)?)', r' \\1 ', plate).strip() # space between text and number\n",
    "    \n",
    "\n",
    "    #plate = re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ', plate).strip() # space before small and capital letters\n",
    "\n",
    "    # string to list to check words\n",
    "    splits = plate.split()\n",
    "    if len(splits) > 3:\n",
    "        for word in splits:\n",
    "            if not (re.search('^[0-9]+$', word)) and len(word) > 4: # if word is not number\n",
    "                if word[0].isupper() and word[-1].islower(): # word start with upper and end lower case, removed from string\n",
    "                    plate = plate.replace(word, \"\").strip()\n",
    "    \n",
    "    return plate\n",
    "\n",
    "\n",
    "def img_ocr(img_path):        \n",
    "    ## To read image_path for ocr\n",
    "    payload = {'isOverlayRequired': ocr_overlay,\n",
    "        'apikey': ocr_api_key,\n",
    "        'language': ocr_lang,\n",
    "        }\n",
    "\n",
    "    with open(img_path, 'rb') as f:\n",
    "        r = requests.post('https://api.ocr.space/parse/image',\n",
    "            files={img_path: f},\n",
    "            data=payload,\n",
    "            )\n",
    "\n",
    "    con = json.loads(r.text)\n",
    "\n",
    "    return con['ParsedResults'][0]['ParsedText']\n",
    "\n",
    "\n",
    "# im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "# # enhance image\n",
    "# im1 = cv2.detailEnhance(im, sigma_s=25, sigma_r=0.15)\n",
    "# # set threshold on image\n",
    "# _ , thresh1 = cv2.threshold(im1, 60, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# OCR\n",
    "plate = img_ocr('/home/hammad/Downloads/license3.jpg').strip()\n",
    "print(plate)\n",
    "\n",
    "# kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
    "# im2 = cv2.filter2D(src=im1, ddepth=-1, kernel=kernel)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California DRIVER LICENSE u 11234568 08/31/201 END NONE LNCARDHOLDER FNIMA 257024TH TREET ANYTOWN, CA 95818 31977 lb Iks DD OOIOO/OOOONNNAN/ANFD/YY 08/31/2009\n",
      "11234568\n"
     ]
    }
   ],
   "source": [
    "len(plate)\n",
    "# mystr = '\\t'.join([line.strip() for line in plate])\n",
    "mystr = plate.replace(\"\\r\", \"\\t\")\n",
    "mystr = mystr.replace(\"\\t\", \" \")\n",
    "mystr = mystr.replace(\"\\n\", \"\")\n",
    "print(mystr)\n",
    "\n",
    "license_number = None\n",
    "word_list = mystr.split() # string to list\n",
    "\n",
    "for word in word_list:\n",
    "    if (re.search('^[0-9]+$', word)) and len(word) == 8: # if word is a number\n",
    "        license_number = word\n",
    "\n",
    "print(license_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.049696774019564\n"
     ]
    }
   ],
   "source": [
    "for dis in distances:\n",
    "    if dis <= 7:\n",
    "        print(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 1st image and store encodings\n",
    "import face_recognition # pip install face-recognition\n",
    "image = cv2.imread(_lic)\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# boxes = face_recognition.face_locations(rgb, model=args[\"detection_method\"])\n",
    "boxes = face_recognition.face_locations(rgb)\n",
    "encodings1 = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "# read 2nd image and store encodings\n",
    "image = cv2.imread(_act1)\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "boxes = face_recognition.face_locations(rgb)\n",
    "encodings2 = face_recognition.face_encodings(rgb, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encodings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now you can compare two encodings\n",
    "# optionally you can pass threshold, by default it is 0.6\n",
    "matches = face_recognition.compare_faces(encodings1, encodings2[0])\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "known_image = face_recognition.load_image_file(ham_act)\n",
    "unknown_image = face_recognition.load_image_file(_act2)\n",
    "\n",
    "biden_encoding = face_recognition.face_encodings(known_image)[0]\n",
    "unknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n",
    "\n",
    "results = face_recognition.compare_faces([biden_encoding], unknown_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknown_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pickle to save data into a file for later use\n",
    "data = {\"encodings\": list_of_embeddings, \"names\": list_of_names}\n",
    "\n",
    "import pickle\n",
    "f = open(\"encodings1\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n",
    "\n",
    "data = pickle.loads(open(\"encodings1\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09894553,  0.03710539,  0.12140112, -0.06968689, -0.04618154,\n",
       "       -0.02325607,  0.02554273,  0.00117452,  0.12625618, -0.06539699,\n",
       "        0.24817342, -0.03510082, -0.17853895, -0.12592055,  0.06113213,\n",
       "        0.09943514, -0.07800653, -0.11726709, -0.04366607, -0.13603181,\n",
       "        0.00731657, -0.01026938, -0.02889294,  0.01736676, -0.19926412,\n",
       "       -0.30530533, -0.05118365, -0.09217598,  0.01016703, -0.11577142,\n",
       "       -0.01402129,  0.06626892, -0.21881552, -0.07556601, -0.05989213,\n",
       "        0.11613798,  0.01260524, -0.05877529,  0.09666663,  0.02629403,\n",
       "       -0.08538578, -0.08446164,  0.03968419,  0.26120317,  0.16975442,\n",
       "        0.06220271,  0.04370676,  0.05502441,  0.03422422, -0.23578916,\n",
       "        0.06754594,  0.05688101,  0.11091635,  0.04913695,  0.01482866,\n",
       "       -0.1057803 ,  0.0305656 ,  0.01871623, -0.17208129,  0.02376074,\n",
       "        0.04391514, -0.05559644, -0.0513586 , -0.00323782,  0.34716722,\n",
       "        0.07585101, -0.08871283, -0.07271168,  0.19027714, -0.09187503,\n",
       "       -0.07648934, -0.01339838, -0.10789905, -0.08158435, -0.29238418,\n",
       "        0.1462442 ,  0.33077407,  0.10890359, -0.17412594,  0.06883043,\n",
       "       -0.18536554,  0.01920893,  0.01212471, -0.00429486, -0.06151868,\n",
       "        0.04962195, -0.12944448,  0.04370526,  0.20360538, -0.01560865,\n",
       "       -0.03439928,  0.1585744 ,  0.00050106, -0.0141432 ,  0.03917155,\n",
       "       -0.01147215, -0.09344003,  0.03588177, -0.09271587, -0.03710464,\n",
       "        0.15585616, -0.06253912,  0.02374561,  0.16673225, -0.23721334,\n",
       "        0.16598861, -0.00850413, -0.04187276,  0.05163712,  0.0387172 ,\n",
       "       -0.13375302, -0.09552757,  0.11923992, -0.23229338,  0.11379621,\n",
       "        0.16937873,  0.05582963,  0.14251566,  0.10919561,  0.08155957,\n",
       "        0.02329342,  0.01474612, -0.13434944, -0.06117829,  0.05426149,\n",
       "       -0.02513186,  0.06480553,  0.12402289])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for val in [True,False]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'license_matched': True}\n"
     ]
    }
   ],
   "source": [
    "if True in [val for val in [False,True]]:\n",
    "    print({\"license_matched\" : True})\n",
    "else:\n",
    "    print({\"license_matched\" : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "from facepplib import FacePP, exceptions # pip install python-facepp\n",
    "import emoji # pip install emoji\n",
    "  \n",
    "   \n",
    "# define global variables\n",
    "face_detection = \"\"\n",
    "faceset_initialize = \"\"\n",
    "face_search = \"\"\n",
    "face_landmarks = \"\"\n",
    "dense_facial_landmarks = \"\"\n",
    "face_attributes = \"\"\n",
    "beauty_score_and_emotion_recognition = \"\"\n",
    "   \n",
    "# define face comparing function\n",
    "def face_comparing(app, Image1, Image2):\n",
    "      \n",
    "    print()\n",
    "    print('-'*30)\n",
    "    print('Comparing Photographs......')\n",
    "    print('-'*30)\n",
    "  \n",
    "   \n",
    "    cmp_ = app.compare.get(image_url1 = Image1,\n",
    "                           image_url2 = Image2)\n",
    "   \n",
    "    print('Photo1', '=', cmp_.image1)\n",
    "    print('Photo2', '=', cmp_.image2)\n",
    "   \n",
    "    # Comparing Photos\n",
    "    if cmp_.confidence > 70:\n",
    "        print('Both photographs are of same person......')\n",
    "    else:\n",
    "        print('Both photographs are of two different persons......')\n",
    "  \n",
    "          \n",
    "# Driver Code \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    # api details\n",
    "    api_key ='xQLsTmMyqp1L2MIt7M3l0h-cQiy0Dwhl'\n",
    "    api_secret ='TyBSGw8NBEP9Tbhv_JbQM18mIlorY6-D'\n",
    "   \n",
    "    try:\n",
    "   \n",
    "        # create a logo of app by using iteration,\n",
    "        # unicode and emoji module-------------\n",
    "        for i in range(1,6):\n",
    "              \n",
    "            for j in range(6,-i):\n",
    "                print(\" \" , end = \" \")\n",
    "                  \n",
    "            for j in range(1,i):\n",
    "                print('\\U0001F600', end =\" \")\n",
    "                  \n",
    "            for j in range(i,0,-1):\n",
    "                print('\\U0001F6A3', end= \" \")\n",
    "                  \n",
    "            for j in range(i,1,-2):\n",
    "                print('\\U0001F62B', end= \" \")\n",
    "                  \n",
    "            print()\n",
    "              \n",
    "        print()\n",
    "   \n",
    "        #print name of the app--------\n",
    "        print(\"\\t\\t\\t\"+\"Photo Comparing App\\n\")\n",
    "       \n",
    "        for i in range(1,6):\n",
    "              \n",
    "            for j in range(6,-i):\n",
    "                print(\" \" , end = \" \")\n",
    "                  \n",
    "            for j in range(1,i):\n",
    "                print(emoji.emojize(\":princess:\"), end =\" \")\n",
    "                  \n",
    "            for j in range(i,0,-1):\n",
    "                print('\\U0001F610', end= \" \")\n",
    "                  \n",
    "            for j in range(i,1,-2):\n",
    "                print(emoji.emojize(\":baby:\"), end= \" \")\n",
    "                  \n",
    "            print()\n",
    "           \n",
    "        # call api\n",
    "        app_ = FacePP(api_key = api_key, \n",
    "                      api_secret = api_secret)\n",
    "        funcs = [\n",
    "            face_detection,\n",
    "            face_comparing_localphoto,\n",
    "            face_comparing_websitephoto,\n",
    "            faceset_initialize,\n",
    "            face_search,\n",
    "            face_landmarks,\n",
    "            dense_facial_landmarks,\n",
    "            face_attributes,\n",
    "            beauty_score_and_emotion_recognition\n",
    "        ]\n",
    "          \n",
    "        # Pair 1\n",
    "        image1 = 'Image 1 link'\n",
    "        image2 = 'Image 2 link'\n",
    "        face_comparing(app_, image1, image2)\n",
    "          \n",
    "        # Pair2\n",
    "        image1 = 'Image 1 link'\n",
    "        image2 = 'Image 2 link'\n",
    "        face_comparing(app_, image1, image2)        \n",
    "   \n",
    "    except exceptions.BaseFacePPError as e:\n",
    "        print('Error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'face_pixels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert each face in the train set to an embedding\u001b[39;00m\n\u001b[1;32m      2\u001b[0m newTrainX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m----> 3\u001b[0m embedding \u001b[38;5;241m=\u001b[39m get_embedding(getModel, \u001b[43mface_pixels\u001b[49m)\n\u001b[1;32m      4\u001b[0m newTrainX\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[1;32m      5\u001b[0m newTrainX \u001b[38;5;241m=\u001b[39m asarray(newTrainX)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'face_pixels' is not defined"
     ]
    }
   ],
   "source": [
    "# convert each face in the train set to an embedding\n",
    "from numpy import savez_compressed\n",
    "\n",
    "Image_Embeddings = list()\n",
    "Image_Embeddings = asarray(embedding_list)\n",
    "print(Image_Embeddings.shape)\n",
    "\n",
    "# convert each face in the test set to an embedding\n",
    "# newTestX = list()\n",
    "# for face_pixels in testX:\n",
    "# \tembedding = get_embedding(model, face_pixels)\n",
    "# \tnewTestX.append(embedding)\n",
    "# newTestX = asarray(newTestX)\n",
    "# print(newTestX.shape)\n",
    "\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('faces-embeddings.npz', Image_Embeddings, trainy, newTestX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(list1, list2):\n",
    "    \"\"\"Distance between two vectors.\"\"\"\n",
    "    squares = [(p-q) ** 2 for p, q in zip(list1, list2)]\n",
    "    return sum(squares) ** .5\n",
    "\n",
    "d2 = distance(source_emb, target_emb1)  \n",
    "print(d2)\n",
    "\n",
    "def image_distances(source_emb, target_emb):\n",
    "    return [(x - y) * (x - y) for x,y in zip(source_emb, target_emb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ((x - y)**2 + (x - y)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
