# -*- coding: utf-8 -*-
"""my_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NKKXJEcZ9CU79qbpNtjPY1lPLQC-Fkl7
"""

# from google.colab import drive

# # Mount Google Drive to access your files
# drive.mount('/content/drive')

import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split

# Define the path to the CSV file
csv_file_path = r"D:\PHD_DOCS\Korea\Yonsei-vnl-coding-assignment-vision-48hrs\Yonsei-vnl-coding-assignment-vision-48hrs\dataset\data\cifar100_nl.csv"
test_csv = r"D:\PHD_DOCS\Korea\Yonsei-vnl-coding-assignment-vision-48hrs\Yonsei-vnl-coding-assignment-vision-48hrs\dataset\data\cifar100_nl_test.csv"

# Read the CSV file into a Pandas DataFrame
df = pd.read_csv(csv_file_path, header=None)
df_test = pd.read_csv(test_csv, header=None)

df.dropna(subset=[1], inplace=True)
df_test.dropna(subset=[1], inplace=True)

# df[0] = (
#     "/content/drive/MyDrive/korea/Yonsei-vnl-coding-assignment-vision-48hrs/dataset/"
#     + df[0]
# )


# Split the dataset into training and validation sets
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# Define a label mapping (convert class names to numeric labels)
class_to_index = {class_name: index for index, class_name in enumerate(df[1].unique())}

# Convert class names to numeric labels in the training and validation DataFrames
train_df[1] = train_df[1].map(class_to_index)
val_df[1] = val_df[1].map(class_to_index)

df_test[1] = df_test[1].map(class_to_index)


# Define a function to load and preprocess images
def load_and_preprocess_image(image_path, label):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_png(
        image, channels=3
    )  # Assuming PNG format, adjust if needed
    image = tf.image.resize(image, (32, 32))  # Resize to the desired input size
    image = tf.image.convert_image_dtype(
        image, tf.float32
    )  # Normalize pixel values to [0, 1]
    return image, label


##Test
test_dataset = tf.data.Dataset.from_tensor_slices((df_test[0], df_test[1]))

test_dataset = test_dataset.map(load_and_preprocess_image)
test_dataset = test_dataset.batch(batch_size=64)
test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)


## Train
train_dataset = tf.data.Dataset.from_tensor_slices((train_df[0], train_df[1]))

# Create a TensorFlow Dataset from the training DataFrame

train_dataset = train_dataset.map(load_and_preprocess_image)
train_dataset = train_dataset.shuffle(buffer_size=10000)
train_dataset = train_dataset.batch(batch_size=64)
train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

# Create a TensorFlow Dataset from the validation DataFrame
val_dataset = tf.data.Dataset.from_tensor_slices((val_df[0], val_df[1]))
val_dataset = val_dataset.map(load_and_preprocess_image)
val_dataset = val_dataset.batch(batch_size=64)
val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

# Optionally, you can apply more data augmentation techniques here if needed

import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

model = keras.Sequential(
    [
        # Convolutional Layer 1: 32 filters, each 3x3, activation ReLU
        keras.layers.Conv2D(32, (3, 3), activation="relu", input_shape=(32, 32, 3)),
        # MaxPooling Layer 1: Pool size 2x2
        keras.layers.MaxPooling2D((2, 2)),
        # Convolutional Layer 2: 64 filters, each 3x3, activation ReLU
        keras.layers.Conv2D(64, (3, 3), activation="relu"),
        # MaxPooling Layer 2: Pool size 2x2
        keras.layers.MaxPooling2D((2, 2)),
        # Convolutional Layer 3: 128 filters, each 3x3, activation ReLU
        keras.layers.Conv2D(128, (3, 3), activation="relu"),
        # Flatten Layer: Flatten the output for the fully connected layers
        keras.layers.Flatten(),
        # Fully Connected Layer 1: 128 neurons, activation ReLU
        keras.layers.Dense(128, activation="relu"),
        # Fully Connected Layer 2: 100 neurons (output layer), activation Softmax
        keras.layers.Dense(100, activation="softmax"),
    ]
)

# Note:
# - Convolutional layers (Conv2D) are responsible for feature extraction.
# - MaxPooling layers reduce spatial dimensions and help with translation invariance.
# - Flatten layer converts the 2D feature maps into a 1D vector for fully connected layers.
# - Fully connected layers (Dense) are responsible for classification.

# You can adjust the number of filters, filter sizes, and other hyperparameters
# according to your specific requirements and experimentation.

# Compile the model
model.compile(
    optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]
)

# Train the model (REPORT2)
history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, verbose=1)

plt.plot(history.history["accuracy"], label="Training Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

# Evaluate the model on the test dataset (assuming you have a test dataset)
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f"Test Accuracy: {test_accuracy}")

# Add test accuracy to the plot
plt.axhline(y=test_accuracy, color="r", linestyle="--", label="Test Accuracy")

plt.legend()
plt.show()
