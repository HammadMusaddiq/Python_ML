{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhbTcIglX1Qt",
        "outputId": "d1c037e1-da0d-4a55-f778-ecfba500fc4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p2Nghg5uX6B4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score, recall_score\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hbfBFVrWX6kq"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from a CSV file\n",
        "df = pd.read_csv('outdata_M.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "QL2ru_OeYF90",
        "outputId": "5a3de86f-7f1d-44ac-ca14-30f3be6ff76b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   NOTES  EVENT_TYPE\n",
              "0      5 January: Beheading of 5 citizens in Douaouda...           2\n",
              "1                Two citizens were beheaded in Hassasna.           2\n",
              "2      Two citizens were killed in a raid on the vill...           2\n",
              "3      4 January: 16 citizens were murdered in the vi...           2\n",
              "4      5 January: Killing of 18 citizens in the Olivi...           2\n",
              "...                                                  ...         ...\n",
              "65528  OLF and Borana ethnic militia attack civilians...           2\n",
              "65529  Ethnic raiders and OLF attack Degodia Somali e...           2\n",
              "65530  OLF and Borana ethnic militia attack civilians...           2\n",
              "65531  University students protest over killing in Wa...           3\n",
              "65532  Garangs SPLA security men raid home of Kerubin...           1\n",
              "\n",
              "[65533 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec87295f-af26-46cc-a81f-96f64ed71bbd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NOTES</th>\n",
              "      <th>EVENT_TYPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 January: Beheading of 5 citizens in Douaouda...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Two citizens were beheaded in Hassasna.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Two citizens were killed in a raid on the vill...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 January: 16 citizens were murdered in the vi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5 January: Killing of 18 citizens in the Olivi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65528</th>\n",
              "      <td>OLF and Borana ethnic militia attack civilians...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65529</th>\n",
              "      <td>Ethnic raiders and OLF attack Degodia Somali e...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65530</th>\n",
              "      <td>OLF and Borana ethnic militia attack civilians...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65531</th>\n",
              "      <td>University students protest over killing in Wa...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65532</th>\n",
              "      <td>Garangs SPLA security men raid home of Kerubin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65533 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec87295f-af26-46cc-a81f-96f64ed71bbd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ec87295f-af26-46cc-a81f-96f64ed71bbd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ec87295f-af26-46cc-a81f-96f64ed71bbd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zluNf_6Wbf01",
        "outputId": "7d3b1981-6b62-4a74-c8d7-4bc7f2d22b72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(65533, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcnpwNwMXojU"
      },
      "source": [
        "# **Text Preprocessing: **\n",
        "Clean the text data by removing stop words, special characters, punctuation, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_WflxqkXkPU",
        "outputId": "670d652c-61b1-4026-a157-fbfc77b7ef3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "# Lowercase text\n",
        "df[\"NOTES\"] = df[\"NOTES\"].str.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "df[\"NOTES\"] = df[\"NOTES\"].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)) if isinstance(x, str) else '')\n",
        "\n",
        "\n",
        "# Remove stop words\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df[\"NOTES\"] = df[\"NOTES\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "# Remove numbers\n",
        "df[\"NOTES\"] = df[\"NOTES\"].apply(lambda x: re.sub(r'\\d', '', str(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "-2C_t_krXkLd",
        "outputId": "be373189-5ae8-4e9f-a1b5-56f35398fd00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   NOTES  EVENT_TYPE\n",
              "0            january beheading  citizens douaouda tipaza           2\n",
              "1                         two citizens beheaded hassasna           2\n",
              "2      two citizens killed raid village hassi el abd ...           2\n",
              "3       january  citizens murdered village benachour ...           2\n",
              "4       january killing  citizens oliviers district d...           2\n",
              "...                                                  ...         ...\n",
              "65528  olf borana ethnic militia attack civilians mas...           2\n",
              "65529  ethnic raiders olf attack degodia somali ethni...           2\n",
              "65530  olf borana ethnic militia attack civilians mas...           2\n",
              "65531  university students protest killing wajir dist...           3\n",
              "65532  garangs spla security men raid home kerubino s...           1\n",
              "\n",
              "[65533 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9fcabc4f-0db9-4d0c-88ed-e9e3756cb45e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NOTES</th>\n",
              "      <th>EVENT_TYPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>january beheading  citizens douaouda tipaza</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>two citizens beheaded hassasna</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>two citizens killed raid village hassi el abd ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>january  citizens murdered village benachour ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>january killing  citizens oliviers district d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65528</th>\n",
              "      <td>olf borana ethnic militia attack civilians mas...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65529</th>\n",
              "      <td>ethnic raiders olf attack degodia somali ethni...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65530</th>\n",
              "      <td>olf borana ethnic militia attack civilians mas...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65531</th>\n",
              "      <td>university students protest killing wajir dist...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65532</th>\n",
              "      <td>garangs spla security men raid home kerubino s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65533 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fcabc4f-0db9-4d0c-88ed-e9e3756cb45e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9fcabc4f-0db9-4d0c-88ed-e9e3756cb45e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9fcabc4f-0db9-4d0c-88ed-e9e3756cb45e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygratB4PXkIL",
        "outputId": "3f8cf956-f119-481f-d0ab-982e6a46696a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NOTES         0\n",
              "EVENT_TYPE    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyjYvfqYXkEj",
        "outputId": "249accb6-ba4d-445b-f905-40615584824c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(65533, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.dropna(axis=0, how=\"any\", thresh=None, subset=None, inplace=False).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2aE3uBgDXkBK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a7cETjBaXj9-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fsTWW7USXj6U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uUQfrJAFFb2P"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zjdFCHsBYO7-"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe['NOTES']\n",
        "        self.targets = dataframe['EVENT_TYPE']\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q085l2-fFRlG"
      },
      "outputs": [],
      "source": [
        "# Sections of config\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VX26TAMHG3w2"
      },
      "outputs": [],
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "C2S9i4BxFWGd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "94ac58e26f3c45469861564941ab3ca4",
            "f1f330798f4f4fbfb878fca6b7920704",
            "aacabf4b1fa0410fa40f70eed86caf8e",
            "6ff50e0be48742608a8a46bfcec18667",
            "78d0c5887b794e8f978380b539e7c937",
            "6cb9f41b732b4d02881d4707bc6880b5",
            "58c97a9ebf5c4ef290eaad5d23e82c6c",
            "3ed075a965c94ee2a0e4756fce818ad9",
            "49b3253aa5e1496d8c7bc02326539d88",
            "e553b1b972914ddb90d48581887b5a59",
            "4ebabbe3be224d79a77bc6847b5e8369",
            "6bfae0298b49426c86e8f14c33922587",
            "96876374d8cb4c369a29e132bff097d9",
            "114cd8db7ea245469b8f2afd12638b8c",
            "5465a932687548e0870232366ad24a20",
            "db5d593c4c384b73928ff7feaaa0f34a",
            "d9047d84171f40a7a53d8591a4184a82",
            "399d10b734ba40ce80999bf3caf446cb",
            "7160bf4626054f458d2e3554e756f3cd",
            "4f4b808068a54a53aca2d04ea757e05f",
            "38384a539f6447d0b9f03f366271ca9b",
            "8f32d10e127e4593b7f3a901b77b1f8a",
            "643318fbeae545a7818cd712a9aa1cd5",
            "0f04aa6254e94e6686821c1e95f6499d",
            "e12ae132c89246e1bb7672339f0abf78",
            "546ea448a1b34f8fb01b25b2cc31656c",
            "f98fc9ca4ca849a2bbdcf1f49135171a",
            "8ef73dfdf8f94bd9ba7c9a03646e06de",
            "7d657fdeceff45328412ddbfd1a686c7",
            "ecac57ecc40744c0a1f8091becf42337",
            "446bc9f0fd38436092baefef8b1eaaca",
            "7a28056c74d74bed9182d772cb91d6aa",
            "3b7da2b038024762a629f371d84a2e7d"
          ]
        },
        "outputId": "90b13f55-4423-4ceb-9c9d-79be598964af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94ac58e26f3c45469861564941ab3ca4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bfae0298b49426c86e8f14c33922587"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "643318fbeae545a7818cd712a9aa1cd5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N3lANrVVFKoG"
      },
      "outputs": [],
      "source": [
        "training_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KAjK4NHFGHxX"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wWhu89aYGHn9"
      },
      "outputs": [],
      "source": [
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "19da3bb79d3449998ad4b5d4cf975fb6",
            "0d2853dc0b6b4104aaec1fb4ad6bd0ac",
            "47d651852b774ed3bb7fb95c837a6437",
            "39ee35eb1aae496a8d7a7c5cbfa4177f",
            "6899e52941e34d048e1d2a1a5385de36",
            "4adcffc6336f4c718da0c421475c6451",
            "1dd6d8240b7243179af663c5d26c5c0a",
            "5738a914d1f14bf39bd52d5bb2681c23",
            "b9eae00a936c4574a2e917bac9134b86",
            "cca2c7926ef04129a8248ae734294245",
            "54a342427c78479589f5323e31bf5735"
          ]
        },
        "id": "n8lGa-DGGHTy",
        "outputId": "54b9136a-4272-45c6-d0de-4a21ad5d0bd8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19da3bb79d3449998ad4b5d4cf975fb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaClass(\n",
              "  (l1): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.5, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        self.l1 = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.l2 = torch.nn.Dropout(0.5)\n",
        "        self.l3 = torch.nn.Linear(768, 6)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        # breakpoint()\n",
        "        # _, output_1= self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        output_2 = self.l2(output_1.pooler_output)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "model = RobertaClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZGQYAp4VY0Ql"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.CrossEntropyLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GT64eoS9IN3F"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZjQ-5PD2IWH0"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    num_of_batches = int(len(train_df)/training_loader.batch_size)\n",
        "    model.train()\n",
        "    running_loss = 0.0    \n",
        "    counter = 0\n",
        "\n",
        "    for data in tqdm(training_loader, total=num_of_batches, desc=\"Epoch No: \" + str(epoch+1), colour=\"blue\"):\n",
        "        counter += 1\n",
        "\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets-1)\n",
        "        tqdm.write(f\"Training Iteration {counter}, Loss: {loss.item()}\")\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss/counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7T5UXghm6i26"
      },
      "outputs": [],
      "source": [
        "def get_class(outputs):\n",
        "    scores = torch.tensor(outputs)\n",
        "    probs = torch.nn.functional.softmax(scores, dim=1)\n",
        "    print(probs)\n",
        "\n",
        "    _, predictions = torch.max(probs, dim=1)\n",
        "    predictions += 1\n",
        "    return predictions.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nxsN7MlkIr9l"
      },
      "outputs": [],
      "source": [
        "def validation(loader):\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    running_loss = 0.0    \n",
        "    counter = 0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            counter += 1\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
        "            loss = loss_fn(outputs, targets-1)\n",
        "            running_loss += loss.item()\n",
        "    loss = running_loss/counter\n",
        "    fin_outputs = get_class(fin_outputs)\n",
        "    return fin_outputs, fin_targets, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6c492a7efa5c4e5cb69a0ed4155cb743",
            "043899dd1cbb4c65835350d26b89eb75",
            "e4fc382a8a124ed28e60e551794c5747",
            "8b2ba646164b43359def87283404b0ed",
            "c33818f8ed2d46d2a393bf4e51e68731",
            "83cec01e601b4b05a5d706eb7e6bdb05",
            "d54dc6674fb948ac8dfef7ccdc5e5c08",
            "55cf8dceb634436a9a2479631b89f8d0",
            "ab051e9273ce41b19149fd6c9fc8841f",
            "410452f0b7cd496da67a7149f26a865a",
            "c87606c0f9e14ac9817ffac8318763de",
            "c95cc0977837400c914be2ae528906cf",
            "b9bccb50996f4df2b273be44cefb92a6",
            "245bd22a3d784361b8050ea21f41d683",
            "855f0df68820494089dda5e1e6956c2a",
            "f12f71553c6b4f32a8829c4c12030c47",
            "d24e3f5b43994a73bdba36a44fef3c09",
            "5c219e5f32484099b5d14d4d6529c2a2",
            "774231e7e54c4090a909ac363555ecaa",
            "bb74b91809464bafa8ba440368f2f904",
            "e95adbcb6cd946f78376d07435ccd148",
            "a3b6e35965c34f8791b0a0f88223976c",
            "8ee41e99bd9f4a6595fc86be0d574edf",
            "2eb998586bbd46218446687aa7dd2e1c",
            "ae5237f9ca424de5a1d5e837377f0874",
            "e03c8f57dcce414ca6bbce344e7ed3ea",
            "67626ce0574b46038688ea0f68ae140d",
            "6a89cb41b9704da19d50b9b9fc340676",
            "d79f4062763b442481e7ac77c8295a72",
            "055569a19fda462791b88b4a3266903d",
            "84586c6072de4132b32d3f50c79940bc",
            "acd6b76021f94316a6b913ee5b67e1a2",
            "03eef5aadf9742b2947ed05193d2e0c6",
            "ddfadc0336c0498f8b3338fab7e72635",
            "4760443aaf944d9ca5687ad408b29d4a",
            "7fea5a03a1b44b1699c78ea383b0ebcf",
            "b1b3559364d04c7094e9ad61255bd382",
            "ea0fe9aee0d34809ae545514914490cc",
            "1671fa7e5fa0462eaca162524c797d4b",
            "3ffc1b07f5da4ec3bcf51b569253b3f7",
            "ff07b390084e4f7280aa06a3fb7f8730",
            "ef75da2f5c6440c2b9b6fb55e545ae07",
            "4fe216d577af427cac7b0222e428b076",
            "1e05b8a091e14edc803444fa96e96e45",
            "53375d19fa8c428cb67647444fa062c4",
            "125e52cf8c01460c89b9541db70dd8ab",
            "f2b80777e75e4a85b68e3e9e844ab5bf",
            "7b747f5a5eb84bc4beb8457be8092e80",
            "7fa0e275f3bb47cb8463f0711b5a229a",
            "7df2b9ef55ea4e23bd26af6292563cf6",
            "672494baee9542b4966aacca02e2e46c",
            "e35d43220176412a923d3e0b3e883d39",
            "75cf99d85c3844fe96de0a3c0089f1ad",
            "171e9625db6d458cb906bbf4735ad53f",
            "bdd86e71d3f14ca19d3fe24ed3992d8f",
            "05322f4214f2468e8dc65933269ad0ba",
            "2b77b8375db74274a09aceb36ef70e6a",
            "ffcf50c3a6ea4a629769bc90f01b5034",
            "0062909d53cf407a9aa0e1bf8d406671",
            "398e0c638d1246e4886688cde8ab9d8d",
            "f3b22310622e4da19a553f444b9aa8d6",
            "0f2e1a63d2724b94854c8fa81c4fb98f",
            "1a3090901faa40e1b742945c4b2440d5",
            "b3150ea59357425c93edc3afbb653d41",
            "83ff390c2d9d49c3890f4e39c4f13940",
            "ef4b90b1276744c9b7dbc1940d95a077",
            "42cf010108004f9e9a749882344f9856",
            "1f673ec4c2cd474b96c014660ff4eb65",
            "2d5b2ff4547e41f2900e23e463f08a26",
            "695ba4bb3b9b4c6696bee26f9fb47cfe",
            "8e19023a118b428ab160431a079ae08d",
            "2914a2479fc04a469d01f75b640f1840",
            "6c189a2c732a4717bff3544fb13ac11f",
            "50f725f9bb4e4e06bdc1f7404f0f2d9c",
            "08c2158f853e4faf8ab75722cfcf436e",
            "112b9b83b0f940efba6d4662eca645ee",
            "e1839709699e47d7a3a0969a33d60b19",
            "61e40208a5c04f5a971a69ce1606347e",
            "8e5c7bf577394a1d968cdae49d4ef405",
            "09bd63e8559b44098b5a5b0311c42efb",
            "fafea01ac2734bff8bf93f478d58d2cc",
            "05f561b1924848a895b7483e98903e3a",
            "a540a3e8c4ce43dc9140ecfad658481f",
            "43ece7139cd74179ab02ada4af26ff29",
            "e8ef03b203ab4bb9abee2833f1407d79",
            "046bbbc662494a79aab47d8683732c80",
            "6bbb626bd3ff41729f8b85ca75cb4d10",
            "8e590d20978c4a7693e7bfe01bd46678",
            "8f6cf0cf57d54d50a14a7abe44018bd1",
            "e43a21f7496741deafb39e69dcfa64e3",
            "f0114abc98e845ea92f42a8f25421406",
            "d581384fe89a41399a56e5d97400afb9",
            "a440e72af980444695fe60c3c971b35b",
            "384c1f876a0d46a498b4f7e0c7f79d06",
            "a8802f0cbac7401982cece00857ab985",
            "2a2409d1bc584eedb76c0b2f11602e8c",
            "d88ad786b701466e80e42a24d336fefa",
            "6039df90bbde45c08b5d374b54997a89",
            "c84d582a066c4d80bf44d9c36a05b380",
            "e1a0588431ab41b48f2ddb645fce31f6",
            "a24e7cf74dbf4868a989e182f8fbd00b",
            "f36270909b6441869e171541d9dc50ad",
            "06acbcb485bc454ab34d083ef452ba82",
            "bfcd3e6d4e804eae8ee7564110697aa5",
            "f7fb56370d9a46f490aa4eb0b862c96b",
            "f23a3e6eeee54473802ff48328c36e5a",
            "e5cd072a7fd94dcea2cc2c1294289079",
            "6ac8cbb4493947b3b14a5a7bd9850274",
            "08a8d1a822bb4904acbaa0db28027f1e",
            "e61c626a92284bce904c9f7107091a57",
            "460f0129a25c4af6a5aaed5b6c17d4a5",
            "d9f47ba2157f48bea8b24427f61ad89f",
            "12cb957de17a490891b2f4b221d3f350",
            "af6e83a9b79a44668ccd6de451a18d09",
            "e5f9e0c1a8ec4878a5c2a4ad554c6e8c",
            "ed4ace53b5394de191616b89bc3bc248",
            "b71bc68ce9624500aa3fa908e5126b6a",
            "1766ebfa683947068187c8a93b7122c5",
            "dd060465c8b94305a220f23ecf79c70e",
            "d0fd1a3a25964d1f85d80948d6c9effc",
            "56d68c5bbade4f718fa51464113c2c04"
          ]
        },
        "id": "Ejt_e_GcJyZ8",
        "outputId": "b14d180a-3d57-4070-c2ea-746b7705631e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c492a7efa5c4e5cb69a0ed4155cb743",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Total Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c95cc0977837400c914be2ae528906cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch No: 1:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 5.476740837097168\n",
            "Training Iteration 1565, Loss: 4.142246246337891\n",
            "Training Iteration 1566, Loss: 4.291141510009766\n",
            "Training Iteration 1567, Loss: 8.08358383178711\n",
            "Training Iteration 1568, Loss: 2.3080925941467285\n",
            "Training Iteration 1569, Loss: 6.173572540283203\n",
            "Training Iteration 1570, Loss: 2.9670050144195557\n",
            "Training Iteration 1571, Loss: 3.9319076538085938\n",
            "Training Iteration 1572, Loss: 7.4030561447143555\n",
            "Training Iteration 1573, Loss: 3.511410713195801\n",
            "Training Iteration 1574, Loss: 7.062469959259033\n",
            "Training Iteration 1575, Loss: 2.5509140491485596\n",
            "Training Iteration 1576, Loss: 3.1586897373199463\n",
            "Training Iteration 1577, Loss: 8.117525100708008\n",
            "Training Iteration 1578, Loss: 6.536108016967773\n",
            "Training Iteration 1579, Loss: 4.543871879577637\n",
            "Training Iteration 1580, Loss: 3.7554030418395996\n",
            "Training Iteration 1581, Loss: 4.950090408325195\n",
            "Training Iteration 1582, Loss: 2.186807632446289\n",
            "Training Iteration 1583, Loss: 4.739782333374023\n",
            "Training Iteration 1584, Loss: 4.13530969619751\n",
            "Training Iteration 1585, Loss: 2.2816152572631836\n",
            "Training Iteration 1586, Loss: 1.922396183013916\n",
            "Training Iteration 1587, Loss: 3.8905582427978516\n",
            "Training Iteration 1588, Loss: 4.545117378234863\n",
            "Training Iteration 1589, Loss: 5.770114421844482\n",
            "Training Iteration 1590, Loss: 4.707418441772461\n",
            "Training Iteration 1591, Loss: 4.183474063873291\n",
            "Training Iteration 1592, Loss: 3.0378684997558594\n",
            "Training Iteration 1593, Loss: 4.913671016693115\n",
            "Training Iteration 1594, Loss: 5.082104682922363\n",
            "Training Iteration 1595, Loss: 5.525421619415283\n",
            "Training Iteration 1596, Loss: 1.3767824172973633\n",
            "Training Iteration 1597, Loss: 4.1080169677734375\n",
            "Training Iteration 1598, Loss: 6.193445682525635\n",
            "Training Iteration 1599, Loss: 8.432863235473633\n",
            "Training Iteration 1600, Loss: 5.1515583992004395\n",
            "Training Iteration 1601, Loss: 10.328031539916992\n",
            "Training Iteration 1602, Loss: 4.04721212387085\n",
            "Training Iteration 1603, Loss: 2.7654755115509033\n",
            "Training Iteration 1604, Loss: 4.042891979217529\n",
            "Training Iteration 1605, Loss: 3.7690296173095703\n",
            "Training Iteration 1606, Loss: 3.936008930206299\n",
            "Training Iteration 1607, Loss: 2.865417003631592\n",
            "Training Iteration 1608, Loss: 4.828319549560547\n",
            "Training Iteration 1609, Loss: 9.669715881347656\n",
            "Training Iteration 1610, Loss: 8.061712265014648\n",
            "Training Iteration 1611, Loss: 3.5051052570343018\n",
            "Training Iteration 1612, Loss: 1.8230977058410645\n",
            "Training Iteration 1613, Loss: 4.950176239013672\n",
            "Training Iteration 1614, Loss: 5.291568756103516\n",
            "Training Iteration 1615, Loss: 4.802126884460449\n",
            "Training Iteration 1616, Loss: 7.899009704589844\n",
            "Training Iteration 1617, Loss: 5.610797882080078\n",
            "Training Iteration 1618, Loss: 4.648034572601318\n",
            "Training Iteration 1619, Loss: 8.077427864074707\n",
            "Training Iteration 1620, Loss: 2.006336212158203\n",
            "Training Iteration 1621, Loss: 2.563807964324951\n",
            "Training Iteration 1622, Loss: 3.4307668209075928\n",
            "Training Iteration 1623, Loss: 2.4719510078430176\n",
            "Training Iteration 1624, Loss: 4.45056676864624\n",
            "Training Iteration 1625, Loss: 2.122926712036133\n",
            "Training Iteration 1626, Loss: 4.427066326141357\n",
            "Training Iteration 1627, Loss: 1.0899262428283691\n",
            "Training Iteration 1628, Loss: 7.078122615814209\n",
            "Training Iteration 1629, Loss: 6.937812805175781\n",
            "Training Iteration 1630, Loss: 6.0838799476623535\n",
            "Training Iteration 1631, Loss: 10.611505508422852\n",
            "Training Iteration 1632, Loss: 6.03672456741333\n",
            "Training Iteration 1633, Loss: 7.205976486206055\n",
            "Training Iteration 1634, Loss: 8.072310447692871\n",
            "Training Iteration 1635, Loss: 5.655862808227539\n",
            "Training Iteration 1636, Loss: 6.987183094024658\n",
            "Training Iteration 1637, Loss: 6.437407970428467\n",
            "Training Iteration 1638, Loss: 4.9276041984558105\n",
            "Training Iteration 1639, Loss: 4.8702521324157715\n",
            "Training Iteration 1640, Loss: 7.2334513664245605\n",
            "Training Iteration 1641, Loss: 10.471506118774414\n",
            "Training Iteration 1642, Loss: 5.517368793487549\n",
            "Training Iteration 1643, Loss: 4.054399013519287\n",
            "Training Iteration 1644, Loss: 4.415446758270264\n",
            "Training Iteration 1645, Loss: 3.745656967163086\n",
            "Training Iteration 1646, Loss: 2.6796655654907227\n",
            "Training Iteration 1647, Loss: 4.408236503601074\n",
            "Training Iteration 1648, Loss: 1.7130779027938843\n",
            "Training Iteration 1649, Loss: 6.632819652557373\n",
            "Training Iteration 1650, Loss: 2.1945602893829346\n",
            "Training Iteration 1651, Loss: 8.60635757446289\n",
            "Training Iteration 1652, Loss: 2.9900577068328857\n",
            "Training Iteration 1653, Loss: 4.259731292724609\n",
            "Training Iteration 1654, Loss: 2.4147629737854004\n",
            "Training Iteration 1655, Loss: 3.7003653049468994\n",
            "Training Iteration 1656, Loss: 5.313352108001709\n",
            "Training Iteration 1657, Loss: 3.7772469520568848\n",
            "Training Iteration 1658, Loss: 6.243302345275879\n",
            "Training Iteration 1659, Loss: 6.339249610900879\n",
            "Training Iteration 1660, Loss: 5.285682678222656\n",
            "Training Iteration 1661, Loss: 4.254138946533203\n",
            "Training Iteration 1662, Loss: 2.6087119579315186\n",
            "Training Iteration 1663, Loss: 2.5232298374176025\n",
            "Training Iteration 1664, Loss: 5.209214210510254\n",
            "Training Iteration 1665, Loss: 3.193342685699463\n",
            "Training Iteration 1666, Loss: 3.981440782546997\n",
            "Training Iteration 1667, Loss: 4.959322929382324\n",
            "Training Iteration 1668, Loss: 4.333215713500977\n",
            "Training Iteration 1669, Loss: 3.0221002101898193\n",
            "Training Iteration 1670, Loss: 7.0137834548950195\n",
            "Training Iteration 1671, Loss: 6.310135364532471\n",
            "Training Iteration 1672, Loss: 3.2795987129211426\n",
            "Training Iteration 1673, Loss: 3.1050143241882324\n",
            "Training Iteration 1674, Loss: 2.917961597442627\n",
            "Training Iteration 1675, Loss: 4.287627220153809\n",
            "Training Iteration 1676, Loss: 5.265434265136719\n",
            "Training Iteration 1677, Loss: 2.536229372024536\n",
            "Training Iteration 1678, Loss: 2.647972822189331\n",
            "Training Iteration 1679, Loss: 2.875736713409424\n",
            "Training Iteration 1680, Loss: 4.2893452644348145\n",
            "Training Iteration 1681, Loss: 3.459836006164551\n",
            "Training Iteration 1682, Loss: 6.986016273498535\n",
            "Training Iteration 1683, Loss: 5.848167896270752\n",
            "Training Iteration 1684, Loss: 3.709444284439087\n",
            "Training Iteration 1685, Loss: 5.109063625335693\n",
            "Training Iteration 1686, Loss: 5.824604034423828\n",
            "Training Iteration 1687, Loss: 4.649674415588379\n",
            "Training Iteration 1688, Loss: 7.914066791534424\n",
            "Training Iteration 1689, Loss: 5.630407810211182\n",
            "Training Iteration 1690, Loss: 3.15869140625\n",
            "Training Iteration 1691, Loss: 5.92998743057251\n",
            "Training Iteration 1692, Loss: 5.377711296081543\n",
            "Training Iteration 1693, Loss: 4.707502841949463\n",
            "Training Iteration 1694, Loss: 7.934542655944824\n",
            "Training Iteration 1695, Loss: 5.922586441040039\n",
            "Training Iteration 1696, Loss: 3.058499813079834\n",
            "Training Iteration 1697, Loss: 3.8204610347747803\n",
            "Training Iteration 1698, Loss: 4.876456260681152\n",
            "Training Iteration 1699, Loss: 2.995678186416626\n",
            "Training Iteration 1700, Loss: 4.781122207641602\n",
            "Training Iteration 1701, Loss: 4.87205171585083\n",
            "Training Iteration 1702, Loss: 3.129530668258667\n",
            "Training Iteration 1703, Loss: 4.299958229064941\n",
            "Training Iteration 1704, Loss: 3.844465970993042\n",
            "Training Iteration 1705, Loss: 5.460138320922852\n",
            "Training Iteration 1706, Loss: 4.353756427764893\n",
            "Training Iteration 1707, Loss: 2.369495391845703\n",
            "Training Iteration 1708, Loss: 2.5258278846740723\n",
            "Training Iteration 1709, Loss: 2.7027735710144043\n",
            "Training Iteration 1710, Loss: 3.1846392154693604\n",
            "Training Iteration 1711, Loss: 4.639442443847656\n",
            "Training Iteration 1712, Loss: 3.8419723510742188\n",
            "Training Iteration 1713, Loss: 4.845546722412109\n",
            "Training Iteration 1714, Loss: 4.412410736083984\n",
            "Training Iteration 1715, Loss: 3.4287729263305664\n",
            "Training Iteration 1716, Loss: 3.123793363571167\n",
            "Training Iteration 1717, Loss: 4.321624279022217\n",
            "Training Iteration 1718, Loss: 3.4569945335388184\n",
            "Training Iteration 1719, Loss: 2.890465259552002\n",
            "Training Iteration 1720, Loss: 3.726628541946411\n",
            "Training Iteration 1721, Loss: 2.6630923748016357\n",
            "Training Iteration 1722, Loss: 5.601312160491943\n",
            "Training Iteration 1723, Loss: 2.0889832973480225\n",
            "Training Iteration 1724, Loss: 5.469971656799316\n",
            "Training Iteration 1725, Loss: 2.450035810470581\n",
            "Training Iteration 1726, Loss: 4.306982517242432\n",
            "Training Iteration 1727, Loss: 4.0950236320495605\n",
            "Training Iteration 1728, Loss: 3.3225226402282715\n",
            "Training Iteration 1729, Loss: 3.869647264480591\n",
            "Training Iteration 1730, Loss: 3.8397164344787598\n",
            "Training Iteration 1731, Loss: 4.515747547149658\n",
            "Training Iteration 1732, Loss: 2.201735258102417\n",
            "Training Iteration 1733, Loss: 4.282835960388184\n",
            "Training Iteration 1734, Loss: 2.9772138595581055\n",
            "Training Iteration 1735, Loss: 3.4626271724700928\n",
            "Training Iteration 1736, Loss: 3.8097915649414062\n",
            "Training Iteration 1737, Loss: 3.8117640018463135\n",
            "Training Iteration 1738, Loss: 4.831882476806641\n",
            "Training Iteration 1739, Loss: 5.265203475952148\n",
            "Training Iteration 1740, Loss: 5.207622528076172\n",
            "Training Iteration 1741, Loss: 2.4852828979492188\n",
            "Training Iteration 1742, Loss: 5.864577293395996\n",
            "Training Iteration 1743, Loss: 3.979001998901367\n",
            "Training Iteration 1744, Loss: 4.339327812194824\n",
            "Training Iteration 1745, Loss: 4.603725910186768\n",
            "Training Iteration 1746, Loss: 6.5638041496276855\n",
            "Training Iteration 1747, Loss: 2.389085054397583\n",
            "Training Iteration 1748, Loss: 4.070076942443848\n",
            "Training Iteration 1749, Loss: 3.2638964653015137\n",
            "Training Iteration 1750, Loss: 3.517212152481079\n",
            "Training Iteration 1751, Loss: 2.2784271240234375\n",
            "Training Iteration 1752, Loss: 4.981153964996338\n",
            "Training Iteration 1753, Loss: 3.0232367515563965\n",
            "Training Iteration 1754, Loss: 14.162317276000977\n",
            "Training Iteration 1755, Loss: 3.5274789333343506\n",
            "Training Iteration 1756, Loss: 5.945836544036865\n",
            "Training Iteration 1757, Loss: 4.247760772705078\n",
            "Training Iteration 1758, Loss: 3.232055187225342\n",
            "Training Iteration 1759, Loss: 3.8054349422454834\n",
            "Training Iteration 1760, Loss: 3.5889599323272705\n",
            "Training Iteration 1761, Loss: 5.839289665222168\n",
            "Training Iteration 1762, Loss: 2.851822853088379\n",
            "Training Iteration 1763, Loss: 4.980635643005371\n",
            "Training Iteration 1764, Loss: 3.580132246017456\n",
            "Training Iteration 1765, Loss: 4.256256103515625\n",
            "Training Iteration 1766, Loss: 6.853577613830566\n",
            "Training Iteration 1767, Loss: 2.3498857021331787\n",
            "Training Iteration 1768, Loss: 5.148542404174805\n",
            "Training Iteration 1769, Loss: 4.625939846038818\n",
            "Training Iteration 1770, Loss: 2.6658103466033936\n",
            "Training Iteration 1771, Loss: 3.6733734607696533\n",
            "Training Iteration 1772, Loss: 4.055001258850098\n",
            "Training Iteration 1773, Loss: 3.7525408267974854\n",
            "Training Iteration 1774, Loss: 4.848181247711182\n",
            "Training Iteration 1775, Loss: 3.9688560962677\n",
            "Training Iteration 1776, Loss: 4.234095096588135\n",
            "Training Iteration 1777, Loss: 5.366219997406006\n",
            "Training Iteration 1778, Loss: 3.6902971267700195\n",
            "Training Iteration 1779, Loss: 4.996451377868652\n",
            "Training Iteration 1780, Loss: 4.03239107131958\n",
            "Training Iteration 1781, Loss: 4.272006034851074\n",
            "Training Iteration 1782, Loss: 6.190943241119385\n",
            "Training Iteration 1783, Loss: 4.091669082641602\n",
            "Training Iteration 1784, Loss: 5.990218162536621\n",
            "Training Iteration 1785, Loss: 4.611132621765137\n",
            "Training Iteration 1786, Loss: 7.793454170227051\n",
            "Training Iteration 1787, Loss: 4.864694118499756\n",
            "Training Iteration 1788, Loss: 1.4125564098358154\n",
            "Training Iteration 1789, Loss: 5.062682151794434\n",
            "Training Iteration 1790, Loss: 4.916018009185791\n",
            "Training Iteration 1791, Loss: 2.942626953125\n",
            "Training Iteration 1792, Loss: 4.482023239135742\n",
            "Training Iteration 1793, Loss: 5.900796890258789\n",
            "Training Iteration 1794, Loss: 2.995849132537842\n",
            "Training Iteration 1795, Loss: 5.229334354400635\n",
            "Training Iteration 1796, Loss: 5.998326778411865\n",
            "Training Iteration 1797, Loss: 3.769745349884033\n",
            "Training Iteration 1798, Loss: 4.825667858123779\n",
            "Training Iteration 1799, Loss: 2.4070582389831543\n",
            "Training Iteration 1800, Loss: 5.2746381759643555\n",
            "Training Iteration 1801, Loss: 3.4693446159362793\n",
            "Training Iteration 1802, Loss: 1.8826634883880615\n",
            "Training Iteration 1803, Loss: 1.4701662063598633\n",
            "Training Iteration 1804, Loss: 3.1382200717926025\n",
            "Training Iteration 1805, Loss: 4.8687357902526855\n",
            "Training Iteration 1806, Loss: 4.155672550201416\n",
            "Training Iteration 1807, Loss: 6.398528575897217\n",
            "Training Iteration 1808, Loss: 3.3647356033325195\n",
            "Training Iteration 1809, Loss: 2.294076442718506\n",
            "Training Iteration 1810, Loss: 2.4852585792541504\n",
            "Training Iteration 1811, Loss: 3.758110284805298\n",
            "Training Iteration 1812, Loss: 3.3510453701019287\n",
            "Training Iteration 1813, Loss: 2.496589422225952\n",
            "Training Iteration 1814, Loss: 3.037777900695801\n",
            "Training Iteration 1815, Loss: 5.723989486694336\n",
            "Training Iteration 1816, Loss: 6.069225788116455\n",
            "Training Iteration 1817, Loss: 4.02109432220459\n",
            "Training Iteration 1818, Loss: 4.408050060272217\n",
            "Training Iteration 1819, Loss: 4.080360412597656\n",
            "Training Iteration 1820, Loss: 2.598604202270508\n",
            "Training Iteration 1821, Loss: 8.668967247009277\n",
            "Training Iteration 1822, Loss: 4.8643269538879395\n",
            "Training Iteration 1823, Loss: 5.728471755981445\n",
            "Training Iteration 1824, Loss: 4.299247741699219\n",
            "Training Iteration 1825, Loss: 2.5692687034606934\n",
            "Training Iteration 1826, Loss: 5.004827499389648\n",
            "Training Iteration 1827, Loss: 5.588077545166016\n",
            "Training Iteration 1828, Loss: 2.5058579444885254\n",
            "Training Iteration 1829, Loss: 4.12321662902832\n",
            "Training Iteration 1830, Loss: 6.282259464263916\n",
            "Training Iteration 1831, Loss: 11.302109718322754\n",
            "Training Iteration 1832, Loss: 5.590753555297852\n",
            "Training Iteration 1833, Loss: 4.142178535461426\n",
            "Training Iteration 1834, Loss: 5.67800235748291\n",
            "Training Iteration 1835, Loss: 2.7103335857391357\n",
            "Training Iteration 1836, Loss: 3.891921043395996\n",
            "Training Iteration 1837, Loss: 7.316285133361816\n",
            "Training Iteration 1838, Loss: 4.879989147186279\n",
            "Training Iteration 1839, Loss: 5.346097946166992\n",
            "Training Iteration 1840, Loss: 2.607588768005371\n",
            "Training Iteration 1841, Loss: 6.629554748535156\n",
            "Training Iteration 1842, Loss: 7.307012557983398\n",
            "Training Iteration 1843, Loss: 4.931364059448242\n",
            "Training Iteration 1844, Loss: 5.127041339874268\n",
            "Training Iteration 1845, Loss: 4.79246187210083\n",
            "Training Iteration 1846, Loss: 11.011049270629883\n",
            "Training Iteration 1847, Loss: 5.005908966064453\n",
            "Training Iteration 1848, Loss: 4.800840377807617\n",
            "Training Iteration 1849, Loss: 4.122148513793945\n",
            "Training Iteration 1850, Loss: 7.449156761169434\n",
            "Training Iteration 1851, Loss: 7.277400970458984\n",
            "Training Iteration 1852, Loss: 6.036026954650879\n",
            "Training Iteration 1853, Loss: 5.253133773803711\n",
            "Training Iteration 1854, Loss: 7.3511223793029785\n",
            "Training Iteration 1855, Loss: 5.5247697830200195\n",
            "Training Iteration 1856, Loss: 2.703411102294922\n",
            "Training Iteration 1857, Loss: 9.736897468566895\n",
            "Training Iteration 1858, Loss: 7.258806228637695\n",
            "Training Iteration 1859, Loss: 3.5306296348571777\n",
            "Training Iteration 1860, Loss: 5.929752826690674\n",
            "Training Iteration 1861, Loss: 3.189218521118164\n",
            "Training Iteration 1862, Loss: 8.911375999450684\n",
            "Training Iteration 1863, Loss: 4.211904048919678\n",
            "Training Iteration 1864, Loss: 4.343297004699707\n",
            "Training Iteration 1865, Loss: 3.9795820713043213\n",
            "Training Iteration 1866, Loss: 2.9430747032165527\n",
            "Training Iteration 1867, Loss: 2.1653389930725098\n",
            "Training Iteration 1868, Loss: 4.126753330230713\n",
            "Training Iteration 1869, Loss: 5.9190754890441895\n",
            "Training Iteration 1870, Loss: 5.837751388549805\n",
            "Training Iteration 1871, Loss: 3.713761806488037\n",
            "Training Iteration 1872, Loss: 5.56443977355957\n",
            "Training Iteration 1873, Loss: 6.687595844268799\n",
            "Training Iteration 1874, Loss: 4.663866996765137\n",
            "Training Iteration 1875, Loss: 5.846845626831055\n",
            "Training Iteration 1876, Loss: 4.718005657196045\n",
            "Training Iteration 1877, Loss: 4.080881118774414\n",
            "Training Iteration 1878, Loss: 4.17057466506958\n",
            "Training Iteration 1879, Loss: 2.3298022747039795\n",
            "Training Iteration 1880, Loss: 4.252140998840332\n",
            "Training Iteration 1881, Loss: 6.264084339141846\n",
            "Training Iteration 1882, Loss: 4.371595859527588\n",
            "Training Iteration 1883, Loss: 5.534170150756836\n",
            "Training Iteration 1884, Loss: 4.881328105926514\n",
            "Training Iteration 1885, Loss: 3.4352364540100098\n",
            "Training Iteration 1886, Loss: 2.6704561710357666\n",
            "Training Iteration 1887, Loss: 5.77391242980957\n",
            "Training Iteration 1888, Loss: 6.24778938293457\n",
            "Training Iteration 1889, Loss: 2.6178109645843506\n",
            "Training Iteration 1890, Loss: 3.431122303009033\n",
            "Training Iteration 1891, Loss: 1.811813473701477\n",
            "Training Iteration 1892, Loss: 2.8073489665985107\n",
            "Training Iteration 1893, Loss: 4.827948093414307\n",
            "Training Iteration 1894, Loss: 6.362873554229736\n",
            "Training Iteration 1895, Loss: 4.881083011627197\n",
            "Training Iteration 1896, Loss: 4.891085147857666\n",
            "Training Iteration 1897, Loss: 5.08275032043457\n",
            "Training Iteration 1898, Loss: 3.609318733215332\n",
            "Training Iteration 1899, Loss: 4.777343273162842\n",
            "Training Iteration 1900, Loss: 3.8610382080078125\n",
            "Training Iteration 1901, Loss: 3.777885913848877\n",
            "Training Iteration 1902, Loss: 4.621938228607178\n",
            "Training Iteration 1903, Loss: 5.729511260986328\n",
            "Training Iteration 1904, Loss: 5.021063327789307\n",
            "Training Iteration 1905, Loss: 4.750982284545898\n",
            "Training Iteration 1906, Loss: 5.314594268798828\n",
            "Training Iteration 1907, Loss: 3.358717441558838\n",
            "Training Iteration 1908, Loss: 4.001460075378418\n",
            "Training Iteration 1909, Loss: 6.670891284942627\n",
            "Training Iteration 1910, Loss: 5.647416114807129\n",
            "Training Iteration 1911, Loss: 2.6559414863586426\n",
            "Training Iteration 1912, Loss: 4.217968463897705\n",
            "Training Iteration 1913, Loss: 1.634338140487671\n",
            "Training Iteration 1914, Loss: 4.069269180297852\n",
            "Training Iteration 1915, Loss: 5.1852006912231445\n",
            "Training Iteration 1916, Loss: 4.628824234008789\n",
            "Training Iteration 1917, Loss: 2.769749164581299\n",
            "Training Iteration 1918, Loss: 6.2392096519470215\n",
            "Training Iteration 1919, Loss: 5.319051265716553\n",
            "Training Iteration 1920, Loss: 5.822145462036133\n",
            "Training Iteration 1921, Loss: 5.925608158111572\n",
            "Training Iteration 1922, Loss: 6.487781047821045\n",
            "Training Iteration 1923, Loss: 2.4318325519561768\n",
            "Training Iteration 1924, Loss: 3.5450451374053955\n",
            "Training Iteration 1925, Loss: 4.429496765136719\n",
            "Training Iteration 1926, Loss: 4.3225016593933105\n",
            "Training Iteration 1927, Loss: 4.053234577178955\n",
            "Training Iteration 1928, Loss: 3.351543426513672\n",
            "Training Iteration 1929, Loss: 3.008784294128418\n",
            "Training Iteration 1930, Loss: 9.448848724365234\n",
            "Training Iteration 1931, Loss: 1.7473251819610596\n",
            "Training Iteration 1932, Loss: 3.8682878017425537\n",
            "Training Iteration 1933, Loss: 3.3998377323150635\n",
            "Training Iteration 1934, Loss: 7.518750190734863\n",
            "Training Iteration 1935, Loss: 5.382574081420898\n",
            "Training Iteration 1936, Loss: 2.89766526222229\n",
            "Training Iteration 1937, Loss: 0.9330629110336304\n",
            "Training Iteration 1938, Loss: 3.8490207195281982\n",
            "Training Iteration 1939, Loss: 4.942479610443115\n",
            "Training Iteration 1940, Loss: 2.376936197280884\n",
            "Training Iteration 1941, Loss: 5.091224670410156\n",
            "Training Iteration 1942, Loss: 9.31438159942627\n",
            "Training Iteration 1943, Loss: 2.22664737701416\n",
            "Training Iteration 1944, Loss: 3.97940993309021\n",
            "Training Iteration 1945, Loss: 4.995415687561035\n",
            "Training Iteration 1946, Loss: 4.573190689086914\n",
            "Training Iteration 1947, Loss: 6.819589138031006\n",
            "Training Iteration 1948, Loss: 6.5176496505737305\n",
            "Training Iteration 1949, Loss: 5.039792537689209\n",
            "Training Iteration 1950, Loss: 3.382563829421997\n",
            "Training Iteration 1951, Loss: 6.397412300109863\n",
            "Training Iteration 1952, Loss: 6.007868766784668\n",
            "Training Iteration 1953, Loss: 7.856330394744873\n",
            "Training Iteration 1954, Loss: 7.405671119689941\n",
            "Training Iteration 1955, Loss: 2.4649784564971924\n",
            "Training Iteration 1956, Loss: 8.513029098510742\n",
            "Training Iteration 1957, Loss: 2.0785746574401855\n",
            "Training Iteration 1958, Loss: 8.463165283203125\n",
            "Training Iteration 1959, Loss: 4.7480244636535645\n",
            "Training Iteration 1960, Loss: 3.40785813331604\n",
            "Training Iteration 1961, Loss: 2.366042375564575\n",
            "Training Iteration 1962, Loss: 10.08632755279541\n",
            "Training Iteration 1963, Loss: 6.721656799316406\n",
            "Training Iteration 1964, Loss: 2.965972423553467\n",
            "Training Iteration 1965, Loss: 4.049052715301514\n",
            "Training Iteration 1966, Loss: 6.283950328826904\n",
            "Training Iteration 1967, Loss: 4.7905097007751465\n",
            "Training Iteration 1968, Loss: 5.668023109436035\n",
            "Training Iteration 1969, Loss: 6.611077308654785\n",
            "Training Iteration 1970, Loss: 5.444908618927002\n",
            "Training Iteration 1971, Loss: 7.300215721130371\n",
            "Training Iteration 1972, Loss: 2.6529369354248047\n",
            "Training Iteration 1973, Loss: 3.396009922027588\n",
            "Training Iteration 1974, Loss: 5.660314083099365\n",
            "Training Iteration 1975, Loss: 3.894843101501465\n",
            "Training Iteration 1976, Loss: 6.763693332672119\n",
            "Training Iteration 1977, Loss: 9.422423362731934\n",
            "Training Iteration 1978, Loss: 2.8869082927703857\n",
            "Training Iteration 1979, Loss: 2.9767637252807617\n",
            "Training Iteration 1980, Loss: 5.985675811767578\n",
            "Training Iteration 1981, Loss: 3.9452905654907227\n",
            "Training Iteration 1982, Loss: 8.375825881958008\n",
            "Training Iteration 1983, Loss: 4.119669437408447\n",
            "Training Iteration 1984, Loss: 5.62437629699707\n",
            "Training Iteration 1985, Loss: 3.6098380088806152\n",
            "Training Iteration 1986, Loss: 2.365896463394165\n",
            "Training Iteration 1987, Loss: 3.1843395233154297\n",
            "Training Iteration 1988, Loss: 5.22365665435791\n",
            "Training Iteration 1989, Loss: 2.6585593223571777\n",
            "Training Iteration 1990, Loss: 2.057398796081543\n",
            "Training Iteration 1991, Loss: 3.036691665649414\n",
            "Training Iteration 1992, Loss: 2.6964728832244873\n",
            "Training Iteration 1993, Loss: 5.2075676918029785\n",
            "Training Iteration 1994, Loss: 7.309658527374268\n",
            "Training Iteration 1995, Loss: 4.085390090942383\n",
            "Training Iteration 1996, Loss: 4.329728603363037\n",
            "Training Iteration 1997, Loss: 6.182892799377441\n",
            "Training Iteration 1998, Loss: 3.593763589859009\n",
            "Training Iteration 1999, Loss: 4.575111389160156\n",
            "Training Iteration 2000, Loss: 3.004587173461914\n",
            "Training Iteration 2001, Loss: 5.053662300109863\n",
            "Training Iteration 2002, Loss: 7.28325080871582\n",
            "Training Iteration 2003, Loss: 7.302829265594482\n",
            "Training Iteration 2004, Loss: 2.1175384521484375\n",
            "Training Iteration 2005, Loss: 6.532785415649414\n",
            "Training Iteration 2006, Loss: 5.542313098907471\n",
            "Training Iteration 2007, Loss: 4.778792858123779\n",
            "Training Iteration 2008, Loss: 6.0736260414123535\n",
            "Training Iteration 2009, Loss: 4.76078987121582\n",
            "Training Iteration 2010, Loss: 3.1419448852539062\n",
            "Training Iteration 2011, Loss: 2.1906604766845703\n",
            "Training Iteration 2012, Loss: 4.646591663360596\n",
            "Training Iteration 2013, Loss: 4.075254440307617\n",
            "Training Iteration 2014, Loss: 4.341868877410889\n",
            "Training Iteration 2015, Loss: 3.392268419265747\n",
            "Training Iteration 2016, Loss: 10.091374397277832\n",
            "Training Iteration 2017, Loss: 8.249380111694336\n",
            "Training Iteration 2018, Loss: 6.939631938934326\n",
            "Training Iteration 2019, Loss: 3.862712860107422\n",
            "Training Iteration 2020, Loss: 6.003313064575195\n",
            "Training Iteration 2021, Loss: 4.695492744445801\n",
            "Training Iteration 2022, Loss: 1.9030245542526245\n",
            "Training Iteration 2023, Loss: 7.44926118850708\n",
            "Training Iteration 2024, Loss: 5.42629861831665\n",
            "Training Iteration 2025, Loss: 7.869604110717773\n",
            "Training Iteration 2026, Loss: 4.982439994812012\n",
            "Training Iteration 2027, Loss: 3.5314290523529053\n",
            "Training Iteration 2028, Loss: 4.193645000457764\n",
            "Training Iteration 2029, Loss: 4.61350679397583\n",
            "Training Iteration 2030, Loss: 6.591489315032959\n",
            "Training Iteration 2031, Loss: 3.5635879039764404\n",
            "Training Iteration 2032, Loss: 2.5427680015563965\n",
            "Training Iteration 2033, Loss: 4.697796821594238\n",
            "Training Iteration 2034, Loss: 7.652969837188721\n",
            "Training Iteration 2035, Loss: 2.708909749984741\n",
            "Training Iteration 2036, Loss: 6.871397018432617\n",
            "Training Iteration 2037, Loss: 4.243678569793701\n",
            "Training Iteration 2038, Loss: 6.467435836791992\n",
            "Training Iteration 2039, Loss: 5.064941883087158\n",
            "Training Iteration 2040, Loss: 3.7871901988983154\n",
            "Training Iteration 2041, Loss: 3.5609006881713867\n",
            "Training Iteration 2042, Loss: 3.1000053882598877\n",
            "Training Iteration 2043, Loss: 2.4246232509613037\n",
            "Training Iteration 2044, Loss: 2.337948799133301\n",
            "Training Iteration 2045, Loss: 4.555879592895508\n",
            "Training Iteration 2046, Loss: 4.7589592933654785\n",
            "Training Iteration 2047, Loss: 4.054261207580566\n",
            "Training Iteration 2048, Loss: 1.7011364698410034\n",
            "Training Iteration 2049, Loss: 2.2086260318756104\n",
            "Training Iteration 2050, Loss: 1.3792542219161987\n",
            "Training Iteration 2051, Loss: 6.347910404205322\n",
            "Training Iteration 2052, Loss: 3.139240264892578\n",
            "Training Iteration 2053, Loss: 4.151148796081543\n",
            "Training Iteration 2054, Loss: 4.112612724304199\n",
            "Training Iteration 2055, Loss: 3.298738718032837\n",
            "Training Iteration 2056, Loss: 2.837773561477661\n",
            "Training Iteration 2057, Loss: 1.1108827590942383\n",
            "Training Iteration 2058, Loss: 4.180026054382324\n",
            "Training Iteration 2059, Loss: 3.898538827896118\n",
            "Training Iteration 2060, Loss: 2.920044422149658\n",
            "Training Iteration 2061, Loss: 6.641541957855225\n",
            "Training Iteration 2062, Loss: 3.8801333904266357\n",
            "Training Iteration 2063, Loss: 2.1891162395477295\n",
            "Training Iteration 2064, Loss: 1.8579984903335571\n",
            "Training Iteration 2065, Loss: 5.459122180938721\n",
            "Training Iteration 2066, Loss: 5.866952896118164\n",
            "Training Iteration 2067, Loss: 4.0705246925354\n",
            "Training Iteration 2068, Loss: 6.289834976196289\n",
            "Training Iteration 2069, Loss: 4.2319655418396\n",
            "Training Iteration 2070, Loss: 5.901014804840088\n",
            "Training Iteration 2071, Loss: 3.681344747543335\n",
            "Training Iteration 2072, Loss: 4.522006034851074\n",
            "Training Iteration 2073, Loss: 5.975554943084717\n",
            "Training Iteration 2074, Loss: 4.469046592712402\n",
            "Training Iteration 2075, Loss: 2.3513996601104736\n",
            "Training Iteration 2076, Loss: 3.2545409202575684\n",
            "Training Iteration 2077, Loss: 3.2648262977600098\n",
            "Training Iteration 2078, Loss: 3.66864275932312\n",
            "Training Iteration 2079, Loss: 4.425497055053711\n",
            "Training Iteration 2080, Loss: 0.914779543876648\n",
            "Training Iteration 2081, Loss: 4.968908309936523\n",
            "Training Iteration 2082, Loss: 4.438057899475098\n",
            "Training Iteration 2083, Loss: 6.509321212768555\n",
            "Training Iteration 2084, Loss: 5.6395087242126465\n",
            "Training Iteration 2085, Loss: 3.863492012023926\n",
            "Training Iteration 2086, Loss: 3.333162546157837\n",
            "Training Iteration 2087, Loss: 4.849283218383789\n",
            "Training Iteration 2088, Loss: 4.192152500152588\n",
            "Training Iteration 2089, Loss: 1.1455386877059937\n",
            "Training Iteration 2090, Loss: 6.813454627990723\n",
            "Training Iteration 2091, Loss: 3.3873462677001953\n",
            "Training Iteration 2092, Loss: 4.114394664764404\n",
            "Training Iteration 2093, Loss: 3.1155529022216797\n",
            "Training Iteration 2094, Loss: 3.697584867477417\n",
            "Training Iteration 2095, Loss: 7.256443023681641\n",
            "Training Iteration 2096, Loss: 6.5429277420043945\n",
            "Training Iteration 2097, Loss: 4.757084369659424\n",
            "Training Iteration 2098, Loss: 3.114064931869507\n",
            "Training Iteration 2099, Loss: 4.1485161781311035\n",
            "Training Iteration 2100, Loss: 3.331120252609253\n",
            "Training Iteration 2101, Loss: 4.969247817993164\n",
            "Training Iteration 2102, Loss: 2.438410758972168\n",
            "Training Iteration 2103, Loss: 1.134635090827942\n",
            "Training Iteration 2104, Loss: 5.690260410308838\n",
            "Training Iteration 2105, Loss: 7.9400315284729\n",
            "Training Iteration 2106, Loss: 4.259350299835205\n",
            "Training Iteration 2107, Loss: 4.047481060028076\n",
            "Training Iteration 2108, Loss: 2.797152519226074\n",
            "Training Iteration 2109, Loss: 6.919689178466797\n",
            "Training Iteration 2110, Loss: 5.482985973358154\n",
            "Training Iteration 2111, Loss: 7.251215934753418\n",
            "Training Iteration 2112, Loss: 4.756455421447754\n",
            "Training Iteration 2113, Loss: 3.2031142711639404\n",
            "Training Iteration 2114, Loss: 6.934227466583252\n",
            "Training Iteration 2115, Loss: 6.325074672698975\n",
            "Training Iteration 2116, Loss: 7.629825592041016\n",
            "Training Iteration 2117, Loss: 7.496493816375732\n",
            "Training Iteration 2118, Loss: 4.956146240234375\n",
            "Training Iteration 2119, Loss: 3.6418559551239014\n",
            "Training Iteration 2120, Loss: 2.1710026264190674\n",
            "Training Iteration 2121, Loss: 3.791994571685791\n",
            "Training Iteration 2122, Loss: 4.918720722198486\n",
            "Training Iteration 2123, Loss: 3.8072669506073\n",
            "Training Iteration 2124, Loss: 3.5369362831115723\n",
            "Training Iteration 2125, Loss: 3.129335641860962\n",
            "Training Iteration 2126, Loss: 5.854743003845215\n",
            "Training Iteration 2127, Loss: 4.256454944610596\n",
            "Training Iteration 2128, Loss: 3.7733123302459717\n",
            "Training Iteration 2129, Loss: 2.4661717414855957\n",
            "Training Iteration 2130, Loss: 4.042861461639404\n",
            "Training Iteration 2131, Loss: 7.552408218383789\n",
            "Training Iteration 2132, Loss: 2.283097982406616\n",
            "Training Iteration 2133, Loss: 3.6194238662719727\n",
            "Training Iteration 2134, Loss: 3.880082845687866\n",
            "Training Iteration 2135, Loss: 2.760572910308838\n",
            "Training Iteration 2136, Loss: 5.11945915222168\n",
            "Training Iteration 2137, Loss: 5.551856517791748\n",
            "Training Iteration 2138, Loss: 1.9495234489440918\n",
            "Training Iteration 2139, Loss: 7.338468551635742\n",
            "Training Iteration 2140, Loss: 3.3780062198638916\n",
            "Training Iteration 2141, Loss: 4.137369632720947\n",
            "Training Iteration 2142, Loss: 2.6331734657287598\n",
            "Training Iteration 2143, Loss: 5.7709832191467285\n",
            "Training Iteration 2144, Loss: 2.9551563262939453\n",
            "Training Iteration 2145, Loss: 5.347139835357666\n",
            "Training Iteration 2146, Loss: 4.940243721008301\n",
            "Training Iteration 2147, Loss: 2.5489702224731445\n",
            "Training Iteration 2148, Loss: 5.578289031982422\n",
            "Training Iteration 2149, Loss: 2.526771068572998\n",
            "Training Iteration 2150, Loss: 5.693076133728027\n",
            "Training Iteration 2151, Loss: 3.700829029083252\n",
            "Training Iteration 2152, Loss: 3.927377700805664\n",
            "Training Iteration 2153, Loss: 2.7449707984924316\n",
            "Training Iteration 2154, Loss: 4.224109172821045\n",
            "Training Iteration 2155, Loss: 5.402777194976807\n",
            "Training Iteration 2156, Loss: 2.1039109230041504\n",
            "Training Iteration 2157, Loss: 4.012856483459473\n",
            "Training Iteration 2158, Loss: 7.382050037384033\n",
            "Training Iteration 2159, Loss: 2.738248825073242\n",
            "Training Iteration 2160, Loss: 4.266101360321045\n",
            "Training Iteration 2161, Loss: 4.427978992462158\n",
            "Training Iteration 2162, Loss: 5.273963928222656\n",
            "Training Iteration 2163, Loss: 5.351134300231934\n",
            "Training Iteration 2164, Loss: 3.4581403732299805\n",
            "Training Iteration 2165, Loss: 1.9904422760009766\n",
            "Training Iteration 2166, Loss: 5.635893821716309\n",
            "Training Iteration 2167, Loss: 6.633721828460693\n",
            "Training Iteration 2168, Loss: 5.429040908813477\n",
            "Training Iteration 2169, Loss: 6.407390117645264\n",
            "Training Iteration 2170, Loss: 3.064785957336426\n",
            "Training Iteration 2171, Loss: 4.465572357177734\n",
            "Training Iteration 2172, Loss: 4.507462024688721\n",
            "Training Iteration 2173, Loss: 5.960760116577148\n",
            "Training Iteration 2174, Loss: 2.2125048637390137\n",
            "Training Iteration 2175, Loss: 4.858189582824707\n",
            "Training Iteration 2176, Loss: 3.4097776412963867\n",
            "Training Iteration 2177, Loss: 5.13422155380249\n",
            "Training Iteration 2178, Loss: 2.3165812492370605\n",
            "Training Iteration 2179, Loss: 6.273648262023926\n",
            "Training Iteration 2180, Loss: 4.498759746551514\n",
            "Training Iteration 2181, Loss: 9.48332691192627\n",
            "Training Iteration 2182, Loss: 4.125185966491699\n",
            "Training Iteration 2183, Loss: 3.06705379486084\n",
            "Training Iteration 2184, Loss: 2.921036720275879\n",
            "Training Iteration 2185, Loss: 4.589579105377197\n",
            "Training Iteration 2186, Loss: 2.307612419128418\n",
            "Training Iteration 2187, Loss: 1.9058479070663452\n",
            "Training Iteration 2188, Loss: 4.445847034454346\n",
            "Training Iteration 2189, Loss: 3.9047374725341797\n",
            "Training Iteration 2190, Loss: 2.81903076171875\n",
            "Training Iteration 2191, Loss: 3.416065216064453\n",
            "Training Iteration 2192, Loss: 5.193741798400879\n",
            "Training Iteration 2193, Loss: 6.855678558349609\n",
            "Training Iteration 2194, Loss: 4.917366027832031\n",
            "Training Iteration 2195, Loss: 3.216594696044922\n",
            "Training Iteration 2196, Loss: 1.7701846361160278\n",
            "Training Iteration 2197, Loss: 5.1107916831970215\n",
            "Training Iteration 2198, Loss: 2.553706169128418\n",
            "Training Iteration 2199, Loss: 2.5218682289123535\n",
            "Training Iteration 2200, Loss: 1.6014690399169922\n",
            "Training Iteration 2201, Loss: 4.080715179443359\n",
            "Training Iteration 2202, Loss: 4.8513383865356445\n",
            "Training Iteration 2203, Loss: 9.288660049438477\n",
            "Training Iteration 2204, Loss: 5.901864528656006\n",
            "Training Iteration 2205, Loss: 2.5871574878692627\n",
            "Training Iteration 2206, Loss: 2.6284165382385254\n",
            "Training Iteration 2207, Loss: 5.798290729522705\n",
            "Training Iteration 2208, Loss: 2.6327309608459473\n",
            "Training Iteration 2209, Loss: 5.531006336212158\n",
            "Training Iteration 2210, Loss: 4.563730716705322\n",
            "Training Iteration 2211, Loss: 5.557503700256348\n",
            "Training Iteration 2212, Loss: 4.662492275238037\n",
            "Training Iteration 2213, Loss: 8.047404289245605\n",
            "Training Iteration 2214, Loss: 5.077713489532471\n",
            "Training Iteration 2215, Loss: 2.627279281616211\n",
            "Training Iteration 2216, Loss: 5.306833267211914\n",
            "Training Iteration 2217, Loss: 2.2047433853149414\n",
            "Training Iteration 2218, Loss: 3.0921757221221924\n",
            "Training Iteration 2219, Loss: 6.082481384277344\n",
            "Training Iteration 2220, Loss: 2.198101043701172\n",
            "Training Iteration 2221, Loss: 3.87790584564209\n",
            "Training Iteration 2222, Loss: 2.2648963928222656\n",
            "Training Iteration 2223, Loss: 3.057360887527466\n",
            "Training Iteration 2224, Loss: 5.064855575561523\n",
            "Training Iteration 2225, Loss: 5.0413312911987305\n",
            "Training Iteration 2226, Loss: 1.4018915891647339\n",
            "Training Iteration 2227, Loss: 7.393261432647705\n",
            "Training Iteration 2228, Loss: 2.724259614944458\n",
            "Training Iteration 2229, Loss: 5.121523380279541\n",
            "Training Iteration 2230, Loss: 2.484788417816162\n",
            "Training Iteration 2231, Loss: 5.486528396606445\n",
            "Training Iteration 2232, Loss: 2.50596022605896\n",
            "Training Iteration 2233, Loss: 3.571809768676758\n",
            "Training Iteration 2234, Loss: 3.4553065299987793\n",
            "Training Iteration 2235, Loss: 4.535025119781494\n",
            "Training Iteration 2236, Loss: 3.890028953552246\n",
            "Training Iteration 2237, Loss: 3.9173929691314697\n",
            "Training Iteration 2238, Loss: 3.954190492630005\n",
            "Training Iteration 2239, Loss: 2.481215476989746\n",
            "Training Iteration 2240, Loss: 3.40993070602417\n",
            "Training Iteration 2241, Loss: 4.385261058807373\n",
            "Training Iteration 2242, Loss: 2.6143877506256104\n",
            "Training Iteration 2243, Loss: 2.2621359825134277\n",
            "Training Iteration 2244, Loss: 5.032044887542725\n",
            "Training Iteration 2245, Loss: 2.6731836795806885\n",
            "Training Iteration 2246, Loss: 1.5050948858261108\n",
            "Training Iteration 2247, Loss: 6.435500144958496\n",
            "Training Iteration 2248, Loss: 5.188821792602539\n",
            "Training Iteration 2249, Loss: 6.260725498199463\n",
            "Training Iteration 2250, Loss: 6.194636344909668\n",
            "Training Iteration 2251, Loss: 4.47838830947876\n",
            "Training Iteration 2252, Loss: 3.1763322353363037\n",
            "Training Iteration 2253, Loss: 5.745750427246094\n",
            "Training Iteration 2254, Loss: 5.56158971786499\n",
            "Training Iteration 2255, Loss: 7.41860294342041\n",
            "Training Iteration 2256, Loss: 5.557448387145996\n",
            "Training Iteration 2257, Loss: 5.692318916320801\n",
            "Training Iteration 2258, Loss: 3.7283387184143066\n",
            "Training Iteration 2259, Loss: 5.885171890258789\n",
            "Training Iteration 2260, Loss: 3.0112130641937256\n",
            "Training Iteration 2261, Loss: 5.712527751922607\n",
            "Training Iteration 2262, Loss: 6.626923561096191\n",
            "Training Iteration 2263, Loss: 9.857122421264648\n",
            "Training Iteration 2264, Loss: 5.376621246337891\n",
            "Training Iteration 2265, Loss: 3.3608596324920654\n",
            "Training Iteration 2266, Loss: 3.686342477798462\n",
            "Training Iteration 2267, Loss: 2.0682358741760254\n",
            "Training Iteration 2268, Loss: 5.815507888793945\n",
            "Training Iteration 2269, Loss: 3.801912784576416\n",
            "Training Iteration 2270, Loss: 4.839798927307129\n",
            "Training Iteration 2271, Loss: 4.448468208312988\n",
            "Training Iteration 2272, Loss: 7.473423004150391\n",
            "Training Iteration 2273, Loss: 3.1765079498291016\n",
            "Training Iteration 2274, Loss: 1.9189670085906982\n",
            "Training Iteration 2275, Loss: 5.340569972991943\n",
            "Training Iteration 2276, Loss: 3.7914915084838867\n",
            "Training Iteration 2277, Loss: 8.550032615661621\n",
            "Training Iteration 2278, Loss: 4.8183698654174805\n",
            "Training Iteration 2279, Loss: 2.1099581718444824\n",
            "Training Iteration 2280, Loss: 3.7639899253845215\n",
            "Training Iteration 2281, Loss: 3.9193034172058105\n",
            "Training Iteration 2282, Loss: 2.815258026123047\n",
            "Training Iteration 2283, Loss: 4.103482246398926\n",
            "Training Iteration 2284, Loss: 4.810675621032715\n",
            "Training Iteration 2285, Loss: 2.839982509613037\n",
            "Training Iteration 2286, Loss: 3.3114891052246094\n",
            "Training Iteration 2287, Loss: 3.333141565322876\n",
            "Training Iteration 2288, Loss: 3.8875041007995605\n",
            "Training Iteration 2289, Loss: 3.0995938777923584\n",
            "Training Iteration 2290, Loss: 5.952415466308594\n",
            "Training Iteration 2291, Loss: 3.876286506652832\n",
            "Training Iteration 2292, Loss: 2.712965965270996\n",
            "Training Iteration 2293, Loss: 2.3411917686462402\n",
            "Training Iteration 2294, Loss: 5.03994083404541\n",
            "Training Iteration 2295, Loss: 4.51500940322876\n",
            "Training Iteration 2296, Loss: 3.6385555267333984\n",
            "Training Iteration 2297, Loss: 5.653584003448486\n",
            "Training Iteration 2298, Loss: 2.2970263957977295\n",
            "Training Iteration 2299, Loss: 4.893126010894775\n",
            "Training Iteration 2300, Loss: 4.235825061798096\n",
            "Training Iteration 2301, Loss: 4.8943376541137695\n",
            "Training Iteration 2302, Loss: 5.752894878387451\n",
            "Training Iteration 2303, Loss: 3.8447265625\n",
            "Training Iteration 2304, Loss: 2.6316957473754883\n",
            "Training Iteration 2305, Loss: 3.50607967376709\n",
            "Training Iteration 2306, Loss: 6.829728126525879\n",
            "Training Iteration 2307, Loss: 9.2701416015625\n",
            "Training Iteration 2308, Loss: 9.100406646728516\n",
            "Training Iteration 2309, Loss: 4.036768913269043\n",
            "Training Iteration 2310, Loss: 8.639878273010254\n",
            "Training Iteration 2311, Loss: 2.931504964828491\n",
            "Training Iteration 2312, Loss: 3.815786361694336\n",
            "Training Iteration 2313, Loss: 5.455148220062256\n",
            "Training Iteration 2314, Loss: 3.9422988891601562\n",
            "Training Iteration 2315, Loss: 6.268583297729492\n",
            "Training Iteration 2316, Loss: 2.0718750953674316\n",
            "Training Iteration 2317, Loss: 3.712780237197876\n",
            "Training Iteration 2318, Loss: 1.9692004919052124\n",
            "Training Iteration 2319, Loss: 3.1557538509368896\n",
            "Training Iteration 2320, Loss: 3.943509340286255\n",
            "Training Iteration 2321, Loss: 6.814383029937744\n",
            "Training Iteration 2322, Loss: 3.2973883152008057\n",
            "Training Iteration 2323, Loss: 4.626202583312988\n",
            "Training Iteration 2324, Loss: 11.29680061340332\n",
            "Training Iteration 2325, Loss: 6.529961585998535\n",
            "Training Iteration 2326, Loss: 4.960792064666748\n",
            "Training Iteration 2327, Loss: 4.540220737457275\n",
            "Training Iteration 2328, Loss: 1.1613643169403076\n",
            "Training Iteration 2329, Loss: 4.1446380615234375\n",
            "Training Iteration 2330, Loss: 8.888463020324707\n",
            "Training Iteration 2331, Loss: 2.7735977172851562\n",
            "Training Iteration 2332, Loss: 3.3217391967773438\n",
            "Training Iteration 2333, Loss: 4.8099822998046875\n",
            "Training Iteration 2334, Loss: 4.8940653800964355\n",
            "Training Iteration 2335, Loss: 4.230791091918945\n",
            "Training Iteration 2336, Loss: 4.869822978973389\n",
            "Training Iteration 2337, Loss: 4.444606781005859\n",
            "Training Iteration 2338, Loss: 5.249534606933594\n",
            "Training Iteration 2339, Loss: 1.878732442855835\n",
            "Training Iteration 2340, Loss: 2.078868865966797\n",
            "Training Iteration 2341, Loss: 4.275362968444824\n",
            "Training Iteration 2342, Loss: 1.8899353742599487\n",
            "Training Iteration 2343, Loss: 5.084167003631592\n",
            "Training Iteration 2344, Loss: 9.346380233764648\n",
            "Training Iteration 2345, Loss: 4.687321186065674\n",
            "Training Iteration 2346, Loss: 3.670868396759033\n",
            "Training Iteration 2347, Loss: 4.969751358032227\n",
            "Training Iteration 2348, Loss: 2.7493648529052734\n",
            "Training Iteration 2349, Loss: 2.799248695373535\n",
            "Training Iteration 2350, Loss: 3.574448823928833\n",
            "Training Iteration 2351, Loss: 6.293664455413818\n",
            "Training Iteration 2352, Loss: 10.128808975219727\n",
            "Training Iteration 2353, Loss: 7.2251739501953125\n",
            "Training Iteration 2354, Loss: 2.409025192260742\n",
            "Training Iteration 2355, Loss: 5.304251194000244\n",
            "Training Iteration 2356, Loss: 4.755167484283447\n",
            "Training Iteration 2357, Loss: 4.284219264984131\n",
            "Training Iteration 2358, Loss: 4.788343906402588\n",
            "Training Iteration 2359, Loss: 5.301202774047852\n",
            "Training Iteration 2360, Loss: 2.981235980987549\n",
            "Training Iteration 2361, Loss: 4.251090049743652\n",
            "Training Iteration 2362, Loss: 6.749255180358887\n",
            "Training Iteration 2363, Loss: 6.272587776184082\n",
            "Training Iteration 2364, Loss: 4.752551555633545\n",
            "Training Iteration 2365, Loss: 2.900101661682129\n",
            "Training Iteration 2366, Loss: 2.999948263168335\n",
            "Training Iteration 2367, Loss: 3.3882575035095215\n",
            "Training Iteration 2368, Loss: 5.193909645080566\n",
            "Training Iteration 2369, Loss: 3.991124153137207\n",
            "Training Iteration 2370, Loss: 3.6687018871307373\n",
            "Training Iteration 2371, Loss: 7.097045421600342\n",
            "Training Iteration 2372, Loss: 4.469181537628174\n",
            "Training Iteration 2373, Loss: 4.143991947174072\n",
            "Training Iteration 2374, Loss: 1.7483348846435547\n",
            "Training Iteration 2375, Loss: 6.54584264755249\n",
            "Training Iteration 2376, Loss: 3.891472339630127\n",
            "Training Iteration 2377, Loss: 4.823487281799316\n",
            "Training Iteration 2378, Loss: 3.3459043502807617\n",
            "Training Iteration 2379, Loss: 2.3192315101623535\n",
            "Training Iteration 2380, Loss: 5.556740760803223\n",
            "Training Iteration 2381, Loss: 5.955356597900391\n",
            "Training Iteration 2382, Loss: 2.2528300285339355\n",
            "Training Iteration 2383, Loss: 2.1864123344421387\n",
            "Training Iteration 2384, Loss: 2.1180710792541504\n",
            "Training Iteration 2385, Loss: 4.966907024383545\n",
            "Training Iteration 2386, Loss: 6.0801682472229\n",
            "Training Iteration 2387, Loss: 3.1448006629943848\n",
            "Training Iteration 2388, Loss: 4.386934280395508\n",
            "Training Iteration 2389, Loss: 6.239253520965576\n",
            "Training Iteration 2390, Loss: 7.235311985015869\n",
            "Training Iteration 2391, Loss: 6.22702693939209\n",
            "Training Iteration 2392, Loss: 4.363386631011963\n",
            "Training Iteration 2393, Loss: 6.139945983886719\n",
            "Training Iteration 2394, Loss: 3.504551887512207\n",
            "Training Iteration 2395, Loss: 4.24531364440918\n",
            "Training Iteration 2396, Loss: 2.955375909805298\n",
            "Training Iteration 2397, Loss: 6.715111255645752\n",
            "Training Iteration 2398, Loss: 6.778275966644287\n",
            "Training Iteration 2399, Loss: 7.884633541107178\n",
            "Training Iteration 2400, Loss: 4.082995414733887\n",
            "Training Iteration 2401, Loss: 4.191166400909424\n",
            "Training Iteration 2402, Loss: 6.365272045135498\n",
            "Training Iteration 2403, Loss: 6.259428977966309\n",
            "Training Iteration 2404, Loss: 7.1360392570495605\n",
            "Training Iteration 2405, Loss: 4.726660251617432\n",
            "Training Iteration 2406, Loss: 3.698853015899658\n",
            "Training Iteration 2407, Loss: 4.652616500854492\n",
            "Training Iteration 2408, Loss: 5.3634209632873535\n",
            "Training Iteration 2409, Loss: 4.876811981201172\n",
            "Training Iteration 2410, Loss: 1.3321806192398071\n",
            "Training Iteration 2411, Loss: 6.78735876083374\n",
            "Training Iteration 2412, Loss: 4.048880577087402\n",
            "Training Iteration 2413, Loss: 5.365579605102539\n",
            "Training Iteration 2414, Loss: 3.859714984893799\n",
            "Training Iteration 2415, Loss: 5.714092254638672\n",
            "Training Iteration 2416, Loss: 4.219391345977783\n",
            "Training Iteration 2417, Loss: 3.450169801712036\n",
            "Training Iteration 2418, Loss: 7.837510108947754\n",
            "Training Iteration 2419, Loss: 2.423736572265625\n",
            "Training Iteration 2420, Loss: 6.398426532745361\n",
            "Training Iteration 2421, Loss: 2.981248140335083\n",
            "Training Iteration 2422, Loss: 1.6982700824737549\n",
            "Training Iteration 2423, Loss: 6.548095226287842\n",
            "Training Iteration 2424, Loss: 4.146002769470215\n",
            "Training Iteration 2425, Loss: 5.503254413604736\n",
            "Training Iteration 2426, Loss: 4.467286586761475\n",
            "Training Iteration 2427, Loss: 3.489454984664917\n",
            "Training Iteration 2428, Loss: 5.652565002441406\n",
            "Training Iteration 2429, Loss: 3.6539466381073\n",
            "Training Iteration 2430, Loss: 5.284895420074463\n",
            "Training Iteration 2431, Loss: 3.7537240982055664\n",
            "Training Iteration 2432, Loss: 3.4668047428131104\n",
            "Training Iteration 2433, Loss: 2.480217456817627\n",
            "Training Iteration 2434, Loss: 4.183821678161621\n",
            "Training Iteration 2435, Loss: 4.146658897399902\n",
            "Training Iteration 2436, Loss: 4.52599573135376\n",
            "Training Iteration 2437, Loss: 4.931617259979248\n",
            "Training Iteration 2438, Loss: 4.231074810028076\n",
            "Training Iteration 2439, Loss: 5.4686598777771\n",
            "Training Iteration 2440, Loss: 3.376595973968506\n",
            "Training Iteration 2441, Loss: 6.117686748504639\n",
            "Training Iteration 2442, Loss: 3.083766460418701\n",
            "Training Iteration 2443, Loss: 3.7108333110809326\n",
            "Training Iteration 2444, Loss: 4.7255778312683105\n",
            "Training Iteration 2445, Loss: 3.0584967136383057\n",
            "Training Iteration 2446, Loss: 7.249112129211426\n",
            "Training Iteration 2447, Loss: 6.368696689605713\n",
            "Training Iteration 2448, Loss: 3.490279197692871\n",
            "Training Iteration 2449, Loss: 3.7540392875671387\n",
            "Training Iteration 2450, Loss: 4.270888328552246\n",
            "Training Iteration 2451, Loss: 1.9148483276367188\n",
            "Training Iteration 2452, Loss: 3.0737743377685547\n",
            "Training Iteration 2453, Loss: 5.3762311935424805\n",
            "Training Iteration 2454, Loss: 3.423067569732666\n",
            "Training Iteration 2455, Loss: 2.3953301906585693\n",
            "Training Iteration 2456, Loss: 5.171314239501953\n",
            "Training Iteration 2457, Loss: 5.023308277130127\n",
            "Training Iteration 2458, Loss: 3.031636953353882\n",
            "Training Iteration 2459, Loss: 4.432986259460449\n",
            "Training Iteration 2460, Loss: 6.30961275100708\n",
            "Training Iteration 2461, Loss: 5.832886695861816\n",
            "Training Iteration 2462, Loss: 5.7350754737854\n",
            "Training Iteration 2463, Loss: 2.1056580543518066\n",
            "Training Iteration 2464, Loss: 6.132338523864746\n",
            "Training Iteration 2465, Loss: 5.25643253326416\n",
            "Training Iteration 2466, Loss: 4.6360883712768555\n",
            "Training Iteration 2467, Loss: 2.9663143157958984\n",
            "Training Iteration 2468, Loss: 6.815289497375488\n",
            "Training Iteration 2469, Loss: 3.4274237155914307\n",
            "Training Iteration 2470, Loss: 3.188324213027954\n",
            "Training Iteration 2471, Loss: 2.196290969848633\n",
            "Training Iteration 2472, Loss: 10.711641311645508\n",
            "Training Iteration 2473, Loss: 10.018669128417969\n",
            "Training Iteration 2474, Loss: 7.85619592666626\n",
            "Training Iteration 2475, Loss: 5.509511947631836\n",
            "Training Iteration 2476, Loss: 3.0688467025756836\n",
            "Training Iteration 2477, Loss: 8.396766662597656\n",
            "Training Iteration 2478, Loss: 5.263771057128906\n",
            "Training Iteration 2479, Loss: 6.1653361320495605\n",
            "Training Iteration 2480, Loss: 7.254455089569092\n",
            "Training Iteration 2481, Loss: 7.157513618469238\n",
            "Training Iteration 2482, Loss: 5.427892684936523\n",
            "Training Iteration 2483, Loss: 3.953326463699341\n",
            "Training Iteration 2484, Loss: 3.9814517498016357\n",
            "Training Iteration 2485, Loss: 3.3281562328338623\n",
            "Training Iteration 2486, Loss: 6.009920597076416\n",
            "Training Iteration 2487, Loss: 3.9979562759399414\n",
            "Training Iteration 2488, Loss: 5.276526927947998\n",
            "Training Iteration 2489, Loss: 8.882152557373047\n",
            "Training Iteration 2490, Loss: 4.149153232574463\n",
            "Training Iteration 2491, Loss: 11.42268180847168\n",
            "Training Iteration 2492, Loss: 2.7974467277526855\n",
            "Training Iteration 2493, Loss: 8.976292610168457\n",
            "Training Iteration 2494, Loss: 4.066247940063477\n",
            "Training Iteration 2495, Loss: 4.633951663970947\n",
            "Training Iteration 2496, Loss: 6.746712684631348\n",
            "Training Iteration 2497, Loss: 6.11635160446167\n",
            "Training Iteration 2498, Loss: 4.611098766326904\n",
            "Training Iteration 2499, Loss: 6.771266937255859\n",
            "Training Iteration 2500, Loss: 3.121936798095703\n",
            "Training Iteration 2501, Loss: 4.85864782333374\n",
            "Training Iteration 2502, Loss: 2.5174474716186523\n",
            "Training Iteration 2503, Loss: 4.430678367614746\n",
            "Training Iteration 2504, Loss: 4.85748291015625\n",
            "Training Iteration 2505, Loss: 2.6875698566436768\n",
            "Training Iteration 2506, Loss: 4.158077239990234\n",
            "Training Iteration 2507, Loss: 5.94870662689209\n",
            "Training Iteration 2508, Loss: 5.394103050231934\n",
            "Training Iteration 2509, Loss: 3.2034289836883545\n",
            "Training Iteration 2510, Loss: 2.421295404434204\n",
            "Training Iteration 2511, Loss: 5.193902969360352\n",
            "Training Iteration 2512, Loss: 4.629608631134033\n",
            "Training Iteration 2513, Loss: 3.0607106685638428\n",
            "Training Iteration 2514, Loss: 3.4672608375549316\n",
            "Training Iteration 2515, Loss: 5.694694995880127\n",
            "Training Iteration 2516, Loss: 3.154689073562622\n",
            "Training Iteration 2517, Loss: 3.5421080589294434\n",
            "Training Iteration 2518, Loss: 5.219094276428223\n",
            "Training Iteration 2519, Loss: 3.652135133743286\n",
            "Training Iteration 2520, Loss: 4.084110736846924\n",
            "Training Iteration 2521, Loss: 5.16996955871582\n",
            "Training Iteration 2522, Loss: 3.119966506958008\n",
            "Training Iteration 2523, Loss: 4.100348949432373\n",
            "Training Iteration 2524, Loss: 5.745560646057129\n",
            "Training Iteration 2525, Loss: 5.368408679962158\n",
            "Training Iteration 2526, Loss: 4.9655256271362305\n",
            "Training Iteration 2527, Loss: 2.7021608352661133\n",
            "Training Iteration 2528, Loss: 5.942569732666016\n",
            "Training Iteration 2529, Loss: 2.802323818206787\n",
            "Training Iteration 2530, Loss: 5.922952175140381\n",
            "Training Iteration 2531, Loss: 5.811199188232422\n",
            "Training Iteration 2532, Loss: 4.7782135009765625\n",
            "Training Iteration 2533, Loss: 3.731778144836426\n",
            "Training Iteration 2534, Loss: 7.095241069793701\n",
            "Training Iteration 2535, Loss: 4.613166809082031\n",
            "Training Iteration 2536, Loss: 3.946566581726074\n",
            "Training Iteration 2537, Loss: 3.052389144897461\n",
            "Training Iteration 2538, Loss: 2.918123960494995\n",
            "Training Iteration 2539, Loss: 6.340910911560059\n",
            "Training Iteration 2540, Loss: 2.9148998260498047\n",
            "Training Iteration 2541, Loss: 4.482507228851318\n",
            "Training Iteration 2542, Loss: 3.5709779262542725\n",
            "Training Iteration 2543, Loss: 4.315771102905273\n",
            "Training Iteration 2544, Loss: 4.758927822113037\n",
            "Training Iteration 2545, Loss: 3.4309778213500977\n",
            "Training Iteration 2546, Loss: 4.002610206604004\n",
            "Training Iteration 2547, Loss: 4.035472869873047\n",
            "Training Iteration 2548, Loss: 7.264779090881348\n",
            "Training Iteration 2549, Loss: 5.411217212677002\n",
            "Training Iteration 2550, Loss: 5.393983364105225\n",
            "Training Iteration 2551, Loss: 2.9926016330718994\n",
            "Training Iteration 2552, Loss: 3.3281641006469727\n",
            "Training Iteration 2553, Loss: 5.857717037200928\n",
            "Training Iteration 2554, Loss: 6.202478408813477\n",
            "Training Iteration 2555, Loss: 6.4089765548706055\n",
            "Training Iteration 2556, Loss: 2.7579667568206787\n",
            "Training Iteration 2557, Loss: 4.61233377456665\n",
            "Training Iteration 2558, Loss: 6.2210869789123535\n",
            "Training Iteration 2559, Loss: 8.884106636047363\n",
            "Training Iteration 2560, Loss: 6.9008893966674805\n",
            "Training Iteration 2561, Loss: 7.130927085876465\n",
            "Training Iteration 2562, Loss: 5.130892276763916\n",
            "Training Iteration 2563, Loss: 3.210890769958496\n",
            "Training Iteration 2564, Loss: 5.842294216156006\n",
            "Training Iteration 2565, Loss: 5.497126579284668\n",
            "Training Iteration 2566, Loss: 5.362433910369873\n",
            "Training Iteration 2567, Loss: 1.8236550092697144\n",
            "Training Iteration 2568, Loss: 3.8627305030822754\n",
            "Training Iteration 2569, Loss: 3.441737413406372\n",
            "Training Iteration 2570, Loss: 6.783384799957275\n",
            "Training Iteration 2571, Loss: 6.293810844421387\n",
            "Training Iteration 2572, Loss: 4.433584213256836\n",
            "Training Iteration 2573, Loss: 1.9036880731582642\n",
            "Training Iteration 2574, Loss: 2.4647395610809326\n",
            "Training Iteration 2575, Loss: 8.663124084472656\n",
            "Training Iteration 2576, Loss: 7.603567123413086\n",
            "Training Iteration 2577, Loss: 5.466835975646973\n",
            "Training Iteration 2578, Loss: 4.419781684875488\n",
            "Training Iteration 2579, Loss: 3.8931477069854736\n",
            "Training Iteration 2580, Loss: 3.9392261505126953\n",
            "Training Iteration 2581, Loss: 4.916999816894531\n",
            "Training Iteration 2582, Loss: 5.999029636383057\n",
            "Training Iteration 2583, Loss: 3.8950765132904053\n",
            "Training Iteration 2584, Loss: 1.9085862636566162\n",
            "Training Iteration 2585, Loss: 1.8684273958206177\n",
            "Training Iteration 2586, Loss: 1.3490132093429565\n",
            "Training Iteration 2587, Loss: 7.383988380432129\n",
            "Training Iteration 2588, Loss: 3.394080400466919\n",
            "Training Iteration 2589, Loss: 2.4661855697631836\n",
            "Training Iteration 2590, Loss: 6.982661247253418\n",
            "Training Iteration 2591, Loss: 2.2852985858917236\n",
            "Training Iteration 2592, Loss: 2.5811879634857178\n",
            "Training Iteration 2593, Loss: 3.73937726020813\n",
            "Training Iteration 2594, Loss: 2.988431930541992\n",
            "Training Iteration 2595, Loss: 4.412154674530029\n",
            "Training Iteration 2596, Loss: 5.24371337890625\n",
            "Training Iteration 2597, Loss: 4.001159191131592\n",
            "Training Iteration 2598, Loss: 4.056582927703857\n",
            "Training Iteration 2599, Loss: 2.9618964195251465\n",
            "Training Iteration 2600, Loss: 3.4661176204681396\n",
            "Training Iteration 2601, Loss: 3.2066433429718018\n",
            "Training Iteration 2602, Loss: 4.612351417541504\n",
            "Training Iteration 2603, Loss: 0.6136535406112671\n",
            "Training Iteration 2604, Loss: 2.8112049102783203\n",
            "Training Iteration 2605, Loss: 4.698775291442871\n",
            "Training Iteration 2606, Loss: 4.3065876960754395\n",
            "Training Iteration 2607, Loss: 2.4805753231048584\n",
            "Training Iteration 2608, Loss: 5.251625061035156\n",
            "Training Iteration 2609, Loss: 5.493152141571045\n",
            "Training Iteration 2610, Loss: 5.699028491973877\n",
            "Training Iteration 2611, Loss: 4.697173595428467\n",
            "Training Iteration 2612, Loss: 2.919534921646118\n",
            "Training Iteration 2613, Loss: 5.804692268371582\n",
            "Training Iteration 2614, Loss: 3.163062334060669\n",
            "Training Iteration 2615, Loss: 3.282818555831909\n",
            "Training Iteration 2616, Loss: 4.429812431335449\n",
            "Training Iteration 2617, Loss: 3.479735851287842\n",
            "Training Iteration 2618, Loss: 9.012970924377441\n",
            "Training Iteration 2619, Loss: 7.391478538513184\n",
            "Training Iteration 2620, Loss: 4.295870780944824\n",
            "Training Iteration 2621, Loss: 3.4107742309570312\n",
            "Training Iteration 2622, Loss: 2.6084957122802734\n",
            "Training Iteration 2623, Loss: 2.7029759883880615\n",
            "Training Iteration 2624, Loss: 5.913360595703125\n",
            "Training Iteration 2625, Loss: 3.3880438804626465\n",
            "Training Iteration 2626, Loss: 2.956543207168579\n",
            "Training Iteration 2627, Loss: 5.048341274261475\n",
            "Training Iteration 2628, Loss: 6.482669830322266\n",
            "Training Iteration 2629, Loss: 2.6184823513031006\n",
            "Training Iteration 2630, Loss: 3.109621047973633\n",
            "Training Iteration 2631, Loss: 6.668684959411621\n",
            "Training Iteration 2632, Loss: 4.352780818939209\n",
            "Training Iteration 2633, Loss: 2.7370777130126953\n",
            "Training Iteration 2634, Loss: 3.97623348236084\n",
            "Training Iteration 2635, Loss: 5.534234046936035\n",
            "Training Iteration 2636, Loss: 3.4358701705932617\n",
            "Training Iteration 2637, Loss: 2.7989892959594727\n",
            "Training Iteration 2638, Loss: 3.717777729034424\n",
            "Training Iteration 2639, Loss: 7.947149276733398\n",
            "Training Iteration 2640, Loss: 4.856523513793945\n",
            "Training Iteration 2641, Loss: 5.824585437774658\n",
            "Training Iteration 2642, Loss: 4.566465854644775\n",
            "Training Iteration 2643, Loss: 5.0887556076049805\n",
            "Training Iteration 2644, Loss: 4.760733604431152\n",
            "Training Iteration 2645, Loss: 4.792089939117432\n",
            "Training Iteration 2646, Loss: 6.6042680740356445\n",
            "Training Iteration 2647, Loss: 3.8112754821777344\n",
            "Training Iteration 2648, Loss: 3.266907215118408\n",
            "Training Iteration 2649, Loss: 4.726365566253662\n",
            "Training Iteration 2650, Loss: 3.383211135864258\n",
            "Training Iteration 2651, Loss: 3.1989035606384277\n",
            "Training Iteration 2652, Loss: 5.534035682678223\n",
            "Training Iteration 2653, Loss: 4.6187968254089355\n",
            "Training Iteration 2654, Loss: 2.6773681640625\n",
            "Training Iteration 2655, Loss: 6.013731479644775\n",
            "Training Iteration 2656, Loss: 5.007623195648193\n",
            "Training Iteration 2657, Loss: 5.2664618492126465\n",
            "Training Iteration 2658, Loss: 6.202420711517334\n",
            "Training Iteration 2659, Loss: 5.710843086242676\n",
            "Training Iteration 2660, Loss: 6.169404983520508\n",
            "Training Iteration 2661, Loss: 5.351651191711426\n",
            "Training Iteration 2662, Loss: 5.82642936706543\n",
            "Training Iteration 2663, Loss: 2.3663883209228516\n",
            "Training Iteration 2664, Loss: 5.583823204040527\n",
            "Training Iteration 2665, Loss: 5.917881965637207\n",
            "Training Iteration 2666, Loss: 5.171804904937744\n",
            "Training Iteration 2667, Loss: 7.011257648468018\n",
            "Training Iteration 2668, Loss: 4.572391510009766\n",
            "Training Iteration 2669, Loss: 3.7541403770446777\n",
            "Training Iteration 2670, Loss: 7.965310096740723\n",
            "Training Iteration 2671, Loss: 4.863968849182129\n",
            "Training Iteration 2672, Loss: 4.473720073699951\n",
            "Training Iteration 2673, Loss: 5.815769672393799\n",
            "Training Iteration 2674, Loss: 4.414261817932129\n",
            "Training Iteration 2675, Loss: 2.814143657684326\n",
            "Training Iteration 2676, Loss: 5.5292534828186035\n",
            "Training Iteration 2677, Loss: 4.472552299499512\n",
            "Training Iteration 2678, Loss: 1.47402822971344\n",
            "Training Iteration 2679, Loss: 5.207546710968018\n",
            "Training Iteration 2680, Loss: 4.683241844177246\n",
            "Training Iteration 2681, Loss: 5.073999881744385\n",
            "Training Iteration 2682, Loss: 6.274822235107422\n",
            "Training Iteration 2683, Loss: 2.8847060203552246\n",
            "Training Iteration 2684, Loss: 7.06689977645874\n",
            "Training Iteration 2685, Loss: 3.9862585067749023\n",
            "Training Iteration 2686, Loss: 2.097315788269043\n",
            "Training Iteration 2687, Loss: 6.712395191192627\n",
            "Training Iteration 2688, Loss: 5.603064060211182\n",
            "Training Iteration 2689, Loss: 2.8877975940704346\n",
            "Training Iteration 2690, Loss: 4.4243550300598145\n",
            "Training Iteration 2691, Loss: 1.1287707090377808\n",
            "Training Iteration 2692, Loss: 4.608607292175293\n",
            "Training Iteration 2693, Loss: 4.330265045166016\n",
            "Training Iteration 2694, Loss: 7.962639808654785\n",
            "Training Iteration 2695, Loss: 6.125034332275391\n",
            "Training Iteration 2696, Loss: 10.499306678771973\n",
            "Training Iteration 2697, Loss: 3.4491353034973145\n",
            "Training Iteration 2698, Loss: 2.71692156791687\n",
            "Training Iteration 2699, Loss: 3.967517137527466\n",
            "Training Iteration 2700, Loss: 2.141934633255005\n",
            "Training Iteration 2701, Loss: 1.8521722555160522\n",
            "Training Iteration 2702, Loss: 5.171854496002197\n",
            "Training Iteration 2703, Loss: 4.736639022827148\n",
            "Training Iteration 2704, Loss: 3.8321526050567627\n",
            "Training Iteration 2705, Loss: 7.893679141998291\n",
            "Training Iteration 2706, Loss: 3.089911699295044\n",
            "Training Iteration 2707, Loss: 6.022543430328369\n",
            "Training Iteration 2708, Loss: 4.177513599395752\n",
            "Training Iteration 2709, Loss: 4.804084777832031\n",
            "Training Iteration 2710, Loss: 4.965989589691162\n",
            "Training Iteration 2711, Loss: 5.361392498016357\n",
            "Training Iteration 2712, Loss: 3.9632482528686523\n",
            "Training Iteration 2713, Loss: 4.393928527832031\n",
            "Training Iteration 2714, Loss: 4.905546188354492\n",
            "Training Iteration 2715, Loss: 6.449406147003174\n",
            "Training Iteration 2716, Loss: 5.12916374206543\n",
            "Training Iteration 2717, Loss: 4.293717384338379\n",
            "Training Iteration 2718, Loss: 5.697670936584473\n",
            "Training Iteration 2719, Loss: 2.245122194290161\n",
            "Training Iteration 2720, Loss: 2.615790843963623\n",
            "Training Iteration 2721, Loss: 4.886481285095215\n",
            "Training Iteration 2722, Loss: 3.0489795207977295\n",
            "Training Iteration 2723, Loss: 5.456642150878906\n",
            "Training Iteration 2724, Loss: 4.353677749633789\n",
            "Training Iteration 2725, Loss: 6.020890235900879\n",
            "Training Iteration 2726, Loss: 4.474966526031494\n",
            "Training Iteration 2727, Loss: 4.6737871170043945\n",
            "Training Iteration 2728, Loss: 1.7908194065093994\n",
            "Training Iteration 2729, Loss: 2.548072576522827\n",
            "Training Iteration 2730, Loss: 3.254190444946289\n",
            "Training Iteration 2731, Loss: 4.8392534255981445\n",
            "Training Iteration 2732, Loss: 4.4910712242126465\n",
            "Training Iteration 2733, Loss: 5.433375358581543\n",
            "Training Iteration 2734, Loss: 4.08335018157959\n",
            "Training Iteration 2735, Loss: 3.161278009414673\n",
            "Training Iteration 2736, Loss: 5.045642852783203\n",
            "Training Iteration 2737, Loss: 4.352821350097656\n",
            "Training Iteration 2738, Loss: 3.236935615539551\n",
            "Training Iteration 2739, Loss: 4.745873928070068\n",
            "Training Iteration 2740, Loss: 4.477843284606934\n",
            "Training Iteration 2741, Loss: 4.499406337738037\n",
            "Training Iteration 2742, Loss: 3.7619900703430176\n",
            "Training Iteration 2743, Loss: 3.4928178787231445\n",
            "Training Iteration 2744, Loss: 4.117044448852539\n",
            "Training Iteration 2745, Loss: 6.05994176864624\n",
            "Training Iteration 2746, Loss: 3.193479061126709\n",
            "Training Iteration 2747, Loss: 2.7442193031311035\n",
            "Training Iteration 2748, Loss: 2.309293746948242\n",
            "Training Iteration 2749, Loss: 6.710251808166504\n",
            "Training Iteration 2750, Loss: 3.688994884490967\n",
            "Training Iteration 2751, Loss: 4.1561055183410645\n",
            "Training Iteration 2752, Loss: 4.783078193664551\n",
            "Training Iteration 2753, Loss: 5.383231163024902\n",
            "Training Iteration 2754, Loss: 4.573747158050537\n",
            "Training Iteration 2755, Loss: 7.2898125648498535\n",
            "Training Iteration 2756, Loss: 4.514984130859375\n",
            "Training Iteration 2757, Loss: 2.225778102874756\n",
            "Training Iteration 2758, Loss: 5.7365288734436035\n",
            "Training Iteration 2759, Loss: 6.530917644500732\n",
            "Training Iteration 2760, Loss: 7.863583564758301\n",
            "Training Iteration 2761, Loss: 5.876326560974121\n",
            "Training Iteration 2762, Loss: 3.6497011184692383\n",
            "Training Iteration 2763, Loss: 6.5883259773254395\n",
            "Training Iteration 2764, Loss: 4.515558242797852\n",
            "Training Iteration 2765, Loss: 2.824982166290283\n",
            "Training Iteration 2766, Loss: 1.512539267539978\n",
            "Training Iteration 2767, Loss: 5.84882926940918\n",
            "Training Iteration 2768, Loss: 4.939851760864258\n",
            "Training Iteration 2769, Loss: 5.148026943206787\n",
            "Training Iteration 2770, Loss: 1.373629093170166\n",
            "Training Iteration 2771, Loss: 6.5836591720581055\n",
            "Training Iteration 2772, Loss: 4.415376663208008\n",
            "Training Iteration 2773, Loss: 3.8478915691375732\n",
            "Training Iteration 2774, Loss: 4.423750400543213\n",
            "Training Iteration 2775, Loss: 4.893953800201416\n",
            "Training Iteration 2776, Loss: 5.34269380569458\n",
            "Training Iteration 2777, Loss: 4.730449676513672\n",
            "Training Iteration 2778, Loss: 4.338731288909912\n",
            "Training Iteration 2779, Loss: 4.545278072357178\n",
            "Training Iteration 2780, Loss: 3.024646282196045\n",
            "Training Iteration 2781, Loss: 3.421846866607666\n",
            "Training Iteration 2782, Loss: 2.1975021362304688\n",
            "Training Iteration 2783, Loss: 2.950917959213257\n",
            "Training Iteration 2784, Loss: 2.9153621196746826\n",
            "Training Iteration 2785, Loss: 4.453370571136475\n",
            "Training Iteration 2786, Loss: 5.884934425354004\n",
            "Training Iteration 2787, Loss: 6.013815402984619\n",
            "Training Iteration 2788, Loss: 7.323916912078857\n",
            "Training Iteration 2789, Loss: 2.6740942001342773\n",
            "Training Iteration 2790, Loss: 4.417081832885742\n",
            "Training Iteration 2791, Loss: 2.6354963779449463\n",
            "Training Iteration 2792, Loss: 2.456303358078003\n",
            "Training Iteration 2793, Loss: 3.924050807952881\n",
            "Training Iteration 2794, Loss: 3.170990467071533\n",
            "Training Iteration 2795, Loss: 3.820859909057617\n",
            "Training Iteration 2796, Loss: 3.4072694778442383\n",
            "Training Iteration 2797, Loss: 3.1392452716827393\n",
            "Training Iteration 2798, Loss: 3.423036575317383\n",
            "Training Iteration 2799, Loss: 4.1316986083984375\n",
            "Training Iteration 2800, Loss: 5.042046070098877\n",
            "Training Iteration 2801, Loss: 5.010449409484863\n",
            "Training Iteration 2802, Loss: 4.527685165405273\n",
            "Training Iteration 2803, Loss: 4.630030632019043\n",
            "Training Iteration 2804, Loss: 4.403642177581787\n",
            "Training Iteration 2805, Loss: 7.0251007080078125\n",
            "Training Iteration 2806, Loss: 3.5353171825408936\n",
            "Training Iteration 2807, Loss: 4.740530490875244\n",
            "Training Iteration 2808, Loss: 2.934077024459839\n",
            "Training Iteration 2809, Loss: 4.448780536651611\n",
            "Training Iteration 2810, Loss: 2.9936776161193848\n",
            "Training Iteration 2811, Loss: 3.4398534297943115\n",
            "Training Iteration 2812, Loss: 3.798142194747925\n",
            "Training Iteration 2813, Loss: 3.4467945098876953\n",
            "Training Iteration 2814, Loss: 3.456855297088623\n",
            "Training Iteration 2815, Loss: 4.565335273742676\n",
            "Training Iteration 2816, Loss: 3.6434919834136963\n",
            "Training Iteration 2817, Loss: 5.228869438171387\n",
            "Training Iteration 2818, Loss: 2.666555404663086\n",
            "Training Iteration 2819, Loss: 4.086991786956787\n",
            "Training Iteration 2820, Loss: 5.175581932067871\n",
            "Training Iteration 2821, Loss: 3.566765785217285\n",
            "Training Iteration 2822, Loss: 3.318279266357422\n",
            "Training Iteration 2823, Loss: 4.961014270782471\n",
            "Training Iteration 2824, Loss: 4.590692520141602\n",
            "Training Iteration 2825, Loss: 3.13897967338562\n",
            "Training Iteration 2826, Loss: 1.2113577127456665\n",
            "Training Iteration 2827, Loss: 6.670086860656738\n",
            "Training Iteration 2828, Loss: 4.462066650390625\n",
            "Training Iteration 2829, Loss: 4.892516136169434\n",
            "Training Iteration 2830, Loss: 1.8714386224746704\n",
            "Training Iteration 2831, Loss: 2.730740785598755\n",
            "Training Iteration 2832, Loss: 2.941887378692627\n",
            "Training Iteration 2833, Loss: 5.270759582519531\n",
            "Training Iteration 2834, Loss: 4.080324649810791\n",
            "Training Iteration 2835, Loss: 4.608532905578613\n",
            "Training Iteration 2836, Loss: 5.412824630737305\n",
            "Training Iteration 2837, Loss: 5.650816917419434\n",
            "Training Iteration 2838, Loss: 6.079665184020996\n",
            "Training Iteration 2839, Loss: 7.412859916687012\n",
            "Training Iteration 2840, Loss: 6.063724517822266\n",
            "Training Iteration 2841, Loss: 1.8311662673950195\n",
            "Training Iteration 2842, Loss: 3.350527048110962\n",
            "Training Iteration 2843, Loss: 4.464232444763184\n",
            "Training Iteration 2844, Loss: 7.772822380065918\n",
            "Training Iteration 2845, Loss: 5.033856391906738\n",
            "Training Iteration 2846, Loss: 6.071002006530762\n",
            "Training Iteration 2847, Loss: 2.6260945796966553\n",
            "Training Iteration 2848, Loss: 6.1428704261779785\n",
            "Training Iteration 2849, Loss: 3.854983329772949\n",
            "Training Iteration 2850, Loss: 4.333037376403809\n",
            "Training Iteration 2851, Loss: 11.620784759521484\n",
            "Training Iteration 2852, Loss: 6.329853057861328\n",
            "Training Iteration 2853, Loss: 6.327425003051758\n",
            "Training Iteration 2854, Loss: 5.896622657775879\n",
            "Training Iteration 2855, Loss: 7.446377277374268\n",
            "Training Iteration 2856, Loss: 4.910695552825928\n",
            "Training Iteration 2857, Loss: 3.210665225982666\n",
            "Training Iteration 2858, Loss: 11.771554946899414\n",
            "Training Iteration 2859, Loss: 4.109676837921143\n",
            "Training Iteration 2860, Loss: 4.967364311218262\n",
            "Training Iteration 2861, Loss: 4.020087242126465\n",
            "Training Iteration 2862, Loss: 3.392958641052246\n",
            "Training Iteration 2863, Loss: 7.622807025909424\n",
            "Training Iteration 2864, Loss: 3.6412253379821777\n",
            "Training Iteration 2865, Loss: 5.506570816040039\n",
            "Training Iteration 2866, Loss: 8.404711723327637\n",
            "Training Iteration 2867, Loss: 2.7560408115386963\n",
            "Training Iteration 2868, Loss: 2.5942180156707764\n",
            "Training Iteration 2869, Loss: 8.368030548095703\n",
            "Training Iteration 2870, Loss: 2.680738687515259\n",
            "Training Iteration 2871, Loss: 2.8677921295166016\n",
            "Training Iteration 2872, Loss: 5.107699871063232\n",
            "Training Iteration 2873, Loss: 2.902543306350708\n",
            "Training Iteration 2874, Loss: 4.648553848266602\n",
            "Training Iteration 2875, Loss: 3.4748337268829346\n",
            "Training Iteration 2876, Loss: 8.530006408691406\n",
            "Training Iteration 2877, Loss: 4.444278240203857\n",
            "Training Iteration 2878, Loss: 5.542997360229492\n",
            "Training Iteration 2879, Loss: 3.1421124935150146\n",
            "Training Iteration 2880, Loss: 4.03928279876709\n",
            "Training Iteration 2881, Loss: 3.7535970211029053\n",
            "Training Iteration 2882, Loss: 2.336599111557007\n",
            "Training Iteration 2883, Loss: 5.778818607330322\n",
            "Training Iteration 2884, Loss: 4.726483345031738\n",
            "Training Iteration 2885, Loss: 3.0627849102020264\n",
            "Training Iteration 2886, Loss: 3.623774766921997\n",
            "Training Iteration 2887, Loss: 4.080867767333984\n",
            "Training Iteration 2888, Loss: 6.325037002563477\n",
            "Training Iteration 2889, Loss: 3.8105595111846924\n",
            "Training Iteration 2890, Loss: 7.944807052612305\n",
            "Training Iteration 2891, Loss: 3.665907144546509\n",
            "Training Iteration 2892, Loss: 3.295307159423828\n",
            "Training Iteration 2893, Loss: 5.1953125\n",
            "Training Iteration 2894, Loss: 5.984766960144043\n",
            "Training Iteration 2895, Loss: 7.630105972290039\n",
            "Training Iteration 2896, Loss: 5.881529808044434\n",
            "Training Iteration 2897, Loss: 6.606842517852783\n",
            "Training Iteration 2898, Loss: 5.89751672744751\n",
            "Training Iteration 2899, Loss: 6.326692581176758\n",
            "Training Iteration 2900, Loss: 5.225815773010254\n",
            "Training Iteration 2901, Loss: 3.9821417331695557\n",
            "Training Iteration 2902, Loss: 3.3711743354797363\n",
            "Training Iteration 2903, Loss: 4.813610553741455\n",
            "Training Iteration 2904, Loss: 4.078590393066406\n",
            "Training Iteration 2905, Loss: 4.553135871887207\n",
            "Training Iteration 2906, Loss: 4.389614105224609\n",
            "Training Iteration 2907, Loss: 4.237959384918213\n",
            "Training Iteration 2908, Loss: 3.7531728744506836\n",
            "Training Iteration 2909, Loss: 2.9980549812316895\n",
            "Training Iteration 2910, Loss: 3.035982608795166\n",
            "Training Iteration 2911, Loss: 2.944620132446289\n",
            "Training Iteration 2912, Loss: 2.3745079040527344\n",
            "Training Iteration 2913, Loss: 5.028968334197998\n",
            "Training Iteration 2914, Loss: 4.432680606842041\n",
            "Training Iteration 2915, Loss: 3.375824451446533\n",
            "Training Iteration 2916, Loss: 3.920927047729492\n",
            "Training Iteration 2917, Loss: 4.7937235832214355\n",
            "Training Iteration 2918, Loss: 2.623420000076294\n",
            "Training Iteration 2919, Loss: 3.648714542388916\n",
            "Training Iteration 2920, Loss: 7.128427028656006\n",
            "Training Iteration 2921, Loss: 7.983161926269531\n",
            "Training Iteration 2922, Loss: 3.437432289123535\n",
            "Training Iteration 2923, Loss: 3.7420926094055176\n",
            "Training Iteration 2924, Loss: 4.100869655609131\n",
            "Training Iteration 2925, Loss: 4.2966766357421875\n",
            "Training Iteration 2926, Loss: 8.117703437805176\n",
            "Training Iteration 2927, Loss: 3.188046932220459\n",
            "Training Iteration 2928, Loss: 3.2140607833862305\n",
            "Training Iteration 2929, Loss: 7.853804588317871\n",
            "Training Iteration 2930, Loss: 2.5787999629974365\n",
            "Training Iteration 2931, Loss: 5.367068290710449\n",
            "Training Iteration 2932, Loss: 3.144956350326538\n",
            "Training Iteration 2933, Loss: 2.9144372940063477\n",
            "Training Iteration 2934, Loss: 3.192863702774048\n",
            "Training Iteration 2935, Loss: 4.805560111999512\n",
            "Training Iteration 2936, Loss: 2.538684368133545\n",
            "Training Iteration 2937, Loss: 3.517714500427246\n",
            "Training Iteration 2938, Loss: 5.406846046447754\n",
            "Training Iteration 2939, Loss: 3.75396728515625\n",
            "Training Iteration 2940, Loss: 3.4554238319396973\n",
            "Training Iteration 2941, Loss: 2.694575309753418\n",
            "Training Iteration 2942, Loss: 1.6179240942001343\n",
            "Training Iteration 2943, Loss: 2.8759140968322754\n",
            "Training Iteration 2944, Loss: 7.715202331542969\n",
            "Training Iteration 2945, Loss: 3.176603317260742\n",
            "Training Iteration 2946, Loss: 7.590721130371094\n",
            "Training Iteration 2947, Loss: 5.7694010734558105\n",
            "Training Iteration 2948, Loss: 3.621391534805298\n",
            "Training Iteration 2949, Loss: 4.209261894226074\n",
            "Training Iteration 2950, Loss: 4.020793914794922\n",
            "Training Iteration 2951, Loss: 3.4269802570343018\n",
            "Training Iteration 2952, Loss: 4.952714443206787\n",
            "Training Iteration 2953, Loss: 3.3579049110412598\n",
            "Training Iteration 2954, Loss: 3.0984809398651123\n",
            "Training Iteration 2955, Loss: 2.9340906143188477\n",
            "Training Iteration 2956, Loss: 4.4420270919799805\n",
            "Training Iteration 2957, Loss: 3.4796903133392334\n",
            "Training Iteration 2958, Loss: 3.02131986618042\n",
            "Training Iteration 2959, Loss: 6.521512031555176\n",
            "Training Iteration 2960, Loss: 7.70070743560791\n",
            "Training Iteration 2961, Loss: 3.79819393157959\n",
            "Training Iteration 2962, Loss: 3.4890668392181396\n",
            "Training Iteration 2963, Loss: 3.7895498275756836\n",
            "Training Iteration 2964, Loss: 3.21669340133667\n",
            "Training Iteration 2965, Loss: 5.218897819519043\n",
            "Training Iteration 2966, Loss: 5.6239848136901855\n",
            "Training Iteration 2967, Loss: 0.9733784794807434\n",
            "Training Iteration 2968, Loss: 5.280312538146973\n",
            "Training Iteration 2969, Loss: 3.3033645153045654\n",
            "Training Iteration 2970, Loss: 4.884390830993652\n",
            "Training Iteration 2971, Loss: 2.8070621490478516\n",
            "Training Iteration 2972, Loss: 3.878117084503174\n",
            "Training Iteration 2973, Loss: 3.430706262588501\n",
            "Training Iteration 2974, Loss: 4.714504718780518\n",
            "Training Iteration 2975, Loss: 2.034788131713867\n",
            "Training Iteration 2976, Loss: 3.82517147064209\n",
            "Training Iteration 2977, Loss: 5.238384246826172\n",
            "Training Iteration 2978, Loss: 4.3166303634643555\n",
            "Training Iteration 2979, Loss: 1.499781847000122\n",
            "Training Iteration 2980, Loss: 3.4092795848846436\n",
            "Training Iteration 2981, Loss: 2.49629807472229\n",
            "Training Iteration 2982, Loss: 5.14769172668457\n",
            "Training Iteration 2983, Loss: 3.595829486846924\n",
            "Training Iteration 2984, Loss: 4.087745666503906\n",
            "Training Iteration 2985, Loss: 5.554346561431885\n",
            "Training Iteration 2986, Loss: 3.915496826171875\n",
            "Training Iteration 2987, Loss: 5.508281707763672\n",
            "Training Iteration 2988, Loss: 2.626619815826416\n",
            "Training Iteration 2989, Loss: 6.300166606903076\n",
            "Training Iteration 2990, Loss: 5.5942182540893555\n",
            "Training Iteration 2991, Loss: 2.6322646141052246\n",
            "Training Iteration 2992, Loss: 2.9513227939605713\n",
            "Training Iteration 2993, Loss: 6.798792839050293\n",
            "Training Iteration 2994, Loss: 3.7461133003234863\n",
            "Training Iteration 2995, Loss: 3.0945966243743896\n",
            "Training Iteration 2996, Loss: 3.340611696243286\n",
            "Training Iteration 2997, Loss: 4.631732940673828\n",
            "Training Iteration 2998, Loss: 4.818337440490723\n",
            "Training Iteration 2999, Loss: 2.6550710201263428\n",
            "Training Iteration 3000, Loss: 5.952192306518555\n",
            "Training Iteration 3001, Loss: 3.0969622135162354\n",
            "Training Iteration 3002, Loss: 4.4111199378967285\n",
            "Training Iteration 3003, Loss: 4.312861442565918\n",
            "Training Iteration 3004, Loss: 5.429229736328125\n",
            "Training Iteration 3005, Loss: 3.4657135009765625\n",
            "Training Iteration 3006, Loss: 4.207974433898926\n",
            "Training Iteration 3007, Loss: 4.490714073181152\n",
            "Training Iteration 3008, Loss: 4.238553047180176\n",
            "Training Iteration 3009, Loss: 4.018956661224365\n",
            "Training Iteration 3010, Loss: 4.067747592926025\n",
            "Training Iteration 3011, Loss: 3.0722544193267822\n",
            "Training Iteration 3012, Loss: 4.283294200897217\n",
            "Training Iteration 3013, Loss: 2.4257915019989014\n",
            "Training Iteration 3014, Loss: 6.311278343200684\n",
            "Training Iteration 3015, Loss: 4.3443427085876465\n",
            "Training Iteration 3016, Loss: 7.93019962310791\n",
            "Training Iteration 3017, Loss: 3.9447247982025146\n",
            "Training Iteration 3018, Loss: 4.3651957511901855\n",
            "Training Iteration 3019, Loss: 5.610284805297852\n",
            "Training Iteration 3020, Loss: 5.241724014282227\n",
            "Training Iteration 3021, Loss: 2.3619046211242676\n",
            "Training Iteration 3022, Loss: 5.41403865814209\n",
            "Training Iteration 3023, Loss: 5.623366832733154\n",
            "Training Iteration 3024, Loss: 3.3548641204833984\n",
            "Training Iteration 3025, Loss: 3.042750358581543\n",
            "Training Iteration 3026, Loss: 4.799349784851074\n",
            "Training Iteration 3027, Loss: 7.211056232452393\n",
            "Training Iteration 3028, Loss: 4.194169044494629\n",
            "Training Iteration 3029, Loss: 3.772200584411621\n",
            "Training Iteration 3030, Loss: 6.25925874710083\n",
            "Training Iteration 3031, Loss: 2.4866037368774414\n",
            "Training Iteration 3032, Loss: 2.755621910095215\n",
            "Training Iteration 3033, Loss: 4.498661994934082\n",
            "Training Iteration 3034, Loss: 2.1953036785125732\n",
            "Training Iteration 3035, Loss: 4.230001926422119\n",
            "Training Iteration 3036, Loss: 3.376817226409912\n",
            "Training Iteration 3037, Loss: 5.690176963806152\n",
            "Training Iteration 3038, Loss: 4.1667890548706055\n",
            "Training Iteration 3039, Loss: 5.730035305023193\n",
            "Training Iteration 3040, Loss: 2.9268393516540527\n",
            "Training Iteration 3041, Loss: 2.8636090755462646\n",
            "Training Iteration 3042, Loss: 1.1763967275619507\n",
            "Training Iteration 3043, Loss: 5.327614784240723\n",
            "Training Iteration 3044, Loss: 3.9807639122009277\n",
            "Training Iteration 3045, Loss: 4.657800197601318\n",
            "Training Iteration 3046, Loss: 5.417435646057129\n",
            "Training Iteration 3047, Loss: 2.7919607162475586\n",
            "Training Iteration 3048, Loss: 4.123295783996582\n",
            "Training Iteration 3049, Loss: 5.6101226806640625\n",
            "Training Iteration 3050, Loss: 5.149216651916504\n",
            "Training Iteration 3051, Loss: 3.5316050052642822\n",
            "Training Iteration 3052, Loss: 2.4636433124542236\n",
            "Training Iteration 3053, Loss: 3.0573394298553467\n",
            "Training Iteration 3054, Loss: 3.5664660930633545\n",
            "Training Iteration 3055, Loss: 3.0925188064575195\n",
            "Training Iteration 3056, Loss: 3.5219333171844482\n",
            "Training Iteration 3057, Loss: 4.162447929382324\n",
            "Training Iteration 3058, Loss: 5.944056034088135\n",
            "Training Iteration 3059, Loss: 6.189222812652588\n",
            "Training Iteration 3060, Loss: 5.533293724060059\n",
            "Training Iteration 3061, Loss: 4.146828651428223\n",
            "Training Iteration 3062, Loss: 3.704041004180908\n",
            "Training Iteration 3063, Loss: 2.959355354309082\n",
            "Training Iteration 3064, Loss: 1.5735557079315186\n",
            "Training Iteration 3065, Loss: 2.5467004776000977\n",
            "Training Iteration 3066, Loss: 5.072667121887207\n",
            "Training Iteration 3067, Loss: 3.6677801609039307\n",
            "Training Iteration 3068, Loss: 2.4947164058685303\n",
            "Training Iteration 3069, Loss: 4.127833366394043\n",
            "Training Iteration 3070, Loss: 2.3044729232788086\n",
            "Training Iteration 3071, Loss: 4.2804036140441895\n",
            "Training Iteration 3072, Loss: 6.132874488830566\n",
            "Training Iteration 3073, Loss: 5.872209548950195\n",
            "Training Iteration 3074, Loss: 3.076896905899048\n",
            "Training Iteration 3075, Loss: 4.961045742034912\n",
            "Training Iteration 3076, Loss: 3.1558613777160645\n",
            "Training Iteration 3077, Loss: 3.9563050270080566\n",
            "Training Iteration 3078, Loss: 4.309754848480225\n",
            "Training Iteration 3079, Loss: 2.7072267532348633\n",
            "Training Iteration 3080, Loss: 5.431035995483398\n",
            "Training Iteration 3081, Loss: 6.567962646484375\n",
            "Training Iteration 3082, Loss: 5.468733310699463\n",
            "Training Iteration 3083, Loss: 4.84308385848999\n",
            "Training Iteration 3084, Loss: 2.1290040016174316\n",
            "Training Iteration 3085, Loss: 4.374753952026367\n",
            "Training Iteration 3086, Loss: 3.9611146450042725\n",
            "Training Iteration 3087, Loss: 4.416390895843506\n",
            "Training Iteration 3088, Loss: 2.3985376358032227\n",
            "Training Iteration 3089, Loss: 2.2712979316711426\n",
            "Training Iteration 3090, Loss: 3.2529778480529785\n",
            "Training Iteration 3091, Loss: 4.412388324737549\n",
            "Training Iteration 3092, Loss: 5.455549240112305\n",
            "Training Iteration 3093, Loss: 2.4975333213806152\n",
            "Training Iteration 3094, Loss: 3.622711420059204\n",
            "Training Iteration 3095, Loss: 5.286510467529297\n",
            "Training Iteration 3096, Loss: 3.1807236671447754\n",
            "Training Iteration 3097, Loss: 3.6414783000946045\n",
            "Training Iteration 3098, Loss: 7.691714763641357\n",
            "Training Iteration 3099, Loss: 7.03779411315918\n",
            "Training Iteration 3100, Loss: 3.750011920928955\n",
            "Training Iteration 3101, Loss: 2.2026631832122803\n",
            "Training Iteration 3102, Loss: 7.557084560394287\n",
            "Training Iteration 3103, Loss: 3.9686532020568848\n",
            "Training Iteration 3104, Loss: 5.935308456420898\n",
            "Training Iteration 3105, Loss: 6.2638397216796875\n",
            "Training Iteration 3106, Loss: 3.8524928092956543\n",
            "Training Iteration 3107, Loss: 5.2844719886779785\n",
            "Training Iteration 3108, Loss: 5.132932186126709\n",
            "Training Iteration 3109, Loss: 3.246277332305908\n",
            "Training Iteration 3110, Loss: 3.7571160793304443\n",
            "Training Iteration 3111, Loss: 4.658889293670654\n",
            "Training Iteration 3112, Loss: 3.262967348098755\n",
            "Training Iteration 3113, Loss: 3.511819839477539\n",
            "Training Iteration 3114, Loss: 5.022151470184326\n",
            "Training Iteration 3115, Loss: 5.526762008666992\n",
            "Training Iteration 3116, Loss: 3.3706517219543457\n",
            "Training Iteration 3117, Loss: 2.0704057216644287\n",
            "Training Iteration 3118, Loss: 4.238593578338623\n",
            "Training Iteration 3119, Loss: 4.745792388916016\n",
            "Training Iteration 3120, Loss: 1.4204427003860474\n",
            "Training Iteration 3121, Loss: 7.030914306640625\n",
            "Training Iteration 3122, Loss: 2.4710593223571777\n",
            "Training Iteration 3123, Loss: 5.971346855163574\n",
            "Training Iteration 3124, Loss: 5.370813369750977\n",
            "Training Iteration 3125, Loss: 5.3381547927856445\n",
            "Training Iteration 3126, Loss: 5.609221458435059\n",
            "Training Iteration 3127, Loss: 5.00019645690918\n",
            "Training Iteration 3128, Loss: 3.786940097808838\n",
            "Training Iteration 3129, Loss: 9.086372375488281\n",
            "Training Iteration 3130, Loss: 6.551318168640137\n",
            "Training Iteration 3131, Loss: 3.3343493938446045\n",
            "Training Iteration 3132, Loss: 5.246222019195557\n",
            "Training Iteration 3133, Loss: 3.405661106109619\n",
            "Training Iteration 3134, Loss: 5.658786296844482\n",
            "Training Iteration 3135, Loss: 4.098731994628906\n",
            "Training Iteration 3136, Loss: 4.637133598327637\n",
            "Training Iteration 3137, Loss: 2.4422876834869385\n",
            "Training Iteration 3138, Loss: 4.678298473358154\n",
            "Training Iteration 3139, Loss: 5.593202114105225\n",
            "Training Iteration 3140, Loss: 5.696108341217041\n",
            "Training Iteration 3141, Loss: 4.060087203979492\n",
            "Training Iteration 3142, Loss: 3.444190502166748\n",
            "Training Iteration 3143, Loss: 5.548192977905273\n",
            "Training Iteration 3144, Loss: 2.6095032691955566\n",
            "Training Iteration 3145, Loss: 2.2679593563079834\n",
            "Training Iteration 3146, Loss: 2.3984222412109375\n",
            "Training Iteration 3147, Loss: 2.903461456298828\n",
            "Training Iteration 3148, Loss: 3.447427988052368\n",
            "Training Iteration 3149, Loss: 5.356779098510742\n",
            "Training Iteration 3150, Loss: 1.3741939067840576\n",
            "Training Iteration 3151, Loss: 5.312289714813232\n",
            "Training Iteration 3152, Loss: 3.7305610179901123\n",
            "Training Iteration 3153, Loss: 3.2639706134796143\n",
            "Training Iteration 3154, Loss: 3.3390045166015625\n",
            "Training Iteration 3155, Loss: 2.3700196743011475\n",
            "Training Iteration 3156, Loss: 4.9065141677856445\n",
            "Training Iteration 3157, Loss: 5.997298717498779\n",
            "Training Iteration 3158, Loss: 5.424211502075195\n",
            "Training Iteration 3159, Loss: 4.083728790283203\n",
            "Training Iteration 3160, Loss: 5.916685104370117\n",
            "Training Iteration 3161, Loss: 5.694881439208984\n",
            "Training Iteration 3162, Loss: 1.691816806793213\n",
            "Training Iteration 3163, Loss: 2.4600167274475098\n",
            "Training Iteration 3164, Loss: 5.837470054626465\n",
            "Training Iteration 3165, Loss: 5.537092208862305\n",
            "Training Iteration 3166, Loss: 4.005517482757568\n",
            "Training Iteration 3167, Loss: 4.7998857498168945\n",
            "Training Iteration 3168, Loss: 4.2667341232299805\n",
            "Training Iteration 3169, Loss: 3.081714153289795\n",
            "Training Iteration 3170, Loss: 4.0872650146484375\n",
            "Training Iteration 3171, Loss: 4.536824703216553\n",
            "Training Iteration 3172, Loss: 5.667169094085693\n",
            "Training Iteration 3173, Loss: 5.181003570556641\n",
            "Training Iteration 3174, Loss: 7.371578216552734\n",
            "Training Iteration 3175, Loss: 3.430487632751465\n",
            "Training Iteration 3176, Loss: 5.336658954620361\n",
            "Training Iteration 3177, Loss: 2.5581469535827637\n",
            "Training Iteration 3178, Loss: 6.9176483154296875\n",
            "Training Iteration 3179, Loss: 4.603108882904053\n",
            "Training Iteration 3180, Loss: 4.673449516296387\n",
            "Training Iteration 3181, Loss: 3.1654775142669678\n",
            "Training Iteration 3182, Loss: 7.981661319732666\n",
            "Training Iteration 3183, Loss: 4.775574684143066\n",
            "Training Iteration 3184, Loss: 4.550865173339844\n",
            "Training Iteration 3185, Loss: 3.3477401733398438\n",
            "Training Iteration 3186, Loss: 2.2522501945495605\n",
            "Training Iteration 3187, Loss: 6.684267997741699\n",
            "Training Iteration 3188, Loss: 6.418028354644775\n",
            "Training Iteration 3189, Loss: 3.729588508605957\n",
            "Training Iteration 3190, Loss: 2.763608455657959\n",
            "Training Iteration 3191, Loss: 3.1288692951202393\n",
            "Training Iteration 3192, Loss: 5.298285961151123\n",
            "Training Iteration 3193, Loss: 2.34346604347229\n",
            "Training Iteration 3194, Loss: 6.677309989929199\n",
            "Training Iteration 3195, Loss: 5.723012924194336\n",
            "Training Iteration 3196, Loss: 4.030169486999512\n",
            "Training Iteration 3197, Loss: 3.389596700668335\n",
            "Training Iteration 3198, Loss: 4.823302745819092\n",
            "Training Iteration 3199, Loss: 6.222492218017578\n",
            "Training Iteration 3200, Loss: 6.105798721313477\n",
            "Training Iteration 3201, Loss: 3.900096893310547\n",
            "Training Iteration 3202, Loss: 6.11533260345459\n",
            "Training Iteration 3203, Loss: 5.958155155181885\n",
            "Training Iteration 3204, Loss: 3.4394075870513916\n",
            "Training Iteration 3205, Loss: 1.8616355657577515\n",
            "Training Iteration 3206, Loss: 3.77484393119812\n",
            "Training Iteration 3207, Loss: 7.5741376876831055\n",
            "Training Iteration 3208, Loss: 4.023545742034912\n",
            "Training Iteration 3209, Loss: 6.458095550537109\n",
            "Training Iteration 3210, Loss: 6.305777072906494\n",
            "Training Iteration 3211, Loss: 2.697939872741699\n",
            "Training Iteration 3212, Loss: 3.664634943008423\n",
            "Training Iteration 3213, Loss: 2.8617358207702637\n",
            "Training Iteration 3214, Loss: 4.058311939239502\n",
            "Training Iteration 3215, Loss: 5.35430908203125\n",
            "Training Iteration 3216, Loss: 4.423348426818848\n",
            "Training Iteration 3217, Loss: 5.977237701416016\n",
            "Training Iteration 3218, Loss: 3.5907750129699707\n",
            "Training Iteration 3219, Loss: 3.7500288486480713\n",
            "Training Iteration 3220, Loss: 4.393362998962402\n",
            "Training Iteration 3221, Loss: 1.6861560344696045\n",
            "Training Iteration 3222, Loss: 4.43489933013916\n",
            "Training Iteration 3223, Loss: 3.146289587020874\n",
            "Training Iteration 3224, Loss: 2.877243757247925\n",
            "Training Iteration 3225, Loss: 6.770569801330566\n",
            "Training Iteration 3226, Loss: 5.276246070861816\n",
            "Training Iteration 3227, Loss: 2.9428162574768066\n",
            "Training Iteration 3228, Loss: 4.187921047210693\n",
            "Training Iteration 3229, Loss: 3.4201152324676514\n",
            "Training Iteration 3230, Loss: 5.56026554107666\n",
            "Training Iteration 3231, Loss: 4.788182735443115\n",
            "Training Iteration 3232, Loss: 5.901482582092285\n",
            "Training Iteration 3233, Loss: 3.9239721298217773\n",
            "Training Iteration 3234, Loss: 4.020269870758057\n",
            "Training Iteration 3235, Loss: 7.195429801940918\n",
            "Training Iteration 3236, Loss: 6.141627311706543\n",
            "Training Iteration 3237, Loss: 5.524597644805908\n",
            "Training Iteration 3238, Loss: 5.440640449523926\n",
            "Training Iteration 3239, Loss: 5.290587425231934\n",
            "Training Iteration 3240, Loss: 5.944571018218994\n",
            "Training Iteration 3241, Loss: 3.7845170497894287\n",
            "Training Iteration 3242, Loss: 3.43908429145813\n",
            "Training Iteration 3243, Loss: 3.748244285583496\n",
            "Training Iteration 3244, Loss: 3.726001262664795\n",
            "Training Iteration 3245, Loss: 8.81159496307373\n",
            "Training Iteration 3246, Loss: 6.707491874694824\n",
            "Training Iteration 3247, Loss: 4.7106757164001465\n",
            "Training Iteration 3248, Loss: 2.323124885559082\n",
            "Training Iteration 3249, Loss: 5.440425395965576\n",
            "Training Iteration 3250, Loss: 4.49600076675415\n",
            "Training Iteration 3251, Loss: 2.460904598236084\n",
            "Training Iteration 3252, Loss: 2.95162296295166\n",
            "Training Iteration 3253, Loss: 3.3309426307678223\n",
            "Training Iteration 3254, Loss: 4.708745002746582\n",
            "Training Iteration 3255, Loss: 3.2527761459350586\n",
            "Training Iteration 3256, Loss: 7.135444164276123\n",
            "Training Iteration 3257, Loss: 6.013215065002441\n",
            "Training Iteration 3258, Loss: 6.103953838348389\n",
            "Training Iteration 3259, Loss: 4.4506306648254395\n",
            "Training Iteration 3260, Loss: 5.049838542938232\n",
            "Training Iteration 3261, Loss: 4.11566162109375\n",
            "Training Iteration 3262, Loss: 1.1749463081359863\n",
            "Training Iteration 3263, Loss: 4.530091285705566\n",
            "Training Iteration 3264, Loss: 5.6966962814331055\n",
            "Training Iteration 3265, Loss: 7.38510799407959\n",
            "Training Iteration 3266, Loss: 0.8434168696403503\n",
            "Training Iteration 3267, Loss: 3.2423524856567383\n",
            "Training Iteration 3268, Loss: 3.39880108833313\n",
            "Training Iteration 3269, Loss: 4.60240364074707\n",
            "Training Iteration 3270, Loss: 4.684653282165527\n",
            "Training Iteration 3271, Loss: 9.853975296020508\n",
            "Training Iteration 3272, Loss: 6.199959754943848\n",
            "Training Iteration 3273, Loss: 6.431403160095215\n",
            "Training Iteration 3274, Loss: 1.968153476715088\n",
            "Training Iteration 3275, Loss: 2.8080451488494873\n",
            "Training Iteration 3276, Loss: 2.8812568187713623\n",
            "Training Iteration 3277, Loss: 6.45973539352417\n",
            "Training Iteration 3278, Loss: 2.9375104904174805\n",
            "Training Iteration 3279, Loss: 3.5925745964050293\n",
            "Training Iteration 3280, Loss: 2.796643018722534\n",
            "Training Iteration 3281, Loss: 4.904185771942139\n",
            "Training Iteration 3282, Loss: 5.430997848510742\n",
            "Training Iteration 3283, Loss: 4.789374351501465\n",
            "Training Iteration 3284, Loss: 3.9665651321411133\n",
            "Training Iteration 3285, Loss: 4.867855548858643\n",
            "Training Iteration 3286, Loss: 4.348195552825928\n",
            "Training Iteration 3287, Loss: 5.825228691101074\n",
            "Training Iteration 3288, Loss: 3.7572145462036133\n",
            "Training Iteration 3289, Loss: 4.795681476593018\n",
            "Training Iteration 3290, Loss: 3.468369483947754\n",
            "Training Iteration 3291, Loss: 2.654818534851074\n",
            "Training Iteration 3292, Loss: 2.9863080978393555\n",
            "Training Iteration 3293, Loss: 3.745420455932617\n",
            "Training Iteration 3294, Loss: 5.405282497406006\n",
            "Training Iteration 3295, Loss: 3.5787224769592285\n",
            "Training Iteration 3296, Loss: 3.274059295654297\n",
            "Training Iteration 3297, Loss: 5.857143878936768\n",
            "Training Iteration 3298, Loss: 3.365018844604492\n",
            "Training Iteration 3299, Loss: 3.0071280002593994\n",
            "Training Iteration 3300, Loss: 5.173591613769531\n",
            "Training Iteration 3301, Loss: 3.3834095001220703\n",
            "Training Iteration 3302, Loss: 4.740116119384766\n",
            "Training Iteration 3303, Loss: 5.963296413421631\n",
            "Training Iteration 3304, Loss: 5.299258232116699\n",
            "Training Iteration 3305, Loss: 5.885550022125244\n",
            "Training Iteration 3306, Loss: 4.145359992980957\n",
            "Training Iteration 3307, Loss: 3.390127420425415\n",
            "Training Iteration 3308, Loss: 4.901667594909668\n",
            "Training Iteration 3309, Loss: 7.033024311065674\n",
            "Training Iteration 3310, Loss: 7.478178024291992\n",
            "Training Iteration 3311, Loss: 4.567839622497559\n",
            "Training Iteration 3312, Loss: 1.9924182891845703\n",
            "Training Iteration 3313, Loss: 7.445067405700684\n",
            "Training Iteration 3314, Loss: 4.351014137268066\n",
            "Training Iteration 3315, Loss: 2.1481807231903076\n",
            "Training Iteration 3316, Loss: 4.427927494049072\n",
            "Training Iteration 3317, Loss: 5.434626579284668\n",
            "Training Iteration 3318, Loss: 5.788329601287842\n",
            "Training Iteration 3319, Loss: 5.225213527679443\n",
            "Training Iteration 3320, Loss: 2.3348073959350586\n",
            "Training Iteration 3321, Loss: 5.587302207946777\n",
            "Training Iteration 3322, Loss: 7.386846542358398\n",
            "Training Iteration 3323, Loss: 6.277417182922363\n",
            "Training Iteration 3324, Loss: 7.565276622772217\n",
            "Training Iteration 3325, Loss: 2.6580073833465576\n",
            "Training Iteration 3326, Loss: 3.2011539936065674\n",
            "Training Iteration 3327, Loss: 5.569759845733643\n",
            "Training Iteration 3328, Loss: 2.343488931655884\n",
            "Training Iteration 3329, Loss: 4.861172676086426\n",
            "Training Iteration 3330, Loss: 3.431748628616333\n",
            "Training Iteration 3331, Loss: 11.618749618530273\n",
            "Training Iteration 3332, Loss: 5.572343349456787\n",
            "Training Iteration 3333, Loss: 4.294550895690918\n",
            "Training Iteration 3334, Loss: 3.6351983547210693\n",
            "Training Iteration 3335, Loss: 8.319061279296875\n",
            "Training Iteration 3336, Loss: 5.83501672744751\n",
            "Training Iteration 3337, Loss: 4.6949076652526855\n",
            "Training Iteration 3338, Loss: 5.985270023345947\n",
            "Training Iteration 3339, Loss: 7.367909908294678\n",
            "Training Iteration 3340, Loss: 6.885772228240967\n",
            "Training Iteration 3341, Loss: 3.876652956008911\n",
            "Training Iteration 3342, Loss: 5.5872392654418945\n",
            "Training Iteration 3343, Loss: 4.960946083068848\n",
            "Training Iteration 3344, Loss: 4.615461349487305\n",
            "Training Iteration 3345, Loss: 5.240833282470703\n",
            "Training Iteration 3346, Loss: 3.352532386779785\n",
            "Training Iteration 3347, Loss: 3.0383822917938232\n",
            "Training Iteration 3348, Loss: 6.774766445159912\n",
            "Training Iteration 3349, Loss: 3.1929423809051514\n",
            "Training Iteration 3350, Loss: 8.542476654052734\n",
            "Training Iteration 3351, Loss: 3.0575413703918457\n",
            "Training Iteration 3352, Loss: 6.240556240081787\n",
            "Training Iteration 3353, Loss: 4.397635459899902\n",
            "Training Iteration 3354, Loss: 6.213113784790039\n",
            "Training Iteration 3355, Loss: 3.4254472255706787\n",
            "Training Iteration 3356, Loss: 3.4575893878936768\n",
            "Training Iteration 3357, Loss: 6.299924850463867\n",
            "Training Iteration 3358, Loss: 6.627564430236816\n",
            "Training Iteration 3359, Loss: 3.9219069480895996\n",
            "Training Iteration 3360, Loss: 8.17076587677002\n",
            "Training Iteration 3361, Loss: 3.379321813583374\n",
            "Training Iteration 3362, Loss: 8.237850189208984\n",
            "Training Iteration 3363, Loss: 2.4088540077209473\n",
            "Training Iteration 3364, Loss: 4.6536335945129395\n",
            "Training Iteration 3365, Loss: 3.26369571685791\n",
            "Training Iteration 3366, Loss: 4.006185054779053\n",
            "Training Iteration 3367, Loss: 5.16703462600708\n",
            "Training Iteration 3368, Loss: 7.610641956329346\n",
            "Training Iteration 3369, Loss: 2.802504301071167\n",
            "Training Iteration 3370, Loss: 3.3071141242980957\n",
            "Training Iteration 3371, Loss: 3.485980272293091\n",
            "Training Iteration 3372, Loss: 4.852601528167725\n",
            "Training Iteration 3373, Loss: 6.673144340515137\n",
            "Training Iteration 3374, Loss: 6.391249179840088\n",
            "Training Iteration 3375, Loss: 3.2645764350891113\n",
            "Training Iteration 3376, Loss: 5.880335330963135\n",
            "Training Iteration 3377, Loss: 4.96498966217041\n",
            "Training Iteration 3378, Loss: 2.877023696899414\n",
            "Training Iteration 3379, Loss: 4.421158313751221\n",
            "Training Iteration 3380, Loss: 2.2925968170166016\n",
            "Training Iteration 3381, Loss: 4.000210285186768\n",
            "Training Iteration 3382, Loss: 2.8684699535369873\n",
            "Training Iteration 3383, Loss: 3.3969879150390625\n",
            "Training Iteration 3384, Loss: 4.415443420410156\n",
            "Training Iteration 3385, Loss: 5.088596820831299\n",
            "Training Iteration 3386, Loss: 3.579726457595825\n",
            "Training Iteration 3387, Loss: 2.7329812049865723\n",
            "Training Iteration 3388, Loss: 2.595654010772705\n",
            "Training Iteration 3389, Loss: 3.323833465576172\n",
            "Training Iteration 3390, Loss: 4.138319492340088\n",
            "Training Iteration 3391, Loss: 4.857702255249023\n",
            "Training Iteration 3392, Loss: 4.332283020019531\n",
            "Training Iteration 3393, Loss: 3.9877703189849854\n",
            "Training Iteration 3394, Loss: 3.7800893783569336\n",
            "Training Iteration 3395, Loss: 5.623448848724365\n",
            "Training Iteration 3396, Loss: 8.057114601135254\n",
            "Training Iteration 3397, Loss: 3.787766933441162\n",
            "Training Iteration 3398, Loss: 4.801699161529541\n",
            "Training Iteration 3399, Loss: 6.112199306488037\n",
            "Training Iteration 3400, Loss: 6.3661088943481445\n",
            "Training Iteration 3401, Loss: 4.502331256866455\n",
            "Training Iteration 3402, Loss: 1.6955300569534302\n",
            "Training Iteration 3403, Loss: 2.9306421279907227\n",
            "Training Iteration 3404, Loss: 8.981729507446289\n",
            "Training Iteration 3405, Loss: 4.205374717712402\n",
            "Training Iteration 3406, Loss: 4.985332489013672\n",
            "Training Iteration 3407, Loss: 5.849293231964111\n",
            "Training Iteration 3408, Loss: 4.153419494628906\n",
            "Training Iteration 3409, Loss: 2.681985378265381\n",
            "Training Iteration 3410, Loss: 4.581274509429932\n",
            "Training Iteration 3411, Loss: 5.442195892333984\n",
            "Training Iteration 3412, Loss: 3.400770902633667\n",
            "Training Iteration 3413, Loss: 4.1882476806640625\n",
            "Training Iteration 3414, Loss: 4.374579906463623\n",
            "Training Iteration 3415, Loss: 6.373897552490234\n",
            "Training Iteration 3416, Loss: 3.1832196712493896\n",
            "Training Iteration 3417, Loss: 6.284361839294434\n",
            "Training Iteration 3418, Loss: 1.3082586526870728\n",
            "Training Iteration 3419, Loss: 5.100363731384277\n",
            "Training Iteration 3420, Loss: 4.228513717651367\n",
            "Training Iteration 3421, Loss: 3.507664442062378\n",
            "Training Iteration 3422, Loss: 2.15556001663208\n",
            "Training Iteration 3423, Loss: 2.4693002700805664\n",
            "Training Iteration 3424, Loss: 2.113241195678711\n",
            "Training Iteration 3425, Loss: 5.0566606521606445\n",
            "Training Iteration 3426, Loss: 3.508437395095825\n",
            "Training Iteration 3427, Loss: 3.843397617340088\n",
            "Training Iteration 3428, Loss: 3.3732194900512695\n",
            "Training Iteration 3429, Loss: 3.8956868648529053\n",
            "Training Iteration 3430, Loss: 6.030864715576172\n",
            "Training Iteration 3431, Loss: 3.0164284706115723\n",
            "Training Iteration 3432, Loss: 3.3032593727111816\n",
            "Training Iteration 3433, Loss: 5.609485149383545\n",
            "Training Iteration 3434, Loss: 3.4275264739990234\n",
            "Training Iteration 3435, Loss: 3.000760316848755\n",
            "Training Iteration 3436, Loss: 4.950654983520508\n",
            "Training Iteration 3437, Loss: 5.00698184967041\n",
            "Training Iteration 3438, Loss: 7.120420455932617\n",
            "Training Iteration 3439, Loss: 5.137568950653076\n",
            "Training Iteration 3440, Loss: 4.119376182556152\n",
            "Training Iteration 3441, Loss: 3.379456043243408\n",
            "Training Iteration 3442, Loss: 5.447481155395508\n",
            "Training Iteration 3443, Loss: 3.287677764892578\n",
            "Training Iteration 3444, Loss: 1.7796084880828857\n",
            "Training Iteration 3445, Loss: 4.6705708503723145\n",
            "Training Iteration 3446, Loss: 4.874205589294434\n",
            "Training Iteration 3447, Loss: 2.3969569206237793\n",
            "Training Iteration 3448, Loss: 6.588095664978027\n",
            "Training Iteration 3449, Loss: 4.309132099151611\n",
            "Training Iteration 3450, Loss: 2.7464962005615234\n",
            "Training Iteration 3451, Loss: 4.657355308532715\n",
            "Training Iteration 3452, Loss: 5.342872142791748\n",
            "Training Iteration 3453, Loss: 5.435291767120361\n",
            "Training Iteration 3454, Loss: 3.3156685829162598\n",
            "Training Iteration 3455, Loss: 5.682592868804932\n",
            "Training Iteration 3456, Loss: 5.04866886138916\n",
            "Training Iteration 3457, Loss: 3.5723414421081543\n",
            "Training Iteration 3458, Loss: 3.0737037658691406\n",
            "Training Iteration 3459, Loss: 4.276359558105469\n",
            "Training Iteration 3460, Loss: 5.603869915008545\n",
            "Training Iteration 3461, Loss: 4.7313337326049805\n",
            "Training Iteration 3462, Loss: 4.509564399719238\n",
            "Training Iteration 3463, Loss: 7.238009452819824\n",
            "Training Iteration 3464, Loss: 4.754002571105957\n",
            "Training Iteration 3465, Loss: 4.115818500518799\n",
            "Training Iteration 3466, Loss: 1.8624026775360107\n",
            "Training Iteration 3467, Loss: 5.282968521118164\n",
            "Training Iteration 3468, Loss: 8.78707218170166\n",
            "Training Iteration 3469, Loss: 8.128545761108398\n",
            "Training Iteration 3470, Loss: 3.5619654655456543\n",
            "Training Iteration 3471, Loss: 9.825287818908691\n",
            "Training Iteration 3472, Loss: 8.753289222717285\n",
            "Training Iteration 3473, Loss: 7.318670749664307\n",
            "Training Iteration 3474, Loss: 5.569639205932617\n",
            "Training Iteration 3475, Loss: 3.6105432510375977\n",
            "Training Iteration 3476, Loss: 4.550389289855957\n",
            "Training Iteration 3477, Loss: 3.714050769805908\n",
            "Training Iteration 3478, Loss: 4.910833835601807\n",
            "Training Iteration 3479, Loss: 4.898691654205322\n",
            "Training Iteration 3480, Loss: 6.009851455688477\n",
            "Training Iteration 3481, Loss: 2.4217236042022705\n",
            "Training Iteration 3482, Loss: 5.151523590087891\n",
            "Training Iteration 3483, Loss: 4.5667243003845215\n",
            "Training Iteration 3484, Loss: 4.317585468292236\n",
            "Training Iteration 3485, Loss: 2.943382978439331\n",
            "Training Iteration 3486, Loss: 5.81423282623291\n",
            "Training Iteration 3487, Loss: 5.772336483001709\n",
            "Training Iteration 3488, Loss: 5.396579265594482\n",
            "Training Iteration 3489, Loss: 3.977628707885742\n",
            "Training Iteration 3490, Loss: 3.4119784832000732\n",
            "Training Iteration 3491, Loss: 4.908855438232422\n",
            "Training Iteration 3492, Loss: 2.6125752925872803\n",
            "Training Iteration 3493, Loss: 5.062934398651123\n",
            "Training Iteration 3494, Loss: 6.470465183258057\n",
            "Training Iteration 3495, Loss: 7.070188999176025\n",
            "Training Iteration 3496, Loss: 3.1796257495880127\n",
            "Training Iteration 3497, Loss: 4.891811370849609\n",
            "Training Iteration 3498, Loss: 4.76174259185791\n",
            "Training Iteration 3499, Loss: 3.0727453231811523\n",
            "Training Iteration 3500, Loss: 4.529904842376709\n",
            "Training Iteration 3501, Loss: 5.141170024871826\n",
            "Training Iteration 3502, Loss: 2.532923698425293\n",
            "Training Iteration 3503, Loss: 5.143071174621582\n",
            "Training Iteration 3504, Loss: 7.336673259735107\n",
            "Training Iteration 3505, Loss: 4.53695821762085\n",
            "Training Iteration 3506, Loss: 4.708096027374268\n",
            "Training Iteration 3507, Loss: 5.428181171417236\n",
            "Training Iteration 3508, Loss: 6.001372337341309\n",
            "Training Iteration 3509, Loss: 7.1143012046813965\n",
            "Training Iteration 3510, Loss: 3.4618148803710938\n",
            "Training Iteration 3511, Loss: 3.959563732147217\n",
            "Training Iteration 3512, Loss: 3.4148383140563965\n",
            "Training Iteration 3513, Loss: 4.724130153656006\n",
            "Training Iteration 3514, Loss: 5.003814697265625\n",
            "Training Iteration 3515, Loss: 3.985574722290039\n",
            "Training Iteration 3516, Loss: 4.048220634460449\n",
            "Training Iteration 3517, Loss: 5.627215385437012\n",
            "Training Iteration 3518, Loss: 4.7812652587890625\n",
            "Training Iteration 3519, Loss: 2.6273858547210693\n",
            "Training Iteration 3520, Loss: 5.489140510559082\n",
            "Training Iteration 3521, Loss: 2.9284160137176514\n",
            "Training Iteration 3522, Loss: 3.736872911453247\n",
            "Training Iteration 3523, Loss: 4.4238176345825195\n",
            "Training Iteration 3524, Loss: 7.296915531158447\n",
            "Training Iteration 3525, Loss: 10.995631217956543\n",
            "Training Iteration 3526, Loss: 6.074099540710449\n",
            "Training Iteration 3527, Loss: 6.8588104248046875\n",
            "Training Iteration 3528, Loss: 4.9759297370910645\n",
            "Training Iteration 3529, Loss: 4.7090744972229\n",
            "Training Iteration 3530, Loss: 5.880887031555176\n",
            "Training Iteration 3531, Loss: 4.011744499206543\n",
            "Training Iteration 3532, Loss: 3.7987053394317627\n",
            "Training Iteration 3533, Loss: 4.261603832244873\n",
            "Training Iteration 3534, Loss: 10.949541091918945\n",
            "Training Iteration 3535, Loss: 9.680000305175781\n",
            "Training Iteration 3536, Loss: 5.178106307983398\n",
            "Training Iteration 3537, Loss: 2.377091646194458\n",
            "Training Iteration 3538, Loss: 10.290414810180664\n",
            "Training Iteration 3539, Loss: 8.707921981811523\n",
            "Training Iteration 3540, Loss: 8.532007217407227\n",
            "Training Iteration 3541, Loss: 7.258927822113037\n",
            "Training Iteration 3542, Loss: 3.9399430751800537\n",
            "Training Iteration 3543, Loss: 8.45421028137207\n",
            "Training Iteration 3544, Loss: 4.259506702423096\n",
            "Training Iteration 3545, Loss: 4.954164505004883\n",
            "Training Iteration 3546, Loss: 6.8630805015563965\n",
            "Training Iteration 3547, Loss: 4.958342552185059\n",
            "Training Iteration 3548, Loss: 4.736850738525391\n",
            "Training Iteration 3549, Loss: 2.3639578819274902\n",
            "Training Iteration 3550, Loss: 4.447750091552734\n",
            "Training Iteration 3551, Loss: 6.4358038902282715\n",
            "Training Iteration 3552, Loss: 4.483773708343506\n",
            "Training Iteration 3553, Loss: 2.9028913974761963\n",
            "Training Iteration 3554, Loss: 1.6043639183044434\n",
            "Training Iteration 3555, Loss: 3.05194354057312\n",
            "Training Iteration 3556, Loss: 3.8095290660858154\n",
            "Training Iteration 3557, Loss: 4.696976184844971\n",
            "Training Iteration 3558, Loss: 3.9518275260925293\n",
            "Training Iteration 3559, Loss: 1.7944809198379517\n",
            "Training Iteration 3560, Loss: 2.8261749744415283\n",
            "Training Iteration 3561, Loss: 7.479550838470459\n",
            "Training Iteration 3562, Loss: 6.914585590362549\n",
            "Training Iteration 3563, Loss: 3.9894137382507324\n",
            "Training Iteration 3564, Loss: 3.2038156986236572\n",
            "Training Iteration 3565, Loss: 3.0880205631256104\n",
            "Training Iteration 3566, Loss: 1.2433782815933228\n",
            "Training Iteration 3567, Loss: 4.382870197296143\n",
            "Training Iteration 3568, Loss: 5.928857326507568\n",
            "Training Iteration 3569, Loss: 5.037591934204102\n",
            "Training Iteration 3570, Loss: 3.885873556137085\n",
            "Training Iteration 3571, Loss: 3.923030376434326\n",
            "Training Iteration 3572, Loss: 3.285576820373535\n",
            "Training Iteration 3573, Loss: 4.54848051071167\n",
            "Training Iteration 3574, Loss: 3.4646434783935547\n",
            "Training Iteration 3575, Loss: 2.1812543869018555\n",
            "Training Iteration 3576, Loss: 3.7340877056121826\n",
            "Training Iteration 3577, Loss: 3.2000181674957275\n",
            "Training Iteration 3578, Loss: 5.865376949310303\n",
            "Training Iteration 3579, Loss: 10.540179252624512\n",
            "Training Iteration 3580, Loss: 9.504365921020508\n",
            "Training Iteration 3581, Loss: 5.24976110458374\n",
            "Training Iteration 3582, Loss: 6.220049858093262\n",
            "Training Iteration 3583, Loss: 5.218811511993408\n",
            "Training Iteration 3584, Loss: 4.972470283508301\n",
            "Training Iteration 3585, Loss: 4.056492805480957\n",
            "Training Iteration 3586, Loss: 6.047595977783203\n",
            "Training Iteration 3587, Loss: 5.942695617675781\n",
            "Training Iteration 3588, Loss: 2.588918685913086\n",
            "Training Iteration 3589, Loss: 6.531237602233887\n",
            "Training Iteration 3590, Loss: 3.2226340770721436\n",
            "Training Iteration 3591, Loss: 5.009451866149902\n",
            "Training Iteration 3592, Loss: 4.4797892570495605\n",
            "Training Iteration 3593, Loss: 3.345677614212036\n",
            "Training Iteration 3594, Loss: 5.877729892730713\n",
            "Training Iteration 3595, Loss: 4.200512886047363\n",
            "Training Iteration 3596, Loss: 6.088629722595215\n",
            "Training Iteration 3597, Loss: 4.277535438537598\n",
            "Training Iteration 3598, Loss: 3.2656023502349854\n",
            "Training Iteration 3599, Loss: 2.030667304992676\n",
            "Training Iteration 3600, Loss: 5.1151933670043945\n",
            "Training Iteration 3601, Loss: 4.006453990936279\n",
            "Training Iteration 3602, Loss: 4.378667831420898\n",
            "Training Iteration 3603, Loss: 4.102643966674805\n",
            "Training Iteration 3604, Loss: 3.385707378387451\n",
            "Training Iteration 3605, Loss: 3.5171871185302734\n",
            "Training Iteration 3606, Loss: 2.318302869796753\n",
            "Training Iteration 3607, Loss: 3.8640828132629395\n",
            "Training Iteration 3608, Loss: 5.52471923828125\n",
            "Training Iteration 3609, Loss: 4.553232669830322\n",
            "Training Iteration 3610, Loss: 1.8497720956802368\n",
            "Training Iteration 3611, Loss: 3.729523181915283\n",
            "Training Iteration 3612, Loss: 3.9796142578125\n",
            "Training Iteration 3613, Loss: 4.859978199005127\n",
            "Training Iteration 3614, Loss: 4.586399555206299\n",
            "Training Iteration 3615, Loss: 4.356767654418945\n",
            "Training Iteration 3616, Loss: 5.2856550216674805\n",
            "Training Iteration 3617, Loss: 2.544085741043091\n",
            "Training Iteration 3618, Loss: 7.705796718597412\n",
            "Training Iteration 3619, Loss: 5.78304386138916\n",
            "Training Iteration 3620, Loss: 3.649846315383911\n",
            "Training Iteration 3621, Loss: 4.322723865509033\n",
            "Training Iteration 3622, Loss: 4.190860748291016\n",
            "Training Iteration 3623, Loss: 5.037899017333984\n",
            "Training Iteration 3624, Loss: 3.251781463623047\n",
            "Training Iteration 3625, Loss: 4.515695095062256\n",
            "Training Iteration 3626, Loss: 4.005795478820801\n",
            "Training Iteration 3627, Loss: 2.1975812911987305\n",
            "Training Iteration 3628, Loss: 7.176143169403076\n",
            "Training Iteration 3629, Loss: 1.758001685142517\n",
            "Training Iteration 3630, Loss: 2.1265783309936523\n",
            "Training Iteration 3631, Loss: 5.8024115562438965\n",
            "Training Iteration 3632, Loss: 2.2437021732330322\n",
            "Training Iteration 3633, Loss: 2.7939717769622803\n",
            "Training Iteration 3634, Loss: 4.374334335327148\n",
            "Training Iteration 3635, Loss: 4.738608360290527\n",
            "Training Iteration 3636, Loss: 4.090824604034424\n",
            "Training Iteration 3637, Loss: 4.032062530517578\n",
            "Training Iteration 3638, Loss: 5.993607521057129\n",
            "Training Iteration 3639, Loss: 4.313416481018066\n",
            "Training Iteration 3640, Loss: 1.6410270929336548\n",
            "Training Iteration 3641, Loss: 3.8079638481140137\n",
            "Training Iteration 3642, Loss: 4.34670352935791\n",
            "Training Iteration 3643, Loss: 4.664216041564941\n",
            "Training Iteration 3644, Loss: 3.860509157180786\n",
            "Training Iteration 3645, Loss: 2.7772796154022217\n",
            "Training Iteration 3646, Loss: 4.571791648864746\n",
            "Training Iteration 3647, Loss: 4.641972064971924\n",
            "Training Iteration 3648, Loss: 2.8817949295043945\n",
            "Training Iteration 3649, Loss: 3.4584317207336426\n",
            "Training Iteration 3650, Loss: 5.800451278686523\n",
            "Training Iteration 3651, Loss: 5.971969127655029\n",
            "Training Iteration 3652, Loss: 3.6406288146972656\n",
            "Training Iteration 3653, Loss: 2.692328929901123\n",
            "Training Iteration 3654, Loss: 3.364455461502075\n",
            "Training Iteration 3655, Loss: 3.1206471920013428\n",
            "Training Iteration 3656, Loss: 4.823690414428711\n",
            "Training Iteration 3657, Loss: 2.757405996322632\n",
            "Training Iteration 3658, Loss: 5.515556812286377\n",
            "Training Iteration 3659, Loss: 3.066176176071167\n",
            "Training Iteration 3660, Loss: 2.926624298095703\n",
            "Training Iteration 3661, Loss: 4.845503807067871\n",
            "Training Iteration 3662, Loss: 2.9226818084716797\n",
            "Training Iteration 3663, Loss: 7.450687408447266\n",
            "Training Iteration 3664, Loss: 2.200960397720337\n",
            "Training Iteration 3665, Loss: 3.6796560287475586\n",
            "Training Iteration 3666, Loss: 4.095834732055664\n",
            "Training Iteration 3667, Loss: 5.427084922790527\n",
            "Training Iteration 3668, Loss: 6.433650970458984\n",
            "Training Iteration 3669, Loss: 3.9586758613586426\n",
            "Training Iteration 3670, Loss: 4.92417049407959\n",
            "Training Iteration 3671, Loss: 5.939935684204102\n",
            "Training Iteration 3672, Loss: 9.75775146484375\n",
            "Training Iteration 3673, Loss: 2.0895097255706787\n",
            "Training Iteration 3674, Loss: 2.441946506500244\n",
            "Training Iteration 3675, Loss: 3.8272500038146973\n",
            "Training Iteration 3676, Loss: 5.807546615600586\n",
            "Training Iteration 3677, Loss: 6.290701866149902\n",
            "Training Iteration 3678, Loss: 8.288564682006836\n",
            "Training Iteration 3679, Loss: 4.174884796142578\n",
            "Training Iteration 3680, Loss: 5.585859298706055\n",
            "Training Iteration 3681, Loss: 4.368537425994873\n",
            "Training Iteration 3682, Loss: 4.686433792114258\n",
            "Training Iteration 3683, Loss: 5.341423511505127\n",
            "Training Iteration 3684, Loss: 4.989491939544678\n",
            "Training Iteration 3685, Loss: 6.576157569885254\n",
            "Training Iteration 3686, Loss: 2.5778729915618896\n",
            "Training Iteration 3687, Loss: 4.92373514175415\n",
            "Training Iteration 3688, Loss: 7.158938884735107\n",
            "Training Iteration 3689, Loss: 6.385229110717773\n",
            "Training Iteration 3690, Loss: 3.593975067138672\n",
            "Training Iteration 3691, Loss: 4.824748516082764\n",
            "Training Iteration 3692, Loss: 1.9210271835327148\n",
            "Training Iteration 3693, Loss: 7.036141395568848\n",
            "Training Iteration 3694, Loss: 3.2726128101348877\n",
            "Training Iteration 3695, Loss: 3.654808521270752\n",
            "Training Iteration 3696, Loss: 5.597333908081055\n",
            "Training Iteration 3697, Loss: 6.527427673339844\n",
            "Training Iteration 3698, Loss: 4.377580165863037\n",
            "Training Iteration 3699, Loss: 4.007763862609863\n",
            "Training Iteration 3700, Loss: 4.634607791900635\n",
            "Training Iteration 3701, Loss: 7.525236129760742\n",
            "Training Iteration 3702, Loss: 7.149981498718262\n",
            "Training Iteration 3703, Loss: 6.927125930786133\n",
            "Training Iteration 3704, Loss: 6.358562469482422\n",
            "Training Iteration 3705, Loss: 2.742762565612793\n",
            "Training Iteration 3706, Loss: 6.475141525268555\n",
            "Training Iteration 3707, Loss: 1.7141753435134888\n",
            "Training Iteration 3708, Loss: 4.609618186950684\n",
            "Training Iteration 3709, Loss: 9.927179336547852\n",
            "Training Iteration 3710, Loss: 6.085402011871338\n",
            "Training Iteration 3711, Loss: 7.2746124267578125\n",
            "Training Iteration 3712, Loss: 1.71432626247406\n",
            "Training Iteration 3713, Loss: 1.839855670928955\n",
            "Training Iteration 3714, Loss: 3.5931556224823\n",
            "Training Iteration 3715, Loss: 3.9030373096466064\n",
            "Training Iteration 3716, Loss: 6.6360015869140625\n",
            "Training Iteration 3717, Loss: 5.584707736968994\n",
            "Training Iteration 3718, Loss: 3.7431511878967285\n",
            "Training Iteration 3719, Loss: 3.6481056213378906\n",
            "Training Iteration 3720, Loss: 5.641092300415039\n",
            "Training Iteration 3721, Loss: 4.8099236488342285\n",
            "Training Iteration 3722, Loss: 6.788694381713867\n",
            "Training Iteration 3723, Loss: 5.041471481323242\n",
            "Training Iteration 3724, Loss: 3.188800811767578\n",
            "Training Iteration 3725, Loss: 5.59412145614624\n",
            "Training Iteration 3726, Loss: 3.6717448234558105\n",
            "Training Iteration 3727, Loss: 6.068832874298096\n",
            "Training Iteration 3728, Loss: 3.0229380130767822\n",
            "Training Iteration 3729, Loss: 5.5671067237854\n",
            "Training Iteration 3730, Loss: 2.344045639038086\n",
            "Training Iteration 3731, Loss: 7.324207782745361\n",
            "Training Iteration 3732, Loss: 3.753065824508667\n",
            "Training Iteration 3733, Loss: 3.514552593231201\n",
            "Training Iteration 3734, Loss: 5.799518585205078\n",
            "Training Iteration 3735, Loss: 3.9301862716674805\n",
            "Training Iteration 3736, Loss: 2.694004774093628\n",
            "Training Iteration 3737, Loss: 5.064430236816406\n",
            "Training Iteration 3738, Loss: 4.2454681396484375\n",
            "Training Iteration 3739, Loss: 2.086674451828003\n",
            "Training Iteration 3740, Loss: 4.288431167602539\n",
            "Training Iteration 3741, Loss: 5.410730361938477\n",
            "Training Iteration 3742, Loss: 3.7685344219207764\n",
            "Training Iteration 3743, Loss: 4.874447822570801\n",
            "Training Iteration 3744, Loss: 3.7125251293182373\n",
            "Training Iteration 3745, Loss: 7.162197113037109\n",
            "Training Iteration 3746, Loss: 2.7765865325927734\n",
            "Training Iteration 3747, Loss: 2.3739969730377197\n",
            "Training Iteration 3748, Loss: 2.4677557945251465\n",
            "Training Iteration 3749, Loss: 7.780707359313965\n",
            "Training Iteration 3750, Loss: 8.46885871887207\n",
            "Training Iteration 3751, Loss: 1.638330101966858\n",
            "Training Iteration 3752, Loss: 4.218624591827393\n",
            "Training Iteration 3753, Loss: 2.0750012397766113\n",
            "Training Iteration 3754, Loss: 6.211976051330566\n",
            "Training Iteration 3755, Loss: 8.406014442443848\n",
            "Training Iteration 3756, Loss: 1.9897468090057373\n",
            "Training Iteration 3757, Loss: 5.443119049072266\n",
            "Training Iteration 3758, Loss: 3.052651882171631\n",
            "Training Iteration 3759, Loss: 5.786905288696289\n",
            "Training Iteration 3760, Loss: 8.296751976013184\n",
            "Training Iteration 3761, Loss: 5.636040210723877\n",
            "Training Iteration 3762, Loss: 5.103810787200928\n",
            "Training Iteration 3763, Loss: 3.2961854934692383\n",
            "Training Iteration 3764, Loss: 5.150032043457031\n",
            "Training Iteration 3765, Loss: 3.875786542892456\n",
            "Training Iteration 3766, Loss: 5.436695575714111\n",
            "Training Iteration 3767, Loss: 5.05804967880249\n",
            "Training Iteration 3768, Loss: 2.665719509124756\n",
            "Training Iteration 3769, Loss: 3.408878803253174\n",
            "Training Iteration 3770, Loss: 5.91624641418457\n",
            "Training Iteration 3771, Loss: 4.64374303817749\n",
            "Training Iteration 3772, Loss: 4.6778459548950195\n",
            "Training Iteration 3773, Loss: 5.076969146728516\n",
            "Training Iteration 3774, Loss: 6.182043552398682\n",
            "Training Iteration 3775, Loss: 4.568075656890869\n",
            "Training Iteration 3776, Loss: 6.623335838317871\n",
            "Training Iteration 3777, Loss: 6.557584285736084\n",
            "Training Iteration 3778, Loss: 6.715767860412598\n",
            "Training Iteration 3779, Loss: 3.8694958686828613\n",
            "Training Iteration 3780, Loss: 3.146590232849121\n",
            "Training Iteration 3781, Loss: 5.254745960235596\n",
            "Training Iteration 3782, Loss: 6.600237846374512\n",
            "Training Iteration 3783, Loss: 12.434799194335938\n",
            "Training Iteration 3784, Loss: 4.88554573059082\n",
            "Training Iteration 3785, Loss: 3.5696136951446533\n",
            "Training Iteration 3786, Loss: 3.782402515411377\n",
            "Training Iteration 3787, Loss: 4.086488723754883\n",
            "Training Iteration 3788, Loss: 5.692623138427734\n",
            "Training Iteration 3789, Loss: 4.905784606933594\n",
            "Training Iteration 3790, Loss: 6.20831823348999\n",
            "Training Iteration 3791, Loss: 2.909600257873535\n",
            "Training Iteration 3792, Loss: 4.96010684967041\n",
            "Training Iteration 3793, Loss: 4.8919477462768555\n",
            "Training Iteration 3794, Loss: 6.542973518371582\n",
            "Training Iteration 3795, Loss: 4.588109493255615\n",
            "Training Iteration 3796, Loss: 4.833827018737793\n",
            "Training Iteration 3797, Loss: 3.2094249725341797\n",
            "Training Iteration 3798, Loss: 4.5749592781066895\n",
            "Training Iteration 3799, Loss: 5.565979957580566\n",
            "Training Iteration 3800, Loss: 5.432757377624512\n",
            "Training Iteration 3801, Loss: 7.500335693359375\n",
            "Training Iteration 3802, Loss: 3.237804889678955\n",
            "Training Iteration 3803, Loss: 5.738296985626221\n",
            "Training Iteration 3804, Loss: 5.216317653656006\n",
            "Training Iteration 3805, Loss: 3.7402894496917725\n",
            "Training Iteration 3806, Loss: 3.82736873626709\n",
            "Training Iteration 3807, Loss: 3.616924524307251\n",
            "Training Iteration 3808, Loss: 4.62162446975708\n",
            "Training Iteration 3809, Loss: 5.505518913269043\n",
            "Training Iteration 3810, Loss: 3.975835084915161\n",
            "Training Iteration 3811, Loss: 3.0565571784973145\n",
            "Training Iteration 3812, Loss: 5.864034652709961\n",
            "Training Iteration 3813, Loss: 3.604198455810547\n",
            "Training Iteration 3814, Loss: 2.8783459663391113\n",
            "Training Iteration 3815, Loss: 2.6822030544281006\n",
            "Training Iteration 3816, Loss: 1.478950023651123\n",
            "Training Iteration 3817, Loss: 3.089081287384033\n",
            "Training Iteration 3818, Loss: 6.524439334869385\n",
            "Training Iteration 3819, Loss: 6.7537665367126465\n",
            "Training Iteration 3820, Loss: 1.5586596727371216\n",
            "Training Iteration 3821, Loss: 5.601474761962891\n",
            "Training Iteration 3822, Loss: 4.909130096435547\n",
            "Training Iteration 3823, Loss: 5.346133232116699\n",
            "Training Iteration 3824, Loss: 3.789346218109131\n",
            "Training Iteration 3825, Loss: 3.8116164207458496\n",
            "Training Iteration 3826, Loss: 4.061851978302002\n",
            "Training Iteration 3827, Loss: 4.831097602844238\n",
            "Training Iteration 3828, Loss: 1.8606431484222412\n",
            "Training Iteration 3829, Loss: 6.89054536819458\n",
            "Training Iteration 3830, Loss: 6.810832500457764\n",
            "Training Iteration 3831, Loss: 3.166588544845581\n",
            "Training Iteration 3832, Loss: 2.721925973892212\n",
            "Training Iteration 3833, Loss: 7.857312202453613\n",
            "Training Iteration 3834, Loss: 5.831339359283447\n",
            "Training Iteration 3835, Loss: 5.462010383605957\n",
            "Training Iteration 3836, Loss: 12.033835411071777\n",
            "Training Iteration 3837, Loss: 6.181408405303955\n",
            "Training Iteration 3838, Loss: 6.2291669845581055\n",
            "Training Iteration 3839, Loss: 2.7253971099853516\n",
            "Training Iteration 3840, Loss: 4.421311378479004\n",
            "Training Iteration 3841, Loss: 4.060092449188232\n",
            "Training Iteration 3842, Loss: 3.2357640266418457\n",
            "Training Iteration 3843, Loss: 3.461477756500244\n",
            "Training Iteration 3844, Loss: 4.463826656341553\n",
            "Training Iteration 3845, Loss: 5.8566484451293945\n",
            "Training Iteration 3846, Loss: 2.562476396560669\n",
            "Training Iteration 3847, Loss: 3.837318181991577\n",
            "Training Iteration 3848, Loss: 6.098954677581787\n",
            "Training Iteration 3849, Loss: 3.205265998840332\n",
            "Training Iteration 3850, Loss: 3.479013204574585\n",
            "Training Iteration 3851, Loss: 4.790215969085693\n",
            "Training Iteration 3852, Loss: 7.798274517059326\n",
            "Training Iteration 3853, Loss: 6.799404621124268\n",
            "Training Iteration 3854, Loss: 4.045567035675049\n",
            "Training Iteration 3855, Loss: 4.860988140106201\n",
            "Training Iteration 3856, Loss: 8.94486141204834\n",
            "Training Iteration 3857, Loss: 8.773200988769531\n",
            "Training Iteration 3858, Loss: 2.186800479888916\n",
            "Training Iteration 3859, Loss: 3.1305603981018066\n",
            "Training Iteration 3860, Loss: 2.831136703491211\n",
            "Training Iteration 3861, Loss: 4.436400413513184\n",
            "Training Iteration 3862, Loss: 3.975539207458496\n",
            "Training Iteration 3863, Loss: 6.2364397048950195\n",
            "Training Iteration 3864, Loss: 5.532994270324707\n",
            "Training Iteration 3865, Loss: 3.8506011962890625\n",
            "Training Iteration 3866, Loss: 8.129316329956055\n",
            "Training Iteration 3867, Loss: 7.217787742614746\n",
            "Training Iteration 3868, Loss: 3.7082791328430176\n",
            "Training Iteration 3869, Loss: 3.778592109680176\n",
            "Training Iteration 3870, Loss: 3.3349618911743164\n",
            "Training Iteration 3871, Loss: 6.41877555847168\n",
            "Training Iteration 3872, Loss: 3.100841760635376\n",
            "Training Iteration 3873, Loss: 4.828198432922363\n",
            "Training Iteration 3874, Loss: 5.371547222137451\n",
            "Training Iteration 3875, Loss: 3.942621946334839\n",
            "Training Iteration 3876, Loss: 2.62192440032959\n",
            "Training Iteration 3877, Loss: 7.4294257164001465\n",
            "Training Iteration 3878, Loss: 1.589563250541687\n",
            "Training Iteration 3879, Loss: 4.350625991821289\n",
            "Training Iteration 3880, Loss: 5.964203357696533\n",
            "Training Iteration 3881, Loss: 2.4619059562683105\n",
            "Training Iteration 3882, Loss: 2.3942928314208984\n",
            "Training Iteration 3883, Loss: 4.800822734832764\n",
            "Training Iteration 3884, Loss: 3.157745122909546\n",
            "Training Iteration 3885, Loss: 4.217019557952881\n",
            "Training Iteration 3886, Loss: 7.983189582824707\n",
            "Training Iteration 3887, Loss: 5.662339687347412\n",
            "Training Iteration 3888, Loss: 1.9296774864196777\n",
            "Training Iteration 3889, Loss: 2.826932430267334\n",
            "Training Iteration 3890, Loss: 4.915947914123535\n",
            "Training Iteration 3891, Loss: 4.0245208740234375\n",
            "Training Iteration 3892, Loss: 3.3576602935791016\n",
            "Training Iteration 3893, Loss: 7.436755180358887\n",
            "Training Iteration 3894, Loss: 3.707266092300415\n",
            "Training Iteration 3895, Loss: 7.474680423736572\n",
            "Training Iteration 3896, Loss: 3.440237045288086\n",
            "Training Iteration 3897, Loss: 5.501128673553467\n",
            "Training Iteration 3898, Loss: 3.8110523223876953\n",
            "Training Iteration 3899, Loss: 3.5535824298858643\n",
            "Training Iteration 3900, Loss: 3.7138638496398926\n",
            "Training Iteration 3901, Loss: 5.317657470703125\n",
            "Training Iteration 3902, Loss: 2.287328004837036\n",
            "Training Iteration 3903, Loss: 3.4765734672546387\n",
            "Training Iteration 3904, Loss: 3.019223690032959\n",
            "Training Iteration 3905, Loss: 6.018304824829102\n",
            "Training Iteration 3906, Loss: 6.745212554931641\n",
            "Training Iteration 3907, Loss: 7.4100213050842285\n",
            "Training Iteration 3908, Loss: 2.392162322998047\n",
            "Training Iteration 3909, Loss: 2.2241458892822266\n",
            "Training Iteration 3910, Loss: 7.916268825531006\n",
            "Training Iteration 3911, Loss: 2.8590755462646484\n",
            "Training Iteration 3912, Loss: 2.7988388538360596\n",
            "Training Iteration 3913, Loss: 5.9023756980896\n",
            "Training Iteration 3914, Loss: 3.5608787536621094\n",
            "Training Iteration 3915, Loss: 2.9449634552001953\n",
            "Training Iteration 3916, Loss: 4.153411865234375\n",
            "Training Iteration 3917, Loss: 4.685862064361572\n",
            "Training Iteration 3918, Loss: 2.482417345046997\n",
            "Training Iteration 3919, Loss: 2.7016077041625977\n",
            "Training Iteration 3920, Loss: 2.389759063720703\n",
            "Training Iteration 3921, Loss: 1.789922833442688\n",
            "Training Iteration 3922, Loss: 2.1944639682769775\n",
            "Training Iteration 3923, Loss: 2.4792208671569824\n",
            "Training Iteration 3924, Loss: 4.207060813903809\n",
            "Training Iteration 3925, Loss: 4.5374932289123535\n",
            "Training Iteration 3926, Loss: 1.7228305339813232\n",
            "Training Iteration 3927, Loss: 4.719840049743652\n",
            "Training Iteration 3928, Loss: 2.7044529914855957\n",
            "Training Iteration 3929, Loss: 4.474897384643555\n",
            "Training Iteration 3930, Loss: 4.258182048797607\n",
            "Training Iteration 3931, Loss: 6.096273899078369\n",
            "Training Iteration 3932, Loss: 3.9429261684417725\n",
            "Training Iteration 3933, Loss: 3.069911479949951\n",
            "Training Iteration 3934, Loss: 4.50460958480835\n",
            "Training Iteration 3935, Loss: 2.9920339584350586\n",
            "Training Iteration 3936, Loss: 4.383744239807129\n",
            "Training Iteration 3937, Loss: 7.038217067718506\n",
            "Training Iteration 3938, Loss: 4.424675464630127\n",
            "Training Iteration 3939, Loss: 7.519075393676758\n",
            "Training Iteration 3940, Loss: 4.276017189025879\n",
            "Training Iteration 3941, Loss: 3.863370895385742\n",
            "Training Iteration 3942, Loss: 3.761631488800049\n",
            "Training Iteration 3943, Loss: 6.62314510345459\n",
            "Training Iteration 3944, Loss: 4.896482467651367\n",
            "Training Iteration 3945, Loss: 4.162352561950684\n",
            "Training Iteration 3946, Loss: 2.213414192199707\n",
            "Training Iteration 3947, Loss: 3.6505157947540283\n",
            "Training Iteration 3948, Loss: 9.01239013671875\n",
            "Training Iteration 3949, Loss: 2.9354991912841797\n",
            "Training Iteration 3950, Loss: 2.9366822242736816\n",
            "Training Iteration 3951, Loss: 5.936579704284668\n",
            "Training Iteration 3952, Loss: 4.048491477966309\n",
            "Training Iteration 3953, Loss: 6.180206298828125\n",
            "Training Iteration 3954, Loss: 10.114404678344727\n",
            "Training Iteration 3955, Loss: 3.28951358795166\n",
            "Training Iteration 3956, Loss: 2.749483346939087\n",
            "Training Iteration 3957, Loss: 4.78816556930542\n",
            "Training Iteration 3958, Loss: 3.5938377380371094\n",
            "Training Iteration 3959, Loss: 3.5208706855773926\n",
            "Training Iteration 3960, Loss: 2.7739880084991455\n",
            "Training Iteration 3961, Loss: 4.696071624755859\n",
            "Training Iteration 3962, Loss: 4.70389461517334\n",
            "Training Iteration 3963, Loss: 4.805418491363525\n",
            "Training Iteration 3964, Loss: 8.340025901794434\n",
            "Training Iteration 3965, Loss: 3.4669125080108643\n",
            "Training Iteration 3966, Loss: 5.7562575340271\n",
            "Training Iteration 3967, Loss: 6.3704915046691895\n",
            "Training Iteration 3968, Loss: 4.481086254119873\n",
            "Training Iteration 3969, Loss: 5.055278301239014\n",
            "Training Iteration 3970, Loss: 6.415645599365234\n",
            "Training Iteration 3971, Loss: 5.2306060791015625\n",
            "Training Iteration 3972, Loss: 3.7809507846832275\n",
            "Training Iteration 3973, Loss: 5.6513872146606445\n",
            "Training Iteration 3974, Loss: 4.863431453704834\n",
            "Training Iteration 3975, Loss: 4.458134651184082\n",
            "Training Iteration 3976, Loss: 3.0219273567199707\n",
            "Training Iteration 3977, Loss: 6.126765727996826\n",
            "Training Iteration 3978, Loss: 8.342108726501465\n",
            "Training Iteration 3979, Loss: 6.740105152130127\n",
            "Training Iteration 3980, Loss: 2.5855298042297363\n",
            "Training Iteration 3981, Loss: 5.793364524841309\n",
            "Training Iteration 3982, Loss: 4.811862945556641\n",
            "Training Iteration 3983, Loss: 5.19302225112915\n",
            "Training Iteration 3984, Loss: 7.4096503257751465\n",
            "Training Iteration 3985, Loss: 3.2650816440582275\n",
            "Training Iteration 3986, Loss: 4.840475082397461\n",
            "Training Iteration 3987, Loss: 3.284142255783081\n",
            "Training Iteration 3988, Loss: 8.290802955627441\n",
            "Training Iteration 3989, Loss: 5.33989953994751\n",
            "Training Iteration 3990, Loss: 7.174724578857422\n",
            "Training Iteration 3991, Loss: 3.355956554412842\n",
            "Training Iteration 3992, Loss: 4.931015491485596\n",
            "Training Iteration 3993, Loss: 5.372859954833984\n",
            "Training Iteration 3994, Loss: 4.501906871795654\n",
            "Training Iteration 3995, Loss: 3.403085470199585\n",
            "Training Iteration 3996, Loss: 2.5216031074523926\n",
            "Training Iteration 3997, Loss: 4.893038749694824\n",
            "Training Iteration 3998, Loss: 3.5844907760620117\n",
            "Training Iteration 3999, Loss: 4.4393839836120605\n",
            "Training Iteration 4000, Loss: 4.833240032196045\n",
            "Training Iteration 4001, Loss: 5.748710632324219\n",
            "Training Iteration 4002, Loss: 3.8244504928588867\n",
            "Training Iteration 4003, Loss: 3.425097942352295\n",
            "Training Iteration 4004, Loss: 3.6223976612091064\n",
            "Training Iteration 4005, Loss: 2.701132297515869\n",
            "Training Iteration 4006, Loss: 3.1861512660980225\n",
            "Training Iteration 4007, Loss: 3.2306933403015137\n",
            "Training Iteration 4008, Loss: 4.9053449630737305\n",
            "Training Iteration 4009, Loss: 4.185460090637207\n",
            "Training Iteration 4010, Loss: 2.4851930141448975\n",
            "Training Iteration 4011, Loss: 3.2709665298461914\n",
            "Training Iteration 4012, Loss: 5.4752888679504395\n",
            "Training Iteration 4013, Loss: 3.6544878482818604\n",
            "Training Iteration 4014, Loss: 5.335803031921387\n",
            "Training Iteration 4015, Loss: 4.436270713806152\n",
            "Training Iteration 4016, Loss: 6.217537879943848\n",
            "Training Iteration 4017, Loss: 5.008243560791016\n",
            "Training Iteration 4018, Loss: 6.761708736419678\n",
            "Training Iteration 4019, Loss: 7.151022911071777\n",
            "Training Iteration 4020, Loss: 4.007063865661621\n",
            "Training Iteration 4021, Loss: 1.3733564615249634\n",
            "Training Iteration 4022, Loss: 3.373809814453125\n",
            "Training Iteration 4023, Loss: 7.808305740356445\n",
            "Training Iteration 4024, Loss: 2.4788477420806885\n",
            "Training Iteration 4025, Loss: 2.5319533348083496\n",
            "Training Iteration 4026, Loss: 5.984029293060303\n",
            "Training Iteration 4027, Loss: 6.761207103729248\n",
            "Training Iteration 4028, Loss: 7.117809295654297\n",
            "Training Iteration 4029, Loss: 7.989110469818115\n",
            "Training Iteration 4030, Loss: 4.878564357757568\n",
            "Training Iteration 4031, Loss: 6.695693016052246\n",
            "Training Iteration 4032, Loss: 3.875765085220337\n",
            "Training Iteration 4033, Loss: 4.606754779815674\n",
            "Training Iteration 4034, Loss: 5.242803573608398\n",
            "Training Iteration 4035, Loss: 4.606115341186523\n",
            "Training Iteration 4036, Loss: 4.5816850662231445\n",
            "Training Iteration 4037, Loss: 3.835561990737915\n",
            "Training Iteration 4038, Loss: 5.251176834106445\n",
            "Training Iteration 4039, Loss: 5.58571195602417\n",
            "Training Iteration 4040, Loss: 6.394193649291992\n",
            "Training Iteration 4041, Loss: 5.1331400871276855\n",
            "Training Iteration 4042, Loss: 3.723217725753784\n",
            "Training Iteration 4043, Loss: 4.675689697265625\n",
            "Training Iteration 4044, Loss: 4.2291741371154785\n",
            "Training Iteration 4045, Loss: 6.356518268585205\n",
            "Training Iteration 4046, Loss: 6.639652729034424\n",
            "Training Iteration 4047, Loss: 5.019577980041504\n",
            "Training Iteration 4048, Loss: 3.8576457500457764\n",
            "Training Iteration 4049, Loss: 4.557487964630127\n",
            "Training Iteration 4050, Loss: 6.89526891708374\n",
            "Training Iteration 4051, Loss: 7.509972095489502\n",
            "Training Iteration 4052, Loss: 3.7155709266662598\n",
            "Training Iteration 4053, Loss: 2.7858638763427734\n",
            "Training Iteration 4054, Loss: 4.505682945251465\n",
            "Training Iteration 4055, Loss: 2.3099846839904785\n",
            "Training Iteration 4056, Loss: 3.17631459236145\n",
            "Training Iteration 4057, Loss: 2.4613935947418213\n",
            "Training Iteration 4058, Loss: 2.820007801055908\n",
            "Training Iteration 4059, Loss: 4.5693206787109375\n",
            "Training Iteration 4060, Loss: 8.751220703125\n",
            "Training Iteration 4061, Loss: 12.415754318237305\n",
            "Training Iteration 4062, Loss: 8.156740188598633\n",
            "Training Iteration 4063, Loss: 3.1849217414855957\n",
            "Training Iteration 4064, Loss: 2.2522501945495605\n",
            "Training Iteration 4065, Loss: 2.8217885494232178\n",
            "Training Iteration 4066, Loss: 9.936713218688965\n",
            "Training Iteration 4067, Loss: 5.288102149963379\n",
            "Training Iteration 4068, Loss: 2.244082450866699\n",
            "Training Iteration 4069, Loss: 9.268903732299805\n",
            "Training Iteration 4070, Loss: 3.3617491722106934\n",
            "Training Iteration 4071, Loss: 6.07605504989624\n",
            "Training Iteration 4072, Loss: 4.577845573425293\n",
            "Training Iteration 4073, Loss: 7.894860744476318\n",
            "Training Iteration 4074, Loss: 6.5921950340271\n",
            "Training Iteration 4075, Loss: 5.277975082397461\n",
            "Training Iteration 4076, Loss: 4.6645402908325195\n",
            "Training Iteration 4077, Loss: 5.550605297088623\n",
            "Training Iteration 4078, Loss: 4.041999340057373\n",
            "Training Iteration 4079, Loss: 2.8091561794281006\n",
            "Training Iteration 4080, Loss: 1.1397995948791504\n",
            "Training Iteration 4081, Loss: 6.78005313873291\n",
            "Training Iteration 4082, Loss: 5.432327747344971\n",
            "Training Iteration 4083, Loss: 2.904177665710449\n",
            "Training Iteration 4084, Loss: 4.354510307312012\n",
            "Training Iteration 4085, Loss: 3.6404643058776855\n",
            "Training Iteration 4086, Loss: 11.314558982849121\n",
            "Training Iteration 4087, Loss: 3.8921825885772705\n",
            "Training Iteration 4088, Loss: 5.207485198974609\n",
            "Training Iteration 4089, Loss: 9.855877876281738\n",
            "Training Iteration 4090, Loss: 1.2797155380249023\n",
            "Training Iteration 4091, Loss: 5.695939064025879\n",
            "Training Iteration 4092, Loss: 2.9429423809051514\n",
            "Training Iteration 4093, Loss: 4.471390247344971\n",
            "Training Iteration 4094, Loss: 4.645606994628906\n",
            "Training Iteration 4095, Loss: 2.7351720333099365\n",
            "Training Iteration 4096, Loss: 3.1135354042053223\n",
            "Training Iteration 4097, Loss: 2.4397521018981934\n",
            "Training Iteration 4098, Loss: 2.6072378158569336\n",
            "Training Iteration 4099, Loss: 4.124435901641846\n",
            "Training Iteration 4100, Loss: 7.091983318328857\n",
            "Training Iteration 4101, Loss: 3.5383758544921875\n",
            "Training Iteration 4102, Loss: 2.1043901443481445\n",
            "Training Iteration 4103, Loss: 4.329229354858398\n",
            "Training Iteration 4104, Loss: 10.668210983276367\n",
            "Training Iteration 4105, Loss: 6.183350563049316\n",
            "Training Iteration 4106, Loss: 5.297031879425049\n",
            "Training Iteration 4107, Loss: 4.040624618530273\n",
            "Training Iteration 4108, Loss: 3.8687262535095215\n",
            "Training Iteration 4109, Loss: 4.21617317199707\n",
            "Training Iteration 4110, Loss: 0.9870250821113586\n",
            "Training Iteration 4111, Loss: 3.007507562637329\n",
            "Training Iteration 4112, Loss: 5.321139335632324\n",
            "Training Iteration 4113, Loss: 3.453817129135132\n",
            "Training Iteration 4114, Loss: 2.471008062362671\n",
            "Training Iteration 4115, Loss: 6.272429466247559\n",
            "Training Iteration 4116, Loss: 3.781247615814209\n",
            "Training Iteration 4117, Loss: 2.615361452102661\n",
            "Training Iteration 4118, Loss: 6.128052711486816\n",
            "Training Iteration 4119, Loss: 5.639153480529785\n",
            "Training Iteration 4120, Loss: 4.196710586547852\n",
            "Training Iteration 4121, Loss: 5.514996528625488\n",
            "Training Iteration 4122, Loss: 4.535916328430176\n",
            "Training Iteration 4123, Loss: 2.431353807449341\n",
            "Training Iteration 4124, Loss: 3.6066231727600098\n",
            "Training Iteration 4125, Loss: 4.264649391174316\n",
            "Training Iteration 4126, Loss: 4.077661514282227\n",
            "Training Iteration 4127, Loss: 4.236120223999023\n",
            "Training Iteration 4128, Loss: 4.054169654846191\n",
            "Training Iteration 4129, Loss: 5.373826026916504\n",
            "Training Iteration 4130, Loss: 4.7208251953125\n",
            "Training Iteration 4131, Loss: 4.972840309143066\n",
            "Training Iteration 4132, Loss: 6.57493257522583\n",
            "Training Iteration 4133, Loss: 2.384756088256836\n",
            "Training Iteration 4134, Loss: 2.4322268962860107\n",
            "Training Iteration 4135, Loss: 3.986097812652588\n",
            "Training Iteration 4136, Loss: 4.671609878540039\n",
            "Training Iteration 4137, Loss: 7.113976001739502\n",
            "Training Iteration 4138, Loss: 4.018263816833496\n",
            "Training Iteration 4139, Loss: 5.816174507141113\n",
            "Training Iteration 4140, Loss: 3.8279924392700195\n",
            "Training Iteration 4141, Loss: 7.842484474182129\n",
            "Training Iteration 4142, Loss: 12.394258499145508\n",
            "Training Iteration 4143, Loss: 4.9327874183654785\n",
            "Training Iteration 4144, Loss: 6.127584457397461\n",
            "Training Iteration 4145, Loss: 8.279897689819336\n",
            "Training Iteration 4146, Loss: 5.114051342010498\n",
            "Training Iteration 4147, Loss: 3.4992823600769043\n",
            "Training Iteration 4148, Loss: 4.861199378967285\n",
            "Training Iteration 4149, Loss: 5.4198713302612305\n",
            "Training Iteration 4150, Loss: 14.914978981018066\n",
            "Training Iteration 4151, Loss: 3.715428590774536\n",
            "Training Iteration 4152, Loss: 1.5463640689849854\n",
            "Training Iteration 4153, Loss: 7.316943645477295\n",
            "Training Iteration 4154, Loss: 7.4072442054748535\n",
            "Training Iteration 4155, Loss: 6.184427261352539\n",
            "Training Iteration 4156, Loss: 5.109482765197754\n",
            "Training Iteration 4157, Loss: 4.862870693206787\n",
            "Training Iteration 4158, Loss: 2.9348905086517334\n",
            "Training Iteration 4159, Loss: 2.7515270709991455\n",
            "Training Iteration 4160, Loss: 5.719871520996094\n",
            "Training Iteration 4161, Loss: 7.539758205413818\n",
            "Training Iteration 4162, Loss: 5.304591178894043\n",
            "Training Iteration 4163, Loss: 5.8742265701293945\n",
            "Training Iteration 4164, Loss: 7.758227348327637\n",
            "Training Iteration 4165, Loss: 3.5498745441436768\n",
            "Training Iteration 4166, Loss: 2.6249189376831055\n",
            "Training Iteration 4167, Loss: 6.581273078918457\n",
            "Training Iteration 4168, Loss: 5.094435691833496\n",
            "Training Iteration 4169, Loss: 9.603477478027344\n",
            "Training Iteration 4170, Loss: 4.175784111022949\n",
            "Training Iteration 4171, Loss: 5.666513442993164\n",
            "Training Iteration 4172, Loss: 2.9361214637756348\n",
            "Training Iteration 4173, Loss: 4.190150737762451\n",
            "Training Iteration 4174, Loss: 3.9673590660095215\n",
            "Training Iteration 4175, Loss: 3.977914571762085\n",
            "Training Iteration 4176, Loss: 6.065728187561035\n",
            "Training Iteration 4177, Loss: 6.360285758972168\n",
            "Training Iteration 4178, Loss: 5.69653844833374\n",
            "Training Iteration 4179, Loss: 2.9286060333251953\n",
            "Training Iteration 4180, Loss: 4.515590190887451\n",
            "Training Iteration 4181, Loss: 3.4247419834136963\n",
            "Training Iteration 4182, Loss: 4.366684436798096\n",
            "Training Iteration 4183, Loss: 3.7321937084198\n",
            "Training Iteration 4184, Loss: 3.7924163341522217\n",
            "Training Iteration 4185, Loss: 5.0738630294799805\n",
            "Training Iteration 4186, Loss: 4.437992095947266\n",
            "Training Iteration 4187, Loss: 5.684638977050781\n",
            "Training Iteration 4188, Loss: 3.4669744968414307\n",
            "Training Iteration 4189, Loss: 4.150148868560791\n",
            "Training Iteration 4190, Loss: 3.411362886428833\n",
            "Training Iteration 4191, Loss: 3.0806069374084473\n",
            "Training Iteration 4192, Loss: 3.370906352996826\n",
            "Training Iteration 4193, Loss: 5.596948146820068\n",
            "Training Iteration 4194, Loss: 3.9047746658325195\n",
            "Training Iteration 4195, Loss: 2.708977699279785\n",
            "Training Iteration 4196, Loss: 4.086340427398682\n",
            "Training Iteration 4197, Loss: 2.982248544692993\n",
            "Training Iteration 4198, Loss: 2.4713144302368164\n",
            "Training Iteration 4199, Loss: 7.430370330810547\n",
            "Training Iteration 4200, Loss: 4.749793529510498\n",
            "Training Iteration 4201, Loss: 4.776778697967529\n",
            "Training Iteration 4202, Loss: 3.9287636280059814\n",
            "Training Iteration 4203, Loss: 4.362624168395996\n",
            "Training Iteration 4204, Loss: 3.5542280673980713\n",
            "Training Iteration 4205, Loss: 6.026418209075928\n",
            "Training Iteration 4206, Loss: 8.439167022705078\n",
            "Training Iteration 4207, Loss: 5.898953437805176\n",
            "Training Iteration 4208, Loss: 4.578497886657715\n",
            "Training Iteration 4209, Loss: 4.436591148376465\n",
            "Training Iteration 4210, Loss: 2.384251594543457\n",
            "Training Iteration 4211, Loss: 6.198720455169678\n",
            "Training Iteration 4212, Loss: 5.1386308670043945\n",
            "Training Iteration 4213, Loss: 4.107123374938965\n",
            "Training Iteration 4214, Loss: 6.208027362823486\n",
            "Training Iteration 4215, Loss: 4.157467842102051\n",
            "Training Iteration 4216, Loss: 5.9358296394348145\n",
            "Training Iteration 4217, Loss: 2.7608418464660645\n",
            "Training Iteration 4218, Loss: 9.521354675292969\n",
            "Training Iteration 4219, Loss: 6.357530117034912\n",
            "Training Iteration 4220, Loss: 4.53408670425415\n",
            "Training Iteration 4221, Loss: 10.40924072265625\n",
            "Training Iteration 4222, Loss: 5.108975887298584\n",
            "Training Iteration 4223, Loss: 2.4043242931365967\n",
            "Training Iteration 4224, Loss: 4.281888484954834\n",
            "Training Iteration 4225, Loss: 2.9587485790252686\n",
            "Training Iteration 4226, Loss: 1.3945567607879639\n",
            "Training Iteration 4227, Loss: 3.6302640438079834\n",
            "Training Iteration 4228, Loss: 5.004072666168213\n",
            "Training Iteration 4229, Loss: 4.717043876647949\n",
            "Training Iteration 4230, Loss: 6.191290855407715\n",
            "Training Iteration 4231, Loss: 3.375685691833496\n",
            "Training Iteration 4232, Loss: 3.0528368949890137\n",
            "Training Iteration 4233, Loss: 8.085805892944336\n",
            "Training Iteration 4234, Loss: 1.6600615978240967\n",
            "Training Iteration 4235, Loss: 3.7091362476348877\n",
            "Training Iteration 4236, Loss: 7.069146156311035\n",
            "Training Iteration 4237, Loss: 4.208486557006836\n",
            "Training Iteration 4238, Loss: 8.004018783569336\n",
            "Training Iteration 4239, Loss: 6.882045269012451\n",
            "Training Iteration 4240, Loss: 3.10882306098938\n",
            "Training Iteration 4241, Loss: 2.575169801712036\n",
            "Training Iteration 4242, Loss: 5.530163764953613\n",
            "Training Iteration 4243, Loss: 5.842411041259766\n",
            "Training Iteration 4244, Loss: 2.995083808898926\n",
            "Training Iteration 4245, Loss: 3.0291645526885986\n",
            "Training Iteration 4246, Loss: 3.366650104522705\n",
            "Training Iteration 4247, Loss: 4.3932785987854\n",
            "Training Iteration 4248, Loss: 2.642206907272339\n",
            "Training Iteration 4249, Loss: 4.940229415893555\n",
            "Training Iteration 4250, Loss: 4.4492669105529785\n",
            "Training Iteration 4251, Loss: 3.099907398223877\n",
            "Training Iteration 4252, Loss: 5.336217880249023\n",
            "Training Iteration 4253, Loss: 2.5963292121887207\n",
            "Training Iteration 4254, Loss: 1.4488433599472046\n",
            "Training Iteration 4255, Loss: 4.901414394378662\n",
            "Training Iteration 4256, Loss: 3.5947484970092773\n",
            "Training Iteration 4257, Loss: 6.403171539306641\n",
            "Training Iteration 4258, Loss: 3.2819910049438477\n",
            "Training Iteration 4259, Loss: 3.7752065658569336\n",
            "Training Iteration 4260, Loss: 5.029301643371582\n",
            "Training Iteration 4261, Loss: 5.211636543273926\n",
            "Training Iteration 4262, Loss: 4.982180595397949\n",
            "Training Iteration 4263, Loss: 3.683072566986084\n",
            "Training Iteration 4264, Loss: 5.880338668823242\n",
            "Training Iteration 4265, Loss: 4.228437900543213\n",
            "Training Iteration 4266, Loss: 5.71394681930542\n",
            "Training Iteration 4267, Loss: 5.036009311676025\n",
            "Training Iteration 4268, Loss: 2.077768325805664\n",
            "Training Iteration 4269, Loss: 3.490955114364624\n",
            "Training Iteration 4270, Loss: 3.781463146209717\n",
            "Training Iteration 4271, Loss: 5.319197177886963\n",
            "Training Iteration 4272, Loss: 3.8493332862854004\n",
            "Training Iteration 4273, Loss: 3.399559259414673\n",
            "Training Iteration 4274, Loss: 5.216598987579346\n",
            "Training Iteration 4275, Loss: 5.420114517211914\n",
            "Training Iteration 4276, Loss: 2.3722429275512695\n",
            "Training Iteration 4277, Loss: 4.336538314819336\n",
            "Training Iteration 4278, Loss: 2.652139663696289\n",
            "Training Iteration 4279, Loss: 2.7948355674743652\n",
            "Training Iteration 4280, Loss: 4.083672046661377\n",
            "Training Iteration 4281, Loss: 6.079661846160889\n",
            "Training Iteration 4282, Loss: 4.415509223937988\n",
            "Training Iteration 4283, Loss: 6.914533615112305\n",
            "Training Iteration 4284, Loss: 3.1578760147094727\n",
            "Training Iteration 4285, Loss: 8.655517578125\n",
            "Training Iteration 4286, Loss: 3.6423816680908203\n",
            "Training Iteration 4287, Loss: 3.695996046066284\n",
            "Training Iteration 4288, Loss: 3.5623419284820557\n",
            "Training Iteration 4289, Loss: 3.716376543045044\n",
            "Training Iteration 4290, Loss: 6.346493244171143\n",
            "Training Iteration 4291, Loss: 8.659218788146973\n",
            "Training Iteration 4292, Loss: 5.094192028045654\n",
            "Training Iteration 4293, Loss: 5.289760589599609\n",
            "Training Iteration 4294, Loss: 4.050065040588379\n",
            "Training Iteration 4295, Loss: 5.286009311676025\n",
            "Training Iteration 4296, Loss: 3.7617931365966797\n",
            "Training Iteration 4297, Loss: 4.789543151855469\n",
            "Training Iteration 4298, Loss: 4.771161079406738\n",
            "Training Iteration 4299, Loss: 4.845094680786133\n",
            "Training Iteration 4300, Loss: 5.916192054748535\n",
            "Training Iteration 4301, Loss: 5.260382652282715\n",
            "Training Iteration 4302, Loss: 2.324124813079834\n",
            "Training Iteration 4303, Loss: 2.017641305923462\n",
            "Training Iteration 4304, Loss: 4.199494361877441\n",
            "Training Iteration 4305, Loss: 3.268507480621338\n",
            "Training Iteration 4306, Loss: 5.457796573638916\n",
            "Training Iteration 4307, Loss: 3.954784870147705\n",
            "Training Iteration 4308, Loss: 1.9202773571014404\n",
            "Training Iteration 4309, Loss: 4.3813090324401855\n",
            "Training Iteration 4310, Loss: 4.898138999938965\n",
            "Training Iteration 4311, Loss: 2.365489959716797\n",
            "Training Iteration 4312, Loss: 1.3769183158874512\n",
            "Training Iteration 4313, Loss: 3.162290573120117\n",
            "Training Iteration 4314, Loss: 4.97818660736084\n",
            "Training Iteration 4315, Loss: 7.090599060058594\n",
            "Training Iteration 4316, Loss: 2.099621295928955\n",
            "Training Iteration 4317, Loss: 2.952073812484741\n",
            "Training Iteration 4318, Loss: 4.718352794647217\n",
            "Training Iteration 4319, Loss: 3.903352737426758\n",
            "Training Iteration 4320, Loss: 4.916386127471924\n",
            "Training Iteration 4321, Loss: 4.404201984405518\n",
            "Training Iteration 4322, Loss: 3.1711392402648926\n",
            "Training Iteration 4323, Loss: 3.118736982345581\n",
            "Training Iteration 4324, Loss: 3.567263603210449\n",
            "Training Iteration 4325, Loss: 2.4749531745910645\n",
            "Training Iteration 4326, Loss: 2.1913821697235107\n",
            "Training Iteration 4327, Loss: 3.7067244052886963\n",
            "Training Iteration 4328, Loss: 3.206789493560791\n",
            "Training Iteration 4329, Loss: 3.4464004039764404\n",
            "Training Iteration 4330, Loss: 5.8469390869140625\n",
            "Training Iteration 4331, Loss: 6.521052837371826\n",
            "Training Iteration 4332, Loss: 4.794892311096191\n",
            "Training Iteration 4333, Loss: 6.229547023773193\n",
            "Training Iteration 4334, Loss: 3.4612793922424316\n",
            "Training Iteration 4335, Loss: 5.4790143966674805\n",
            "Training Iteration 4336, Loss: 5.111567497253418\n",
            "Training Iteration 4337, Loss: 4.807895660400391\n",
            "Training Iteration 4338, Loss: 3.751681327819824\n",
            "Training Iteration 4339, Loss: 5.521589756011963\n",
            "Training Iteration 4340, Loss: 5.996591567993164\n",
            "Training Iteration 4341, Loss: 6.734729766845703\n",
            "Training Iteration 4342, Loss: 9.835131645202637\n",
            "Training Iteration 4343, Loss: 5.56131649017334\n",
            "Training Iteration 4344, Loss: 2.213459014892578\n",
            "Training Iteration 4345, Loss: 4.0082011222839355\n",
            "Training Iteration 4346, Loss: 6.9000067710876465\n",
            "Training Iteration 4347, Loss: 6.437938213348389\n",
            "Training Iteration 4348, Loss: 3.9190073013305664\n",
            "Training Iteration 4349, Loss: 4.570316791534424\n",
            "Training Iteration 4350, Loss: 3.48663067817688\n",
            "Training Iteration 4351, Loss: 6.773916721343994\n",
            "Training Iteration 4352, Loss: 12.465675354003906\n",
            "Training Iteration 4353, Loss: 11.720779418945312\n",
            "Training Iteration 4354, Loss: 7.564028263092041\n",
            "Training Iteration 4355, Loss: 6.885814189910889\n",
            "Training Iteration 4356, Loss: 4.426036834716797\n",
            "Training Iteration 4357, Loss: 6.196260929107666\n",
            "Training Iteration 4358, Loss: 3.2384660243988037\n",
            "Training Iteration 4359, Loss: 4.00000524520874\n",
            "Training Iteration 4360, Loss: 3.8381686210632324\n",
            "Training Iteration 4361, Loss: 5.79173469543457\n",
            "Training Iteration 4362, Loss: 9.634112358093262\n",
            "Training Iteration 4363, Loss: 10.134642601013184\n",
            "Training Iteration 4364, Loss: 3.8947360515594482\n",
            "Training Iteration 4365, Loss: 7.472809314727783\n",
            "Training Iteration 4366, Loss: 5.2865447998046875\n",
            "Training Iteration 4367, Loss: 5.227569580078125\n",
            "Training Iteration 4368, Loss: 6.003880500793457\n",
            "Training Iteration 4369, Loss: 6.593526363372803\n",
            "Training Iteration 4370, Loss: 3.6790833473205566\n",
            "Training Iteration 4371, Loss: 5.098948001861572\n",
            "Training Iteration 4372, Loss: 5.110147953033447\n",
            "Training Iteration 4373, Loss: 3.969883680343628\n",
            "Training Iteration 4374, Loss: 5.173761367797852\n",
            "Training Iteration 4375, Loss: 5.708986759185791\n",
            "Training Iteration 4376, Loss: 4.032344341278076\n",
            "Training Iteration 4377, Loss: 7.150047302246094\n",
            "Training Iteration 4378, Loss: 3.046126365661621\n",
            "Training Iteration 4379, Loss: 3.5671205520629883\n",
            "Training Iteration 4380, Loss: 5.438887596130371\n",
            "Training Iteration 4381, Loss: 5.283329010009766\n",
            "Training Iteration 4382, Loss: 5.011144161224365\n",
            "Training Iteration 4383, Loss: 9.343389511108398\n",
            "Training Iteration 4384, Loss: 4.4173970222473145\n",
            "Training Iteration 4385, Loss: 4.133646011352539\n",
            "Training Iteration 4386, Loss: 5.615601539611816\n",
            "Training Iteration 4387, Loss: 5.128362655639648\n",
            "Training Iteration 4388, Loss: 4.7513885498046875\n",
            "Training Iteration 4389, Loss: 3.9267325401306152\n",
            "Training Iteration 4390, Loss: 5.085165977478027\n",
            "Training Iteration 4391, Loss: 5.397777080535889\n",
            "Training Iteration 4392, Loss: 3.1126348972320557\n",
            "Training Iteration 4393, Loss: 3.748993396759033\n",
            "Training Iteration 4394, Loss: 4.155155181884766\n",
            "Training Iteration 4395, Loss: 5.461883068084717\n",
            "Training Iteration 4396, Loss: 4.493579864501953\n",
            "Training Iteration 4397, Loss: 7.051095962524414\n",
            "Training Iteration 4398, Loss: 2.644604206085205\n",
            "Training Iteration 4399, Loss: 2.7668144702911377\n",
            "Training Iteration 4400, Loss: 2.2209653854370117\n",
            "Training Iteration 4401, Loss: 6.374972343444824\n",
            "Training Iteration 4402, Loss: 5.780305862426758\n",
            "Training Iteration 4403, Loss: 7.701585292816162\n",
            "Training Iteration 4404, Loss: 2.7978386878967285\n",
            "Training Iteration 4405, Loss: 5.453364372253418\n",
            "Training Iteration 4406, Loss: 3.2945942878723145\n",
            "Training Iteration 4407, Loss: 2.523834228515625\n",
            "Training Iteration 4408, Loss: 6.796740531921387\n",
            "Training Iteration 4409, Loss: 4.489645481109619\n",
            "Training Iteration 4410, Loss: 8.545534133911133\n",
            "Training Iteration 4411, Loss: 3.7378158569335938\n",
            "Training Iteration 4412, Loss: 3.768895149230957\n",
            "Training Iteration 4413, Loss: 5.258489608764648\n",
            "Training Iteration 4414, Loss: 6.432193756103516\n",
            "Training Iteration 4415, Loss: 8.601282119750977\n",
            "Training Iteration 4416, Loss: 3.6755499839782715\n",
            "Training Iteration 4417, Loss: 4.899569511413574\n",
            "Training Iteration 4418, Loss: 3.752032518386841\n",
            "Training Iteration 4419, Loss: 4.741633415222168\n",
            "Training Iteration 4420, Loss: 5.689201354980469\n",
            "Training Iteration 4421, Loss: 4.262996196746826\n",
            "Training Iteration 4422, Loss: 6.79559326171875\n",
            "Training Iteration 4423, Loss: 6.481940269470215\n",
            "Training Iteration 4424, Loss: 4.112525463104248\n",
            "Training Iteration 4425, Loss: 2.420304775238037\n",
            "Training Iteration 4426, Loss: 4.595575332641602\n",
            "Training Iteration 4427, Loss: 3.878727436065674\n",
            "Training Iteration 4428, Loss: 2.0488481521606445\n",
            "Training Iteration 4429, Loss: 2.2034988403320312\n",
            "Training Iteration 4430, Loss: 3.852116823196411\n",
            "Training Iteration 4431, Loss: 2.9203271865844727\n",
            "Training Iteration 4432, Loss: 4.4852705001831055\n",
            "Training Iteration 4433, Loss: 4.93776798248291\n",
            "Training Iteration 4434, Loss: 1.9939544200897217\n",
            "Training Iteration 4435, Loss: 3.7444703578948975\n",
            "Training Iteration 4436, Loss: 2.980344295501709\n",
            "Training Iteration 4437, Loss: 6.542989730834961\n",
            "Training Iteration 4438, Loss: 4.569628715515137\n",
            "Training Iteration 4439, Loss: 3.536475896835327\n",
            "Training Iteration 4440, Loss: 4.0933051109313965\n",
            "Training Iteration 4441, Loss: 5.284964561462402\n",
            "Training Iteration 4442, Loss: 9.017115592956543\n",
            "Training Iteration 4443, Loss: 5.218772888183594\n",
            "Training Iteration 4444, Loss: 7.590066909790039\n",
            "Training Iteration 4445, Loss: 8.255525588989258\n",
            "Training Iteration 4446, Loss: 3.3505611419677734\n",
            "Training Iteration 4447, Loss: 4.861337661743164\n",
            "Training Iteration 4448, Loss: 6.574520111083984\n",
            "Training Iteration 4449, Loss: 5.428635597229004\n",
            "Training Iteration 4450, Loss: 4.35426139831543\n",
            "Training Iteration 4451, Loss: 4.383214950561523\n",
            "Training Iteration 4452, Loss: 2.887139081954956\n",
            "Training Iteration 4453, Loss: 3.0334908962249756\n",
            "Training Iteration 4454, Loss: 6.130276203155518\n",
            "Training Iteration 4455, Loss: 5.194470405578613\n",
            "Training Iteration 4456, Loss: 5.885810852050781\n",
            "Training Iteration 4457, Loss: 5.755551338195801\n",
            "Training Iteration 4458, Loss: 4.628481864929199\n",
            "Training Iteration 4459, Loss: 3.188939094543457\n",
            "Training Iteration 4460, Loss: 2.695861339569092\n",
            "Training Iteration 4461, Loss: 4.147177219390869\n",
            "Training Iteration 4462, Loss: 5.2060017585754395\n",
            "Training Iteration 4463, Loss: 3.6152946949005127\n",
            "Training Iteration 4464, Loss: 4.042079448699951\n",
            "Training Iteration 4465, Loss: 6.18880558013916\n",
            "Training Iteration 4466, Loss: 2.5172691345214844\n",
            "Training Iteration 4467, Loss: 3.4891979694366455\n",
            "Training Iteration 4468, Loss: 10.197317123413086\n",
            "Training Iteration 4469, Loss: 6.67018985748291\n",
            "Training Iteration 4470, Loss: 7.549510478973389\n",
            "Training Iteration 4471, Loss: 4.004086017608643\n",
            "Training Iteration 4472, Loss: 2.3904073238372803\n",
            "Training Iteration 4473, Loss: 5.527373313903809\n",
            "Training Iteration 4474, Loss: 4.831239223480225\n",
            "Training Iteration 4475, Loss: 5.536650657653809\n",
            "Training Iteration 4476, Loss: 5.294071197509766\n",
            "Training Iteration 4477, Loss: 4.924445152282715\n",
            "Training Iteration 4478, Loss: 4.247758388519287\n",
            "Training Iteration 4479, Loss: 4.35084342956543\n",
            "Training Iteration 4480, Loss: 5.72348690032959\n",
            "Training Iteration 4481, Loss: 5.722013473510742\n",
            "Training Iteration 4482, Loss: 3.5303332805633545\n",
            "Training Iteration 4483, Loss: 4.795752048492432\n",
            "Training Iteration 4484, Loss: 5.02370548248291\n",
            "Training Iteration 4485, Loss: 5.142115116119385\n",
            "Training Iteration 4486, Loss: 4.798002243041992\n",
            "Training Iteration 4487, Loss: 4.243608474731445\n",
            "Training Iteration 4488, Loss: 5.320466041564941\n",
            "Training Iteration 4489, Loss: 4.165317058563232\n",
            "Training Iteration 4490, Loss: 6.522984504699707\n",
            "Training Iteration 4491, Loss: 5.77103328704834\n",
            "Training Iteration 4492, Loss: 4.737597465515137\n",
            "Training Iteration 4493, Loss: 3.9859466552734375\n",
            "Training Iteration 4494, Loss: 3.9148635864257812\n",
            "Training Iteration 4495, Loss: 5.119450569152832\n",
            "Training Iteration 4496, Loss: 5.443726539611816\n",
            "Training Iteration 4497, Loss: 3.3077774047851562\n",
            "Training Iteration 4498, Loss: 6.470207214355469\n",
            "Training Iteration 4499, Loss: 5.910897731781006\n",
            "Training Iteration 4500, Loss: 5.7376790046691895\n",
            "Training Iteration 4501, Loss: 4.557139873504639\n",
            "Training Iteration 4502, Loss: 8.678417205810547\n",
            "Training Iteration 4503, Loss: 4.207176208496094\n",
            "Training Iteration 4504, Loss: 4.029747486114502\n",
            "Training Iteration 4505, Loss: 2.9114606380462646\n",
            "Training Iteration 4506, Loss: 7.973939895629883\n",
            "Training Iteration 4507, Loss: 4.932252407073975\n",
            "Training Iteration 4508, Loss: 2.5207254886627197\n",
            "Training Iteration 4509, Loss: 3.6723036766052246\n",
            "Training Iteration 4510, Loss: 6.514725208282471\n",
            "Training Iteration 4511, Loss: 4.674436569213867\n",
            "Training Iteration 4512, Loss: 4.251285552978516\n",
            "Training Iteration 4513, Loss: 2.2748515605926514\n",
            "Training Iteration 4514, Loss: 2.917896270751953\n",
            "Training Iteration 4515, Loss: 4.152738094329834\n",
            "Training Iteration 4516, Loss: 5.464719772338867\n",
            "Training Iteration 4517, Loss: 2.96838116645813\n",
            "Training Iteration 4518, Loss: 5.083515644073486\n",
            "Training Iteration 4519, Loss: 5.407144069671631\n",
            "Training Iteration 4520, Loss: 4.582086563110352\n",
            "Training Iteration 4521, Loss: 3.798842191696167\n",
            "Training Iteration 4522, Loss: 3.689124822616577\n",
            "Training Iteration 4523, Loss: 5.797799587249756\n",
            "Training Iteration 4524, Loss: 2.205916166305542\n",
            "Training Iteration 4525, Loss: 2.0817086696624756\n",
            "Training Iteration 4526, Loss: 7.301429748535156\n",
            "Training Iteration 4527, Loss: 4.7661333084106445\n",
            "Training Iteration 4528, Loss: 4.905860424041748\n",
            "Training Iteration 4529, Loss: 3.5115742683410645\n",
            "Training Iteration 4530, Loss: 4.3669633865356445\n",
            "Training Iteration 4531, Loss: 5.217616558074951\n",
            "Training Iteration 4532, Loss: 3.531960964202881\n",
            "Training Iteration 4533, Loss: 6.160571575164795\n",
            "Training Iteration 4534, Loss: 4.260395526885986\n",
            "Training Iteration 4535, Loss: 3.381793737411499\n",
            "Training Iteration 4536, Loss: 2.2441680431365967\n",
            "Training Iteration 4537, Loss: 3.4576706886291504\n",
            "Training Iteration 4538, Loss: 3.5283920764923096\n",
            "Training Iteration 4539, Loss: 4.689060211181641\n",
            "Training Iteration 4540, Loss: 3.0145273208618164\n",
            "Training Iteration 4541, Loss: 2.6856532096862793\n",
            "Training Iteration 4542, Loss: 3.7340288162231445\n",
            "Training Iteration 4543, Loss: 4.578849792480469\n",
            "Training Iteration 4544, Loss: 6.790741920471191\n",
            "Training Iteration 4545, Loss: 5.66935396194458\n",
            "Training Iteration 4546, Loss: 4.741297721862793\n",
            "Training Iteration 4547, Loss: 3.957883596420288\n",
            "Training Iteration 4548, Loss: 4.138239860534668\n",
            "Training Iteration 4549, Loss: 1.8435360193252563\n",
            "Training Iteration 4550, Loss: 2.640057325363159\n",
            "Training Iteration 4551, Loss: 2.926854133605957\n",
            "Training Iteration 4552, Loss: 3.090818166732788\n",
            "Training Iteration 4553, Loss: 2.10963773727417\n",
            "Training Iteration 4554, Loss: 5.295348644256592\n",
            "Training Iteration 4555, Loss: 6.8438310623168945\n",
            "Training Iteration 4556, Loss: 5.6879706382751465\n",
            "Training Iteration 4557, Loss: 3.7096445560455322\n",
            "Training Iteration 4558, Loss: 4.2574992179870605\n",
            "Training Iteration 4559, Loss: 2.2878060340881348\n",
            "Training Iteration 4560, Loss: 5.548524856567383\n",
            "Training Iteration 4561, Loss: 4.924588680267334\n",
            "Training Iteration 4562, Loss: 3.0876805782318115\n",
            "Training Iteration 4563, Loss: 4.303654670715332\n",
            "Training Iteration 4564, Loss: 3.540524482727051\n",
            "Training Iteration 4565, Loss: 3.308560609817505\n",
            "Training Iteration 4566, Loss: 4.664801597595215\n",
            "Training Iteration 4567, Loss: 3.121151924133301\n",
            "Training Iteration 4568, Loss: 2.3541464805603027\n",
            "Training Iteration 4569, Loss: 3.5780763626098633\n",
            "Training Iteration 4570, Loss: 5.2410712242126465\n",
            "Training Iteration 4571, Loss: 3.764618158340454\n",
            "Training Iteration 4572, Loss: 4.800128936767578\n",
            "Training Iteration 4573, Loss: 6.331394672393799\n",
            "Training Iteration 4574, Loss: 6.0764312744140625\n",
            "Training Iteration 4575, Loss: 7.326285362243652\n",
            "Training Iteration 4576, Loss: 6.452601432800293\n",
            "Training Iteration 4577, Loss: 1.796839714050293\n",
            "Training Iteration 4578, Loss: 5.30751895904541\n",
            "Training Iteration 4579, Loss: 4.434836387634277\n",
            "Training Iteration 4580, Loss: 4.968866348266602\n",
            "Training Iteration 4581, Loss: 6.348546981811523\n",
            "Training Iteration 4582, Loss: 3.8635919094085693\n",
            "Training Iteration 4583, Loss: 6.529067039489746\n",
            "Training Iteration 4584, Loss: 7.84422492980957\n",
            "Training Iteration 4585, Loss: 11.614137649536133\n",
            "Training Iteration 4586, Loss: 5.8858771324157715\n",
            "Training Iteration 4587, Loss: 5.52947998046875\n",
            "Training Iteration 4588, Loss: 4.805998802185059\n",
            "Training Iteration 4589, Loss: 4.0085835456848145\n",
            "Training Iteration 4590, Loss: 2.759460687637329\n",
            "Training Iteration 4591, Loss: 3.4131343364715576\n",
            "Training Iteration 4592, Loss: 7.990705490112305\n",
            "Training Iteration 4593, Loss: 5.384194850921631\n",
            "Training Iteration 4594, Loss: 5.890799522399902\n",
            "Training Iteration 4595, Loss: 10.139039993286133\n",
            "Training Iteration 4596, Loss: 4.535735607147217\n",
            "Training Iteration 4597, Loss: 4.7163872718811035\n",
            "Training Iteration 4598, Loss: 9.021699905395508\n",
            "Training Iteration 4599, Loss: 3.2461352348327637\n",
            "Training Iteration 4600, Loss: 3.082585096359253\n",
            "Training Iteration 4601, Loss: 4.060298442840576\n",
            "Training Iteration 4602, Loss: 5.113073348999023\n",
            "Training Iteration 4603, Loss: 4.3499650955200195\n",
            "Training Iteration 4604, Loss: 1.9166793823242188\n",
            "Training Iteration 4605, Loss: 4.298725128173828\n",
            "Training Iteration 4606, Loss: 5.116918563842773\n",
            "Training Iteration 4607, Loss: 4.54854154586792\n",
            "Training Iteration 4608, Loss: 6.198712348937988\n",
            "Training Iteration 4609, Loss: 2.0160372257232666\n",
            "Training Iteration 4610, Loss: 3.3806185722351074\n",
            "Training Iteration 4611, Loss: 3.487527847290039\n",
            "Training Iteration 4612, Loss: 3.2539944648742676\n",
            "Training Iteration 4613, Loss: 4.630182266235352\n",
            "Training Iteration 4614, Loss: 8.281238555908203\n",
            "Training Iteration 4615, Loss: 5.200377464294434\n",
            "Training Iteration 4616, Loss: 1.9730173349380493\n",
            "Training Iteration 4617, Loss: 4.009067058563232\n",
            "Training Iteration 4618, Loss: 2.4036121368408203\n",
            "Training Iteration 4619, Loss: 7.2780046463012695\n",
            "Training Iteration 4620, Loss: 4.286172866821289\n",
            "Training Iteration 4621, Loss: 2.1213808059692383\n",
            "Training Iteration 4622, Loss: 3.1789941787719727\n",
            "Training Iteration 4623, Loss: 4.352277755737305\n",
            "Training Iteration 4624, Loss: 5.131293773651123\n",
            "Training Iteration 4625, Loss: 10.168341636657715\n",
            "Training Iteration 4626, Loss: 4.3309197425842285\n",
            "Training Iteration 4627, Loss: 4.5497026443481445\n",
            "Training Iteration 4628, Loss: 4.542544841766357\n",
            "Training Iteration 4629, Loss: 5.0739898681640625\n",
            "Training Iteration 4630, Loss: 6.170543193817139\n",
            "Training Iteration 4631, Loss: 2.635190725326538\n",
            "Training Iteration 4632, Loss: 6.431896686553955\n",
            "Training Iteration 4633, Loss: 6.496793746948242\n",
            "Training Iteration 4634, Loss: 4.0328803062438965\n",
            "Training Iteration 4635, Loss: 3.5307416915893555\n",
            "Training Iteration 4636, Loss: 3.757154703140259\n",
            "Training Iteration 4637, Loss: 4.084311008453369\n",
            "Training Iteration 4638, Loss: 4.212519645690918\n",
            "Training Iteration 4639, Loss: 5.514459133148193\n",
            "Training Iteration 4640, Loss: 4.92659330368042\n",
            "Training Iteration 4641, Loss: 4.95113468170166\n",
            "Training Iteration 4642, Loss: 5.369422912597656\n",
            "Training Iteration 4643, Loss: 1.8818731307983398\n",
            "Training Iteration 4644, Loss: 3.7037951946258545\n",
            "Training Iteration 4645, Loss: 2.970075845718384\n",
            "Training Iteration 4646, Loss: 3.541914463043213\n",
            "Training Iteration 4647, Loss: 5.2808942794799805\n",
            "Training Iteration 4648, Loss: 4.250331401824951\n",
            "Training Iteration 4649, Loss: 4.9766459465026855\n",
            "Training Iteration 4650, Loss: 10.318443298339844\n",
            "Training Iteration 4651, Loss: 3.649676561355591\n",
            "Training Iteration 4652, Loss: 4.094990253448486\n",
            "Training Iteration 4653, Loss: 3.975640296936035\n",
            "Training Iteration 4654, Loss: 5.093410491943359\n",
            "Training Iteration 4655, Loss: 3.883876323699951\n",
            "Training Iteration 4656, Loss: 3.6103708744049072\n",
            "Training Iteration 4657, Loss: 4.600179672241211\n",
            "Training Iteration 4658, Loss: 5.610887050628662\n",
            "Training Iteration 4659, Loss: 3.4938697814941406\n",
            "Training Iteration 4660, Loss: 4.15552282333374\n",
            "Training Iteration 4661, Loss: 7.862771511077881\n",
            "Training Iteration 4662, Loss: 3.6012284755706787\n",
            "Training Iteration 4663, Loss: 4.45738410949707\n",
            "Training Iteration 4664, Loss: 4.159939289093018\n",
            "Training Iteration 4665, Loss: 5.573296546936035\n",
            "Training Iteration 4666, Loss: 0.9987462759017944\n",
            "Training Iteration 4667, Loss: 3.6631476879119873\n",
            "Training Iteration 4668, Loss: 3.0746970176696777\n",
            "Training Iteration 4669, Loss: 5.255334377288818\n",
            "Training Iteration 4670, Loss: 4.231288433074951\n",
            "Training Iteration 4671, Loss: 2.9005515575408936\n",
            "Training Iteration 4672, Loss: 2.5498080253601074\n",
            "Training Iteration 4673, Loss: 6.439602375030518\n",
            "Training Iteration 4674, Loss: 5.551445960998535\n",
            "Training Iteration 4675, Loss: 5.8372931480407715\n",
            "Training Iteration 4676, Loss: 3.008984088897705\n",
            "Training Iteration 4677, Loss: 4.4024338722229\n",
            "Training Iteration 4678, Loss: 7.830887317657471\n",
            "Training Iteration 4679, Loss: 1.8563377857208252\n",
            "Training Iteration 4680, Loss: 5.829746723175049\n",
            "Training Iteration 4681, Loss: 2.8186147212982178\n",
            "Training Iteration 4682, Loss: 5.347285270690918\n",
            "Training Iteration 4683, Loss: 4.513721942901611\n",
            "Training Iteration 4684, Loss: 5.154576301574707\n",
            "Training Iteration 4685, Loss: 2.520092487335205\n",
            "Training Iteration 4686, Loss: 4.1009955406188965\n",
            "Training Iteration 4687, Loss: 4.190118789672852\n",
            "Training Iteration 4688, Loss: 6.485561370849609\n",
            "Training Iteration 4689, Loss: 2.471059560775757\n",
            "Training Iteration 4690, Loss: 3.3777315616607666\n",
            "Training Iteration 4691, Loss: 2.2151238918304443\n",
            "Training Iteration 4692, Loss: 3.0775842666625977\n",
            "Training Iteration 4693, Loss: 4.809937953948975\n",
            "Training Iteration 4694, Loss: 4.559298515319824\n",
            "Training Iteration 4695, Loss: 2.9518110752105713\n",
            "Training Iteration 4696, Loss: 5.200434684753418\n",
            "Training Iteration 4697, Loss: 8.982744216918945\n",
            "Training Iteration 4698, Loss: 4.847482204437256\n",
            "Training Iteration 4699, Loss: 3.3344337940216064\n",
            "Training Iteration 4700, Loss: 3.623628616333008\n",
            "Training Iteration 4701, Loss: 3.797272205352783\n",
            "Training Iteration 4702, Loss: 3.567887544631958\n",
            "Training Iteration 4703, Loss: 2.5339291095733643\n",
            "Training Iteration 4704, Loss: 5.8546624183654785\n",
            "Training Iteration 4705, Loss: 5.5681047439575195\n",
            "Training Iteration 4706, Loss: 1.9535150527954102\n",
            "Training Iteration 4707, Loss: 3.1477208137512207\n",
            "Training Iteration 4708, Loss: 5.118497848510742\n",
            "Training Iteration 4709, Loss: 3.8258070945739746\n",
            "Training Iteration 4710, Loss: 3.72208833694458\n",
            "Training Iteration 4711, Loss: 4.896425247192383\n",
            "Training Iteration 4712, Loss: 1.6558189392089844\n",
            "Training Iteration 4713, Loss: 3.5124099254608154\n",
            "Training Iteration 4714, Loss: 4.826159477233887\n",
            "Training Iteration 4715, Loss: 7.514144420623779\n",
            "Training Iteration 4716, Loss: 1.8559606075286865\n",
            "Training Iteration 4717, Loss: 3.684673309326172\n",
            "Training Iteration 4718, Loss: 2.933495283126831\n",
            "Training Iteration 4719, Loss: 4.6498003005981445\n",
            "Training Iteration 4720, Loss: 5.067687034606934\n",
            "Training Iteration 4721, Loss: 2.965193510055542\n",
            "Training Iteration 4722, Loss: 2.863416910171509\n",
            "Training Iteration 4723, Loss: 1.5700933933258057\n",
            "Training Iteration 4724, Loss: 3.6255314350128174\n",
            "Training Iteration 4725, Loss: 3.23862624168396\n",
            "Training Iteration 4726, Loss: 5.012006759643555\n",
            "Training Iteration 4727, Loss: 3.4663476943969727\n",
            "Training Iteration 4728, Loss: 8.61532974243164\n",
            "Training Iteration 4729, Loss: 4.031891822814941\n",
            "Training Iteration 4730, Loss: 5.156615257263184\n",
            "Training Iteration 4731, Loss: 2.9445533752441406\n",
            "Training Iteration 4732, Loss: 4.476483345031738\n",
            "Training Iteration 4733, Loss: 3.2521021366119385\n",
            "Training Iteration 4734, Loss: 5.044803142547607\n",
            "Training Iteration 4735, Loss: 4.113933563232422\n",
            "Training Iteration 4736, Loss: 6.339749336242676\n",
            "Training Iteration 4737, Loss: 3.596092462539673\n",
            "Training Iteration 4738, Loss: 3.7800307273864746\n",
            "Training Iteration 4739, Loss: 3.574049949645996\n",
            "Training Iteration 4740, Loss: 3.898331642150879\n",
            "Training Iteration 4741, Loss: 3.384263277053833\n",
            "Training Iteration 4742, Loss: 7.4312639236450195\n",
            "Training Iteration 4743, Loss: 2.970843553543091\n",
            "Training Iteration 4744, Loss: 3.862961530685425\n",
            "Training Iteration 4745, Loss: 7.602744102478027\n",
            "Training Iteration 4746, Loss: 1.823264241218567\n",
            "Training Iteration 4747, Loss: 4.521563529968262\n",
            "Training Iteration 4748, Loss: 3.1230151653289795\n",
            "Training Iteration 4749, Loss: 4.121166706085205\n",
            "Training Iteration 4750, Loss: 5.031396865844727\n",
            "Training Iteration 4751, Loss: 5.396319389343262\n",
            "Training Iteration 4752, Loss: 4.914445400238037\n",
            "Training Iteration 4753, Loss: 4.542613506317139\n",
            "Training Iteration 4754, Loss: 4.320178031921387\n",
            "Training Iteration 4755, Loss: 6.553251266479492\n",
            "Training Iteration 4756, Loss: 6.256782531738281\n",
            "Training Iteration 4757, Loss: 3.90529727935791\n",
            "Training Iteration 4758, Loss: 5.1816229820251465\n",
            "Training Iteration 4759, Loss: 4.1560187339782715\n",
            "Training Iteration 4760, Loss: 8.601408958435059\n",
            "Training Iteration 4761, Loss: 7.013542652130127\n",
            "Training Iteration 4762, Loss: 4.640310764312744\n",
            "Training Iteration 4763, Loss: 6.399051189422607\n",
            "Training Iteration 4764, Loss: 8.860699653625488\n",
            "Training Iteration 4765, Loss: 5.778672218322754\n",
            "Training Iteration 4766, Loss: 7.153531074523926\n",
            "Training Iteration 4767, Loss: 2.342273235321045\n",
            "Training Iteration 4768, Loss: 9.332701683044434\n",
            "Training Iteration 4769, Loss: 4.2955241203308105\n",
            "Training Iteration 4770, Loss: 5.36729621887207\n",
            "Training Iteration 4771, Loss: 5.160717964172363\n",
            "Training Iteration 4772, Loss: 4.712507724761963\n",
            "Training Iteration 4773, Loss: 8.338696479797363\n",
            "Training Iteration 4774, Loss: 5.551517009735107\n",
            "Training Iteration 4775, Loss: 5.344756603240967\n",
            "Training Iteration 4776, Loss: 6.884842872619629\n",
            "Training Iteration 4777, Loss: 6.436277866363525\n",
            "Training Iteration 4778, Loss: 5.77524471282959\n",
            "Training Iteration 4779, Loss: 3.491612672805786\n",
            "Training Iteration 4780, Loss: 4.845724105834961\n",
            "Training Iteration 4781, Loss: 4.450845718383789\n",
            "Training Iteration 4782, Loss: 6.57030725479126\n",
            "Training Iteration 4783, Loss: 6.453402042388916\n",
            "Training Iteration 4784, Loss: 1.3325566053390503\n",
            "Training Iteration 4785, Loss: 3.859358310699463\n",
            "Training Iteration 4786, Loss: 3.2467129230499268\n",
            "Training Iteration 4787, Loss: 3.3039040565490723\n",
            "Training Iteration 4788, Loss: 4.860169410705566\n",
            "Training Iteration 4789, Loss: 5.382480621337891\n",
            "Training Iteration 4790, Loss: 8.701797485351562\n",
            "Training Iteration 4791, Loss: 3.1602699756622314\n",
            "Training Iteration 4792, Loss: 4.633645534515381\n",
            "Training Iteration 4793, Loss: 3.699653148651123\n",
            "Training Iteration 4794, Loss: 4.423775672912598\n",
            "Training Iteration 4795, Loss: 3.279804229736328\n",
            "Training Iteration 4796, Loss: 2.625248670578003\n",
            "Training Iteration 4797, Loss: 3.3686325550079346\n",
            "Training Iteration 4798, Loss: 3.5149455070495605\n",
            "Training Iteration 4799, Loss: 2.7904064655303955\n",
            "Training Iteration 4800, Loss: 6.174151420593262\n",
            "Training Iteration 4801, Loss: 3.450975179672241\n",
            "Training Iteration 4802, Loss: 4.242665767669678\n",
            "Training Iteration 4803, Loss: 1.3433643579483032\n",
            "Training Iteration 4804, Loss: 6.643150806427002\n",
            "Training Iteration 4805, Loss: 7.489933967590332\n",
            "Training Iteration 4806, Loss: 3.632429599761963\n",
            "Training Iteration 4807, Loss: 6.968207836151123\n",
            "Training Iteration 4808, Loss: 5.24344539642334\n",
            "Training Iteration 4809, Loss: 5.2930707931518555\n",
            "Training Iteration 4810, Loss: 6.396515846252441\n",
            "Training Iteration 4811, Loss: 6.23714017868042\n",
            "Training Iteration 4812, Loss: 6.82135534286499\n",
            "Training Iteration 4813, Loss: 5.988062858581543\n",
            "Training Iteration 4814, Loss: 5.217349529266357\n",
            "Training Iteration 4815, Loss: 6.412657737731934\n",
            "Training Iteration 4816, Loss: 4.85377311706543\n",
            "Training Iteration 4817, Loss: 2.3984179496765137\n",
            "Training Iteration 4818, Loss: 3.58046293258667\n",
            "Training Iteration 4819, Loss: 9.528129577636719\n",
            "Training Iteration 4820, Loss: 3.4266676902770996\n",
            "Training Iteration 4821, Loss: 5.388732433319092\n",
            "Training Iteration 4822, Loss: 4.286776542663574\n",
            "Training Iteration 4823, Loss: 4.216020584106445\n",
            "Training Iteration 4824, Loss: 7.421709060668945\n",
            "Training Iteration 4825, Loss: 6.680195331573486\n",
            "Training Iteration 4826, Loss: 4.143517017364502\n",
            "Training Iteration 4827, Loss: 6.559080600738525\n",
            "Training Iteration 4828, Loss: 3.5682260990142822\n",
            "Training Iteration 4829, Loss: 4.28127384185791\n",
            "Training Iteration 4830, Loss: 4.627323150634766\n",
            "Training Iteration 4831, Loss: 4.814984321594238\n",
            "Training Iteration 4832, Loss: 1.6597230434417725\n",
            "Training Iteration 4833, Loss: 4.480342864990234\n",
            "Training Iteration 4834, Loss: 3.966979503631592\n",
            "Training Iteration 4835, Loss: 4.942983150482178\n",
            "Training Iteration 4836, Loss: 4.821659564971924\n",
            "Training Iteration 4837, Loss: 5.921018600463867\n",
            "Training Iteration 4838, Loss: 5.767232894897461\n",
            "Training Iteration 4839, Loss: 8.452754974365234\n",
            "Training Iteration 4840, Loss: 7.066953659057617\n",
            "Training Iteration 4841, Loss: 4.141298770904541\n",
            "Training Iteration 4842, Loss: 6.144803047180176\n",
            "Training Iteration 4843, Loss: 8.482068061828613\n",
            "Training Iteration 4844, Loss: 1.6887176036834717\n",
            "Training Iteration 4845, Loss: 6.914007663726807\n",
            "Training Iteration 4846, Loss: 3.8708157539367676\n",
            "Training Iteration 4847, Loss: 4.739241600036621\n",
            "Training Iteration 4848, Loss: 5.747824192047119\n",
            "Training Iteration 4849, Loss: 5.412261009216309\n",
            "Training Iteration 4850, Loss: 4.147730827331543\n",
            "Training Iteration 4851, Loss: 1.4183824062347412\n",
            "Training Iteration 4852, Loss: 4.123569488525391\n",
            "Training Iteration 4853, Loss: 3.664968967437744\n",
            "Training Iteration 4854, Loss: 2.208374261856079\n",
            "Training Iteration 4855, Loss: 5.697802543640137\n",
            "Training Iteration 4856, Loss: 1.8063621520996094\n",
            "Training Iteration 4857, Loss: 2.8145694732666016\n",
            "Training Iteration 4858, Loss: 3.0080087184906006\n",
            "Training Iteration 4859, Loss: 4.637613773345947\n",
            "Training Iteration 4860, Loss: 5.204686164855957\n",
            "Training Iteration 4861, Loss: 5.387133598327637\n",
            "Training Iteration 4862, Loss: 3.533452033996582\n",
            "Training Iteration 4863, Loss: 3.951094627380371\n",
            "Training Iteration 4864, Loss: 4.26504373550415\n",
            "Training Iteration 4865, Loss: 4.209300518035889\n",
            "Training Iteration 4866, Loss: 9.2709321975708\n",
            "Training Iteration 4867, Loss: 4.581952095031738\n",
            "Training Iteration 4868, Loss: 7.170456409454346\n",
            "Training Iteration 4869, Loss: 6.744431018829346\n",
            "Training Iteration 4870, Loss: 3.5612244606018066\n",
            "Training Iteration 4871, Loss: 3.7982017993927\n",
            "Training Iteration 4872, Loss: 4.122371196746826\n",
            "Training Iteration 4873, Loss: 2.584007978439331\n",
            "Training Iteration 4874, Loss: 4.744963645935059\n",
            "Training Iteration 4875, Loss: 4.399665355682373\n",
            "Training Iteration 4876, Loss: 6.141453742980957\n",
            "Training Iteration 4877, Loss: 4.989634990692139\n",
            "Training Iteration 4878, Loss: 8.565723419189453\n",
            "Training Iteration 4879, Loss: 6.620737075805664\n",
            "Training Iteration 4880, Loss: 3.7538092136383057\n",
            "Training Iteration 4881, Loss: 2.1459460258483887\n",
            "Training Iteration 4882, Loss: 4.630059242248535\n",
            "Training Iteration 4883, Loss: 7.597122669219971\n",
            "Training Iteration 4884, Loss: 6.512386322021484\n",
            "Training Iteration 4885, Loss: 5.95373010635376\n",
            "Training Iteration 4886, Loss: 7.439115047454834\n",
            "Training Iteration 4887, Loss: 2.318558931350708\n",
            "Training Iteration 4888, Loss: 6.234776020050049\n",
            "Training Iteration 4889, Loss: 6.4143853187561035\n",
            "Training Iteration 4890, Loss: 6.22337532043457\n",
            "Training Iteration 4891, Loss: 3.4394116401672363\n",
            "Training Iteration 4892, Loss: 3.1381397247314453\n",
            "Training Iteration 4893, Loss: 5.680199146270752\n",
            "Training Iteration 4894, Loss: 2.66337251663208\n",
            "Training Iteration 4895, Loss: 7.215656280517578\n",
            "Training Iteration 4896, Loss: 4.649456977844238\n",
            "Training Iteration 4897, Loss: 7.2249836921691895\n",
            "Training Iteration 4898, Loss: 6.594606399536133\n",
            "Training Iteration 4899, Loss: 1.4789503812789917\n",
            "Training Iteration 4900, Loss: 4.140850067138672\n",
            "Training Iteration 4901, Loss: 9.401739120483398\n",
            "Training Iteration 4902, Loss: 4.477279186248779\n",
            "Training Iteration 4903, Loss: 3.247638463973999\n",
            "Training Iteration 4904, Loss: 4.413308620452881\n",
            "Training Iteration 4905, Loss: 4.6102471351623535\n",
            "Training Iteration 4906, Loss: 5.11010217666626\n",
            "Training Iteration 4907, Loss: 3.3590800762176514\n",
            "Training Iteration 4908, Loss: 4.5086493492126465\n",
            "Training Iteration 4909, Loss: 3.8547565937042236\n",
            "Training Iteration 4910, Loss: 2.21770977973938\n",
            "Training Iteration 4911, Loss: 5.206116676330566\n",
            "Training Iteration 4912, Loss: 4.27670431137085\n",
            "Training Iteration 4913, Loss: 2.957855463027954\n",
            "Training Iteration 4914, Loss: 3.9760820865631104\n",
            "Training Iteration 4915, Loss: 8.479275703430176\n",
            "Training Iteration 4916, Loss: 8.658836364746094\n",
            "Training Iteration 4917, Loss: 3.7538065910339355\n",
            "Training Iteration 4918, Loss: 3.486945390701294\n",
            "Training Iteration 4919, Loss: 6.092041015625\n",
            "Training Iteration 4920, Loss: 4.775119304656982\n",
            "Training Iteration 4921, Loss: 3.5875613689422607\n",
            "Training Iteration 4922, Loss: 4.414592742919922\n",
            "Training Iteration 4923, Loss: 8.446573257446289\n",
            "Training Iteration 4924, Loss: 6.827198028564453\n",
            "Training Iteration 4925, Loss: 1.704887866973877\n",
            "Training Iteration 4926, Loss: 6.118711948394775\n",
            "Training Iteration 4927, Loss: 8.448148727416992\n",
            "Training Iteration 4928, Loss: 4.576108455657959\n",
            "Training Iteration 4929, Loss: 6.74653434753418\n",
            "Training Iteration 4930, Loss: 7.8935980796813965\n",
            "Training Iteration 4931, Loss: 3.8635315895080566\n",
            "Training Iteration 4932, Loss: 3.799649477005005\n",
            "Training Iteration 4933, Loss: 4.3374834060668945\n",
            "Training Iteration 4934, Loss: 6.281094551086426\n",
            "Training Iteration 4935, Loss: 2.029548406600952\n",
            "Training Iteration 4936, Loss: 3.2519750595092773\n",
            "Training Iteration 4937, Loss: 3.0198898315429688\n",
            "Training Iteration 4938, Loss: 3.608776569366455\n",
            "Training Iteration 4939, Loss: 3.150954008102417\n",
            "Training Iteration 4940, Loss: 8.412374496459961\n",
            "Training Iteration 4941, Loss: 7.278321266174316\n",
            "Training Iteration 4942, Loss: 9.220458030700684\n",
            "Training Iteration 4943, Loss: 3.4797143936157227\n",
            "Training Iteration 4944, Loss: 5.070369243621826\n",
            "Training Iteration 4945, Loss: 3.7402756214141846\n",
            "Training Iteration 4946, Loss: 7.376368045806885\n",
            "Training Iteration 4947, Loss: 3.272829055786133\n",
            "Training Iteration 4948, Loss: 3.346404552459717\n",
            "Training Iteration 4949, Loss: 7.227268695831299\n",
            "Training Iteration 4950, Loss: 2.336350202560425\n",
            "Training Iteration 4951, Loss: 11.375070571899414\n",
            "Training Iteration 4952, Loss: 2.5133988857269287\n",
            "Training Iteration 4953, Loss: 2.8925836086273193\n",
            "Training Iteration 4954, Loss: 4.859867095947266\n",
            "Training Iteration 4955, Loss: 2.849522590637207\n",
            "Training Iteration 4956, Loss: 7.7029571533203125\n",
            "Training Iteration 4957, Loss: 6.590482711791992\n",
            "Training Iteration 4958, Loss: 3.819915533065796\n",
            "Training Iteration 4959, Loss: 7.6301069259643555\n",
            "Training Iteration 4960, Loss: 6.164124965667725\n",
            "Training Iteration 4961, Loss: 3.590636730194092\n",
            "Training Iteration 4962, Loss: 3.4657185077667236\n",
            "Training Iteration 4963, Loss: 3.3688712120056152\n",
            "Training Iteration 4964, Loss: 6.469600677490234\n",
            "Training Iteration 4965, Loss: 4.84684419631958\n",
            "Training Iteration 4966, Loss: 5.32082462310791\n",
            "Training Iteration 4967, Loss: 6.438368797302246\n",
            "Training Iteration 4968, Loss: 2.517291784286499\n",
            "Training Iteration 4969, Loss: 5.330620765686035\n",
            "Training Iteration 4970, Loss: 4.107263565063477\n",
            "Training Iteration 4971, Loss: 7.265299320220947\n",
            "Training Iteration 4972, Loss: 4.40568733215332\n",
            "Training Iteration 4973, Loss: 3.797579765319824\n",
            "Training Iteration 4974, Loss: 3.4685921669006348\n",
            "Training Iteration 4975, Loss: 5.041290283203125\n",
            "Training Iteration 4976, Loss: 3.1036338806152344\n",
            "Training Iteration 4977, Loss: 3.9242959022521973\n",
            "Training Iteration 4978, Loss: 4.272374153137207\n",
            "Training Iteration 4979, Loss: 6.4384565353393555\n",
            "Training Iteration 4980, Loss: 3.969386100769043\n",
            "Training Iteration 4981, Loss: 5.447368144989014\n",
            "Training Iteration 4982, Loss: 4.435654163360596\n",
            "Training Iteration 4983, Loss: 1.6099097728729248\n",
            "Training Iteration 4984, Loss: 5.4666900634765625\n",
            "Training Iteration 4985, Loss: 4.88309907913208\n",
            "Training Iteration 4986, Loss: 3.329904079437256\n",
            "Training Iteration 4987, Loss: 3.183150291442871\n",
            "Training Iteration 4988, Loss: 4.729391098022461\n",
            "Training Iteration 4989, Loss: 3.0346360206604004\n",
            "Training Iteration 4990, Loss: 3.2540950775146484\n",
            "Training Iteration 4991, Loss: 6.386927604675293\n",
            "Training Iteration 4992, Loss: 8.0220365524292\n",
            "Training Iteration 4993, Loss: 5.70193338394165\n",
            "Training Iteration 4994, Loss: 4.707356929779053\n",
            "Training Iteration 4995, Loss: 6.881621360778809\n",
            "Training Iteration 4996, Loss: 4.715860366821289\n",
            "Training Iteration 4997, Loss: 3.7772960662841797\n",
            "Training Iteration 4998, Loss: 5.696314811706543\n",
            "Training Iteration 4999, Loss: 3.812082529067993\n",
            "Training Iteration 5000, Loss: 6.924239158630371\n",
            "Training Iteration 5001, Loss: 6.154703140258789\n",
            "Training Iteration 5002, Loss: 4.592203140258789\n",
            "Training Iteration 5003, Loss: 5.102601051330566\n",
            "Training Iteration 5004, Loss: 5.207967758178711\n",
            "Training Iteration 5005, Loss: 7.882906436920166\n",
            "Training Iteration 5006, Loss: 7.9293012619018555\n",
            "Training Iteration 5007, Loss: 2.792707681655884\n",
            "Training Iteration 5008, Loss: 2.558206558227539\n",
            "Training Iteration 5009, Loss: 3.035581588745117\n",
            "Training Iteration 5010, Loss: 4.228091716766357\n",
            "Training Iteration 5011, Loss: 5.191793918609619\n",
            "Training Iteration 5012, Loss: 5.947132587432861\n",
            "Training Iteration 5013, Loss: 1.8891165256500244\n",
            "Training Iteration 5014, Loss: 6.150251388549805\n",
            "Training Iteration 5015, Loss: 6.847661018371582\n",
            "Training Iteration 5016, Loss: 4.427585601806641\n",
            "Training Iteration 5017, Loss: 4.986795425415039\n",
            "Training Iteration 5018, Loss: 6.921105861663818\n",
            "Training Iteration 5019, Loss: 5.7242817878723145\n",
            "Training Iteration 5020, Loss: 2.586805582046509\n",
            "Training Iteration 5021, Loss: 8.34715461730957\n",
            "Training Iteration 5022, Loss: 4.248441219329834\n",
            "Training Iteration 5023, Loss: 2.450723171234131\n",
            "Training Iteration 5024, Loss: 2.300344705581665\n",
            "Training Iteration 5025, Loss: 4.90749454498291\n",
            "Training Iteration 5026, Loss: 6.468058109283447\n",
            "Training Iteration 5027, Loss: 3.800023078918457\n",
            "Training Iteration 5028, Loss: 6.085414409637451\n",
            "Training Iteration 5029, Loss: 4.110761642456055\n",
            "Training Iteration 5030, Loss: 2.71140456199646\n",
            "Training Iteration 5031, Loss: 2.379596710205078\n",
            "Training Iteration 5032, Loss: 5.4106903076171875\n",
            "Training Iteration 5033, Loss: 5.013026714324951\n",
            "Training Iteration 5034, Loss: 4.101200580596924\n",
            "Training Iteration 5035, Loss: 6.63873291015625\n",
            "Training Iteration 5036, Loss: 3.5412094593048096\n",
            "Training Iteration 5037, Loss: 6.457691192626953\n",
            "Training Iteration 5038, Loss: 6.791492462158203\n",
            "Training Iteration 5039, Loss: 4.588298797607422\n",
            "Training Iteration 5040, Loss: 4.490556716918945\n",
            "Training Iteration 5041, Loss: 3.3667006492614746\n",
            "Training Iteration 5042, Loss: 5.899044513702393\n",
            "Training Iteration 5043, Loss: 3.6234967708587646\n",
            "Training Iteration 5044, Loss: 6.851498126983643\n",
            "Training Iteration 5045, Loss: 5.250265121459961\n",
            "Training Iteration 5046, Loss: 7.153914451599121\n",
            "Training Iteration 5047, Loss: 3.7925875186920166\n",
            "Training Iteration 5048, Loss: 6.065654277801514\n",
            "Training Iteration 5049, Loss: 7.944859504699707\n",
            "Training Iteration 5050, Loss: 4.253551483154297\n",
            "Training Iteration 5051, Loss: 7.55401611328125\n",
            "Training Iteration 5052, Loss: 4.167856693267822\n",
            "Training Iteration 5053, Loss: 5.079460620880127\n",
            "Training Iteration 5054, Loss: 2.5137741565704346\n",
            "Training Iteration 5055, Loss: 4.114744186401367\n",
            "Training Iteration 5056, Loss: 4.029496669769287\n",
            "Training Iteration 5057, Loss: 2.6169826984405518\n",
            "Training Iteration 5058, Loss: 4.142609596252441\n",
            "Training Iteration 5059, Loss: 5.880862712860107\n",
            "Training Iteration 5060, Loss: 1.6875243186950684\n",
            "Training Iteration 5061, Loss: 3.9330923557281494\n",
            "Training Iteration 5062, Loss: 6.505062580108643\n",
            "Training Iteration 5063, Loss: 2.544757843017578\n",
            "Training Iteration 5064, Loss: 6.185321807861328\n",
            "Training Iteration 5065, Loss: 4.41592264175415\n",
            "Training Iteration 5066, Loss: 2.971379518508911\n",
            "Training Iteration 5067, Loss: 3.149729013442993\n",
            "Training Iteration 5068, Loss: 2.8071749210357666\n",
            "Training Iteration 5069, Loss: 7.875825881958008\n",
            "Training Iteration 5070, Loss: 3.3437700271606445\n",
            "Training Iteration 5071, Loss: 6.533033847808838\n",
            "Training Iteration 5072, Loss: 3.3411476612091064\n",
            "Training Iteration 5073, Loss: 3.034747838973999\n",
            "Training Iteration 5074, Loss: 7.623783111572266\n",
            "Training Iteration 5075, Loss: 3.862297296524048\n",
            "Training Iteration 5076, Loss: 10.547871589660645\n",
            "Training Iteration 5077, Loss: 4.827800273895264\n",
            "Training Iteration 5078, Loss: 2.8792226314544678\n",
            "Training Iteration 5079, Loss: 5.09065055847168\n",
            "Training Iteration 5080, Loss: 5.526216983795166\n",
            "Training Iteration 5081, Loss: 3.057671070098877\n",
            "Training Iteration 5082, Loss: 3.5753865242004395\n",
            "Training Iteration 5083, Loss: 3.871786594390869\n",
            "Training Iteration 5084, Loss: 3.108369827270508\n",
            "Training Iteration 5085, Loss: 2.20961856842041\n",
            "Training Iteration 5086, Loss: 5.576523780822754\n",
            "Training Iteration 5087, Loss: 2.724404811859131\n",
            "Training Iteration 5088, Loss: 2.9681198596954346\n",
            "Training Iteration 5089, Loss: 3.1948978900909424\n",
            "Training Iteration 5090, Loss: 3.189262866973877\n",
            "Training Iteration 5091, Loss: 5.441404342651367\n",
            "Training Iteration 5092, Loss: 2.5675628185272217\n",
            "Training Iteration 5093, Loss: 5.359961032867432\n",
            "Training Iteration 5094, Loss: 1.5375752449035645\n",
            "Training Iteration 5095, Loss: 3.2853972911834717\n",
            "Training Iteration 5096, Loss: 3.4057364463806152\n",
            "Training Iteration 5097, Loss: 2.6528377532958984\n",
            "Training Iteration 5098, Loss: 6.48109769821167\n",
            "Training Iteration 5099, Loss: 3.530876398086548\n",
            "Training Iteration 5100, Loss: 5.172933101654053\n",
            "Training Iteration 5101, Loss: 6.509526252746582\n",
            "Training Iteration 5102, Loss: 6.3174285888671875\n",
            "Training Iteration 5103, Loss: 4.965951919555664\n",
            "Training Iteration 5104, Loss: 4.828779220581055\n",
            "Training Iteration 5105, Loss: 4.949347972869873\n",
            "Training Iteration 5106, Loss: 4.59642219543457\n",
            "Training Iteration 5107, Loss: 5.16224479675293\n",
            "Training Iteration 5108, Loss: 3.6800613403320312\n",
            "Training Iteration 5109, Loss: 4.2723188400268555\n",
            "Training Iteration 5110, Loss: 4.9427313804626465\n",
            "Training Iteration 5111, Loss: 4.139103889465332\n",
            "Training Iteration 5112, Loss: 3.3404502868652344\n",
            "Training Iteration 5113, Loss: 5.626617908477783\n",
            "Training Iteration 5114, Loss: 4.2473955154418945\n",
            "Training Iteration 5115, Loss: 5.529410362243652\n",
            "Training Iteration 5116, Loss: 2.400859832763672\n",
            "Training Iteration 5117, Loss: 3.3270740509033203\n",
            "Training Iteration 5118, Loss: 4.531507968902588\n",
            "Training Iteration 5119, Loss: 5.985121726989746\n",
            "Training Iteration 5120, Loss: 3.7359585762023926\n",
            "Training Iteration 5121, Loss: 4.703038692474365\n",
            "Training Iteration 5122, Loss: 3.012899160385132\n",
            "Training Iteration 5123, Loss: 3.6666598320007324\n",
            "Training Iteration 5124, Loss: 3.3553051948547363\n",
            "Training Iteration 5125, Loss: 4.0492024421691895\n",
            "Training Iteration 5126, Loss: 4.358656406402588\n",
            "Training Iteration 5127, Loss: 10.124041557312012\n",
            "Training Iteration 5128, Loss: 8.661966323852539\n",
            "Training Iteration 5129, Loss: 4.297265529632568\n",
            "Training Iteration 5130, Loss: 4.331117630004883\n",
            "Training Iteration 5131, Loss: 4.4764580726623535\n",
            "Training Iteration 5132, Loss: 3.5048587322235107\n",
            "Training Iteration 5133, Loss: 4.0757551193237305\n",
            "Training Iteration 5134, Loss: 4.268647193908691\n",
            "Training Iteration 5135, Loss: 4.148285388946533\n",
            "Training Iteration 5136, Loss: 3.317268133163452\n",
            "Training Iteration 5137, Loss: 5.437330722808838\n",
            "Training Iteration 5138, Loss: 4.613204479217529\n",
            "Training Iteration 5139, Loss: 4.728759765625\n",
            "Training Iteration 5140, Loss: 3.1131200790405273\n",
            "Training Iteration 5141, Loss: 4.681167125701904\n",
            "Training Iteration 5142, Loss: 4.6538472175598145\n",
            "Training Iteration 5143, Loss: 6.286055088043213\n",
            "Training Iteration 5144, Loss: 3.4179701805114746\n",
            "Training Iteration 5145, Loss: 4.474307060241699\n",
            "Training Iteration 5146, Loss: 3.4081084728240967\n",
            "Training Iteration 5147, Loss: 3.3662352561950684\n",
            "Training Iteration 5148, Loss: 4.119106292724609\n",
            "Training Iteration 5149, Loss: 4.303511142730713\n",
            "Training Iteration 5150, Loss: 3.0147581100463867\n",
            "Training Iteration 5151, Loss: 4.1975202560424805\n",
            "Training Iteration 5152, Loss: 5.246249198913574\n",
            "Training Iteration 5153, Loss: 2.962171792984009\n",
            "Training Iteration 5154, Loss: 6.863225936889648\n",
            "Training Iteration 5155, Loss: 8.423216819763184\n",
            "Training Iteration 5156, Loss: 3.4928836822509766\n",
            "Training Iteration 5157, Loss: 3.902820348739624\n",
            "Training Iteration 5158, Loss: 4.9214606285095215\n",
            "Training Iteration 5159, Loss: 6.718450546264648\n",
            "Training Iteration 5160, Loss: 7.92884635925293\n",
            "Training Iteration 5161, Loss: 9.70782470703125\n",
            "Training Iteration 5162, Loss: 3.303436040878296\n",
            "Training Iteration 5163, Loss: 4.216933727264404\n",
            "Training Iteration 5164, Loss: 3.6250052452087402\n",
            "Training Iteration 5165, Loss: 5.352017879486084\n",
            "Training Iteration 5166, Loss: 5.137882232666016\n",
            "Training Iteration 5167, Loss: 4.697381496429443\n",
            "Training Iteration 5168, Loss: 5.476868152618408\n",
            "Training Iteration 5169, Loss: 4.325308322906494\n",
            "Training Iteration 5170, Loss: 6.302871227264404\n",
            "Training Iteration 5171, Loss: 4.9401350021362305\n",
            "Training Iteration 5172, Loss: 5.23938512802124\n",
            "Training Iteration 5173, Loss: 3.718522548675537\n",
            "Training Iteration 5174, Loss: 5.738038063049316\n",
            "Training Iteration 5175, Loss: 5.04349946975708\n",
            "Training Iteration 5176, Loss: 3.33347749710083\n",
            "Training Iteration 5177, Loss: 2.5874009132385254\n",
            "Training Iteration 5178, Loss: 4.521001815795898\n",
            "Training Iteration 5179, Loss: 3.906359910964966\n",
            "Training Iteration 5180, Loss: 3.456123113632202\n",
            "Training Iteration 5181, Loss: 4.615270137786865\n",
            "Training Iteration 5182, Loss: 4.851170539855957\n",
            "Training Iteration 5183, Loss: 4.210720539093018\n",
            "Training Iteration 5184, Loss: 2.5700316429138184\n",
            "Training Iteration 5185, Loss: 7.889410018920898\n",
            "Training Iteration 5186, Loss: 5.068778038024902\n",
            "Training Iteration 5187, Loss: 2.778128147125244\n",
            "Training Iteration 5188, Loss: 6.021982669830322\n",
            "Training Iteration 5189, Loss: 5.806066513061523\n",
            "Training Iteration 5190, Loss: 7.223256587982178\n",
            "Training Iteration 5191, Loss: 7.581668376922607\n",
            "Training Iteration 5192, Loss: 7.9346113204956055\n",
            "Training Iteration 5193, Loss: 3.671865940093994\n",
            "Training Iteration 5194, Loss: 4.808878421783447\n",
            "Training Iteration 5195, Loss: 4.464513301849365\n",
            "Training Iteration 5196, Loss: 3.4509668350219727\n",
            "Training Iteration 5197, Loss: 6.087182521820068\n",
            "Training Iteration 5198, Loss: 5.362983226776123\n",
            "Training Iteration 5199, Loss: 4.2192840576171875\n",
            "Training Iteration 5200, Loss: 5.023440837860107\n",
            "Training Iteration 5201, Loss: 4.011549949645996\n",
            "Training Iteration 5202, Loss: 4.492847919464111\n",
            "Training Iteration 5203, Loss: 4.601731300354004\n",
            "Training Iteration 5204, Loss: 5.610963344573975\n",
            "Training Iteration 5205, Loss: 4.727645397186279\n",
            "Training Iteration 5206, Loss: 5.382790565490723\n",
            "Training Iteration 5207, Loss: 3.9355506896972656\n",
            "Training Iteration 5208, Loss: 4.177483081817627\n",
            "Training Iteration 5209, Loss: 4.171573638916016\n",
            "Training Iteration 5210, Loss: 4.483503818511963\n",
            "Training Iteration 5211, Loss: 6.1117048263549805\n",
            "Training Iteration 5212, Loss: 6.077943325042725\n",
            "Training Iteration 5213, Loss: 3.9805757999420166\n",
            "Training Iteration 5214, Loss: 4.533002853393555\n",
            "Training Iteration 5215, Loss: 3.44966197013855\n",
            "Training Iteration 5216, Loss: 7.269994735717773\n",
            "Training Iteration 5217, Loss: 5.450481414794922\n",
            "Training Iteration 5218, Loss: 3.8629157543182373\n",
            "Training Iteration 5219, Loss: 4.12645959854126\n",
            "Training Iteration 5220, Loss: 2.654209613800049\n",
            "Training Iteration 5221, Loss: 5.549731254577637\n",
            "Training Iteration 5222, Loss: 4.66071891784668\n",
            "Training Iteration 5223, Loss: 5.608964443206787\n",
            "Training Iteration 5224, Loss: 3.975935935974121\n",
            "Training Iteration 5225, Loss: 7.058021068572998\n",
            "Training Iteration 5226, Loss: 6.088557243347168\n",
            "Training Iteration 5227, Loss: 4.939863204956055\n",
            "Training Iteration 5228, Loss: 4.123152732849121\n",
            "Training Iteration 5229, Loss: 4.334662437438965\n",
            "Training Iteration 5230, Loss: 3.537323474884033\n",
            "Training Iteration 5231, Loss: 4.263666152954102\n",
            "Training Iteration 5232, Loss: 1.6275142431259155\n",
            "Training Iteration 5233, Loss: 4.255547046661377\n",
            "Training Iteration 5234, Loss: 3.4331541061401367\n",
            "Training Iteration 5235, Loss: 3.7452521324157715\n",
            "Training Iteration 5236, Loss: 7.011762619018555\n",
            "Training Iteration 5237, Loss: 3.1568613052368164\n",
            "Training Iteration 5238, Loss: 5.860628604888916\n",
            "Training Iteration 5239, Loss: 3.436708450317383\n",
            "Training Iteration 5240, Loss: 3.647432327270508\n",
            "Training Iteration 5241, Loss: 4.430184841156006\n",
            "Training Iteration 5242, Loss: 2.2022008895874023\n",
            "Training Iteration 5243, Loss: 6.224402904510498\n",
            "Training Iteration 5244, Loss: 4.111153602600098\n",
            "Training Iteration 5245, Loss: 2.3954992294311523\n",
            "Training Iteration 5246, Loss: 5.517197608947754\n",
            "Training Iteration 5247, Loss: 3.514284133911133\n",
            "Training Iteration 5248, Loss: 3.5055534839630127\n",
            "Training Iteration 5249, Loss: 7.230232238769531\n",
            "Training Iteration 5250, Loss: 5.714111804962158\n",
            "Training Iteration 5251, Loss: 5.945003986358643\n",
            "Training Iteration 5252, Loss: 5.064206600189209\n",
            "Training Iteration 5253, Loss: 7.380344390869141\n",
            "Training Iteration 5254, Loss: 6.273229598999023\n",
            "Training Iteration 5255, Loss: 4.162609577178955\n",
            "Training Iteration 5256, Loss: 6.091063976287842\n",
            "Training Iteration 5257, Loss: 2.5958213806152344\n",
            "Training Iteration 5258, Loss: 3.670468807220459\n",
            "Training Iteration 5259, Loss: 3.5618669986724854\n",
            "Training Iteration 5260, Loss: 3.3615338802337646\n",
            "Training Iteration 5261, Loss: 3.710425853729248\n",
            "Training Iteration 5262, Loss: 4.312829971313477\n",
            "Training Iteration 5263, Loss: 5.107062339782715\n",
            "Training Iteration 5264, Loss: 3.1693058013916016\n",
            "Training Iteration 5265, Loss: 2.877362012863159\n",
            "Training Iteration 5266, Loss: 2.5048420429229736\n",
            "Training Iteration 5267, Loss: 2.832719087600708\n",
            "Training Iteration 5268, Loss: 4.160861015319824\n",
            "Training Iteration 5269, Loss: 5.0474419593811035\n",
            "Training Iteration 5270, Loss: 3.7446322441101074\n",
            "Training Iteration 5271, Loss: 2.8505892753601074\n",
            "Training Iteration 5272, Loss: 5.446547031402588\n",
            "Training Iteration 5273, Loss: 8.034871101379395\n",
            "Training Iteration 5274, Loss: 5.834451675415039\n",
            "Training Iteration 5275, Loss: 4.627904415130615\n",
            "Training Iteration 5276, Loss: 1.9539947509765625\n",
            "Training Iteration 5277, Loss: 3.672877311706543\n",
            "Training Iteration 5278, Loss: 5.769953727722168\n",
            "Training Iteration 5279, Loss: 6.18141508102417\n",
            "Training Iteration 5280, Loss: 4.666123390197754\n",
            "Training Iteration 5281, Loss: 5.457423210144043\n",
            "Training Iteration 5282, Loss: 7.410575866699219\n",
            "Training Iteration 5283, Loss: 3.8148295879364014\n",
            "Training Iteration 5284, Loss: 5.172348499298096\n",
            "Training Iteration 5285, Loss: 5.182060241699219\n",
            "Training Iteration 5286, Loss: 3.819965124130249\n",
            "Training Iteration 5287, Loss: 8.060290336608887\n",
            "Training Iteration 5288, Loss: 6.503866195678711\n",
            "Training Iteration 5289, Loss: 7.623485088348389\n",
            "Training Iteration 5290, Loss: 5.039780616760254\n",
            "Training Iteration 5291, Loss: 1.8078151941299438\n",
            "Training Iteration 5292, Loss: 3.8109657764434814\n",
            "Training Iteration 5293, Loss: 4.944677352905273\n",
            "Training Iteration 5294, Loss: 7.856122016906738\n",
            "Training Iteration 5295, Loss: 4.670650959014893\n",
            "Training Iteration 5296, Loss: 2.3942673206329346\n",
            "Training Iteration 5297, Loss: 3.7499005794525146\n",
            "Training Iteration 5298, Loss: 2.383748769760132\n",
            "Training Iteration 5299, Loss: 4.247552394866943\n",
            "Training Iteration 5300, Loss: 4.796504497528076\n",
            "Training Iteration 5301, Loss: 8.876562118530273\n",
            "Training Iteration 5302, Loss: 5.377789497375488\n",
            "Training Iteration 5303, Loss: 5.427842617034912\n",
            "Training Iteration 5304, Loss: 7.563121795654297\n",
            "Training Iteration 5305, Loss: 6.386487007141113\n",
            "Training Iteration 5306, Loss: 6.366278171539307\n",
            "Training Iteration 5307, Loss: 3.3974175453186035\n",
            "Training Iteration 5308, Loss: 6.595601558685303\n",
            "Training Iteration 5309, Loss: 3.0210213661193848\n",
            "Training Iteration 5310, Loss: 5.216482162475586\n",
            "Training Iteration 5311, Loss: 4.301784515380859\n",
            "Training Iteration 5312, Loss: 5.743691444396973\n",
            "Training Iteration 5313, Loss: 4.20601224899292\n",
            "Training Iteration 5314, Loss: 7.131309509277344\n",
            "Training Iteration 5315, Loss: 6.98728609085083\n",
            "Training Iteration 5316, Loss: 3.2724545001983643\n",
            "Training Iteration 5317, Loss: 11.907950401306152\n",
            "Training Iteration 5318, Loss: 8.98370361328125\n",
            "Training Iteration 5319, Loss: 4.910760402679443\n",
            "Training Iteration 5320, Loss: 5.324728965759277\n",
            "Training Iteration 5321, Loss: 3.521048069000244\n",
            "Training Iteration 5322, Loss: 3.138190269470215\n",
            "Training Iteration 5323, Loss: 8.326064109802246\n",
            "Training Iteration 5324, Loss: 5.673498153686523\n",
            "Training Iteration 5325, Loss: 4.354377269744873\n",
            "Training Iteration 5326, Loss: 6.1971564292907715\n",
            "Training Iteration 5327, Loss: 7.343915939331055\n",
            "Training Iteration 5328, Loss: 6.010298728942871\n",
            "Training Iteration 5329, Loss: 6.690038681030273\n",
            "Training Iteration 5330, Loss: 5.069770336151123\n",
            "Training Iteration 5331, Loss: 10.581830978393555\n",
            "Training Iteration 5332, Loss: 4.789577960968018\n",
            "Training Iteration 5333, Loss: 5.973615646362305\n",
            "Training Iteration 5334, Loss: 10.497122764587402\n",
            "Training Iteration 5335, Loss: 4.30123233795166\n",
            "Training Iteration 5336, Loss: 5.159571647644043\n",
            "Training Iteration 5337, Loss: 6.057984828948975\n",
            "Training Iteration 5338, Loss: 10.66769790649414\n",
            "Training Iteration 5339, Loss: 6.003915786743164\n",
            "Training Iteration 5340, Loss: 5.296590328216553\n",
            "Training Iteration 5341, Loss: 2.3519062995910645\n",
            "Training Iteration 5342, Loss: 6.902763366699219\n",
            "Training Iteration 5343, Loss: 3.7643628120422363\n",
            "Training Iteration 5344, Loss: 7.794160842895508\n",
            "Training Iteration 5345, Loss: 4.15858268737793\n",
            "Training Iteration 5346, Loss: 4.0888824462890625\n",
            "Training Iteration 5347, Loss: 4.503012180328369\n",
            "Training Iteration 5348, Loss: 6.804814338684082\n",
            "Training Iteration 5349, Loss: 4.969521999359131\n",
            "Training Iteration 5350, Loss: 4.656163215637207\n",
            "Training Iteration 5351, Loss: 6.257809162139893\n",
            "Training Iteration 5352, Loss: 3.6428163051605225\n",
            "Training Iteration 5353, Loss: 3.4103012084960938\n",
            "Training Iteration 5354, Loss: 4.288846969604492\n",
            "Training Iteration 5355, Loss: 4.129659175872803\n",
            "Training Iteration 5356, Loss: 5.75433874130249\n",
            "Training Iteration 5357, Loss: 2.01638126373291\n",
            "Training Iteration 5358, Loss: 2.987133026123047\n",
            "Training Iteration 5359, Loss: 6.174111366271973\n",
            "Training Iteration 5360, Loss: 2.643930673599243\n",
            "Training Iteration 5361, Loss: 6.386120796203613\n",
            "Training Iteration 5362, Loss: 2.7249698638916016\n",
            "Training Iteration 5363, Loss: 2.8398165702819824\n",
            "Training Iteration 5364, Loss: 2.8970203399658203\n",
            "Training Iteration 5365, Loss: 5.258182048797607\n",
            "Training Iteration 5366, Loss: 3.3590915203094482\n",
            "Training Iteration 5367, Loss: 4.111784934997559\n",
            "Training Iteration 5368, Loss: 3.736767530441284\n",
            "Training Iteration 5369, Loss: 3.02728533744812\n",
            "Training Iteration 5370, Loss: 3.1658780574798584\n",
            "Training Iteration 5371, Loss: 3.676635503768921\n",
            "Training Iteration 5372, Loss: 5.049269676208496\n",
            "Training Iteration 5373, Loss: 5.124847412109375\n",
            "Training Iteration 5374, Loss: 6.381781578063965\n",
            "Training Iteration 5375, Loss: 5.214261054992676\n",
            "Training Iteration 5376, Loss: 2.942558526992798\n",
            "Training Iteration 5377, Loss: 2.3491125106811523\n",
            "Training Iteration 5378, Loss: 2.02142596244812\n",
            "Training Iteration 5379, Loss: 2.6514034271240234\n",
            "Training Iteration 5380, Loss: 4.210233688354492\n",
            "Training Iteration 5381, Loss: 3.84409761428833\n",
            "Training Iteration 5382, Loss: 5.435667991638184\n",
            "Training Iteration 5383, Loss: 5.080314636230469\n",
            "Training Iteration 5384, Loss: 3.2448956966400146\n",
            "Training Iteration 5385, Loss: 4.450148582458496\n",
            "Training Iteration 5386, Loss: 3.998243808746338\n",
            "Training Iteration 5387, Loss: 3.7916479110717773\n",
            "Training Iteration 5388, Loss: 2.6555612087249756\n",
            "Training Iteration 5389, Loss: 5.645215034484863\n",
            "Training Iteration 5390, Loss: 4.0873589515686035\n",
            "Training Iteration 5391, Loss: 4.070481300354004\n",
            "Training Iteration 5392, Loss: 7.683310031890869\n",
            "Training Iteration 5393, Loss: 3.055316925048828\n",
            "Training Iteration 5394, Loss: 3.109783411026001\n",
            "Training Iteration 5395, Loss: 2.931048631668091\n",
            "Training Iteration 5396, Loss: 4.1760663986206055\n",
            "Training Iteration 5397, Loss: 4.414063453674316\n",
            "Training Iteration 5398, Loss: 4.036978721618652\n",
            "Training Iteration 5399, Loss: 5.559916973114014\n",
            "Training Iteration 5400, Loss: 5.796657085418701\n",
            "Training Iteration 5401, Loss: 4.4780473709106445\n",
            "Training Iteration 5402, Loss: 4.415744781494141\n",
            "Training Iteration 5403, Loss: 4.267874717712402\n",
            "Training Iteration 5404, Loss: 4.710179805755615\n",
            "Training Iteration 5405, Loss: 5.653351306915283\n",
            "Training Iteration 5406, Loss: 4.542943477630615\n",
            "Training Iteration 5407, Loss: 4.999967575073242\n",
            "Training Iteration 5408, Loss: 6.864384651184082\n",
            "Training Iteration 5409, Loss: 2.0015835762023926\n",
            "Training Iteration 5410, Loss: 5.797755241394043\n",
            "Training Iteration 5411, Loss: 5.066871166229248\n",
            "Training Iteration 5412, Loss: 4.493330001831055\n",
            "Training Iteration 5413, Loss: 2.010770082473755\n",
            "Training Iteration 5414, Loss: 10.150566101074219\n",
            "Training Iteration 5415, Loss: 4.1422224044799805\n",
            "Training Iteration 5416, Loss: 5.234454154968262\n",
            "Training Iteration 5417, Loss: 3.128164291381836\n",
            "Training Iteration 5418, Loss: 4.083254337310791\n",
            "Training Iteration 5419, Loss: 5.010876655578613\n",
            "Training Iteration 5420, Loss: 5.29461669921875\n",
            "Training Iteration 5421, Loss: 3.0572824478149414\n",
            "Training Iteration 5422, Loss: 5.509425640106201\n",
            "Training Iteration 5423, Loss: 2.241607666015625\n",
            "Training Iteration 5424, Loss: 5.970748424530029\n",
            "Training Iteration 5425, Loss: 5.255092144012451\n",
            "Training Iteration 5426, Loss: 5.117584705352783\n",
            "Training Iteration 5427, Loss: 2.047478675842285\n",
            "Training Iteration 5428, Loss: 5.548253536224365\n",
            "Training Iteration 5429, Loss: 5.623980522155762\n",
            "Training Iteration 5430, Loss: 4.6114912033081055\n",
            "Training Iteration 5431, Loss: 2.89568829536438\n",
            "Training Iteration 5432, Loss: 3.628631353378296\n",
            "Training Iteration 5433, Loss: 3.8085837364196777\n",
            "Training Iteration 5434, Loss: 3.609436273574829\n",
            "Training Iteration 5435, Loss: 4.931231498718262\n",
            "Training Iteration 5436, Loss: 7.147527694702148\n",
            "Training Iteration 5437, Loss: 7.591839790344238\n",
            "Training Iteration 5438, Loss: 6.4433159828186035\n",
            "Training Iteration 5439, Loss: 3.4468302726745605\n",
            "Training Iteration 5440, Loss: 2.508679151535034\n",
            "Training Iteration 5441, Loss: 5.080730438232422\n",
            "Training Iteration 5442, Loss: 7.765296459197998\n",
            "Training Iteration 5443, Loss: 4.166553020477295\n",
            "Training Iteration 5444, Loss: 7.239903926849365\n",
            "Training Iteration 5445, Loss: 4.412499904632568\n",
            "Training Iteration 5446, Loss: 5.629589080810547\n",
            "Training Iteration 5447, Loss: 5.256261348724365\n",
            "Training Iteration 5448, Loss: 2.511014699935913\n",
            "Training Iteration 5449, Loss: 3.632556438446045\n",
            "Training Iteration 5450, Loss: 4.189442157745361\n",
            "Training Iteration 5451, Loss: 4.819035530090332\n",
            "Training Iteration 5452, Loss: 5.2680888175964355\n",
            "Training Iteration 5453, Loss: 5.176584720611572\n",
            "Training Iteration 5454, Loss: 6.008728981018066\n",
            "Training Iteration 5455, Loss: 4.8015923500061035\n",
            "Training Iteration 5456, Loss: 3.844147205352783\n",
            "Training Iteration 5457, Loss: 1.8036081790924072\n",
            "Training Iteration 5458, Loss: 6.360633850097656\n",
            "Training Iteration 5459, Loss: 5.267344951629639\n",
            "Training Iteration 5460, Loss: 6.533107757568359\n",
            "Training Iteration 5461, Loss: 1.9170695543289185\n",
            "Training Iteration 5462, Loss: 5.721164226531982\n",
            "Training Iteration 5463, Loss: 6.437791347503662\n",
            "Training Iteration 5464, Loss: 3.892336845397949\n",
            "Training Iteration 5465, Loss: 5.5929765701293945\n",
            "Training Iteration 5466, Loss: 5.94960355758667\n",
            "Training Iteration 5467, Loss: 6.667428016662598\n",
            "Training Iteration 5468, Loss: 3.122711181640625\n",
            "Training Iteration 5469, Loss: 2.0586416721343994\n",
            "Training Iteration 5470, Loss: 3.453549385070801\n",
            "Training Iteration 5471, Loss: 5.341731071472168\n",
            "Training Iteration 5472, Loss: 5.166356563568115\n",
            "Training Iteration 5473, Loss: 5.397061824798584\n",
            "Training Iteration 5474, Loss: 2.1690499782562256\n",
            "Training Iteration 5475, Loss: 4.366765975952148\n",
            "Training Iteration 5476, Loss: 6.1487226486206055\n",
            "Training Iteration 5477, Loss: 10.326642990112305\n",
            "Training Iteration 5478, Loss: 3.4168405532836914\n",
            "Training Iteration 5479, Loss: 5.727667808532715\n",
            "Training Iteration 5480, Loss: 6.996608734130859\n",
            "Training Iteration 5481, Loss: 4.775360107421875\n",
            "Training Iteration 5482, Loss: 4.698878765106201\n",
            "Training Iteration 5483, Loss: 3.079221248626709\n",
            "Training Iteration 5484, Loss: 4.605100154876709\n",
            "Training Iteration 5485, Loss: 8.365379333496094\n",
            "Training Iteration 5486, Loss: 8.99250602722168\n",
            "Training Iteration 5487, Loss: 7.191326141357422\n",
            "Training Iteration 5488, Loss: 3.9033596515655518\n",
            "Training Iteration 5489, Loss: 5.006316184997559\n",
            "Training Iteration 5490, Loss: 3.6684131622314453\n",
            "Training Iteration 5491, Loss: 4.276474475860596\n",
            "Training Iteration 5492, Loss: 4.797051429748535\n",
            "Training Iteration 5493, Loss: 3.9974679946899414\n",
            "Training Iteration 5494, Loss: 2.8580291271209717\n",
            "Training Iteration 5495, Loss: 7.977208137512207\n",
            "Training Iteration 5496, Loss: 1.5923517942428589\n",
            "Training Iteration 5497, Loss: 6.601889133453369\n",
            "Training Iteration 5498, Loss: 3.716362714767456\n",
            "Training Iteration 5499, Loss: 3.0273919105529785\n",
            "Training Iteration 5500, Loss: 5.359298229217529\n",
            "Training Iteration 5501, Loss: 6.143791675567627\n",
            "Training Iteration 5502, Loss: 5.942557334899902\n",
            "Training Iteration 5503, Loss: 6.92592716217041\n",
            "Training Iteration 5504, Loss: 1.6265798807144165\n",
            "Training Iteration 5505, Loss: 3.6508734226226807\n",
            "Training Iteration 5506, Loss: 2.7617006301879883\n",
            "Training Iteration 5507, Loss: 2.361562967300415\n",
            "Training Iteration 5508, Loss: 5.604763984680176\n",
            "Training Iteration 5509, Loss: 5.853075981140137\n",
            "Training Iteration 5510, Loss: 9.468742370605469\n",
            "Training Iteration 5511, Loss: 5.961743354797363\n",
            "Training Iteration 5512, Loss: 2.243048667907715\n",
            "Training Iteration 5513, Loss: 4.134890079498291\n",
            "Training Iteration 5514, Loss: 1.6952906847000122\n",
            "Training Iteration 5515, Loss: 3.9619107246398926\n",
            "Training Iteration 5516, Loss: 3.6873042583465576\n",
            "Training Iteration 5517, Loss: 4.487010955810547\n",
            "Training Iteration 5518, Loss: 4.983717918395996\n",
            "Training Iteration 5519, Loss: 2.9352405071258545\n",
            "Training Iteration 5520, Loss: 6.009204864501953\n",
            "Training Iteration 5521, Loss: 2.275377035140991\n",
            "Training Iteration 5522, Loss: 2.564603567123413\n",
            "Training Iteration 5523, Loss: 4.062360763549805\n",
            "Training Iteration 5524, Loss: 1.9983491897583008\n",
            "Training Iteration 5525, Loss: 7.309781074523926\n",
            "Training Iteration 5526, Loss: 2.2863388061523438\n",
            "Training Iteration 5527, Loss: 2.984816074371338\n",
            "Training Iteration 5528, Loss: 4.6884870529174805\n",
            "Training Iteration 5529, Loss: 3.3161444664001465\n",
            "Training Iteration 5530, Loss: 3.1972429752349854\n",
            "Training Iteration 5531, Loss: 5.872715950012207\n",
            "Training Iteration 5532, Loss: 5.108686923980713\n",
            "Training Iteration 5533, Loss: 7.229034900665283\n",
            "Training Iteration 5534, Loss: 4.36384391784668\n",
            "Training Iteration 5535, Loss: 4.7400641441345215\n",
            "Training Iteration 5536, Loss: 3.5775561332702637\n",
            "Training Iteration 5537, Loss: 7.2964982986450195\n",
            "Training Iteration 5538, Loss: 5.56005859375\n",
            "Training Iteration 5539, Loss: 2.111311912536621\n",
            "Training Iteration 5540, Loss: 2.840019464492798\n",
            "Training Iteration 5541, Loss: 4.586082935333252\n",
            "Training Iteration 5542, Loss: 3.234449863433838\n",
            "Training Iteration 5543, Loss: 5.263336658477783\n",
            "Training Iteration 5544, Loss: 3.2365005016326904\n",
            "Training Iteration 5545, Loss: 5.213070392608643\n",
            "Training Iteration 5546, Loss: 3.9939584732055664\n",
            "Training Iteration 5547, Loss: 2.351029396057129\n",
            "Training Iteration 5548, Loss: 2.2048521041870117\n",
            "Training Iteration 5549, Loss: 5.4873199462890625\n",
            "Training Iteration 5550, Loss: 2.8441758155822754\n",
            "Training Iteration 5551, Loss: 3.7097654342651367\n",
            "Training Iteration 5552, Loss: 6.234712600708008\n",
            "Training Iteration 5553, Loss: 4.850647926330566\n",
            "Training Iteration 5554, Loss: 4.78656530380249\n",
            "Training Iteration 5555, Loss: 6.356993198394775\n",
            "Training Iteration 5556, Loss: 4.847002029418945\n",
            "Training Iteration 5557, Loss: 4.2903618812561035\n",
            "Training Iteration 5558, Loss: 2.8232316970825195\n",
            "Training Iteration 5559, Loss: 5.347322463989258\n",
            "Training Iteration 5560, Loss: 5.465690612792969\n",
            "Training Iteration 5561, Loss: 3.9294917583465576\n",
            "Training Iteration 5562, Loss: 3.8146729469299316\n",
            "Training Iteration 5563, Loss: 7.141990661621094\n",
            "Training Iteration 5564, Loss: 3.6381218433380127\n",
            "Training Iteration 5565, Loss: 5.287054061889648\n",
            "Training Iteration 5566, Loss: 3.2257602214813232\n",
            "Training Iteration 5567, Loss: 4.817336559295654\n",
            "Training Iteration 5568, Loss: 3.0615060329437256\n",
            "Training Iteration 5569, Loss: 8.548856735229492\n",
            "Training Iteration 5570, Loss: 6.805495262145996\n",
            "Training Iteration 5571, Loss: 4.2569756507873535\n",
            "Training Iteration 5572, Loss: 4.494542121887207\n",
            "Training Iteration 5573, Loss: 3.5119266510009766\n",
            "Training Iteration 5574, Loss: 5.5174689292907715\n",
            "Training Iteration 5575, Loss: 6.104795932769775\n",
            "Training Iteration 5576, Loss: 2.554718017578125\n",
            "Training Iteration 5577, Loss: 4.393274307250977\n",
            "Training Iteration 5578, Loss: 6.433157920837402\n",
            "Training Iteration 5579, Loss: 9.150023460388184\n",
            "Training Iteration 5580, Loss: 4.202116966247559\n",
            "Training Iteration 5581, Loss: 3.7695610523223877\n",
            "Training Iteration 5582, Loss: 2.6153204441070557\n",
            "Training Iteration 5583, Loss: 3.4460012912750244\n",
            "Training Iteration 5584, Loss: 9.133838653564453\n",
            "Training Iteration 5585, Loss: 4.587349891662598\n",
            "Training Iteration 5586, Loss: 4.610996723175049\n",
            "Training Iteration 5587, Loss: 1.6938635110855103\n",
            "Training Iteration 5588, Loss: 7.412087917327881\n",
            "Training Iteration 5589, Loss: 7.140610694885254\n",
            "Training Iteration 5590, Loss: 7.565454483032227\n",
            "Training Iteration 5591, Loss: 5.094033241271973\n",
            "Training Iteration 5592, Loss: 4.044463157653809\n",
            "Training Iteration 5593, Loss: 5.679790019989014\n",
            "Training Iteration 5594, Loss: 3.6067521572113037\n",
            "Training Iteration 5595, Loss: 4.292057514190674\n",
            "Training Iteration 5596, Loss: 4.634984970092773\n",
            "Training Iteration 5597, Loss: 5.249940872192383\n",
            "Training Iteration 5598, Loss: 5.296263694763184\n",
            "Training Iteration 5599, Loss: 4.29425048828125\n",
            "Training Iteration 5600, Loss: 5.415539264678955\n",
            "Training Iteration 5601, Loss: 4.239274024963379\n",
            "Training Iteration 5602, Loss: 3.9961206912994385\n",
            "Training Iteration 5603, Loss: 3.5269668102264404\n",
            "Training Iteration 5604, Loss: 2.861140727996826\n",
            "Training Iteration 5605, Loss: 5.514917373657227\n",
            "Training Iteration 5606, Loss: 4.7187676429748535\n",
            "Training Iteration 5607, Loss: 3.0316011905670166\n",
            "Training Iteration 5608, Loss: 3.2007133960723877\n",
            "Training Iteration 5609, Loss: 4.449039936065674\n",
            "Training Iteration 5610, Loss: 6.257347106933594\n",
            "Training Iteration 5611, Loss: 2.7987208366394043\n",
            "Training Iteration 5612, Loss: 4.968388557434082\n",
            "Training Iteration 5613, Loss: 3.5702450275421143\n",
            "Training Iteration 5614, Loss: 7.24141263961792\n",
            "Training Iteration 5615, Loss: 6.571254253387451\n",
            "Training Iteration 5616, Loss: 2.472074270248413\n",
            "Training Iteration 5617, Loss: 6.521481990814209\n",
            "Training Iteration 5618, Loss: 3.914478302001953\n",
            "Training Iteration 5619, Loss: 5.628351211547852\n",
            "Training Iteration 5620, Loss: 4.482853889465332\n",
            "Training Iteration 5621, Loss: 3.4590539932250977\n",
            "Training Iteration 5622, Loss: 3.4893739223480225\n",
            "Training Iteration 5623, Loss: 3.499213218688965\n",
            "Training Iteration 5624, Loss: 5.847586154937744\n",
            "Training Iteration 5625, Loss: 2.4792449474334717\n",
            "Training Iteration 5626, Loss: 1.8137058019638062\n",
            "Training Iteration 5627, Loss: 7.060803413391113\n",
            "Training Iteration 5628, Loss: 4.485004425048828\n",
            "Training Iteration 5629, Loss: 2.4973795413970947\n",
            "Training Iteration 5630, Loss: 4.5657782554626465\n",
            "Training Iteration 5631, Loss: 2.556927442550659\n",
            "Training Iteration 5632, Loss: 3.713778018951416\n",
            "Training Iteration 5633, Loss: 5.183403491973877\n",
            "Training Iteration 5634, Loss: 3.70479154586792\n",
            "Training Iteration 5635, Loss: 3.163850784301758\n",
            "Training Iteration 5636, Loss: 3.2831246852874756\n",
            "Training Iteration 5637, Loss: 2.1668927669525146\n",
            "Training Iteration 5638, Loss: 8.767786026000977\n",
            "Training Iteration 5639, Loss: 6.3101396560668945\n",
            "Training Iteration 5640, Loss: 3.3813819885253906\n",
            "Training Iteration 5641, Loss: 4.861031532287598\n",
            "Training Iteration 5642, Loss: 6.223588943481445\n",
            "Training Iteration 5643, Loss: 5.869025707244873\n",
            "Training Iteration 5644, Loss: 4.572668075561523\n",
            "Training Iteration 5645, Loss: 4.262011528015137\n",
            "Training Iteration 5646, Loss: 2.9338860511779785\n",
            "Training Iteration 5647, Loss: 3.4964911937713623\n",
            "Training Iteration 5648, Loss: 4.046512126922607\n",
            "Training Iteration 5649, Loss: 1.85093092918396\n",
            "Training Iteration 5650, Loss: 6.504082202911377\n",
            "Training Iteration 5651, Loss: 5.859976291656494\n",
            "Training Iteration 5652, Loss: 9.362690925598145\n",
            "Training Iteration 5653, Loss: 6.919357776641846\n",
            "Training Iteration 5654, Loss: 3.0911705493927\n",
            "Training Iteration 5655, Loss: 2.370558023452759\n",
            "Training Iteration 5656, Loss: 2.3973264694213867\n",
            "Training Iteration 5657, Loss: 5.1244425773620605\n",
            "Training Iteration 5658, Loss: 10.66888427734375\n",
            "Training Iteration 5659, Loss: 5.668724536895752\n",
            "Training Iteration 5660, Loss: 2.7928342819213867\n",
            "Training Iteration 5661, Loss: 5.950845241546631\n",
            "Training Iteration 5662, Loss: 3.8769006729125977\n",
            "Training Iteration 5663, Loss: 3.7425973415374756\n",
            "Training Iteration 5664, Loss: 6.712490558624268\n",
            "Training Iteration 5665, Loss: 5.0517258644104\n",
            "Training Iteration 5666, Loss: 4.540368556976318\n",
            "Training Iteration 5667, Loss: 5.36904764175415\n",
            "Training Iteration 5668, Loss: 2.416295289993286\n",
            "Training Iteration 5669, Loss: 2.301645517349243\n",
            "Training Iteration 5670, Loss: 4.3793721199035645\n",
            "Training Iteration 5671, Loss: 7.557173252105713\n",
            "Training Iteration 5672, Loss: 6.147163391113281\n",
            "Training Iteration 5673, Loss: 2.4141697883605957\n",
            "Training Iteration 5674, Loss: 2.759840965270996\n",
            "Training Iteration 5675, Loss: 2.9695491790771484\n",
            "Training Iteration 5676, Loss: 4.875161647796631\n",
            "Training Iteration 5677, Loss: 1.697888731956482\n",
            "Training Iteration 5678, Loss: 5.8218183517456055\n",
            "Training Iteration 5679, Loss: 6.661802291870117\n",
            "Training Iteration 5680, Loss: 5.581117630004883\n",
            "Training Iteration 5681, Loss: 3.786533832550049\n",
            "Training Iteration 5682, Loss: 3.401263475418091\n",
            "Training Iteration 5683, Loss: 6.220398902893066\n",
            "Training Iteration 5684, Loss: 7.1168317794799805\n",
            "Training Iteration 5685, Loss: 6.47150993347168\n",
            "Training Iteration 5686, Loss: 6.23110818862915\n",
            "Training Iteration 5687, Loss: 3.677156925201416\n",
            "Training Iteration 5688, Loss: 4.189948081970215\n",
            "Training Iteration 5689, Loss: 4.612168788909912\n",
            "Training Iteration 5690, Loss: 3.7739434242248535\n",
            "Training Iteration 5691, Loss: 3.8151392936706543\n",
            "Training Iteration 5692, Loss: 2.673157215118408\n",
            "Training Iteration 5693, Loss: 3.2395055294036865\n",
            "Training Iteration 5694, Loss: 4.127926826477051\n",
            "Training Iteration 5695, Loss: 3.406399726867676\n",
            "Training Iteration 5696, Loss: 3.7761569023132324\n",
            "Training Iteration 5697, Loss: 5.574708938598633\n",
            "Training Iteration 5698, Loss: 9.23917293548584\n",
            "Training Iteration 5699, Loss: 3.340564250946045\n",
            "Training Iteration 5700, Loss: 3.3870882987976074\n",
            "Training Iteration 5701, Loss: 4.17081356048584\n",
            "Training Iteration 5702, Loss: 2.494908094406128\n",
            "Training Iteration 5703, Loss: 5.386147499084473\n",
            "Training Iteration 5704, Loss: 5.743137836456299\n",
            "Training Iteration 5705, Loss: 3.7369637489318848\n",
            "Training Iteration 5706, Loss: 4.186063289642334\n",
            "Training Iteration 5707, Loss: 3.5646395683288574\n",
            "Training Iteration 5708, Loss: 4.663821220397949\n",
            "Training Iteration 5709, Loss: 4.847054958343506\n",
            "Training Iteration 5710, Loss: 2.526069402694702\n",
            "Training Iteration 5711, Loss: 3.5019187927246094\n",
            "Training Iteration 5712, Loss: 5.630587100982666\n",
            "Training Iteration 5713, Loss: 3.5465378761291504\n",
            "Training Iteration 5714, Loss: 5.157675743103027\n",
            "Training Iteration 5715, Loss: 4.0672407150268555\n",
            "Training Iteration 5716, Loss: 7.345223903656006\n",
            "Training Iteration 5717, Loss: 6.287343978881836\n",
            "Training Iteration 5718, Loss: 7.523111820220947\n",
            "Training Iteration 5719, Loss: 5.232945919036865\n",
            "Training Iteration 5720, Loss: 2.3156964778900146\n",
            "Training Iteration 5721, Loss: 3.364466428756714\n",
            "Training Iteration 5722, Loss: 3.7092676162719727\n",
            "Training Iteration 5723, Loss: 5.694936752319336\n",
            "Training Iteration 5724, Loss: 7.52049446105957\n",
            "Training Iteration 5725, Loss: 8.121917724609375\n",
            "Training Iteration 5726, Loss: 4.122750759124756\n",
            "Training Iteration 5727, Loss: 4.220766067504883\n",
            "Training Iteration 5728, Loss: 6.726977825164795\n",
            "Training Iteration 5729, Loss: 8.082046508789062\n",
            "Training Iteration 5730, Loss: 4.359906196594238\n",
            "Training Iteration 5731, Loss: 7.047377109527588\n",
            "Training Iteration 5732, Loss: 8.279082298278809\n",
            "Training Iteration 5733, Loss: 3.567436933517456\n",
            "Training Iteration 5734, Loss: 3.5511879920959473\n",
            "Training Iteration 5735, Loss: 8.242998123168945\n",
            "Training Iteration 5736, Loss: 2.3386049270629883\n",
            "Training Iteration 5737, Loss: 2.7346112728118896\n",
            "Training Iteration 5738, Loss: 2.6143860816955566\n",
            "Training Iteration 5739, Loss: 3.7043676376342773\n",
            "Training Iteration 5740, Loss: 8.27380084991455\n",
            "Training Iteration 5741, Loss: 6.368112087249756\n",
            "Training Iteration 5742, Loss: 6.821660995483398\n",
            "Training Iteration 5743, Loss: 4.749511241912842\n",
            "Training Iteration 5744, Loss: 3.096552848815918\n",
            "Training Iteration 5745, Loss: 2.9581503868103027\n",
            "Training Iteration 5746, Loss: 4.280773162841797\n",
            "Training Iteration 5747, Loss: 5.1021342277526855\n",
            "Training Iteration 5748, Loss: 3.6563332080841064\n",
            "Training Iteration 5749, Loss: 2.5911922454833984\n",
            "Training Iteration 5750, Loss: 4.559540748596191\n",
            "Training Iteration 5751, Loss: 2.6725826263427734\n",
            "Training Iteration 5752, Loss: 3.1730480194091797\n",
            "Training Iteration 5753, Loss: 5.793837070465088\n",
            "Training Iteration 5754, Loss: 4.35692834854126\n",
            "Training Iteration 5755, Loss: 4.960224151611328\n",
            "Training Iteration 5756, Loss: 4.771632194519043\n",
            "Training Iteration 5757, Loss: 4.558813571929932\n",
            "Training Iteration 5758, Loss: 3.4884233474731445\n",
            "Training Iteration 5759, Loss: 7.264190673828125\n",
            "Training Iteration 5760, Loss: 2.9067471027374268\n",
            "Training Iteration 5761, Loss: 2.8628602027893066\n",
            "Training Iteration 5762, Loss: 5.017210006713867\n",
            "Training Iteration 5763, Loss: 6.070513725280762\n",
            "Training Iteration 5764, Loss: 6.414787769317627\n",
            "Training Iteration 5765, Loss: 5.80497932434082\n",
            "Training Iteration 5766, Loss: 6.329809188842773\n",
            "Training Iteration 5767, Loss: 2.0066072940826416\n",
            "Training Iteration 5768, Loss: 8.360447883605957\n",
            "Training Iteration 5769, Loss: 7.143325328826904\n",
            "Training Iteration 5770, Loss: 5.816289901733398\n",
            "Training Iteration 5771, Loss: 7.523766994476318\n",
            "Training Iteration 5772, Loss: 4.641531467437744\n",
            "Training Iteration 5773, Loss: 3.8211913108825684\n",
            "Training Iteration 5774, Loss: 4.199770450592041\n",
            "Training Iteration 5775, Loss: 7.112740516662598\n",
            "Training Iteration 5776, Loss: 7.336030006408691\n",
            "Training Iteration 5777, Loss: 4.777556896209717\n",
            "Training Iteration 5778, Loss: 4.595705986022949\n",
            "Training Iteration 5779, Loss: 3.8328115940093994\n",
            "Training Iteration 5780, Loss: 3.5954301357269287\n",
            "Training Iteration 5781, Loss: 5.634734153747559\n",
            "Training Iteration 5782, Loss: 2.782400131225586\n",
            "Training Iteration 5783, Loss: 4.3337602615356445\n",
            "Training Iteration 5784, Loss: 2.478771686553955\n",
            "Training Iteration 5785, Loss: 4.607465744018555\n",
            "Training Iteration 5786, Loss: 2.7972187995910645\n",
            "Training Iteration 5787, Loss: 10.096151351928711\n",
            "Training Iteration 5788, Loss: 3.002619981765747\n",
            "Training Iteration 5789, Loss: 4.617833137512207\n",
            "Training Iteration 5790, Loss: 4.309604167938232\n",
            "Training Iteration 5791, Loss: 4.454475402832031\n",
            "Training Iteration 5792, Loss: 4.134523391723633\n",
            "Training Iteration 5793, Loss: 5.0655436515808105\n",
            "Training Iteration 5794, Loss: 5.9925537109375\n",
            "Training Iteration 5795, Loss: 2.6213018894195557\n",
            "Training Iteration 5796, Loss: 7.021986961364746\n",
            "Training Iteration 5797, Loss: 3.5875742435455322\n",
            "Training Iteration 5798, Loss: 4.760642051696777\n",
            "Training Iteration 5799, Loss: 2.8338522911071777\n",
            "Training Iteration 5800, Loss: 5.364490032196045\n",
            "Training Iteration 5801, Loss: 2.659759521484375\n",
            "Training Iteration 5802, Loss: 4.0913238525390625\n",
            "Training Iteration 5803, Loss: 2.9055652618408203\n",
            "Training Iteration 5804, Loss: 5.553963661193848\n",
            "Training Iteration 5805, Loss: 3.8555245399475098\n",
            "Training Iteration 5806, Loss: 6.081936836242676\n",
            "Training Iteration 5807, Loss: 6.069687843322754\n",
            "Training Iteration 5808, Loss: 4.003593921661377\n",
            "Training Iteration 5809, Loss: 5.671727180480957\n",
            "Training Iteration 5810, Loss: 2.762211561203003\n",
            "Training Iteration 5811, Loss: 3.9748034477233887\n",
            "Training Iteration 5812, Loss: 6.144592761993408\n",
            "Training Iteration 5813, Loss: 7.885930061340332\n",
            "Training Iteration 5814, Loss: 4.574375152587891\n",
            "Training Iteration 5815, Loss: 3.122713565826416\n",
            "Training Iteration 5816, Loss: 6.530327796936035\n",
            "Training Iteration 5817, Loss: 4.440088748931885\n",
            "Training Iteration 5818, Loss: 2.9495882987976074\n",
            "Training Iteration 5819, Loss: 3.3081092834472656\n",
            "Training Iteration 5820, Loss: 3.4260408878326416\n",
            "Training Iteration 5821, Loss: 3.8628904819488525\n",
            "Training Iteration 5822, Loss: 7.31007719039917\n",
            "Training Iteration 5823, Loss: 6.516341209411621\n",
            "Training Iteration 5824, Loss: 5.574036121368408\n",
            "Training Iteration 5825, Loss: 3.65305233001709\n",
            "Training Iteration 5826, Loss: 4.847205638885498\n",
            "Training Iteration 5827, Loss: 3.622756004333496\n",
            "Training Iteration 5828, Loss: 4.19409704208374\n",
            "Training Iteration 5829, Loss: 8.08197021484375\n",
            "Training Iteration 5830, Loss: 3.801368236541748\n",
            "Training Iteration 5831, Loss: 7.270358562469482\n",
            "Training Iteration 5832, Loss: 6.125950336456299\n",
            "Training Iteration 5833, Loss: 5.425865173339844\n",
            "Training Iteration 5834, Loss: 4.046765327453613\n",
            "Training Iteration 5835, Loss: 1.996092438697815\n",
            "Training Iteration 5836, Loss: 5.127353668212891\n",
            "Training Iteration 5837, Loss: 7.35391902923584\n",
            "Training Iteration 5838, Loss: 5.534106254577637\n",
            "Training Iteration 5839, Loss: 5.339215278625488\n",
            "Training Iteration 5840, Loss: 6.538051605224609\n",
            "Training Iteration 5841, Loss: 8.26855754852295\n",
            "Training Iteration 5842, Loss: 5.830901145935059\n",
            "Training Iteration 5843, Loss: 6.478848457336426\n",
            "Training Iteration 5844, Loss: 5.289292812347412\n",
            "Training Iteration 5845, Loss: 3.8687310218811035\n",
            "Training Iteration 5846, Loss: 3.209887981414795\n",
            "Training Iteration 5847, Loss: 2.7001326084136963\n",
            "Training Iteration 5848, Loss: 2.07266902923584\n",
            "Training Iteration 5849, Loss: 3.7094500064849854\n",
            "Training Iteration 5850, Loss: 7.059234619140625\n",
            "Training Iteration 5851, Loss: 4.986735820770264\n",
            "Training Iteration 5852, Loss: 2.0036778450012207\n",
            "Training Iteration 5853, Loss: 3.5039186477661133\n",
            "Training Iteration 5854, Loss: 3.7912020683288574\n",
            "Training Iteration 5855, Loss: 6.675215244293213\n",
            "Training Iteration 5856, Loss: 6.429983139038086\n",
            "Training Iteration 5857, Loss: 9.284043312072754\n",
            "Training Iteration 5858, Loss: 3.044286012649536\n",
            "Training Iteration 5859, Loss: 4.3026323318481445\n",
            "Training Iteration 5860, Loss: 5.410117149353027\n",
            "Training Iteration 5861, Loss: 8.254842758178711\n",
            "Training Iteration 5862, Loss: 2.9014406204223633\n",
            "Training Iteration 5863, Loss: 4.457551002502441\n",
            "Training Iteration 5864, Loss: 4.043997287750244\n",
            "Training Iteration 5865, Loss: 3.1544008255004883\n",
            "Training Iteration 5866, Loss: 7.388012886047363\n",
            "Training Iteration 5867, Loss: 4.901212215423584\n",
            "Training Iteration 5868, Loss: 7.292713165283203\n",
            "Training Iteration 5869, Loss: 4.693634033203125\n",
            "Training Iteration 5870, Loss: 2.982635021209717\n",
            "Training Iteration 5871, Loss: 3.208156108856201\n",
            "Training Iteration 5872, Loss: 3.614532470703125\n",
            "Training Iteration 5873, Loss: 3.7443976402282715\n",
            "Training Iteration 5874, Loss: 2.367643356323242\n",
            "Training Iteration 5875, Loss: 1.9228070974349976\n",
            "Training Iteration 5876, Loss: 8.321488380432129\n",
            "Training Iteration 5877, Loss: 4.202059745788574\n",
            "Training Iteration 5878, Loss: 2.2620320320129395\n",
            "Training Iteration 5879, Loss: 3.5270133018493652\n",
            "Training Iteration 5880, Loss: 6.3656415939331055\n",
            "Training Iteration 5881, Loss: 3.394195795059204\n",
            "Training Iteration 5882, Loss: 2.358431339263916\n",
            "Training Iteration 5883, Loss: 7.264912128448486\n",
            "Training Iteration 5884, Loss: 4.002890586853027\n",
            "Training Iteration 5885, Loss: 7.781006336212158\n",
            "Training Iteration 5886, Loss: 5.48258113861084\n",
            "Training Iteration 5887, Loss: 1.9465417861938477\n",
            "Training Iteration 5888, Loss: 14.166494369506836\n",
            "Training Iteration 5889, Loss: 5.400271892547607\n",
            "Training Iteration 5890, Loss: 7.511878967285156\n",
            "Training Iteration 5891, Loss: 11.03971004486084\n",
            "Training Iteration 5892, Loss: 5.041752815246582\n",
            "Training Iteration 5893, Loss: 6.274746894836426\n",
            "Training Iteration 5894, Loss: 4.376028537750244\n",
            "Training Iteration 5895, Loss: 7.065510272979736\n",
            "Training Iteration 5896, Loss: 3.3018741607666016\n",
            "Training Iteration 5897, Loss: 4.123242378234863\n",
            "Training Iteration 5898, Loss: 2.638828992843628\n",
            "Training Iteration 5899, Loss: 3.607503890991211\n",
            "Training Iteration 5900, Loss: 3.8826727867126465\n",
            "Training Iteration 5901, Loss: 3.8569436073303223\n",
            "Training Iteration 5902, Loss: 3.073098659515381\n",
            "Training Iteration 5903, Loss: 4.372708320617676\n",
            "Training Iteration 5904, Loss: 2.428455352783203\n",
            "Training Iteration 5905, Loss: 2.815544843673706\n",
            "Training Iteration 5906, Loss: 7.379270076751709\n",
            "Training Iteration 5907, Loss: 4.337550163269043\n",
            "Training Iteration 5908, Loss: 2.9107255935668945\n",
            "Training Iteration 5909, Loss: 6.064755439758301\n",
            "Training Iteration 5910, Loss: 5.449726104736328\n",
            "Training Iteration 5911, Loss: 3.434140920639038\n",
            "Training Iteration 5912, Loss: 4.632260799407959\n",
            "Training Iteration 5913, Loss: 3.1786434650421143\n",
            "Training Iteration 5914, Loss: 3.444005012512207\n",
            "Training Iteration 5915, Loss: 6.183361053466797\n",
            "Training Iteration 5916, Loss: 5.082158088684082\n",
            "Training Iteration 5917, Loss: 3.9033570289611816\n",
            "Training Iteration 5918, Loss: 2.140688419342041\n",
            "Training Iteration 5919, Loss: 9.409046173095703\n",
            "Training Iteration 5920, Loss: 3.2098042964935303\n",
            "Training Iteration 5921, Loss: 4.950280666351318\n",
            "Training Iteration 5922, Loss: 3.5018863677978516\n",
            "Training Iteration 5923, Loss: 7.798180103302002\n",
            "Training Iteration 5924, Loss: 7.246467590332031\n",
            "Training Iteration 5925, Loss: 3.143829107284546\n",
            "Training Iteration 5926, Loss: 4.102144718170166\n",
            "Training Iteration 5927, Loss: 3.548877239227295\n",
            "Training Iteration 5928, Loss: 3.6020562648773193\n",
            "Training Iteration 5929, Loss: 6.870759963989258\n",
            "Training Iteration 5930, Loss: 3.7402915954589844\n",
            "Training Iteration 5931, Loss: 1.9023524522781372\n",
            "Training Iteration 5932, Loss: 4.437602519989014\n",
            "Training Iteration 5933, Loss: 1.938515067100525\n",
            "Training Iteration 5934, Loss: 4.112387180328369\n",
            "Training Iteration 5935, Loss: 2.7401208877563477\n",
            "Training Iteration 5936, Loss: 4.218059539794922\n",
            "Training Iteration 5937, Loss: 2.8275492191314697\n",
            "Training Iteration 5938, Loss: 2.2049829959869385\n",
            "Training Iteration 5939, Loss: 3.0415515899658203\n",
            "Training Iteration 5940, Loss: 3.3373236656188965\n",
            "Training Iteration 5941, Loss: 3.7336714267730713\n",
            "Training Iteration 5942, Loss: 7.106635570526123\n",
            "Training Iteration 5943, Loss: 2.0953562259674072\n",
            "Training Iteration 5944, Loss: 4.37713623046875\n",
            "Training Iteration 5945, Loss: 4.842519283294678\n",
            "Training Iteration 5946, Loss: 7.127262592315674\n",
            "Training Iteration 5947, Loss: 2.6101365089416504\n",
            "Training Iteration 5948, Loss: 3.906480312347412\n",
            "Training Iteration 5949, Loss: 4.626879692077637\n",
            "Training Iteration 5950, Loss: 3.9842729568481445\n",
            "Training Iteration 5951, Loss: 5.122597694396973\n",
            "Training Iteration 5952, Loss: 3.7899017333984375\n",
            "Training Iteration 5953, Loss: 3.1388516426086426\n",
            "Training Iteration 5954, Loss: 3.106395959854126\n",
            "Training Iteration 5955, Loss: 2.1693480014801025\n",
            "Training Iteration 5956, Loss: 3.5696942806243896\n",
            "Training Iteration 5957, Loss: 4.5033135414123535\n",
            "Training Iteration 5958, Loss: 2.669987201690674\n",
            "Training Iteration 5959, Loss: 4.261815547943115\n",
            "Training Iteration 5960, Loss: 7.246647834777832\n",
            "Training Iteration 5961, Loss: 6.787855625152588\n",
            "Training Iteration 5962, Loss: 4.750637531280518\n",
            "Training Iteration 5963, Loss: 3.5991666316986084\n",
            "Training Iteration 5964, Loss: 3.106281042098999\n",
            "Training Iteration 5965, Loss: 5.228507041931152\n",
            "Training Iteration 5966, Loss: 4.621262073516846\n",
            "Training Iteration 5967, Loss: 3.2444868087768555\n",
            "Training Iteration 5968, Loss: 5.692915916442871\n",
            "Training Iteration 5969, Loss: 5.426653861999512\n",
            "Training Iteration 5970, Loss: 2.6821141242980957\n",
            "Training Iteration 5971, Loss: 4.41769552230835\n",
            "Training Iteration 5972, Loss: 5.506693363189697\n",
            "Training Iteration 5973, Loss: 3.50376033782959\n",
            "Training Iteration 5974, Loss: 5.741333961486816\n",
            "Training Iteration 5975, Loss: 5.3369622230529785\n",
            "Training Iteration 5976, Loss: 4.733489990234375\n",
            "Training Iteration 5977, Loss: 6.5238213539123535\n",
            "Training Iteration 5978, Loss: 2.7816858291625977\n",
            "Training Iteration 5979, Loss: 2.6338837146759033\n",
            "Training Iteration 5980, Loss: 2.348522901535034\n",
            "Training Iteration 5981, Loss: 3.2881245613098145\n",
            "Training Iteration 5982, Loss: 3.0101094245910645\n",
            "Training Iteration 5983, Loss: 5.399896621704102\n",
            "Training Iteration 5984, Loss: 4.788776874542236\n",
            "Training Iteration 5985, Loss: 1.16497802734375\n",
            "Training Iteration 5986, Loss: 5.391520977020264\n",
            "Training Iteration 5987, Loss: 3.1933417320251465\n",
            "Training Iteration 5988, Loss: 3.950256824493408\n",
            "Training Iteration 5989, Loss: 3.289501667022705\n",
            "Training Iteration 5990, Loss: 6.986505031585693\n",
            "Training Iteration 5991, Loss: 4.678683757781982\n",
            "Training Iteration 5992, Loss: 4.395519256591797\n",
            "Training Iteration 5993, Loss: 3.7201883792877197\n",
            "Training Iteration 5994, Loss: 6.946670055389404\n",
            "Training Iteration 5995, Loss: 6.585740566253662\n",
            "Training Iteration 5996, Loss: 2.4628000259399414\n",
            "Training Iteration 5997, Loss: 6.848401069641113\n",
            "Training Iteration 5998, Loss: 6.637444019317627\n",
            "Training Iteration 5999, Loss: 5.295539855957031\n",
            "Training Iteration 6000, Loss: 4.708443641662598\n",
            "Training Iteration 6001, Loss: 4.2660231590271\n",
            "Training Iteration 6002, Loss: 4.049277305603027\n",
            "Training Iteration 6003, Loss: 4.600464820861816\n",
            "Training Iteration 6004, Loss: 6.738816261291504\n",
            "Training Iteration 6005, Loss: 5.782310485839844\n",
            "Training Iteration 6006, Loss: 4.0287065505981445\n",
            "Training Iteration 6007, Loss: 5.6021552085876465\n",
            "Training Iteration 6008, Loss: 4.715444087982178\n",
            "Training Iteration 6009, Loss: 7.420526027679443\n",
            "Training Iteration 6010, Loss: 5.397441387176514\n",
            "Training Iteration 6011, Loss: 2.799503803253174\n",
            "Training Iteration 6012, Loss: 3.995419979095459\n",
            "Training Iteration 6013, Loss: 8.713153839111328\n",
            "Training Iteration 6014, Loss: 5.777458190917969\n",
            "Training Iteration 6015, Loss: 2.1292762756347656\n",
            "Training Iteration 6016, Loss: 1.5264517068862915\n",
            "Training Iteration 6017, Loss: 2.376279592514038\n",
            "Training Iteration 6018, Loss: 5.226306915283203\n",
            "Training Iteration 6019, Loss: 5.247441291809082\n",
            "Training Iteration 6020, Loss: 4.2562103271484375\n",
            "Training Iteration 6021, Loss: 4.961681365966797\n",
            "Training Iteration 6022, Loss: 3.3019328117370605\n",
            "Training Iteration 6023, Loss: 4.630954742431641\n",
            "Training Iteration 6024, Loss: 5.242345809936523\n",
            "Training Iteration 6025, Loss: 5.2359724044799805\n",
            "Training Iteration 6026, Loss: 4.032910346984863\n",
            "Training Iteration 6027, Loss: 4.451137542724609\n",
            "Training Iteration 6028, Loss: 7.067956924438477\n",
            "Training Iteration 6029, Loss: 3.554474353790283\n",
            "Training Iteration 6030, Loss: 3.208540678024292\n",
            "Training Iteration 6031, Loss: 6.988133907318115\n",
            "Training Iteration 6032, Loss: 8.520051956176758\n",
            "Training Iteration 6033, Loss: 4.34241247177124\n",
            "Training Iteration 6034, Loss: 3.9984660148620605\n",
            "Training Iteration 6035, Loss: 5.991178035736084\n",
            "Training Iteration 6036, Loss: 2.908299207687378\n",
            "Training Iteration 6037, Loss: 5.1931233406066895\n",
            "Training Iteration 6038, Loss: 3.2101473808288574\n",
            "Training Iteration 6039, Loss: 3.9925947189331055\n",
            "Training Iteration 6040, Loss: 3.0729339122772217\n",
            "Training Iteration 6041, Loss: 5.673635482788086\n",
            "Training Iteration 6042, Loss: 1.4311798810958862\n",
            "Training Iteration 6043, Loss: 7.046433448791504\n",
            "Training Iteration 6044, Loss: 7.0694708824157715\n",
            "Training Iteration 6045, Loss: 9.068819999694824\n",
            "Training Iteration 6046, Loss: 4.952051639556885\n",
            "Training Iteration 6047, Loss: 2.5465333461761475\n",
            "Training Iteration 6048, Loss: 4.757652282714844\n",
            "Training Iteration 6049, Loss: 3.867968797683716\n",
            "Training Iteration 6050, Loss: 5.9773759841918945\n",
            "Training Iteration 6051, Loss: 4.370597839355469\n",
            "Training Iteration 6052, Loss: 6.198217868804932\n",
            "Training Iteration 6053, Loss: 4.7537055015563965\n",
            "Training Iteration 6054, Loss: 3.3606693744659424\n",
            "Training Iteration 6055, Loss: 4.231215000152588\n",
            "Training Iteration 6056, Loss: 4.399744510650635\n",
            "Training Iteration 6057, Loss: 4.319942951202393\n",
            "Training Iteration 6058, Loss: 4.7951579093933105\n",
            "Training Iteration 6059, Loss: 2.4972128868103027\n",
            "Training Iteration 6060, Loss: 3.695204734802246\n",
            "Training Iteration 6061, Loss: 2.3071420192718506\n",
            "Training Iteration 6062, Loss: 6.030420780181885\n",
            "Training Iteration 6063, Loss: 2.0498499870300293\n",
            "Training Iteration 6064, Loss: 1.9847972393035889\n",
            "Training Iteration 6065, Loss: 2.472423553466797\n",
            "Training Iteration 6066, Loss: 4.7046074867248535\n",
            "Training Iteration 6067, Loss: 8.350172996520996\n",
            "Training Iteration 6068, Loss: 3.7518961429595947\n",
            "Training Iteration 6069, Loss: 4.133839130401611\n",
            "Training Iteration 6070, Loss: 2.250868797302246\n",
            "Training Iteration 6071, Loss: 3.417691230773926\n",
            "Training Iteration 6072, Loss: 3.4910056591033936\n",
            "Training Iteration 6073, Loss: 6.110558986663818\n",
            "Training Iteration 6074, Loss: 5.16301965713501\n",
            "Training Iteration 6075, Loss: 4.515212059020996\n",
            "Training Iteration 6076, Loss: 4.295590877532959\n",
            "Training Iteration 6077, Loss: 8.397928237915039\n",
            "Training Iteration 6078, Loss: 4.492481708526611\n",
            "Training Iteration 6079, Loss: 4.77534818649292\n",
            "Training Iteration 6080, Loss: 4.803032875061035\n",
            "Training Iteration 6081, Loss: 5.132417678833008\n",
            "Training Iteration 6082, Loss: 3.0864503383636475\n",
            "Training Iteration 6083, Loss: 2.0757057666778564\n",
            "Training Iteration 6084, Loss: 3.9083468914031982\n",
            "Training Iteration 6085, Loss: 5.144137382507324\n",
            "Training Iteration 6086, Loss: 2.301342010498047\n",
            "Training Iteration 6087, Loss: 4.889227867126465\n",
            "Training Iteration 6088, Loss: 3.3349368572235107\n",
            "Training Iteration 6089, Loss: 3.956369638442993\n",
            "Training Iteration 6090, Loss: 3.722417116165161\n",
            "Training Iteration 6091, Loss: 5.523379802703857\n",
            "Training Iteration 6092, Loss: 5.819418907165527\n",
            "Training Iteration 6093, Loss: 3.1757657527923584\n",
            "Training Iteration 6094, Loss: 3.172219753265381\n",
            "Training Iteration 6095, Loss: 2.1385412216186523\n",
            "Training Iteration 6096, Loss: 5.735637187957764\n",
            "Training Iteration 6097, Loss: 5.783940315246582\n",
            "Training Iteration 6098, Loss: 5.151942253112793\n",
            "Training Iteration 6099, Loss: 6.648097515106201\n",
            "Training Iteration 6100, Loss: 3.772239923477173\n",
            "Training Iteration 6101, Loss: 9.6105318069458\n",
            "Training Iteration 6102, Loss: 2.4197323322296143\n",
            "Training Iteration 6103, Loss: 5.81897497177124\n",
            "Training Iteration 6104, Loss: 5.020215034484863\n",
            "Training Iteration 6105, Loss: 6.357923984527588\n",
            "Training Iteration 6106, Loss: 3.636688232421875\n",
            "Training Iteration 6107, Loss: 4.039007186889648\n",
            "Training Iteration 6108, Loss: 2.745974540710449\n",
            "Training Iteration 6109, Loss: 4.105204105377197\n",
            "Training Iteration 6110, Loss: 2.7342710494995117\n",
            "Training Iteration 6111, Loss: 4.15387487411499\n",
            "Training Iteration 6112, Loss: 4.861817359924316\n",
            "Training Iteration 6113, Loss: 4.9620771408081055\n",
            "Training Iteration 6114, Loss: 4.023095607757568\n",
            "Training Iteration 6115, Loss: 6.409777641296387\n",
            "Training Iteration 6116, Loss: 11.033117294311523\n",
            "Training Iteration 6117, Loss: 4.231182098388672\n",
            "Training Iteration 6118, Loss: 1.7339882850646973\n",
            "Training Iteration 6119, Loss: 5.369874954223633\n",
            "Training Iteration 6120, Loss: 8.072596549987793\n",
            "Training Iteration 6121, Loss: 3.731389045715332\n",
            "Training Iteration 6122, Loss: 5.034184455871582\n",
            "Training Iteration 6123, Loss: 3.067009687423706\n",
            "Training Iteration 6124, Loss: 5.59975004196167\n",
            "Training Iteration 6125, Loss: 3.5747127532958984\n",
            "Training Iteration 6126, Loss: 5.496499061584473\n",
            "Training Iteration 6127, Loss: 4.910087585449219\n",
            "Training Iteration 6128, Loss: 9.418343544006348\n",
            "Training Iteration 6129, Loss: 4.0302886962890625\n",
            "Training Iteration 6130, Loss: 6.912364959716797\n",
            "Training Iteration 6131, Loss: 4.9754228591918945\n",
            "Training Iteration 6132, Loss: 4.507713317871094\n",
            "Training Iteration 6133, Loss: 3.7992987632751465\n",
            "Training Iteration 6134, Loss: 7.854051113128662\n",
            "Training Iteration 6135, Loss: 3.2707250118255615\n",
            "Training Iteration 6136, Loss: 3.329019784927368\n",
            "Training Iteration 6137, Loss: 3.6348485946655273\n",
            "Training Iteration 6138, Loss: 4.541833400726318\n",
            "Training Iteration 6139, Loss: 5.642553329467773\n",
            "Training Iteration 6140, Loss: 10.597944259643555\n",
            "Training Iteration 6141, Loss: 6.259279727935791\n",
            "Training Iteration 6142, Loss: 8.937638282775879\n",
            "Training Iteration 6143, Loss: 5.849828243255615\n",
            "Training Iteration 6144, Loss: 4.2537055015563965\n",
            "Training Iteration 6145, Loss: 7.0792131423950195\n",
            "Training Iteration 6146, Loss: 4.416997909545898\n",
            "Training Iteration 6147, Loss: 5.006992816925049\n",
            "Training Iteration 6148, Loss: 4.357249736785889\n",
            "Training Iteration 6149, Loss: 7.447137832641602\n",
            "Training Iteration 6150, Loss: 5.555141925811768\n",
            "Training Iteration 6151, Loss: 7.010705471038818\n",
            "Training Iteration 6152, Loss: 4.86998176574707\n",
            "Training Iteration 6153, Loss: 8.800628662109375\n",
            "Training Iteration 6154, Loss: 3.5526857376098633\n",
            "Training Iteration 6155, Loss: 1.824453592300415\n",
            "Training Iteration 6156, Loss: 4.538288116455078\n",
            "Training Iteration 6157, Loss: 4.475589275360107\n",
            "Training Iteration 6158, Loss: 6.977304458618164\n",
            "Training Iteration 6159, Loss: 6.797056674957275\n",
            "Training Iteration 6160, Loss: 5.811431407928467\n",
            "Training Iteration 6161, Loss: 5.717734336853027\n",
            "Training Iteration 6162, Loss: 4.273991584777832\n",
            "Training Iteration 6163, Loss: 7.01519250869751\n",
            "Training Iteration 6164, Loss: 9.092409133911133\n",
            "Training Iteration 6165, Loss: 6.865607261657715\n",
            "Training Iteration 6166, Loss: 6.075958728790283\n",
            "Training Iteration 6167, Loss: 7.244065761566162\n",
            "Training Iteration 6168, Loss: 3.5023021697998047\n",
            "Training Iteration 6169, Loss: 9.764699935913086\n",
            "Training Iteration 6170, Loss: 3.824899673461914\n",
            "Training Iteration 6171, Loss: 3.4414596557617188\n",
            "Training Iteration 6172, Loss: 5.148157119750977\n",
            "Training Iteration 6173, Loss: 7.476274013519287\n",
            "Training Iteration 6174, Loss: 6.454254627227783\n",
            "Training Iteration 6175, Loss: 6.6285929679870605\n",
            "Training Iteration 6176, Loss: 8.008588790893555\n",
            "Training Iteration 6177, Loss: 3.461621046066284\n",
            "Training Iteration 6178, Loss: 5.802027225494385\n",
            "Training Iteration 6179, Loss: 8.371953964233398\n",
            "Training Iteration 6180, Loss: 4.391848564147949\n",
            "Training Iteration 6181, Loss: 2.5674567222595215\n",
            "Training Iteration 6182, Loss: 3.616318464279175\n",
            "Training Iteration 6183, Loss: 2.9341964721679688\n",
            "Training Iteration 6184, Loss: 2.1175785064697266\n",
            "Training Iteration 6185, Loss: 5.872133731842041\n",
            "Training Iteration 6186, Loss: 3.8253159523010254\n",
            "Training Iteration 6187, Loss: 3.2419214248657227\n",
            "Training Iteration 6188, Loss: 3.2356295585632324\n",
            "Training Iteration 6189, Loss: 1.6723051071166992\n",
            "Training Iteration 6190, Loss: 3.028578758239746\n",
            "Training Iteration 6191, Loss: 3.910776138305664\n",
            "Training Iteration 6192, Loss: 4.857458591461182\n",
            "Training Iteration 6193, Loss: 5.57144021987915\n",
            "Training Iteration 6194, Loss: 2.8349945545196533\n",
            "Training Iteration 6195, Loss: 7.380372047424316\n",
            "Training Iteration 6196, Loss: 6.259053707122803\n",
            "Training Iteration 6197, Loss: 4.833467960357666\n",
            "Training Iteration 6198, Loss: 3.2325263023376465\n",
            "Training Iteration 6199, Loss: 4.319782733917236\n",
            "Training Iteration 6200, Loss: 5.562792778015137\n",
            "Training Iteration 6201, Loss: 6.706255912780762\n",
            "Training Iteration 6202, Loss: 9.41435718536377\n",
            "Training Iteration 6203, Loss: 5.139053821563721\n",
            "Training Iteration 6204, Loss: 5.124917030334473\n",
            "Training Iteration 6205, Loss: 5.150993824005127\n",
            "Training Iteration 6206, Loss: 5.9282708168029785\n",
            "Training Iteration 6207, Loss: 4.4440460205078125\n",
            "Training Iteration 6208, Loss: 9.25108814239502\n",
            "Training Iteration 6209, Loss: 10.614724159240723\n",
            "Training Iteration 6210, Loss: 6.894131660461426\n",
            "Training Iteration 6211, Loss: 5.385464191436768\n",
            "Training Iteration 6212, Loss: 3.2261645793914795\n",
            "Training Iteration 6213, Loss: 8.231571197509766\n",
            "Training Iteration 6214, Loss: 6.194878578186035\n",
            "Training Iteration 6215, Loss: 7.645308017730713\n",
            "Training Iteration 6216, Loss: 3.6074295043945312\n",
            "Training Iteration 6217, Loss: 6.504195213317871\n",
            "Training Iteration 6218, Loss: 3.2846055030822754\n",
            "Training Iteration 6219, Loss: 2.2985706329345703\n",
            "Training Iteration 6220, Loss: 5.0557661056518555\n",
            "Training Iteration 6221, Loss: 5.422126770019531\n",
            "Training Iteration 6222, Loss: 5.9170823097229\n",
            "Training Iteration 6223, Loss: 6.393245220184326\n",
            "Training Iteration 6224, Loss: 4.053502082824707\n",
            "Training Iteration 6225, Loss: 4.255729675292969\n",
            "Training Iteration 6226, Loss: 11.836362838745117\n",
            "Training Iteration 6227, Loss: 8.514604568481445\n",
            "Training Iteration 6228, Loss: 5.27017879486084\n",
            "Training Iteration 6229, Loss: 2.956514358520508\n",
            "Training Iteration 6230, Loss: 5.2769927978515625\n",
            "Training Iteration 6231, Loss: 5.048962116241455\n",
            "Training Iteration 6232, Loss: 8.925862312316895\n",
            "Training Iteration 6233, Loss: 10.254950523376465\n",
            "Training Iteration 6234, Loss: 6.297183990478516\n",
            "Training Iteration 6235, Loss: 7.569654941558838\n",
            "Training Iteration 6236, Loss: 5.987972736358643\n",
            "Training Iteration 6237, Loss: 6.534286022186279\n",
            "Training Iteration 6238, Loss: 5.731536388397217\n",
            "Training Iteration 6239, Loss: 6.301077365875244\n",
            "Training Iteration 6240, Loss: 4.16218900680542\n",
            "Training Iteration 6241, Loss: 7.0884480476379395\n",
            "Training Iteration 6242, Loss: 3.2345216274261475\n",
            "Training Iteration 6243, Loss: 5.245222091674805\n",
            "Training Iteration 6244, Loss: 8.739542961120605\n",
            "Training Iteration 6245, Loss: 4.872661590576172\n",
            "Training Iteration 6246, Loss: 6.234931468963623\n",
            "Training Iteration 6247, Loss: 6.162721157073975\n",
            "Training Iteration 6248, Loss: 4.478767395019531\n",
            "Training Iteration 6249, Loss: 6.53381872177124\n",
            "Training Iteration 6250, Loss: 4.784764289855957\n",
            "Training Iteration 6251, Loss: 5.284687519073486\n",
            "Training Iteration 6252, Loss: 4.428586483001709\n",
            "Training Iteration 6253, Loss: 3.9391422271728516\n",
            "Training Iteration 6254, Loss: 3.087846279144287\n",
            "Training Iteration 6255, Loss: 4.648907661437988\n",
            "Training Iteration 6256, Loss: 3.992521286010742\n",
            "Training Iteration 6257, Loss: 5.589852333068848\n",
            "Training Iteration 6258, Loss: 6.738189697265625\n",
            "Training Iteration 6259, Loss: 3.5333011150360107\n",
            "Training Iteration 6260, Loss: 1.7950108051300049\n",
            "Training Iteration 6261, Loss: 4.137245178222656\n",
            "Training Iteration 6262, Loss: 3.217318058013916\n",
            "Training Iteration 6263, Loss: 4.642190933227539\n",
            "Training Iteration 6264, Loss: 6.031660556793213\n",
            "Training Iteration 6265, Loss: 4.530139923095703\n",
            "Training Iteration 6266, Loss: 9.893853187561035\n",
            "Training Iteration 6267, Loss: 5.757002353668213\n",
            "Training Iteration 6268, Loss: 3.9482040405273438\n",
            "Training Iteration 6269, Loss: 5.5951128005981445\n",
            "Training Iteration 6270, Loss: 3.0648257732391357\n",
            "Training Iteration 6271, Loss: 4.694030284881592\n",
            "Training Iteration 6272, Loss: 4.177008152008057\n",
            "Training Iteration 6273, Loss: 4.839028835296631\n",
            "Training Iteration 6274, Loss: 4.276103973388672\n",
            "Training Iteration 6275, Loss: 5.63908576965332\n",
            "Training Iteration 6276, Loss: 3.752739191055298\n",
            "Training Iteration 6277, Loss: 9.775494575500488\n",
            "Training Iteration 6278, Loss: 5.953789234161377\n",
            "Training Iteration 6279, Loss: 4.1652069091796875\n",
            "Training Iteration 6280, Loss: 6.368936061859131\n",
            "Training Iteration 6281, Loss: 3.56262469291687\n",
            "Training Iteration 6282, Loss: 4.070300102233887\n",
            "Training Iteration 6283, Loss: 4.461812496185303\n",
            "Training Iteration 6284, Loss: 5.613015651702881\n",
            "Training Iteration 6285, Loss: 6.246317386627197\n",
            "Training Iteration 6286, Loss: 5.756540298461914\n",
            "Training Iteration 6287, Loss: 5.070396423339844\n",
            "Training Iteration 6288, Loss: 5.84999418258667\n",
            "Training Iteration 6289, Loss: 5.491682529449463\n",
            "Training Iteration 6290, Loss: 4.580188274383545\n",
            "Training Iteration 6291, Loss: 3.8707122802734375\n",
            "Training Iteration 6292, Loss: 4.164273738861084\n",
            "Training Iteration 6293, Loss: 3.204761266708374\n",
            "Training Iteration 6294, Loss: 5.726393222808838\n",
            "Training Iteration 6295, Loss: 8.719491004943848\n",
            "Training Iteration 6296, Loss: 5.964624404907227\n",
            "Training Iteration 6297, Loss: 7.002254486083984\n",
            "Training Iteration 6298, Loss: 4.059723854064941\n",
            "Training Iteration 6299, Loss: 4.352534294128418\n",
            "Training Iteration 6300, Loss: 3.3961567878723145\n",
            "Training Iteration 6301, Loss: 5.600118160247803\n",
            "Training Iteration 6302, Loss: 6.17935848236084\n",
            "Training Iteration 6303, Loss: 5.104241371154785\n",
            "Training Iteration 6304, Loss: 4.203181743621826\n",
            "Training Iteration 6305, Loss: 3.8448233604431152\n",
            "Training Iteration 6306, Loss: 4.871270656585693\n",
            "Training Iteration 6307, Loss: 4.263533592224121\n",
            "Training Iteration 6308, Loss: 4.967047214508057\n",
            "Training Iteration 6309, Loss: 2.3779475688934326\n",
            "Training Iteration 6310, Loss: 2.6885786056518555\n",
            "Training Iteration 6311, Loss: 4.9906816482543945\n",
            "Training Iteration 6312, Loss: 3.052985906600952\n",
            "Training Iteration 6313, Loss: 4.906588077545166\n",
            "Training Iteration 6314, Loss: 2.429649829864502\n",
            "Training Iteration 6315, Loss: 3.3835668563842773\n",
            "Training Iteration 6316, Loss: 4.694197654724121\n",
            "Training Iteration 6317, Loss: 4.401063919067383\n",
            "Training Iteration 6318, Loss: 4.296778202056885\n",
            "Training Iteration 6319, Loss: 4.738607406616211\n",
            "Training Iteration 6320, Loss: 3.5421195030212402\n",
            "Training Iteration 6321, Loss: 5.744344711303711\n",
            "Training Iteration 6322, Loss: 5.746848106384277\n",
            "Training Iteration 6323, Loss: 3.3489389419555664\n",
            "Training Iteration 6324, Loss: 6.862004280090332\n",
            "Training Iteration 6325, Loss: 3.1149942874908447\n",
            "Training Iteration 6326, Loss: 2.336134672164917\n",
            "Training Iteration 6327, Loss: 3.7399089336395264\n",
            "Training Iteration 6328, Loss: 7.601657390594482\n",
            "Training Iteration 6329, Loss: 4.936427116394043\n",
            "Training Iteration 6330, Loss: 4.062140941619873\n",
            "Training Iteration 6331, Loss: 1.1285868883132935\n",
            "Training Iteration 6332, Loss: 4.889465808868408\n",
            "Training Iteration 6333, Loss: 6.649838924407959\n",
            "Training Iteration 6334, Loss: 3.185732364654541\n",
            "Training Iteration 6335, Loss: 3.7304978370666504\n",
            "Training Iteration 6336, Loss: 3.9272382259368896\n",
            "Training Iteration 6337, Loss: 3.6346118450164795\n",
            "Training Iteration 6338, Loss: 7.207098484039307\n",
            "Training Iteration 6339, Loss: 6.694969177246094\n",
            "Training Iteration 6340, Loss: 5.9731645584106445\n",
            "Training Iteration 6341, Loss: 2.384761333465576\n",
            "Training Iteration 6342, Loss: 2.4158802032470703\n",
            "Training Iteration 6343, Loss: 3.5513699054718018\n",
            "Training Iteration 6344, Loss: 6.37516975402832\n",
            "Training Iteration 6345, Loss: 3.4923248291015625\n",
            "Training Iteration 6346, Loss: 2.850019931793213\n",
            "Training Iteration 6347, Loss: 8.270645141601562\n",
            "Training Iteration 6348, Loss: 7.12422513961792\n",
            "Training Iteration 6349, Loss: 5.574622631072998\n",
            "Training Iteration 6350, Loss: 5.015867710113525\n",
            "Training Iteration 6351, Loss: 6.62706184387207\n",
            "Training Iteration 6352, Loss: 3.0449562072753906\n",
            "Training Iteration 6353, Loss: 3.704988479614258\n",
            "Training Iteration 6354, Loss: 4.533562660217285\n",
            "Training Iteration 6355, Loss: 5.267315864562988\n",
            "Training Iteration 6356, Loss: 5.313977241516113\n",
            "Training Iteration 6357, Loss: 7.101606369018555\n",
            "Training Iteration 6358, Loss: 4.759396553039551\n",
            "Training Iteration 6359, Loss: 5.3293986320495605\n",
            "Training Iteration 6360, Loss: 6.160118103027344\n",
            "Training Iteration 6361, Loss: 1.90691077709198\n",
            "Training Iteration 6362, Loss: 4.098587512969971\n",
            "Training Iteration 6363, Loss: 1.7044047117233276\n",
            "Training Iteration 6364, Loss: 5.251570224761963\n",
            "Training Iteration 6365, Loss: 6.873162269592285\n",
            "Training Iteration 6366, Loss: 5.130302906036377\n",
            "Training Iteration 6367, Loss: 4.275932312011719\n",
            "Training Iteration 6368, Loss: 4.1325459480285645\n",
            "Training Iteration 6369, Loss: 2.825165033340454\n",
            "Training Iteration 6370, Loss: 3.4491465091705322\n",
            "Training Iteration 6371, Loss: 7.484921455383301\n",
            "Training Iteration 6372, Loss: 4.622547626495361\n",
            "Training Iteration 6373, Loss: 4.6029181480407715\n",
            "Training Iteration 6374, Loss: 4.135978698730469\n",
            "Training Iteration 6375, Loss: 3.5900208950042725\n",
            "Training Iteration 6376, Loss: 4.503345012664795\n",
            "Training Iteration 6377, Loss: 6.003747940063477\n",
            "Training Iteration 6378, Loss: 3.2869715690612793\n",
            "Training Iteration 6379, Loss: 3.0557448863983154\n",
            "Training Iteration 6380, Loss: 3.7935283184051514\n",
            "Training Iteration 6381, Loss: 4.560986042022705\n",
            "Training Iteration 6382, Loss: 3.174612283706665\n",
            "Training Iteration 6383, Loss: 5.866875171661377\n",
            "Training Iteration 6384, Loss: 6.102634429931641\n",
            "Training Iteration 6385, Loss: 4.376298427581787\n",
            "Training Iteration 6386, Loss: 3.380613327026367\n",
            "Training Iteration 6387, Loss: 9.693819046020508\n",
            "Training Iteration 6388, Loss: 5.866105556488037\n",
            "Training Iteration 6389, Loss: 4.328911781311035\n",
            "Training Iteration 6390, Loss: 3.8054375648498535\n",
            "Training Iteration 6391, Loss: 6.286713600158691\n",
            "Training Iteration 6392, Loss: 4.207988262176514\n",
            "Training Iteration 6393, Loss: 5.479612827301025\n",
            "Training Iteration 6394, Loss: 3.840559482574463\n",
            "Training Iteration 6395, Loss: 6.504477024078369\n",
            "Training Iteration 6396, Loss: 11.15625286102295\n",
            "Training Iteration 6397, Loss: 8.778931617736816\n",
            "Training Iteration 6398, Loss: 6.458817958831787\n",
            "Training Iteration 6399, Loss: 1.9592727422714233\n",
            "Training Iteration 6400, Loss: 5.642807960510254\n",
            "Training Iteration 6401, Loss: 4.751269817352295\n",
            "Training Iteration 6402, Loss: 9.41717529296875\n",
            "Training Iteration 6403, Loss: 4.332677841186523\n",
            "Training Iteration 6404, Loss: 11.403182983398438\n",
            "Training Iteration 6405, Loss: 4.341403007507324\n",
            "Training Iteration 6406, Loss: 4.087664604187012\n",
            "Training Iteration 6407, Loss: 4.305551052093506\n",
            "Training Iteration 6408, Loss: 4.8547282218933105\n",
            "Training Iteration 6409, Loss: 5.06586217880249\n",
            "Training Iteration 6410, Loss: 4.366069316864014\n",
            "Training Iteration 6411, Loss: 4.607020854949951\n",
            "Training Iteration 6412, Loss: 5.786755084991455\n",
            "Training Iteration 6413, Loss: 4.509702205657959\n",
            "Training Iteration 6414, Loss: 8.631662368774414\n",
            "Training Iteration 6415, Loss: 1.402968168258667\n",
            "Training Iteration 6416, Loss: 3.9075119495391846\n",
            "Training Iteration 6417, Loss: 6.583429336547852\n",
            "Training Iteration 6418, Loss: 4.639987945556641\n",
            "Training Iteration 6419, Loss: 3.648827314376831\n",
            "Training Iteration 6420, Loss: 5.299504280090332\n",
            "Training Iteration 6421, Loss: 4.922825813293457\n",
            "Training Iteration 6422, Loss: 5.290126323699951\n",
            "Training Iteration 6423, Loss: 4.868579387664795\n",
            "Training Iteration 6424, Loss: 8.486677169799805\n",
            "Training Iteration 6425, Loss: 9.552655220031738\n",
            "Training Iteration 6426, Loss: 1.8877861499786377\n",
            "Training Iteration 6427, Loss: 5.998527526855469\n",
            "Training Iteration 6428, Loss: 6.798491477966309\n",
            "Training Iteration 6429, Loss: 5.5297346115112305\n",
            "Training Iteration 6430, Loss: 4.946203231811523\n",
            "Training Iteration 6431, Loss: 3.9608702659606934\n",
            "Training Iteration 6432, Loss: 5.1617512702941895\n",
            "Training Iteration 6433, Loss: 3.9875612258911133\n",
            "Training Iteration 6434, Loss: 7.89323091506958\n",
            "Training Iteration 6435, Loss: 7.064533710479736\n",
            "Training Iteration 6436, Loss: 6.252017498016357\n",
            "Training Iteration 6437, Loss: 3.101560115814209\n",
            "Training Iteration 6438, Loss: 3.80194354057312\n",
            "Training Iteration 6439, Loss: 7.334536552429199\n",
            "Training Iteration 6440, Loss: 8.17507553100586\n",
            "Training Iteration 6441, Loss: 7.58167839050293\n",
            "Training Iteration 6442, Loss: 4.573910713195801\n",
            "Training Iteration 6443, Loss: 6.594926357269287\n",
            "Training Iteration 6444, Loss: 8.37916088104248\n",
            "Training Iteration 6445, Loss: 6.66447639465332\n",
            "Training Iteration 6446, Loss: 7.0368242263793945\n",
            "Training Iteration 6447, Loss: 5.166664123535156\n",
            "Training Iteration 6448, Loss: 2.951530694961548\n",
            "Training Iteration 6449, Loss: 7.313111305236816\n",
            "Training Iteration 6450, Loss: 4.644805431365967\n",
            "Training Iteration 6451, Loss: 8.636157989501953\n",
            "Training Iteration 6452, Loss: 4.184775352478027\n",
            "Training Iteration 6453, Loss: 8.811240196228027\n",
            "Training Iteration 6454, Loss: 8.924503326416016\n",
            "Training Iteration 6455, Loss: 4.523441791534424\n",
            "Training Iteration 6456, Loss: 3.2923617362976074\n",
            "Training Iteration 6457, Loss: 4.951531410217285\n",
            "Training Iteration 6458, Loss: 8.12575912475586\n",
            "Training Iteration 6459, Loss: 4.014883995056152\n",
            "Training Iteration 6460, Loss: 3.1700077056884766\n",
            "Training Iteration 6461, Loss: 2.0282740592956543\n",
            "Training Iteration 6462, Loss: 3.1276235580444336\n",
            "Training Iteration 6463, Loss: 6.948375225067139\n",
            "Training Iteration 6464, Loss: 3.5235588550567627\n",
            "Training Iteration 6465, Loss: 4.090803146362305\n",
            "Training Iteration 6466, Loss: 5.436674118041992\n",
            "Training Iteration 6467, Loss: 2.7258706092834473\n",
            "Training Iteration 6468, Loss: 6.934398651123047\n",
            "Training Iteration 6469, Loss: 3.4486794471740723\n",
            "Training Iteration 6470, Loss: 3.8987698554992676\n",
            "Training Iteration 6471, Loss: 5.201320171356201\n",
            "Training Iteration 6472, Loss: 5.119650840759277\n",
            "Training Iteration 6473, Loss: 2.712745189666748\n",
            "Training Iteration 6474, Loss: 3.984938144683838\n",
            "Training Iteration 6475, Loss: 3.916863203048706\n",
            "Training Iteration 6476, Loss: 4.292009353637695\n",
            "Training Iteration 6477, Loss: 5.023557662963867\n",
            "Training Iteration 6478, Loss: 3.3789403438568115\n",
            "Training Iteration 6479, Loss: 2.1158621311187744\n",
            "Training Iteration 6480, Loss: 4.808364391326904\n",
            "Training Iteration 6481, Loss: 3.8304715156555176\n",
            "Training Iteration 6482, Loss: 1.9919183254241943\n",
            "Training Iteration 6483, Loss: 3.0610833168029785\n",
            "Training Iteration 6484, Loss: 6.150076866149902\n",
            "Training Iteration 6485, Loss: 4.493550777435303\n",
            "Training Iteration 6486, Loss: 4.2798171043396\n",
            "Training Iteration 6487, Loss: 4.147743225097656\n",
            "Training Iteration 6488, Loss: 5.337445259094238\n",
            "Training Iteration 6489, Loss: 10.91002082824707\n",
            "Training Iteration 6490, Loss: 6.947992324829102\n",
            "Training Iteration 6491, Loss: 8.686723709106445\n",
            "Training Iteration 6492, Loss: 3.2962608337402344\n",
            "Training Iteration 6493, Loss: 3.3866517543792725\n",
            "Training Iteration 6494, Loss: 6.629988670349121\n",
            "Training Iteration 6495, Loss: 6.137566089630127\n",
            "Training Iteration 6496, Loss: 6.686521053314209\n",
            "Training Iteration 6497, Loss: 5.931460380554199\n",
            "Training Iteration 6498, Loss: 4.099541187286377\n",
            "Training Iteration 6499, Loss: 3.2911782264709473\n",
            "Training Iteration 6500, Loss: 4.963048458099365\n",
            "Training Iteration 6501, Loss: 6.352746486663818\n",
            "Training Iteration 6502, Loss: 7.460310935974121\n",
            "Training Iteration 6503, Loss: 4.693709373474121\n",
            "Training Iteration 6504, Loss: 9.735560417175293\n",
            "Training Iteration 6505, Loss: 10.66427230834961\n",
            "Training Iteration 6506, Loss: 4.366767883300781\n",
            "Training Iteration 6507, Loss: 6.210302829742432\n",
            "Training Iteration 6508, Loss: 8.146212577819824\n",
            "Training Iteration 6509, Loss: 9.253765106201172\n",
            "Training Iteration 6510, Loss: 5.025803089141846\n",
            "Training Iteration 6511, Loss: 5.528565406799316\n",
            "Training Iteration 6512, Loss: 4.23312520980835\n",
            "Training Iteration 6513, Loss: 2.112989664077759\n",
            "Training Iteration 6514, Loss: 4.30570650100708\n",
            "Training Iteration 6515, Loss: 4.8028693199157715\n",
            "Training Iteration 6516, Loss: 3.155817985534668\n",
            "Training Iteration 6517, Loss: 4.629508972167969\n",
            "Training Iteration 6518, Loss: 4.810112953186035\n",
            "Training Iteration 6519, Loss: 1.723608374595642\n",
            "Training Iteration 6520, Loss: 4.935232162475586\n",
            "Training Iteration 6521, Loss: 2.1785213947296143\n",
            "Training Iteration 6522, Loss: 2.890225887298584\n",
            "Training Iteration 6523, Loss: 2.9395499229431152\n",
            "Training Iteration 6524, Loss: 3.582130193710327\n",
            "Training Iteration 6525, Loss: 3.247396469116211\n",
            "Training Iteration 6526, Loss: 3.029052495956421\n",
            "Training Iteration 6527, Loss: 3.492560386657715\n",
            "Training Iteration 6528, Loss: 8.475760459899902\n",
            "Training Iteration 6529, Loss: 6.661779403686523\n",
            "Training Iteration 6530, Loss: 4.616940975189209\n",
            "Training Iteration 6531, Loss: 4.956872463226318\n",
            "Training Iteration 6532, Loss: 5.990787029266357\n",
            "Training Iteration 6533, Loss: 3.8692784309387207\n",
            "Training Iteration 6534, Loss: 6.604732036590576\n",
            "Training Iteration 6535, Loss: 2.630434513092041\n",
            "Training Iteration 6536, Loss: 4.546403408050537\n",
            "Training Iteration 6537, Loss: 5.281294822692871\n",
            "Training Iteration 6538, Loss: 3.0876572132110596\n",
            "Training Iteration 6539, Loss: 3.175495147705078\n",
            "Training Iteration 6540, Loss: 4.829937934875488\n",
            "Training Iteration 6541, Loss: 3.8786277770996094\n",
            "Training Iteration 6542, Loss: 4.033520698547363\n",
            "Training Iteration 6543, Loss: 2.4331042766571045\n",
            "Training Iteration 6544, Loss: 2.8688933849334717\n",
            "Training Iteration 6545, Loss: 3.3457140922546387\n",
            "Training Iteration 6546, Loss: 2.440368890762329\n",
            "Training Iteration 6547, Loss: 4.177613258361816\n",
            "Training Iteration 6548, Loss: 6.4400129318237305\n",
            "Training Iteration 6549, Loss: 4.608059883117676\n",
            "Training Iteration 6550, Loss: 3.000434160232544\n",
            "Training Iteration 6551, Loss: 8.410907745361328\n",
            "Training Iteration 6552, Loss: 2.7393012046813965\n",
            "Training Iteration 6553, Loss: 5.443224906921387\n",
            "Training Iteration 6554, Loss: 2.825061321258545\n",
            "tensor([[9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        ...,\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07]])\n",
            "Training loss for epcoh 1: 3.6506657880601048\n",
            "Training accuracy for epoch 1: 0.30667989165681153\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        ...,\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07],\n",
            "        [9.6013e-01, 2.4537e-02, 1.3370e-02, 4.9185e-04, 1.4719e-03, 3.2627e-07]])\n",
            "Validation loss for epcoh 1: 3.6647503227086387\n",
            "Test accuracy for epoch 1: 0.3103685053788052\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ee41e99bd9f4a6595fc86be0d574edf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch No: 2:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Iteration 1, Loss: 5.577312469482422\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 5.436380863189697\n",
            "Training Iteration 1565, Loss: 6.543172359466553\n",
            "Training Iteration 1566, Loss: 4.09991455078125\n",
            "Training Iteration 1567, Loss: 4.605649948120117\n",
            "Training Iteration 1568, Loss: 2.8721776008605957\n",
            "Training Iteration 1569, Loss: 4.214337348937988\n",
            "Training Iteration 1570, Loss: 5.164185523986816\n",
            "Training Iteration 1571, Loss: 5.787470817565918\n",
            "Training Iteration 1572, Loss: 7.026432037353516\n",
            "Training Iteration 1573, Loss: 5.173123359680176\n",
            "Training Iteration 1574, Loss: 4.802278518676758\n",
            "Training Iteration 1575, Loss: 8.005227088928223\n",
            "Training Iteration 1576, Loss: 6.445741653442383\n",
            "Training Iteration 1577, Loss: 2.499340057373047\n",
            "Training Iteration 1578, Loss: 2.9497318267822266\n",
            "Training Iteration 1579, Loss: 4.792680740356445\n",
            "Training Iteration 1580, Loss: 5.042891502380371\n",
            "Training Iteration 1581, Loss: 4.956450939178467\n",
            "Training Iteration 1582, Loss: 3.364658832550049\n",
            "Training Iteration 1583, Loss: 5.08590030670166\n",
            "Training Iteration 1584, Loss: 5.8597798347473145\n",
            "Training Iteration 1585, Loss: 3.424759864807129\n",
            "Training Iteration 1586, Loss: 3.615274429321289\n",
            "Training Iteration 1587, Loss: 6.930713653564453\n",
            "Training Iteration 1588, Loss: 5.299455165863037\n",
            "Training Iteration 1589, Loss: 2.6962146759033203\n",
            "Training Iteration 1590, Loss: 5.828614234924316\n",
            "Training Iteration 1591, Loss: 5.118789196014404\n",
            "Training Iteration 1592, Loss: 4.441372394561768\n",
            "Training Iteration 1593, Loss: 5.234455108642578\n",
            "Training Iteration 1594, Loss: 2.9633142948150635\n",
            "Training Iteration 1595, Loss: 4.528314113616943\n",
            "Training Iteration 1596, Loss: 3.5360639095306396\n",
            "Training Iteration 1597, Loss: 3.0735020637512207\n",
            "Training Iteration 1598, Loss: 6.086203575134277\n",
            "Training Iteration 1599, Loss: 3.4532272815704346\n",
            "Training Iteration 1600, Loss: 3.6550228595733643\n",
            "Training Iteration 1601, Loss: 4.4799113273620605\n",
            "Training Iteration 1602, Loss: 3.48881196975708\n",
            "Training Iteration 1603, Loss: 3.6774544715881348\n",
            "Training Iteration 1604, Loss: 5.71101188659668\n",
            "Training Iteration 1605, Loss: 4.9861249923706055\n",
            "Training Iteration 1606, Loss: 1.6120941638946533\n",
            "Training Iteration 1607, Loss: 6.037355422973633\n",
            "Training Iteration 1608, Loss: 4.719857215881348\n",
            "Training Iteration 1609, Loss: 5.349143028259277\n",
            "Training Iteration 1610, Loss: 5.640995979309082\n",
            "Training Iteration 1611, Loss: 4.525102138519287\n",
            "Training Iteration 1612, Loss: 2.934962749481201\n",
            "Training Iteration 1613, Loss: 4.589882850646973\n",
            "Training Iteration 1614, Loss: 3.8198678493499756\n",
            "Training Iteration 1615, Loss: 4.441949367523193\n",
            "Training Iteration 1616, Loss: 5.137389659881592\n",
            "Training Iteration 1617, Loss: 2.730142593383789\n",
            "Training Iteration 1618, Loss: 1.9075472354888916\n",
            "Training Iteration 1619, Loss: 3.70245623588562\n",
            "Training Iteration 1620, Loss: 5.093361854553223\n",
            "Training Iteration 1621, Loss: 3.0478062629699707\n",
            "Training Iteration 1622, Loss: 7.243877410888672\n",
            "Training Iteration 1623, Loss: 5.3663482666015625\n",
            "Training Iteration 1624, Loss: 4.730039596557617\n",
            "Training Iteration 1625, Loss: 6.753467559814453\n",
            "Training Iteration 1626, Loss: 8.611109733581543\n",
            "Training Iteration 1627, Loss: 2.6689281463623047\n",
            "Training Iteration 1628, Loss: 5.0561394691467285\n",
            "Training Iteration 1629, Loss: 4.990394592285156\n",
            "Training Iteration 1630, Loss: 3.901310920715332\n",
            "Training Iteration 1631, Loss: 4.73689603805542\n",
            "Training Iteration 1632, Loss: 2.9697012901306152\n",
            "Training Iteration 1633, Loss: 8.120269775390625\n",
            "Training Iteration 1634, Loss: 3.721856117248535\n",
            "Training Iteration 1635, Loss: 4.213470458984375\n",
            "Training Iteration 1636, Loss: 3.559469223022461\n",
            "Training Iteration 1637, Loss: 4.895444393157959\n",
            "Training Iteration 1638, Loss: 5.907196998596191\n",
            "Training Iteration 1639, Loss: 5.9421610832214355\n",
            "Training Iteration 1640, Loss: 1.9993259906768799\n",
            "Training Iteration 1641, Loss: 3.669595241546631\n",
            "Training Iteration 1642, Loss: 4.430314540863037\n",
            "Training Iteration 1643, Loss: 5.377158164978027\n",
            "Training Iteration 1644, Loss: 4.58665657043457\n",
            "Training Iteration 1645, Loss: 5.996262550354004\n",
            "Training Iteration 1646, Loss: 5.6792473793029785\n",
            "Training Iteration 1647, Loss: 2.6855881214141846\n",
            "Training Iteration 1648, Loss: 4.973402500152588\n",
            "Training Iteration 1649, Loss: 3.5278961658477783\n",
            "Training Iteration 1650, Loss: 4.658322811126709\n",
            "Training Iteration 1651, Loss: 6.483514308929443\n",
            "Training Iteration 1652, Loss: 4.401061534881592\n",
            "Training Iteration 1653, Loss: 4.074625492095947\n",
            "Training Iteration 1654, Loss: 4.945280075073242\n",
            "Training Iteration 1655, Loss: 2.875250816345215\n",
            "Training Iteration 1656, Loss: 3.543163299560547\n",
            "Training Iteration 1657, Loss: 9.126649856567383\n",
            "Training Iteration 1658, Loss: 7.135986328125\n",
            "Training Iteration 1659, Loss: 2.7143430709838867\n",
            "Training Iteration 1660, Loss: 8.63410472869873\n",
            "Training Iteration 1661, Loss: 2.8421683311462402\n",
            "Training Iteration 1662, Loss: 5.064335823059082\n",
            "Training Iteration 1663, Loss: 5.296445846557617\n",
            "Training Iteration 1664, Loss: 3.3133797645568848\n",
            "Training Iteration 1665, Loss: 2.694258451461792\n",
            "Training Iteration 1666, Loss: 3.033721685409546\n",
            "Training Iteration 1667, Loss: 4.786685943603516\n",
            "Training Iteration 1668, Loss: 4.879668235778809\n",
            "Training Iteration 1669, Loss: 1.641904592514038\n",
            "Training Iteration 1670, Loss: 3.1222894191741943\n",
            "Training Iteration 1671, Loss: 3.931095600128174\n",
            "Training Iteration 1672, Loss: 3.7523353099823\n",
            "Training Iteration 1673, Loss: 7.185579776763916\n",
            "Training Iteration 1674, Loss: 3.78955340385437\n",
            "Training Iteration 1675, Loss: 4.506545543670654\n",
            "Training Iteration 1676, Loss: 5.313167572021484\n",
            "Training Iteration 1677, Loss: 4.2364044189453125\n",
            "Training Iteration 1678, Loss: 2.8044075965881348\n",
            "Training Iteration 1679, Loss: 2.098905086517334\n",
            "Training Iteration 1680, Loss: 5.524308681488037\n",
            "Training Iteration 1681, Loss: 2.191455841064453\n",
            "Training Iteration 1682, Loss: 5.102521896362305\n",
            "Training Iteration 1683, Loss: 4.4097795486450195\n",
            "Training Iteration 1684, Loss: 2.8675711154937744\n",
            "Training Iteration 1685, Loss: 2.934615135192871\n",
            "Training Iteration 1686, Loss: 5.322107315063477\n",
            "Training Iteration 1687, Loss: 3.0411643981933594\n",
            "Training Iteration 1688, Loss: 1.9553778171539307\n",
            "Training Iteration 1689, Loss: 6.312624454498291\n",
            "Training Iteration 1690, Loss: 4.141699314117432\n",
            "Training Iteration 1691, Loss: 2.7541751861572266\n",
            "Training Iteration 1692, Loss: 4.354488372802734\n",
            "Training Iteration 1693, Loss: 6.342288017272949\n",
            "Training Iteration 1694, Loss: 5.535589694976807\n",
            "Training Iteration 1695, Loss: 5.700714111328125\n",
            "Training Iteration 1696, Loss: 1.7960847616195679\n",
            "Training Iteration 1697, Loss: 3.0570406913757324\n",
            "Training Iteration 1698, Loss: 3.4277284145355225\n",
            "Training Iteration 1699, Loss: 3.518777370452881\n",
            "Training Iteration 1700, Loss: 2.9553096294403076\n",
            "Training Iteration 1701, Loss: 5.083162307739258\n",
            "Training Iteration 1702, Loss: 6.431751251220703\n",
            "Training Iteration 1703, Loss: 2.8614978790283203\n",
            "Training Iteration 1704, Loss: 6.823246955871582\n",
            "Training Iteration 1705, Loss: 2.0866901874542236\n",
            "Training Iteration 1706, Loss: 5.131129741668701\n",
            "Training Iteration 1707, Loss: 4.950982093811035\n",
            "Training Iteration 1708, Loss: 6.211974620819092\n",
            "Training Iteration 1709, Loss: 4.341405391693115\n",
            "Training Iteration 1710, Loss: 5.505107879638672\n",
            "Training Iteration 1711, Loss: 5.147024631500244\n",
            "Training Iteration 1712, Loss: 3.980179786682129\n",
            "Training Iteration 1713, Loss: 3.6761419773101807\n",
            "Training Iteration 1714, Loss: 6.085128307342529\n",
            "Training Iteration 1715, Loss: 5.48464298248291\n",
            "Training Iteration 1716, Loss: 3.6323537826538086\n",
            "Training Iteration 1717, Loss: 6.471124649047852\n",
            "Training Iteration 1718, Loss: 4.170586109161377\n",
            "Training Iteration 1719, Loss: 5.711604118347168\n",
            "Training Iteration 1720, Loss: 2.97391939163208\n",
            "Training Iteration 1721, Loss: 7.0044636726379395\n",
            "Training Iteration 1722, Loss: 3.852830410003662\n",
            "Training Iteration 1723, Loss: 5.290431022644043\n",
            "Training Iteration 1724, Loss: 4.267483234405518\n",
            "Training Iteration 1725, Loss: 3.0937328338623047\n",
            "Training Iteration 1726, Loss: 4.110969543457031\n",
            "Training Iteration 1727, Loss: 4.228704929351807\n",
            "Training Iteration 1728, Loss: 7.010118007659912\n",
            "Training Iteration 1729, Loss: 6.298579692840576\n",
            "Training Iteration 1730, Loss: 3.1207919120788574\n",
            "Training Iteration 1731, Loss: 6.033397674560547\n",
            "Training Iteration 1732, Loss: 3.116446018218994\n",
            "Training Iteration 1733, Loss: 7.277976989746094\n",
            "Training Iteration 1734, Loss: 1.0080872774124146\n",
            "Training Iteration 1735, Loss: 5.010418891906738\n",
            "Training Iteration 1736, Loss: 3.023151397705078\n",
            "Training Iteration 1737, Loss: 3.4062767028808594\n",
            "Training Iteration 1738, Loss: 5.9376678466796875\n",
            "Training Iteration 1739, Loss: 5.8181633949279785\n",
            "Training Iteration 1740, Loss: 4.024242401123047\n",
            "Training Iteration 1741, Loss: 3.8784942626953125\n",
            "Training Iteration 1742, Loss: 2.908033847808838\n",
            "Training Iteration 1743, Loss: 5.634749412536621\n",
            "Training Iteration 1744, Loss: 2.590707302093506\n",
            "Training Iteration 1745, Loss: 3.913733720779419\n",
            "Training Iteration 1746, Loss: 2.9239399433135986\n",
            "Training Iteration 1747, Loss: 4.568426609039307\n",
            "Training Iteration 1748, Loss: 4.754280090332031\n",
            "Training Iteration 1749, Loss: 4.517843246459961\n",
            "Training Iteration 1750, Loss: 3.9763550758361816\n",
            "Training Iteration 1751, Loss: 6.266121864318848\n",
            "Training Iteration 1752, Loss: 5.11625862121582\n",
            "Training Iteration 1753, Loss: 5.530765056610107\n",
            "Training Iteration 1754, Loss: 5.306192874908447\n",
            "Training Iteration 1755, Loss: 5.241752624511719\n",
            "Training Iteration 1756, Loss: 3.0447206497192383\n",
            "Training Iteration 1757, Loss: 5.519803524017334\n",
            "Training Iteration 1758, Loss: 4.006886959075928\n",
            "Training Iteration 1759, Loss: 4.55042028427124\n",
            "Training Iteration 1760, Loss: 5.576956748962402\n",
            "Training Iteration 1761, Loss: 4.513877868652344\n",
            "Training Iteration 1762, Loss: 3.5174643993377686\n",
            "Training Iteration 1763, Loss: 5.192307949066162\n",
            "Training Iteration 1764, Loss: 4.874285697937012\n",
            "Training Iteration 1765, Loss: 5.302721977233887\n",
            "Training Iteration 1766, Loss: 5.296985149383545\n",
            "Training Iteration 1767, Loss: 3.525465965270996\n",
            "Training Iteration 1768, Loss: 5.204108238220215\n",
            "Training Iteration 1769, Loss: 5.129505157470703\n",
            "Training Iteration 1770, Loss: 2.8978917598724365\n",
            "Training Iteration 1771, Loss: 0.8594722747802734\n",
            "Training Iteration 1772, Loss: 3.9552133083343506\n",
            "Training Iteration 1773, Loss: 6.23130989074707\n",
            "Training Iteration 1774, Loss: 5.962594032287598\n",
            "Training Iteration 1775, Loss: 4.201694488525391\n",
            "Training Iteration 1776, Loss: 2.5534873008728027\n",
            "Training Iteration 1777, Loss: 5.335264205932617\n",
            "Training Iteration 1778, Loss: 2.8746156692504883\n",
            "Training Iteration 1779, Loss: 5.190467834472656\n",
            "Training Iteration 1780, Loss: 2.89308500289917\n",
            "Training Iteration 1781, Loss: 3.432887554168701\n",
            "Training Iteration 1782, Loss: 3.6386187076568604\n",
            "Training Iteration 1783, Loss: 3.615725517272949\n",
            "Training Iteration 1784, Loss: 4.665481090545654\n",
            "Training Iteration 1785, Loss: 6.958397388458252\n",
            "Training Iteration 1786, Loss: 4.048743724822998\n",
            "Training Iteration 1787, Loss: 7.551578998565674\n",
            "Training Iteration 1788, Loss: 5.055399417877197\n",
            "Training Iteration 1789, Loss: 3.0649867057800293\n",
            "Training Iteration 1790, Loss: 8.999277114868164\n",
            "Training Iteration 1791, Loss: 4.318970680236816\n",
            "Training Iteration 1792, Loss: 1.4250675439834595\n",
            "Training Iteration 1793, Loss: 4.098803520202637\n",
            "Training Iteration 1794, Loss: 4.328595161437988\n",
            "Training Iteration 1795, Loss: 4.762650012969971\n",
            "Training Iteration 1796, Loss: 3.5487780570983887\n",
            "Training Iteration 1797, Loss: 3.777451276779175\n",
            "Training Iteration 1798, Loss: 4.227524280548096\n",
            "Training Iteration 1799, Loss: 5.4965033531188965\n",
            "Training Iteration 1800, Loss: 3.574956178665161\n",
            "Training Iteration 1801, Loss: 2.3746097087860107\n",
            "Training Iteration 1802, Loss: 6.833591461181641\n",
            "Training Iteration 1803, Loss: 5.5004353523254395\n",
            "Training Iteration 1804, Loss: 5.015378952026367\n",
            "Training Iteration 1805, Loss: 3.8911330699920654\n",
            "Training Iteration 1806, Loss: 3.8094229698181152\n",
            "Training Iteration 1807, Loss: 7.874227046966553\n",
            "Training Iteration 1808, Loss: 5.42429780960083\n",
            "Training Iteration 1809, Loss: 4.352780818939209\n",
            "Training Iteration 1810, Loss: 1.616833209991455\n",
            "Training Iteration 1811, Loss: 5.176342010498047\n",
            "Training Iteration 1812, Loss: 2.665881872177124\n",
            "Training Iteration 1813, Loss: 8.288195610046387\n",
            "Training Iteration 1814, Loss: 6.104831695556641\n",
            "Training Iteration 1815, Loss: 2.393338680267334\n",
            "Training Iteration 1816, Loss: 4.121965408325195\n",
            "Training Iteration 1817, Loss: 5.420989036560059\n",
            "Training Iteration 1818, Loss: 4.028365612030029\n",
            "Training Iteration 1819, Loss: 1.3850860595703125\n",
            "Training Iteration 1820, Loss: 7.517082214355469\n",
            "Training Iteration 1821, Loss: 6.05274772644043\n",
            "Training Iteration 1822, Loss: 3.6125361919403076\n",
            "Training Iteration 1823, Loss: 7.872823715209961\n",
            "Training Iteration 1824, Loss: 2.298048734664917\n",
            "Training Iteration 1825, Loss: 4.981183052062988\n",
            "Training Iteration 1826, Loss: 3.0687906742095947\n",
            "Training Iteration 1827, Loss: 3.283224105834961\n",
            "Training Iteration 1828, Loss: 1.6256539821624756\n",
            "Training Iteration 1829, Loss: 3.8829383850097656\n",
            "Training Iteration 1830, Loss: 2.5814785957336426\n",
            "Training Iteration 1831, Loss: 5.17379903793335\n",
            "Training Iteration 1832, Loss: 5.254489898681641\n",
            "Training Iteration 1833, Loss: 4.442347049713135\n",
            "Training Iteration 1834, Loss: 4.095473289489746\n",
            "Training Iteration 1835, Loss: 4.839029788970947\n",
            "Training Iteration 1836, Loss: 4.1409149169921875\n",
            "Training Iteration 1837, Loss: 8.24679946899414\n",
            "Training Iteration 1838, Loss: 4.938645362854004\n",
            "Training Iteration 1839, Loss: 2.7330660820007324\n",
            "Training Iteration 1840, Loss: 3.659806728363037\n",
            "Training Iteration 1841, Loss: 4.913560390472412\n",
            "Training Iteration 1842, Loss: 5.231186866760254\n",
            "Training Iteration 1843, Loss: 3.749645233154297\n",
            "Training Iteration 1844, Loss: 2.7323038578033447\n",
            "Training Iteration 1845, Loss: 1.4682152271270752\n",
            "Training Iteration 1846, Loss: 5.090684413909912\n",
            "Training Iteration 1847, Loss: 8.318175315856934\n",
            "Training Iteration 1848, Loss: 4.301034927368164\n",
            "Training Iteration 1849, Loss: 2.3312602043151855\n",
            "Training Iteration 1850, Loss: 2.8760781288146973\n",
            "Training Iteration 1851, Loss: 2.628746271133423\n",
            "Training Iteration 1852, Loss: 5.85127592086792\n",
            "Training Iteration 1853, Loss: 3.5130085945129395\n",
            "Training Iteration 1854, Loss: 4.349878787994385\n",
            "Training Iteration 1855, Loss: 3.325850486755371\n",
            "Training Iteration 1856, Loss: 3.454328775405884\n",
            "Training Iteration 1857, Loss: 3.3178722858428955\n",
            "Training Iteration 1858, Loss: 2.4904353618621826\n",
            "Training Iteration 1859, Loss: 4.69713020324707\n",
            "Training Iteration 1860, Loss: 7.3203444480896\n",
            "Training Iteration 1861, Loss: 3.2671306133270264\n",
            "Training Iteration 1862, Loss: 2.001081705093384\n",
            "Training Iteration 1863, Loss: 5.261220932006836\n",
            "Training Iteration 1864, Loss: 3.881992816925049\n",
            "Training Iteration 1865, Loss: 3.647602081298828\n",
            "Training Iteration 1866, Loss: 6.216064453125\n",
            "Training Iteration 1867, Loss: 4.467167854309082\n",
            "Training Iteration 1868, Loss: 5.313173294067383\n",
            "Training Iteration 1869, Loss: 4.991164207458496\n",
            "Training Iteration 1870, Loss: 3.244739532470703\n",
            "Training Iteration 1871, Loss: 3.7452046871185303\n",
            "Training Iteration 1872, Loss: 2.9730775356292725\n",
            "Training Iteration 1873, Loss: 3.7237226963043213\n",
            "Training Iteration 1874, Loss: 1.9473556280136108\n",
            "Training Iteration 1875, Loss: 2.3479654788970947\n",
            "Training Iteration 1876, Loss: 4.136770248413086\n",
            "Training Iteration 1877, Loss: 9.015433311462402\n",
            "Training Iteration 1878, Loss: 3.8964388370513916\n",
            "Training Iteration 1879, Loss: 3.937392234802246\n",
            "Training Iteration 1880, Loss: 4.831167221069336\n",
            "Training Iteration 1881, Loss: 4.971732139587402\n",
            "Training Iteration 1882, Loss: 3.342923164367676\n",
            "Training Iteration 1883, Loss: 4.790239334106445\n",
            "Training Iteration 1884, Loss: 5.213538646697998\n",
            "Training Iteration 1885, Loss: 2.135160446166992\n",
            "Training Iteration 1886, Loss: 3.636483669281006\n",
            "Training Iteration 1887, Loss: 5.078700542449951\n",
            "Training Iteration 1888, Loss: 6.021000862121582\n",
            "Training Iteration 1889, Loss: 2.5238046646118164\n",
            "Training Iteration 1890, Loss: 2.84708833694458\n",
            "Training Iteration 1891, Loss: 2.5171802043914795\n",
            "Training Iteration 1892, Loss: 2.3500893115997314\n",
            "Training Iteration 1893, Loss: 4.364420413970947\n",
            "Training Iteration 1894, Loss: 2.6480627059936523\n",
            "Training Iteration 1895, Loss: 3.0502490997314453\n",
            "Training Iteration 1896, Loss: 5.063425064086914\n",
            "Training Iteration 1897, Loss: 1.962245225906372\n",
            "Training Iteration 1898, Loss: 6.724372863769531\n",
            "Training Iteration 1899, Loss: 3.922921895980835\n",
            "Training Iteration 1900, Loss: 5.22529411315918\n",
            "Training Iteration 1901, Loss: 4.144471645355225\n",
            "Training Iteration 1902, Loss: 6.128325939178467\n",
            "Training Iteration 1903, Loss: 4.961967468261719\n",
            "Training Iteration 1904, Loss: 2.818450450897217\n",
            "Training Iteration 1905, Loss: 5.758422374725342\n",
            "Training Iteration 1906, Loss: 8.588133811950684\n",
            "Training Iteration 1907, Loss: 6.9234418869018555\n",
            "Training Iteration 1908, Loss: 8.383508682250977\n",
            "Training Iteration 1909, Loss: 3.9691781997680664\n",
            "Training Iteration 1910, Loss: 3.2875404357910156\n",
            "Training Iteration 1911, Loss: 4.2251787185668945\n",
            "Training Iteration 1912, Loss: 2.9367587566375732\n",
            "Training Iteration 1913, Loss: 4.095273017883301\n",
            "Training Iteration 1914, Loss: 4.853787899017334\n",
            "Training Iteration 1915, Loss: 3.79567813873291\n",
            "Training Iteration 1916, Loss: 5.074406623840332\n",
            "Training Iteration 1917, Loss: 6.04434871673584\n",
            "Training Iteration 1918, Loss: 4.999447345733643\n",
            "Training Iteration 1919, Loss: 5.5712175369262695\n",
            "Training Iteration 1920, Loss: 5.688985824584961\n",
            "Training Iteration 1921, Loss: 2.5098493099212646\n",
            "Training Iteration 1922, Loss: 6.849215030670166\n",
            "Training Iteration 1923, Loss: 3.164356231689453\n",
            "Training Iteration 1924, Loss: 3.040398359298706\n",
            "Training Iteration 1925, Loss: 4.0820393562316895\n",
            "Training Iteration 1926, Loss: 3.6205995082855225\n",
            "Training Iteration 1927, Loss: 2.525240421295166\n",
            "Training Iteration 1928, Loss: 9.733747482299805\n",
            "Training Iteration 1929, Loss: 1.6742337942123413\n",
            "Training Iteration 1930, Loss: 5.590677261352539\n",
            "Training Iteration 1931, Loss: 5.835029602050781\n",
            "Training Iteration 1932, Loss: 7.769778728485107\n",
            "Training Iteration 1933, Loss: 4.930600166320801\n",
            "Training Iteration 1934, Loss: 1.5328220129013062\n",
            "Training Iteration 1935, Loss: 3.833010196685791\n",
            "Training Iteration 1936, Loss: 3.6299002170562744\n",
            "Training Iteration 1937, Loss: 4.350042343139648\n",
            "Training Iteration 1938, Loss: 3.813628673553467\n",
            "Training Iteration 1939, Loss: 4.617485523223877\n",
            "Training Iteration 1940, Loss: 1.9377751350402832\n",
            "Training Iteration 1941, Loss: 2.441528558731079\n",
            "Training Iteration 1942, Loss: 5.320977210998535\n",
            "Training Iteration 1943, Loss: 2.671269178390503\n",
            "Training Iteration 1944, Loss: 2.7797069549560547\n",
            "Training Iteration 1945, Loss: 10.613237380981445\n",
            "Training Iteration 1946, Loss: 1.9322543144226074\n",
            "Training Iteration 1947, Loss: 4.668481826782227\n",
            "Training Iteration 1948, Loss: 2.7558934688568115\n",
            "Training Iteration 1949, Loss: 4.707444667816162\n",
            "Training Iteration 1950, Loss: 5.338902950286865\n",
            "Training Iteration 1951, Loss: 5.042754173278809\n",
            "Training Iteration 1952, Loss: 3.6168689727783203\n",
            "Training Iteration 1953, Loss: 2.7344472408294678\n",
            "Training Iteration 1954, Loss: 3.4573545455932617\n",
            "Training Iteration 1955, Loss: 4.216803550720215\n",
            "Training Iteration 1956, Loss: 4.256905555725098\n",
            "Training Iteration 1957, Loss: 3.9447600841522217\n",
            "Training Iteration 1958, Loss: 3.737992525100708\n",
            "Training Iteration 1959, Loss: 3.095538854598999\n",
            "Training Iteration 1960, Loss: 5.397331714630127\n",
            "Training Iteration 1961, Loss: 4.096588134765625\n",
            "Training Iteration 1962, Loss: 3.636390447616577\n",
            "Training Iteration 1963, Loss: 2.538862705230713\n",
            "Training Iteration 1964, Loss: 5.852461814880371\n",
            "Training Iteration 1965, Loss: 1.8281720876693726\n",
            "Training Iteration 1966, Loss: 5.085864543914795\n",
            "Training Iteration 1967, Loss: 1.4225417375564575\n",
            "Training Iteration 1968, Loss: 2.206789970397949\n",
            "Training Iteration 1969, Loss: 3.2652249336242676\n",
            "Training Iteration 1970, Loss: 7.107498645782471\n",
            "Training Iteration 1971, Loss: 4.729885101318359\n",
            "Training Iteration 1972, Loss: 6.4518938064575195\n",
            "Training Iteration 1973, Loss: 6.108644008636475\n",
            "Training Iteration 1974, Loss: 4.943819999694824\n",
            "Training Iteration 1975, Loss: 3.573967933654785\n",
            "Training Iteration 1976, Loss: 3.6285669803619385\n",
            "Training Iteration 1977, Loss: 5.879663944244385\n",
            "Training Iteration 1978, Loss: 2.939469575881958\n",
            "Training Iteration 1979, Loss: 3.8096587657928467\n",
            "Training Iteration 1980, Loss: 4.592019557952881\n",
            "Training Iteration 1981, Loss: 4.337837219238281\n",
            "Training Iteration 1982, Loss: 3.2923569679260254\n",
            "Training Iteration 1983, Loss: 4.267625331878662\n",
            "Training Iteration 1984, Loss: 2.9670252799987793\n",
            "Training Iteration 1985, Loss: 7.307232856750488\n",
            "Training Iteration 1986, Loss: 2.524949073791504\n",
            "Training Iteration 1987, Loss: 3.018691301345825\n",
            "Training Iteration 1988, Loss: 4.968150615692139\n",
            "Training Iteration 1989, Loss: 7.451845169067383\n",
            "Training Iteration 1990, Loss: 5.908524990081787\n",
            "Training Iteration 1991, Loss: 6.385917663574219\n",
            "Training Iteration 1992, Loss: 4.079773902893066\n",
            "Training Iteration 1993, Loss: 3.993328332901001\n",
            "Training Iteration 1994, Loss: 6.093557834625244\n",
            "Training Iteration 1995, Loss: 5.574075698852539\n",
            "Training Iteration 1996, Loss: 3.380756378173828\n",
            "Training Iteration 1997, Loss: 3.9660892486572266\n",
            "Training Iteration 1998, Loss: 3.363619089126587\n",
            "Training Iteration 1999, Loss: 2.2948155403137207\n",
            "Training Iteration 2000, Loss: 4.054965496063232\n",
            "Training Iteration 2001, Loss: 3.3771979808807373\n",
            "Training Iteration 2002, Loss: 4.23075008392334\n",
            "Training Iteration 2003, Loss: 7.685765266418457\n",
            "Training Iteration 2004, Loss: 4.687023162841797\n",
            "Training Iteration 2005, Loss: 5.337460994720459\n",
            "Training Iteration 2006, Loss: 3.3486440181732178\n",
            "Training Iteration 2007, Loss: 3.037166118621826\n",
            "Training Iteration 2008, Loss: 1.3577135801315308\n",
            "Training Iteration 2009, Loss: 5.525269031524658\n",
            "Training Iteration 2010, Loss: 4.739954948425293\n",
            "Training Iteration 2011, Loss: 6.486059665679932\n",
            "Training Iteration 2012, Loss: 3.8504090309143066\n",
            "Training Iteration 2013, Loss: 4.114341735839844\n",
            "Training Iteration 2014, Loss: 2.1794049739837646\n",
            "Training Iteration 2015, Loss: 7.070911407470703\n",
            "Training Iteration 2016, Loss: 2.4008095264434814\n",
            "Training Iteration 2017, Loss: 5.8378190994262695\n",
            "Training Iteration 2018, Loss: 2.799975872039795\n",
            "Training Iteration 2019, Loss: 3.2232561111450195\n",
            "Training Iteration 2020, Loss: 2.021214246749878\n",
            "Training Iteration 2021, Loss: 6.34510612487793\n",
            "Training Iteration 2022, Loss: 5.688651084899902\n",
            "Training Iteration 2023, Loss: 5.6950154304504395\n",
            "Training Iteration 2024, Loss: 6.145962238311768\n",
            "Training Iteration 2025, Loss: 5.547739028930664\n",
            "Training Iteration 2026, Loss: 4.010244369506836\n",
            "Training Iteration 2027, Loss: 3.0847198963165283\n",
            "Training Iteration 2028, Loss: 4.750689506530762\n",
            "Training Iteration 2029, Loss: 5.739527225494385\n",
            "Training Iteration 2030, Loss: 5.896856784820557\n",
            "Training Iteration 2031, Loss: 2.8051376342773438\n",
            "Training Iteration 2032, Loss: 6.060474395751953\n",
            "Training Iteration 2033, Loss: 3.9978413581848145\n",
            "Training Iteration 2034, Loss: 4.968912601470947\n",
            "Training Iteration 2035, Loss: 4.117049217224121\n",
            "Training Iteration 2036, Loss: 4.913758754730225\n",
            "Training Iteration 2037, Loss: 7.146785736083984\n",
            "Training Iteration 2038, Loss: 4.4543328285217285\n",
            "Training Iteration 2039, Loss: 4.301143646240234\n",
            "Training Iteration 2040, Loss: 2.616521120071411\n",
            "Training Iteration 2041, Loss: 4.87227201461792\n",
            "Training Iteration 2042, Loss: 3.6498098373413086\n",
            "Training Iteration 2043, Loss: 5.090322971343994\n",
            "Training Iteration 2044, Loss: 7.7157816886901855\n",
            "Training Iteration 2045, Loss: 9.442211151123047\n",
            "Training Iteration 2046, Loss: 4.218884468078613\n",
            "Training Iteration 2047, Loss: 7.249784469604492\n",
            "Training Iteration 2048, Loss: 7.324419975280762\n",
            "Training Iteration 2049, Loss: 7.077269554138184\n",
            "Training Iteration 2050, Loss: 7.633505821228027\n",
            "Training Iteration 2051, Loss: 4.789458751678467\n",
            "Training Iteration 2052, Loss: 5.920327663421631\n",
            "Training Iteration 2053, Loss: 2.3336520195007324\n",
            "Training Iteration 2054, Loss: 3.64523983001709\n",
            "Training Iteration 2055, Loss: 5.10479211807251\n",
            "Training Iteration 2056, Loss: 2.7404041290283203\n",
            "Training Iteration 2057, Loss: 5.39981746673584\n",
            "Training Iteration 2058, Loss: 2.213721513748169\n",
            "Training Iteration 2059, Loss: 2.3985118865966797\n",
            "Training Iteration 2060, Loss: 5.744365692138672\n",
            "Training Iteration 2061, Loss: 7.08248233795166\n",
            "Training Iteration 2062, Loss: 3.9011683464050293\n",
            "Training Iteration 2063, Loss: 4.3029465675354\n",
            "Training Iteration 2064, Loss: 3.5597519874572754\n",
            "Training Iteration 2065, Loss: 2.883181095123291\n",
            "Training Iteration 2066, Loss: 5.0771074295043945\n",
            "Training Iteration 2067, Loss: 1.7604587078094482\n",
            "Training Iteration 2068, Loss: 2.8558342456817627\n",
            "Training Iteration 2069, Loss: 4.652358055114746\n",
            "Training Iteration 2070, Loss: 6.251041412353516\n",
            "Training Iteration 2071, Loss: 2.096644163131714\n",
            "Training Iteration 2072, Loss: 6.397980213165283\n",
            "Training Iteration 2073, Loss: 3.4500391483306885\n",
            "Training Iteration 2074, Loss: 6.283871173858643\n",
            "Training Iteration 2075, Loss: 6.694316864013672\n",
            "Training Iteration 2076, Loss: 2.9051754474639893\n",
            "Training Iteration 2077, Loss: 5.887324810028076\n",
            "Training Iteration 2078, Loss: 4.847685813903809\n",
            "Training Iteration 2079, Loss: 8.355801582336426\n",
            "Training Iteration 2080, Loss: 2.2158799171447754\n",
            "Training Iteration 2081, Loss: 4.325099468231201\n",
            "Training Iteration 2082, Loss: 6.0646467208862305\n",
            "Training Iteration 2083, Loss: 7.544747829437256\n",
            "Training Iteration 2084, Loss: 1.73360013961792\n",
            "Training Iteration 2085, Loss: 6.382604598999023\n",
            "Training Iteration 2086, Loss: 5.327998161315918\n",
            "Training Iteration 2087, Loss: 4.757702350616455\n",
            "Training Iteration 2088, Loss: 3.706641674041748\n",
            "Training Iteration 2089, Loss: 2.921496868133545\n",
            "Training Iteration 2090, Loss: 4.14231014251709\n",
            "Training Iteration 2091, Loss: 2.435353994369507\n",
            "Training Iteration 2092, Loss: 6.771448135375977\n",
            "Training Iteration 2093, Loss: 6.017032623291016\n",
            "Training Iteration 2094, Loss: 0.6613926887512207\n",
            "Training Iteration 2095, Loss: 5.003957748413086\n",
            "Training Iteration 2096, Loss: 3.453827381134033\n",
            "Training Iteration 2097, Loss: 5.5517473220825195\n",
            "Training Iteration 2098, Loss: 6.152768611907959\n",
            "Training Iteration 2099, Loss: 4.023340702056885\n",
            "Training Iteration 2100, Loss: 5.290748596191406\n",
            "Training Iteration 2101, Loss: 5.987406253814697\n",
            "Training Iteration 2102, Loss: 5.535672187805176\n",
            "Training Iteration 2103, Loss: 10.200479507446289\n",
            "Training Iteration 2104, Loss: 5.204156875610352\n",
            "Training Iteration 2105, Loss: 4.5080246925354\n",
            "Training Iteration 2106, Loss: 6.984890460968018\n",
            "Training Iteration 2107, Loss: 5.299372673034668\n",
            "Training Iteration 2108, Loss: 3.7225546836853027\n",
            "Training Iteration 2109, Loss: 2.4676268100738525\n",
            "Training Iteration 2110, Loss: 5.618101596832275\n",
            "Training Iteration 2111, Loss: 4.4303083419799805\n",
            "Training Iteration 2112, Loss: 1.162424921989441\n",
            "Training Iteration 2113, Loss: 9.087623596191406\n",
            "Training Iteration 2114, Loss: 6.817295074462891\n",
            "Training Iteration 2115, Loss: 6.249744892120361\n",
            "Training Iteration 2116, Loss: 4.3802409172058105\n",
            "Training Iteration 2117, Loss: 4.490999221801758\n",
            "Training Iteration 2118, Loss: 6.494113922119141\n",
            "Training Iteration 2119, Loss: 5.224552631378174\n",
            "Training Iteration 2120, Loss: 4.437761306762695\n",
            "Training Iteration 2121, Loss: 4.467910289764404\n",
            "Training Iteration 2122, Loss: 4.101919174194336\n",
            "Training Iteration 2123, Loss: 3.3822009563446045\n",
            "Training Iteration 2124, Loss: 5.63723087310791\n",
            "Training Iteration 2125, Loss: 4.79240083694458\n",
            "Training Iteration 2126, Loss: 7.706918239593506\n",
            "Training Iteration 2127, Loss: 4.318609237670898\n",
            "Training Iteration 2128, Loss: 2.8212032318115234\n",
            "Training Iteration 2129, Loss: 6.16655969619751\n",
            "Training Iteration 2130, Loss: 6.5204668045043945\n",
            "Training Iteration 2131, Loss: 6.314539432525635\n",
            "Training Iteration 2132, Loss: 9.213847160339355\n",
            "Training Iteration 2133, Loss: 5.653626441955566\n",
            "Training Iteration 2134, Loss: 3.4965131282806396\n",
            "Training Iteration 2135, Loss: 2.495234251022339\n",
            "Training Iteration 2136, Loss: 3.8054349422454834\n",
            "Training Iteration 2137, Loss: 5.582438945770264\n",
            "Training Iteration 2138, Loss: 5.109023571014404\n",
            "Training Iteration 2139, Loss: 4.803461074829102\n",
            "Training Iteration 2140, Loss: 4.151013374328613\n",
            "Training Iteration 2141, Loss: 4.592783451080322\n",
            "Training Iteration 2142, Loss: 3.573040246963501\n",
            "Training Iteration 2143, Loss: 4.592549800872803\n",
            "Training Iteration 2144, Loss: 3.4641823768615723\n",
            "Training Iteration 2145, Loss: 6.495059490203857\n",
            "Training Iteration 2146, Loss: 4.441068172454834\n",
            "Training Iteration 2147, Loss: 6.73117208480835\n",
            "Training Iteration 2148, Loss: 6.161499977111816\n",
            "Training Iteration 2149, Loss: 4.581117630004883\n",
            "Training Iteration 2150, Loss: 6.297842502593994\n",
            "Training Iteration 2151, Loss: 3.607684373855591\n",
            "Training Iteration 2152, Loss: 6.380894184112549\n",
            "Training Iteration 2153, Loss: 9.397211074829102\n",
            "Training Iteration 2154, Loss: 6.338036060333252\n",
            "Training Iteration 2155, Loss: 7.717123031616211\n",
            "Training Iteration 2156, Loss: 2.397369384765625\n",
            "Training Iteration 2157, Loss: 3.0048916339874268\n",
            "Training Iteration 2158, Loss: 6.036739826202393\n",
            "Training Iteration 2159, Loss: 3.1212918758392334\n",
            "Training Iteration 2160, Loss: 7.228451728820801\n",
            "Training Iteration 2161, Loss: 7.528404235839844\n",
            "Training Iteration 2162, Loss: 2.5524089336395264\n",
            "Training Iteration 2163, Loss: 2.7197370529174805\n",
            "Training Iteration 2164, Loss: 3.4087870121002197\n",
            "Training Iteration 2165, Loss: 4.599491119384766\n",
            "Training Iteration 2166, Loss: 4.367701530456543\n",
            "Training Iteration 2167, Loss: 2.2685277462005615\n",
            "Training Iteration 2168, Loss: 4.946958541870117\n",
            "Training Iteration 2169, Loss: 3.9025113582611084\n",
            "Training Iteration 2170, Loss: 3.835629463195801\n",
            "Training Iteration 2171, Loss: 3.7316534519195557\n",
            "Training Iteration 2172, Loss: 2.8986191749572754\n",
            "Training Iteration 2173, Loss: 4.384851455688477\n",
            "Training Iteration 2174, Loss: 2.983182191848755\n",
            "Training Iteration 2175, Loss: 7.342433929443359\n",
            "Training Iteration 2176, Loss: 2.8118250370025635\n",
            "Training Iteration 2177, Loss: 3.971428871154785\n",
            "Training Iteration 2178, Loss: 9.232312202453613\n",
            "Training Iteration 2179, Loss: 3.3490653038024902\n",
            "Training Iteration 2180, Loss: 5.130260467529297\n",
            "Training Iteration 2181, Loss: 4.682693958282471\n",
            "Training Iteration 2182, Loss: 5.3455400466918945\n",
            "Training Iteration 2183, Loss: 3.759634494781494\n",
            "Training Iteration 2184, Loss: 7.398754119873047\n",
            "Training Iteration 2185, Loss: 6.822476387023926\n",
            "Training Iteration 2186, Loss: 3.0643413066864014\n",
            "Training Iteration 2187, Loss: 3.945971727371216\n",
            "Training Iteration 2188, Loss: 4.215314865112305\n",
            "Training Iteration 2189, Loss: 6.124262809753418\n",
            "Training Iteration 2190, Loss: 4.148247718811035\n",
            "Training Iteration 2191, Loss: 3.5936124324798584\n",
            "Training Iteration 2192, Loss: 5.796234607696533\n",
            "Training Iteration 2193, Loss: 2.2993104457855225\n",
            "Training Iteration 2194, Loss: 6.025996208190918\n",
            "Training Iteration 2195, Loss: 0.9780007600784302\n",
            "Training Iteration 2196, Loss: 2.008945941925049\n",
            "Training Iteration 2197, Loss: 4.959783554077148\n",
            "Training Iteration 2198, Loss: 5.6182122230529785\n",
            "Training Iteration 2199, Loss: 6.620838165283203\n",
            "Training Iteration 2200, Loss: 4.7786712646484375\n",
            "Training Iteration 2201, Loss: 6.134195804595947\n",
            "Training Iteration 2202, Loss: 6.918857097625732\n",
            "Training Iteration 2203, Loss: 2.8961446285247803\n",
            "Training Iteration 2204, Loss: 5.747679710388184\n",
            "Training Iteration 2205, Loss: 7.183019161224365\n",
            "Training Iteration 2206, Loss: 6.652949333190918\n",
            "Training Iteration 2207, Loss: 6.370794296264648\n",
            "Training Iteration 2208, Loss: 6.463104724884033\n",
            "Training Iteration 2209, Loss: 7.190149307250977\n",
            "Training Iteration 2210, Loss: 4.423899173736572\n",
            "Training Iteration 2211, Loss: 2.8512325286865234\n",
            "Training Iteration 2212, Loss: 5.514653205871582\n",
            "Training Iteration 2213, Loss: 5.328463554382324\n",
            "Training Iteration 2214, Loss: 3.0339503288269043\n",
            "Training Iteration 2215, Loss: 3.4455032348632812\n",
            "Training Iteration 2216, Loss: 7.408407211303711\n",
            "Training Iteration 2217, Loss: 5.501832962036133\n",
            "Training Iteration 2218, Loss: 8.879002571105957\n",
            "Training Iteration 2219, Loss: 3.0334312915802\n",
            "Training Iteration 2220, Loss: 3.4958584308624268\n",
            "Training Iteration 2221, Loss: 2.0154716968536377\n",
            "Training Iteration 2222, Loss: 6.481306076049805\n",
            "Training Iteration 2223, Loss: 4.064849376678467\n",
            "Training Iteration 2224, Loss: 4.372992038726807\n",
            "Training Iteration 2225, Loss: 3.5762338638305664\n",
            "Training Iteration 2226, Loss: 6.194027900695801\n",
            "Training Iteration 2227, Loss: 2.8132567405700684\n",
            "Training Iteration 2228, Loss: 4.940334796905518\n",
            "Training Iteration 2229, Loss: 4.0934038162231445\n",
            "Training Iteration 2230, Loss: 7.65938138961792\n",
            "Training Iteration 2231, Loss: 2.620133876800537\n",
            "Training Iteration 2232, Loss: 4.121807098388672\n",
            "Training Iteration 2233, Loss: 3.9893455505371094\n",
            "Training Iteration 2234, Loss: 1.3639100790023804\n",
            "Training Iteration 2235, Loss: 2.557377576828003\n",
            "Training Iteration 2236, Loss: 3.614565849304199\n",
            "Training Iteration 2237, Loss: 3.72589111328125\n",
            "Training Iteration 2238, Loss: 5.731171131134033\n",
            "Training Iteration 2239, Loss: 6.3091020584106445\n",
            "Training Iteration 2240, Loss: 7.969722747802734\n",
            "Training Iteration 2241, Loss: 8.212496757507324\n",
            "Training Iteration 2242, Loss: 8.595666885375977\n",
            "Training Iteration 2243, Loss: 4.170333385467529\n",
            "Training Iteration 2244, Loss: 3.6916258335113525\n",
            "Training Iteration 2245, Loss: 7.979445457458496\n",
            "Training Iteration 2246, Loss: 9.834548950195312\n",
            "Training Iteration 2247, Loss: 7.224785327911377\n",
            "Training Iteration 2248, Loss: 3.4705820083618164\n",
            "Training Iteration 2249, Loss: 5.857347011566162\n",
            "Training Iteration 2250, Loss: 2.118190050125122\n",
            "Training Iteration 2251, Loss: 4.471418857574463\n",
            "Training Iteration 2252, Loss: 5.854477882385254\n",
            "Training Iteration 2253, Loss: 8.506070137023926\n",
            "Training Iteration 2254, Loss: 1.700635552406311\n",
            "Training Iteration 2255, Loss: 5.1738996505737305\n",
            "Training Iteration 2256, Loss: 3.886406660079956\n",
            "Training Iteration 2257, Loss: 5.799931526184082\n",
            "Training Iteration 2258, Loss: 5.081605911254883\n",
            "Training Iteration 2259, Loss: 3.4193458557128906\n",
            "Training Iteration 2260, Loss: 4.991894721984863\n",
            "Training Iteration 2261, Loss: 4.471712589263916\n",
            "Training Iteration 2262, Loss: 5.927456855773926\n",
            "Training Iteration 2263, Loss: 5.223431587219238\n",
            "Training Iteration 2264, Loss: 1.3458354473114014\n",
            "Training Iteration 2265, Loss: 3.162010669708252\n",
            "Training Iteration 2266, Loss: 4.884096145629883\n",
            "Training Iteration 2267, Loss: 6.157484531402588\n",
            "Training Iteration 2268, Loss: 4.640552043914795\n",
            "Training Iteration 2269, Loss: 4.591976165771484\n",
            "Training Iteration 2270, Loss: 3.4746294021606445\n",
            "Training Iteration 2271, Loss: 1.1675653457641602\n",
            "Training Iteration 2272, Loss: 4.947617053985596\n",
            "Training Iteration 2273, Loss: 5.478488922119141\n",
            "Training Iteration 2274, Loss: 4.9432373046875\n",
            "Training Iteration 2275, Loss: 3.608407497406006\n",
            "Training Iteration 2276, Loss: 3.200701951980591\n",
            "Training Iteration 2277, Loss: 3.2121143341064453\n",
            "Training Iteration 2278, Loss: 5.015110969543457\n",
            "Training Iteration 2279, Loss: 3.440603256225586\n",
            "Training Iteration 2280, Loss: 4.321278095245361\n",
            "Training Iteration 2281, Loss: 3.321983814239502\n",
            "Training Iteration 2282, Loss: 6.227291107177734\n",
            "Training Iteration 2283, Loss: 4.082578182220459\n",
            "Training Iteration 2284, Loss: 5.68595027923584\n",
            "Training Iteration 2285, Loss: 5.75209379196167\n",
            "Training Iteration 2286, Loss: 6.0025200843811035\n",
            "Training Iteration 2287, Loss: 3.4189610481262207\n",
            "Training Iteration 2288, Loss: 6.247387886047363\n",
            "Training Iteration 2289, Loss: 3.2139410972595215\n",
            "Training Iteration 2290, Loss: 5.261534690856934\n",
            "Training Iteration 2291, Loss: 4.350687026977539\n",
            "Training Iteration 2292, Loss: 2.70259165763855\n",
            "Training Iteration 2293, Loss: 5.209384918212891\n",
            "Training Iteration 2294, Loss: 2.8065285682678223\n",
            "Training Iteration 2295, Loss: 2.8914153575897217\n",
            "Training Iteration 2296, Loss: 1.974790334701538\n",
            "Training Iteration 2297, Loss: 6.067753791809082\n",
            "Training Iteration 2298, Loss: 2.8767988681793213\n",
            "Training Iteration 2299, Loss: 6.015719890594482\n",
            "Training Iteration 2300, Loss: 3.75994610786438\n",
            "Training Iteration 2301, Loss: 4.382571697235107\n",
            "Training Iteration 2302, Loss: 5.017435073852539\n",
            "Training Iteration 2303, Loss: 5.42647123336792\n",
            "Training Iteration 2304, Loss: 3.86161470413208\n",
            "Training Iteration 2305, Loss: 3.6216678619384766\n",
            "Training Iteration 2306, Loss: 4.25839376449585\n",
            "Training Iteration 2307, Loss: 3.8774287700653076\n",
            "Training Iteration 2308, Loss: 3.429780960083008\n",
            "Training Iteration 2309, Loss: 4.025123596191406\n",
            "Training Iteration 2310, Loss: 3.580904722213745\n",
            "Training Iteration 2311, Loss: 5.758310317993164\n",
            "Training Iteration 2312, Loss: 3.905280113220215\n",
            "Training Iteration 2313, Loss: 4.903337001800537\n",
            "Training Iteration 2314, Loss: 3.072779655456543\n",
            "Training Iteration 2315, Loss: 6.28495979309082\n",
            "Training Iteration 2316, Loss: 4.77910041809082\n",
            "Training Iteration 2317, Loss: 5.157186508178711\n",
            "Training Iteration 2318, Loss: 3.50278639793396\n",
            "Training Iteration 2319, Loss: 4.534807205200195\n",
            "Training Iteration 2320, Loss: 1.2033299207687378\n",
            "Training Iteration 2321, Loss: 3.648545980453491\n",
            "Training Iteration 2322, Loss: 3.4081006050109863\n",
            "Training Iteration 2323, Loss: 4.104193687438965\n",
            "Training Iteration 2324, Loss: 3.3608059883117676\n",
            "Training Iteration 2325, Loss: 2.257913827896118\n",
            "Training Iteration 2326, Loss: 2.0421767234802246\n",
            "Training Iteration 2327, Loss: 3.6242198944091797\n",
            "Training Iteration 2328, Loss: 6.078534126281738\n",
            "Training Iteration 2329, Loss: 4.136763572692871\n",
            "Training Iteration 2330, Loss: 3.4172565937042236\n",
            "Training Iteration 2331, Loss: 4.556670188903809\n",
            "Training Iteration 2332, Loss: 2.744033098220825\n",
            "Training Iteration 2333, Loss: 3.935286283493042\n",
            "Training Iteration 2334, Loss: 2.793590545654297\n",
            "Training Iteration 2335, Loss: 2.06695818901062\n",
            "Training Iteration 2336, Loss: 2.239931344985962\n",
            "Training Iteration 2337, Loss: 3.7201693058013916\n",
            "Training Iteration 2338, Loss: 3.6963329315185547\n",
            "Training Iteration 2339, Loss: 2.958065986633301\n",
            "Training Iteration 2340, Loss: 2.7875747680664062\n",
            "Training Iteration 2341, Loss: 4.548326015472412\n",
            "Training Iteration 2342, Loss: 3.5010793209075928\n",
            "Training Iteration 2343, Loss: 3.2856030464172363\n",
            "Training Iteration 2344, Loss: 3.166670322418213\n",
            "Training Iteration 2345, Loss: 4.198968410491943\n",
            "Training Iteration 2346, Loss: 2.5176446437835693\n",
            "Training Iteration 2347, Loss: 3.628187656402588\n",
            "Training Iteration 2348, Loss: 5.2734551429748535\n",
            "Training Iteration 2349, Loss: 3.8122966289520264\n",
            "Training Iteration 2350, Loss: 4.40584659576416\n",
            "Training Iteration 2351, Loss: 5.2145466804504395\n",
            "Training Iteration 2352, Loss: 5.294734477996826\n",
            "Training Iteration 2353, Loss: 3.7903287410736084\n",
            "Training Iteration 2354, Loss: 2.8323488235473633\n",
            "Training Iteration 2355, Loss: 4.176260948181152\n",
            "Training Iteration 2356, Loss: 3.3151278495788574\n",
            "Training Iteration 2357, Loss: 2.9455995559692383\n",
            "Training Iteration 2358, Loss: 5.779817581176758\n",
            "Training Iteration 2359, Loss: 4.962534427642822\n",
            "Training Iteration 2360, Loss: 4.408304691314697\n",
            "Training Iteration 2361, Loss: 3.900613784790039\n",
            "Training Iteration 2362, Loss: 1.8681457042694092\n",
            "Training Iteration 2363, Loss: 3.4788618087768555\n",
            "Training Iteration 2364, Loss: 4.6793622970581055\n",
            "Training Iteration 2365, Loss: 3.8743016719818115\n",
            "Training Iteration 2366, Loss: 3.976950168609619\n",
            "Training Iteration 2367, Loss: 4.041027069091797\n",
            "Training Iteration 2368, Loss: 4.131253242492676\n",
            "Training Iteration 2369, Loss: 5.290792465209961\n",
            "Training Iteration 2370, Loss: 2.507207155227661\n",
            "Training Iteration 2371, Loss: 4.9904584884643555\n",
            "Training Iteration 2372, Loss: 2.3615787029266357\n",
            "Training Iteration 2373, Loss: 1.5583925247192383\n",
            "Training Iteration 2374, Loss: 4.856865406036377\n",
            "Training Iteration 2375, Loss: 4.071777820587158\n",
            "Training Iteration 2376, Loss: 4.279449462890625\n",
            "Training Iteration 2377, Loss: 4.561695098876953\n",
            "Training Iteration 2378, Loss: 2.0985329151153564\n",
            "Training Iteration 2379, Loss: 7.384253978729248\n",
            "Training Iteration 2380, Loss: 3.656024932861328\n",
            "Training Iteration 2381, Loss: 5.39743709564209\n",
            "Training Iteration 2382, Loss: 3.6959404945373535\n",
            "Training Iteration 2383, Loss: 3.182490348815918\n",
            "Training Iteration 2384, Loss: 5.096382141113281\n",
            "Training Iteration 2385, Loss: 6.415983200073242\n",
            "Training Iteration 2386, Loss: 6.533908843994141\n",
            "Training Iteration 2387, Loss: 5.089751720428467\n",
            "Training Iteration 2388, Loss: 3.2301933765411377\n",
            "Training Iteration 2389, Loss: 2.74233078956604\n",
            "Training Iteration 2390, Loss: 4.928926944732666\n",
            "Training Iteration 2391, Loss: 4.910834312438965\n",
            "Training Iteration 2392, Loss: 3.0297069549560547\n",
            "Training Iteration 2393, Loss: 6.281085968017578\n",
            "Training Iteration 2394, Loss: 2.9701614379882812\n",
            "Training Iteration 2395, Loss: 4.916698455810547\n",
            "Training Iteration 2396, Loss: 4.5043511390686035\n",
            "Training Iteration 2397, Loss: 4.241992950439453\n",
            "Training Iteration 2398, Loss: 3.378321647644043\n",
            "Training Iteration 2399, Loss: 3.8828067779541016\n",
            "Training Iteration 2400, Loss: 4.624935626983643\n",
            "Training Iteration 2401, Loss: 4.543239593505859\n",
            "Training Iteration 2402, Loss: 5.579737663269043\n",
            "Training Iteration 2403, Loss: 3.3633742332458496\n",
            "Training Iteration 2404, Loss: 4.29241418838501\n",
            "Training Iteration 2405, Loss: 4.945389747619629\n",
            "Training Iteration 2406, Loss: 4.206611156463623\n",
            "Training Iteration 2407, Loss: 5.46319580078125\n",
            "Training Iteration 2408, Loss: 1.0428966283798218\n",
            "Training Iteration 2409, Loss: 6.606637954711914\n",
            "Training Iteration 2410, Loss: 9.269222259521484\n",
            "Training Iteration 2411, Loss: 3.9618778228759766\n",
            "Training Iteration 2412, Loss: 2.6369946002960205\n",
            "Training Iteration 2413, Loss: 3.0710697174072266\n",
            "Training Iteration 2414, Loss: 4.615582466125488\n",
            "Training Iteration 2415, Loss: 6.457977771759033\n",
            "Training Iteration 2416, Loss: 11.123734474182129\n",
            "Training Iteration 2417, Loss: 6.1061787605285645\n",
            "Training Iteration 2418, Loss: 7.155498027801514\n",
            "Training Iteration 2419, Loss: 2.908189296722412\n",
            "Training Iteration 2420, Loss: 4.433798313140869\n",
            "Training Iteration 2421, Loss: 4.571059703826904\n",
            "Training Iteration 2422, Loss: 5.352503776550293\n",
            "Training Iteration 2423, Loss: 6.619357109069824\n",
            "Training Iteration 2424, Loss: 3.100353479385376\n",
            "Training Iteration 2425, Loss: 4.418379783630371\n",
            "Training Iteration 2426, Loss: 3.5905089378356934\n",
            "Training Iteration 2427, Loss: 3.189326763153076\n",
            "Training Iteration 2428, Loss: 2.8617563247680664\n",
            "Training Iteration 2429, Loss: 3.2134199142456055\n",
            "Training Iteration 2430, Loss: 3.013442039489746\n",
            "Training Iteration 2431, Loss: 5.216482162475586\n",
            "Training Iteration 2432, Loss: 2.497011184692383\n",
            "Training Iteration 2433, Loss: 3.141627550125122\n",
            "Training Iteration 2434, Loss: 4.033834457397461\n",
            "Training Iteration 2435, Loss: 8.58696460723877\n",
            "Training Iteration 2436, Loss: 3.231534481048584\n",
            "Training Iteration 2437, Loss: 2.694789409637451\n",
            "Training Iteration 2438, Loss: 1.7454437017440796\n",
            "Training Iteration 2439, Loss: 4.032325744628906\n",
            "Training Iteration 2440, Loss: 7.059250831604004\n",
            "Training Iteration 2441, Loss: 5.552101135253906\n",
            "Training Iteration 2442, Loss: 8.381050109863281\n",
            "Training Iteration 2443, Loss: 7.020662784576416\n",
            "Training Iteration 2444, Loss: 9.03880500793457\n",
            "Training Iteration 2445, Loss: 5.583691120147705\n",
            "Training Iteration 2446, Loss: 4.080462455749512\n",
            "Training Iteration 2447, Loss: 4.306402206420898\n",
            "Training Iteration 2448, Loss: 4.0893144607543945\n",
            "Training Iteration 2449, Loss: 3.3797154426574707\n",
            "Training Iteration 2450, Loss: 5.700058937072754\n",
            "Training Iteration 2451, Loss: 5.5655107498168945\n",
            "Training Iteration 2452, Loss: 5.371289253234863\n",
            "Training Iteration 2453, Loss: 7.881576061248779\n",
            "Training Iteration 2454, Loss: 3.3307957649230957\n",
            "Training Iteration 2455, Loss: 6.948386192321777\n",
            "Training Iteration 2456, Loss: 5.114071846008301\n",
            "Training Iteration 2457, Loss: 6.038853645324707\n",
            "Training Iteration 2458, Loss: 7.320043563842773\n",
            "Training Iteration 2459, Loss: 4.2973222732543945\n",
            "Training Iteration 2460, Loss: 5.640750408172607\n",
            "Training Iteration 2461, Loss: 5.4152512550354\n",
            "Training Iteration 2462, Loss: 2.7045369148254395\n",
            "Training Iteration 2463, Loss: 2.703500986099243\n",
            "Training Iteration 2464, Loss: 6.268677711486816\n",
            "Training Iteration 2465, Loss: 2.5256459712982178\n",
            "Training Iteration 2466, Loss: 1.9122750759124756\n",
            "Training Iteration 2467, Loss: 3.767101287841797\n",
            "Training Iteration 2468, Loss: 2.0033316612243652\n",
            "Training Iteration 2469, Loss: 6.83815336227417\n",
            "Training Iteration 2470, Loss: 5.0405144691467285\n",
            "Training Iteration 2471, Loss: 2.992978572845459\n",
            "Training Iteration 2472, Loss: 4.48576545715332\n",
            "Training Iteration 2473, Loss: 3.6913697719573975\n",
            "Training Iteration 2474, Loss: 8.250774383544922\n",
            "Training Iteration 2475, Loss: 4.081079959869385\n",
            "Training Iteration 2476, Loss: 6.086302757263184\n",
            "Training Iteration 2477, Loss: 3.482454776763916\n",
            "Training Iteration 2478, Loss: 3.698639392852783\n",
            "Training Iteration 2479, Loss: 4.587190628051758\n",
            "Training Iteration 2480, Loss: 2.332249402999878\n",
            "Training Iteration 2481, Loss: 2.8397202491760254\n",
            "Training Iteration 2482, Loss: 4.698427200317383\n",
            "Training Iteration 2483, Loss: 3.6526451110839844\n",
            "Training Iteration 2484, Loss: 4.181268692016602\n",
            "Training Iteration 2485, Loss: 3.493635892868042\n",
            "Training Iteration 2486, Loss: 4.79319953918457\n",
            "Training Iteration 2487, Loss: 2.5666921138763428\n",
            "Training Iteration 2488, Loss: 5.174524307250977\n",
            "Training Iteration 2489, Loss: 4.1231794357299805\n",
            "Training Iteration 2490, Loss: 2.101118803024292\n",
            "Training Iteration 2491, Loss: 3.482959747314453\n",
            "Training Iteration 2492, Loss: 5.061640739440918\n",
            "Training Iteration 2493, Loss: 3.4308626651763916\n",
            "Training Iteration 2494, Loss: 2.2257089614868164\n",
            "Training Iteration 2495, Loss: 2.894207239151001\n",
            "Training Iteration 2496, Loss: 5.072261810302734\n",
            "Training Iteration 2497, Loss: 2.6355786323547363\n",
            "Training Iteration 2498, Loss: 5.387804985046387\n",
            "Training Iteration 2499, Loss: 6.3747477531433105\n",
            "Training Iteration 2500, Loss: 5.464745044708252\n",
            "Training Iteration 2501, Loss: 3.662327289581299\n",
            "Training Iteration 2502, Loss: 5.119441032409668\n",
            "Training Iteration 2503, Loss: 5.455010890960693\n",
            "Training Iteration 2504, Loss: 5.521531105041504\n",
            "Training Iteration 2505, Loss: 4.805054664611816\n",
            "Training Iteration 2506, Loss: 4.016876220703125\n",
            "Training Iteration 2507, Loss: 2.6782400608062744\n",
            "Training Iteration 2508, Loss: 5.825885772705078\n",
            "Training Iteration 2509, Loss: 4.279453277587891\n",
            "Training Iteration 2510, Loss: 2.004228115081787\n",
            "Training Iteration 2511, Loss: 7.351554870605469\n",
            "Training Iteration 2512, Loss: 3.1253671646118164\n",
            "Training Iteration 2513, Loss: 3.100931167602539\n",
            "Training Iteration 2514, Loss: 3.3155994415283203\n",
            "Training Iteration 2515, Loss: 4.944936275482178\n",
            "Training Iteration 2516, Loss: 2.2424912452697754\n",
            "Training Iteration 2517, Loss: 2.7560622692108154\n",
            "Training Iteration 2518, Loss: 2.9164280891418457\n",
            "Training Iteration 2519, Loss: 3.523196220397949\n",
            "Training Iteration 2520, Loss: 3.602626323699951\n",
            "Training Iteration 2521, Loss: 3.2901031970977783\n",
            "Training Iteration 2522, Loss: 4.536506652832031\n",
            "Training Iteration 2523, Loss: 3.3777692317962646\n",
            "Training Iteration 2524, Loss: 2.5860118865966797\n",
            "Training Iteration 2525, Loss: 4.617220401763916\n",
            "Training Iteration 2526, Loss: 4.278738021850586\n",
            "Training Iteration 2527, Loss: 5.878743648529053\n",
            "Training Iteration 2528, Loss: 2.569169044494629\n",
            "Training Iteration 2529, Loss: 2.3274779319763184\n",
            "Training Iteration 2530, Loss: 5.115387439727783\n",
            "Training Iteration 2531, Loss: 3.8009119033813477\n",
            "Training Iteration 2532, Loss: 2.851259231567383\n",
            "Training Iteration 2533, Loss: 5.010305881500244\n",
            "Training Iteration 2534, Loss: 4.298708915710449\n",
            "Training Iteration 2535, Loss: 4.132113933563232\n",
            "Training Iteration 2536, Loss: 3.2949633598327637\n",
            "Training Iteration 2537, Loss: 6.8409576416015625\n",
            "Training Iteration 2538, Loss: 3.5180797576904297\n",
            "Training Iteration 2539, Loss: 1.5781038999557495\n",
            "Training Iteration 2540, Loss: 6.301259994506836\n",
            "Training Iteration 2541, Loss: 2.7100679874420166\n",
            "Training Iteration 2542, Loss: 3.8134846687316895\n",
            "Training Iteration 2543, Loss: 5.089813232421875\n",
            "Training Iteration 2544, Loss: 5.013636589050293\n",
            "Training Iteration 2545, Loss: 4.76609468460083\n",
            "Training Iteration 2546, Loss: 3.292665481567383\n",
            "Training Iteration 2547, Loss: 8.131219863891602\n",
            "Training Iteration 2548, Loss: 3.7830445766448975\n",
            "Training Iteration 2549, Loss: 4.924781799316406\n",
            "Training Iteration 2550, Loss: 3.4972007274627686\n",
            "Training Iteration 2551, Loss: 3.1197147369384766\n",
            "Training Iteration 2552, Loss: 7.271327495574951\n",
            "Training Iteration 2553, Loss: 5.738158702850342\n",
            "Training Iteration 2554, Loss: 4.473550319671631\n",
            "Training Iteration 2555, Loss: 3.46291446685791\n",
            "Training Iteration 2556, Loss: 2.1760826110839844\n",
            "Training Iteration 2557, Loss: 4.901873588562012\n",
            "Training Iteration 2558, Loss: 3.1877925395965576\n",
            "Training Iteration 2559, Loss: 2.1287975311279297\n",
            "Training Iteration 2560, Loss: 3.451075315475464\n",
            "Training Iteration 2561, Loss: 3.2282705307006836\n",
            "Training Iteration 2562, Loss: 2.4456591606140137\n",
            "Training Iteration 2563, Loss: 4.342896938323975\n",
            "Training Iteration 2564, Loss: 2.2363767623901367\n",
            "Training Iteration 2565, Loss: 4.545310020446777\n",
            "Training Iteration 2566, Loss: 4.607003211975098\n",
            "Training Iteration 2567, Loss: 4.3522539138793945\n",
            "Training Iteration 2568, Loss: 3.811492681503296\n",
            "Training Iteration 2569, Loss: 3.195683479309082\n",
            "Training Iteration 2570, Loss: 5.876664638519287\n",
            "Training Iteration 2571, Loss: 2.8751091957092285\n",
            "Training Iteration 2572, Loss: 3.0247554779052734\n",
            "Training Iteration 2573, Loss: 3.5630545616149902\n",
            "Training Iteration 2574, Loss: 2.5908806324005127\n",
            "Training Iteration 2575, Loss: 4.9342732429504395\n",
            "Training Iteration 2576, Loss: 6.719119071960449\n",
            "Training Iteration 2577, Loss: 6.889865875244141\n",
            "Training Iteration 2578, Loss: 5.859602928161621\n",
            "Training Iteration 2579, Loss: 3.5401976108551025\n",
            "Training Iteration 2580, Loss: 7.658580303192139\n",
            "Training Iteration 2581, Loss: 6.998175621032715\n",
            "Training Iteration 2582, Loss: 4.966873645782471\n",
            "Training Iteration 2583, Loss: 8.377462387084961\n",
            "Training Iteration 2584, Loss: 4.431158542633057\n",
            "Training Iteration 2585, Loss: 3.0816972255706787\n",
            "Training Iteration 2586, Loss: 4.904364109039307\n",
            "Training Iteration 2587, Loss: 2.203634262084961\n",
            "Training Iteration 2588, Loss: 5.524645805358887\n",
            "Training Iteration 2589, Loss: 7.669646739959717\n",
            "Training Iteration 2590, Loss: 2.871225595474243\n",
            "Training Iteration 2591, Loss: 4.51959228515625\n",
            "Training Iteration 2592, Loss: 3.4302077293395996\n",
            "Training Iteration 2593, Loss: 4.498061180114746\n",
            "Training Iteration 2594, Loss: 4.2981696128845215\n",
            "Training Iteration 2595, Loss: 3.9741625785827637\n",
            "Training Iteration 2596, Loss: 4.907959938049316\n",
            "Training Iteration 2597, Loss: 3.5831398963928223\n",
            "Training Iteration 2598, Loss: 2.4536616802215576\n",
            "Training Iteration 2599, Loss: 2.9962141513824463\n",
            "Training Iteration 2600, Loss: 5.724948406219482\n",
            "Training Iteration 2601, Loss: 6.060846328735352\n",
            "Training Iteration 2602, Loss: 4.958056926727295\n",
            "Training Iteration 2603, Loss: 1.8857632875442505\n",
            "Training Iteration 2604, Loss: 1.9919190406799316\n",
            "Training Iteration 2605, Loss: 2.5478644371032715\n",
            "Training Iteration 2606, Loss: 3.0409679412841797\n",
            "Training Iteration 2607, Loss: 4.703273773193359\n",
            "Training Iteration 2608, Loss: 2.877687931060791\n",
            "Training Iteration 2609, Loss: 4.4172868728637695\n",
            "Training Iteration 2610, Loss: 4.086923599243164\n",
            "Training Iteration 2611, Loss: 4.292849540710449\n",
            "Training Iteration 2612, Loss: 4.218066692352295\n",
            "Training Iteration 2613, Loss: 4.423139572143555\n",
            "Training Iteration 2614, Loss: 5.069780349731445\n",
            "Training Iteration 2615, Loss: 5.003436088562012\n",
            "Training Iteration 2616, Loss: 4.255336761474609\n",
            "Training Iteration 2617, Loss: 4.148617744445801\n",
            "Training Iteration 2618, Loss: 3.605243682861328\n",
            "Training Iteration 2619, Loss: 1.6495075225830078\n",
            "Training Iteration 2620, Loss: 4.555667400360107\n",
            "Training Iteration 2621, Loss: 6.282756328582764\n",
            "Training Iteration 2622, Loss: 5.318121433258057\n",
            "Training Iteration 2623, Loss: 5.9218316078186035\n",
            "Training Iteration 2624, Loss: 2.74747633934021\n",
            "Training Iteration 2625, Loss: 5.414259910583496\n",
            "Training Iteration 2626, Loss: 4.556716442108154\n",
            "Training Iteration 2627, Loss: 3.2856054306030273\n",
            "Training Iteration 2628, Loss: 1.9381968975067139\n",
            "Training Iteration 2629, Loss: 5.042354106903076\n",
            "Training Iteration 2630, Loss: 4.148402690887451\n",
            "Training Iteration 2631, Loss: 3.0862317085266113\n",
            "Training Iteration 2632, Loss: 1.6817626953125\n",
            "Training Iteration 2633, Loss: 2.3313522338867188\n",
            "Training Iteration 2634, Loss: 4.314842700958252\n",
            "Training Iteration 2635, Loss: 4.5707902908325195\n",
            "Training Iteration 2636, Loss: 5.900485515594482\n",
            "Training Iteration 2637, Loss: 8.132118225097656\n",
            "Training Iteration 2638, Loss: 5.601303577423096\n",
            "Training Iteration 2639, Loss: 2.1234211921691895\n",
            "Training Iteration 2640, Loss: 4.441090106964111\n",
            "Training Iteration 2641, Loss: 2.7394442558288574\n",
            "Training Iteration 2642, Loss: 4.211774826049805\n",
            "Training Iteration 2643, Loss: 3.6917929649353027\n",
            "Training Iteration 2644, Loss: 3.1623971462249756\n",
            "Training Iteration 2645, Loss: 3.222721815109253\n",
            "Training Iteration 2646, Loss: 3.865959644317627\n",
            "Training Iteration 2647, Loss: 4.147162914276123\n",
            "Training Iteration 2648, Loss: 5.462404251098633\n",
            "Training Iteration 2649, Loss: 4.588981628417969\n",
            "Training Iteration 2650, Loss: 3.960423707962036\n",
            "Training Iteration 2651, Loss: 3.05188250541687\n",
            "Training Iteration 2652, Loss: 2.338192939758301\n",
            "Training Iteration 2653, Loss: 5.930085182189941\n",
            "Training Iteration 2654, Loss: 4.666765213012695\n",
            "Training Iteration 2655, Loss: 5.355299472808838\n",
            "Training Iteration 2656, Loss: 4.047563552856445\n",
            "Training Iteration 2657, Loss: 5.470088481903076\n",
            "Training Iteration 2658, Loss: 4.688050270080566\n",
            "Training Iteration 2659, Loss: 6.785000801086426\n",
            "Training Iteration 2660, Loss: 6.757412433624268\n",
            "Training Iteration 2661, Loss: 4.336400985717773\n",
            "Training Iteration 2662, Loss: 5.954137802124023\n",
            "Training Iteration 2663, Loss: 3.4420812129974365\n",
            "Training Iteration 2664, Loss: 3.841676712036133\n",
            "Training Iteration 2665, Loss: 4.083541393280029\n",
            "Training Iteration 2666, Loss: 6.175181865692139\n",
            "Training Iteration 2667, Loss: 2.934971570968628\n",
            "Training Iteration 2668, Loss: 4.837385654449463\n",
            "Training Iteration 2669, Loss: 7.032188415527344\n",
            "Training Iteration 2670, Loss: 1.5599321126937866\n",
            "Training Iteration 2671, Loss: 5.103424072265625\n",
            "Training Iteration 2672, Loss: 2.1514225006103516\n",
            "Training Iteration 2673, Loss: 5.0153489112854\n",
            "Training Iteration 2674, Loss: 7.070442199707031\n",
            "Training Iteration 2675, Loss: 2.711562395095825\n",
            "Training Iteration 2676, Loss: 3.0430245399475098\n",
            "Training Iteration 2677, Loss: 4.780952453613281\n",
            "Training Iteration 2678, Loss: 4.099464416503906\n",
            "Training Iteration 2679, Loss: 6.697193622589111\n",
            "Training Iteration 2680, Loss: 5.731686592102051\n",
            "Training Iteration 2681, Loss: 5.750295162200928\n",
            "Training Iteration 2682, Loss: 3.2937159538269043\n",
            "Training Iteration 2683, Loss: 1.934441328048706\n",
            "Training Iteration 2684, Loss: 5.435103893280029\n",
            "Training Iteration 2685, Loss: 2.204873561859131\n",
            "Training Iteration 2686, Loss: 4.632518768310547\n",
            "Training Iteration 2687, Loss: 8.876452445983887\n",
            "Training Iteration 2688, Loss: 8.21647834777832\n",
            "Training Iteration 2689, Loss: 3.8247196674346924\n",
            "Training Iteration 2690, Loss: 6.124446868896484\n",
            "Training Iteration 2691, Loss: 3.5385546684265137\n",
            "Training Iteration 2692, Loss: 5.050959587097168\n",
            "Training Iteration 2693, Loss: 9.721494674682617\n",
            "Training Iteration 2694, Loss: 3.809839963912964\n",
            "Training Iteration 2695, Loss: 2.5844883918762207\n",
            "Training Iteration 2696, Loss: 6.855709075927734\n",
            "Training Iteration 2697, Loss: 7.85349178314209\n",
            "Training Iteration 2698, Loss: 4.227354526519775\n",
            "Training Iteration 2699, Loss: 6.677306175231934\n",
            "Training Iteration 2700, Loss: 6.454812526702881\n",
            "Training Iteration 2701, Loss: 3.8059163093566895\n",
            "Training Iteration 2702, Loss: 2.645418167114258\n",
            "Training Iteration 2703, Loss: 3.562248706817627\n",
            "Training Iteration 2704, Loss: 4.567209720611572\n",
            "Training Iteration 2705, Loss: 5.05232048034668\n",
            "Training Iteration 2706, Loss: 3.895312786102295\n",
            "Training Iteration 2707, Loss: 2.0939323902130127\n",
            "Training Iteration 2708, Loss: 7.5005669593811035\n",
            "Training Iteration 2709, Loss: 5.8370041847229\n",
            "Training Iteration 2710, Loss: 7.7319536209106445\n",
            "Training Iteration 2711, Loss: 5.224234580993652\n",
            "Training Iteration 2712, Loss: 2.217341184616089\n",
            "Training Iteration 2713, Loss: 1.5552949905395508\n",
            "Training Iteration 2714, Loss: 8.066737174987793\n",
            "Training Iteration 2715, Loss: 4.586878299713135\n",
            "Training Iteration 2716, Loss: 4.009139537811279\n",
            "Training Iteration 2717, Loss: 5.283208847045898\n",
            "Training Iteration 2718, Loss: 4.967684745788574\n",
            "Training Iteration 2719, Loss: 2.9690186977386475\n",
            "Training Iteration 2720, Loss: 2.653552293777466\n",
            "Training Iteration 2721, Loss: 4.424524784088135\n",
            "Training Iteration 2722, Loss: 5.217938423156738\n",
            "Training Iteration 2723, Loss: 3.1519105434417725\n",
            "Training Iteration 2724, Loss: 4.930877685546875\n",
            "Training Iteration 2725, Loss: 3.697911024093628\n",
            "Training Iteration 2726, Loss: 4.170012950897217\n",
            "Training Iteration 2727, Loss: 1.4063276052474976\n",
            "Training Iteration 2728, Loss: 2.041644334793091\n",
            "Training Iteration 2729, Loss: 2.1785433292388916\n",
            "Training Iteration 2730, Loss: 3.649810791015625\n",
            "Training Iteration 2731, Loss: 5.365019798278809\n",
            "Training Iteration 2732, Loss: 4.558075904846191\n",
            "Training Iteration 2733, Loss: 6.4356489181518555\n",
            "Training Iteration 2734, Loss: 4.489140033721924\n",
            "Training Iteration 2735, Loss: 7.550219535827637\n",
            "Training Iteration 2736, Loss: 7.9246978759765625\n",
            "Training Iteration 2737, Loss: 4.681259632110596\n",
            "Training Iteration 2738, Loss: 4.346253871917725\n",
            "Training Iteration 2739, Loss: 2.2111687660217285\n",
            "Training Iteration 2740, Loss: 6.743305683135986\n",
            "Training Iteration 2741, Loss: 2.99149751663208\n",
            "Training Iteration 2742, Loss: 2.7097606658935547\n",
            "Training Iteration 2743, Loss: 5.890308380126953\n",
            "Training Iteration 2744, Loss: 5.772510051727295\n",
            "Training Iteration 2745, Loss: 2.814791679382324\n",
            "Training Iteration 2746, Loss: 6.285998821258545\n",
            "Training Iteration 2747, Loss: 6.513945579528809\n",
            "Training Iteration 2748, Loss: 1.7639057636260986\n",
            "Training Iteration 2749, Loss: 6.1015448570251465\n",
            "Training Iteration 2750, Loss: 4.13760232925415\n",
            "Training Iteration 2751, Loss: 5.538609504699707\n",
            "Training Iteration 2752, Loss: 4.8667097091674805\n",
            "Training Iteration 2753, Loss: 3.447510004043579\n",
            "Training Iteration 2754, Loss: 6.88683557510376\n",
            "Training Iteration 2755, Loss: 3.375925064086914\n",
            "Training Iteration 2756, Loss: 2.109020948410034\n",
            "Training Iteration 2757, Loss: 4.858755111694336\n",
            "Training Iteration 2758, Loss: 5.981669902801514\n",
            "Training Iteration 2759, Loss: 3.2208683490753174\n",
            "Training Iteration 2760, Loss: 4.557368755340576\n",
            "Training Iteration 2761, Loss: 5.02209997177124\n",
            "Training Iteration 2762, Loss: 4.852380752563477\n",
            "Training Iteration 2763, Loss: 4.317255020141602\n",
            "Training Iteration 2764, Loss: 4.204957008361816\n",
            "Training Iteration 2765, Loss: 4.370178699493408\n",
            "Training Iteration 2766, Loss: 4.228005409240723\n",
            "Training Iteration 2767, Loss: 2.8571767807006836\n",
            "Training Iteration 2768, Loss: 7.402530670166016\n",
            "Training Iteration 2769, Loss: 3.008420467376709\n",
            "Training Iteration 2770, Loss: 3.63375186920166\n",
            "Training Iteration 2771, Loss: 7.73725700378418\n",
            "Training Iteration 2772, Loss: 7.010608196258545\n",
            "Training Iteration 2773, Loss: 4.202507495880127\n",
            "Training Iteration 2774, Loss: 5.40117073059082\n",
            "Training Iteration 2775, Loss: 3.1156604290008545\n",
            "Training Iteration 2776, Loss: 6.432234764099121\n",
            "Training Iteration 2777, Loss: 6.8373284339904785\n",
            "Training Iteration 2778, Loss: 3.467409610748291\n",
            "Training Iteration 2779, Loss: 6.378681182861328\n",
            "Training Iteration 2780, Loss: 6.11630392074585\n",
            "Training Iteration 2781, Loss: 3.203453779220581\n",
            "Training Iteration 2782, Loss: 5.302584171295166\n",
            "Training Iteration 2783, Loss: 3.8348073959350586\n",
            "Training Iteration 2784, Loss: 4.40970516204834\n",
            "Training Iteration 2785, Loss: 1.7608269453048706\n",
            "Training Iteration 2786, Loss: 4.216153144836426\n",
            "Training Iteration 2787, Loss: 7.933406352996826\n",
            "Training Iteration 2788, Loss: 2.3927512168884277\n",
            "Training Iteration 2789, Loss: 3.6796774864196777\n",
            "Training Iteration 2790, Loss: 3.7045702934265137\n",
            "Training Iteration 2791, Loss: 3.226367235183716\n",
            "Training Iteration 2792, Loss: 4.542054653167725\n",
            "Training Iteration 2793, Loss: 4.492331504821777\n",
            "Training Iteration 2794, Loss: 3.2980434894561768\n",
            "Training Iteration 2795, Loss: 4.0053181648254395\n",
            "Training Iteration 2796, Loss: 3.787816047668457\n",
            "Training Iteration 2797, Loss: 5.975669860839844\n",
            "Training Iteration 2798, Loss: 3.9693686962127686\n",
            "Training Iteration 2799, Loss: 3.90690541267395\n",
            "Training Iteration 2800, Loss: 9.409226417541504\n",
            "Training Iteration 2801, Loss: 3.2887134552001953\n",
            "Training Iteration 2802, Loss: 8.017561912536621\n",
            "Training Iteration 2803, Loss: 4.755041599273682\n",
            "Training Iteration 2804, Loss: 4.377999782562256\n",
            "Training Iteration 2805, Loss: 2.9968836307525635\n",
            "Training Iteration 2806, Loss: 2.409515142440796\n",
            "Training Iteration 2807, Loss: 4.000805377960205\n",
            "Training Iteration 2808, Loss: 6.035290241241455\n",
            "Training Iteration 2809, Loss: 8.073014259338379\n",
            "Training Iteration 2810, Loss: 5.089371681213379\n",
            "Training Iteration 2811, Loss: 5.615575790405273\n",
            "Training Iteration 2812, Loss: 6.07242488861084\n",
            "Training Iteration 2813, Loss: 8.920022964477539\n",
            "Training Iteration 2814, Loss: 6.225098133087158\n",
            "Training Iteration 2815, Loss: 5.432283878326416\n",
            "Training Iteration 2816, Loss: 3.4046077728271484\n",
            "Training Iteration 2817, Loss: 3.408773422241211\n",
            "Training Iteration 2818, Loss: 2.179151773452759\n",
            "Training Iteration 2819, Loss: 6.882162570953369\n",
            "Training Iteration 2820, Loss: 2.493868827819824\n",
            "Training Iteration 2821, Loss: 1.6044775247573853\n",
            "Training Iteration 2822, Loss: 8.466947555541992\n",
            "Training Iteration 2823, Loss: 8.283487319946289\n",
            "Training Iteration 2824, Loss: 4.679873466491699\n",
            "Training Iteration 2825, Loss: 3.8438405990600586\n",
            "Training Iteration 2826, Loss: 3.284611701965332\n",
            "Training Iteration 2827, Loss: 4.540963172912598\n",
            "Training Iteration 2828, Loss: 9.082274436950684\n",
            "Training Iteration 2829, Loss: 5.515389919281006\n",
            "Training Iteration 2830, Loss: 3.4388790130615234\n",
            "Training Iteration 2831, Loss: 10.214876174926758\n",
            "Training Iteration 2832, Loss: 8.17954158782959\n",
            "Training Iteration 2833, Loss: 7.598008632659912\n",
            "Training Iteration 2834, Loss: 5.555331230163574\n",
            "Training Iteration 2835, Loss: 4.875714302062988\n",
            "Training Iteration 2836, Loss: 4.862220287322998\n",
            "Training Iteration 2837, Loss: 4.654584884643555\n",
            "Training Iteration 2838, Loss: 2.100879192352295\n",
            "Training Iteration 2839, Loss: 6.634939670562744\n",
            "Training Iteration 2840, Loss: 5.403386116027832\n",
            "Training Iteration 2841, Loss: 5.533105373382568\n",
            "Training Iteration 2842, Loss: 3.072998046875\n",
            "Training Iteration 2843, Loss: 4.028572082519531\n",
            "Training Iteration 2844, Loss: 4.448924541473389\n",
            "Training Iteration 2845, Loss: 5.676399230957031\n",
            "Training Iteration 2846, Loss: 3.877379894256592\n",
            "Training Iteration 2847, Loss: 7.752562999725342\n",
            "Training Iteration 2848, Loss: 4.7568278312683105\n",
            "Training Iteration 2849, Loss: 3.3478426933288574\n",
            "Training Iteration 2850, Loss: 2.8389170169830322\n",
            "Training Iteration 2851, Loss: 2.8907556533813477\n",
            "Training Iteration 2852, Loss: 2.568699836730957\n",
            "Training Iteration 2853, Loss: 4.497739315032959\n",
            "Training Iteration 2854, Loss: 3.5820093154907227\n",
            "Training Iteration 2855, Loss: 2.7107462882995605\n",
            "Training Iteration 2856, Loss: 4.576981544494629\n",
            "Training Iteration 2857, Loss: 3.893960952758789\n",
            "Training Iteration 2858, Loss: 1.689572811126709\n",
            "Training Iteration 2859, Loss: 4.488379955291748\n",
            "Training Iteration 2860, Loss: 7.044386863708496\n",
            "Training Iteration 2861, Loss: 9.278780937194824\n",
            "Training Iteration 2862, Loss: 7.748831272125244\n",
            "Training Iteration 2863, Loss: 3.068243980407715\n",
            "Training Iteration 2864, Loss: 3.025108814239502\n",
            "Training Iteration 2865, Loss: 3.5955047607421875\n",
            "Training Iteration 2866, Loss: 7.782713890075684\n",
            "Training Iteration 2867, Loss: 7.652552604675293\n",
            "Training Iteration 2868, Loss: 4.542718410491943\n",
            "Training Iteration 2869, Loss: 3.8245086669921875\n",
            "Training Iteration 2870, Loss: 1.7095927000045776\n",
            "Training Iteration 2871, Loss: 6.774524688720703\n",
            "Training Iteration 2872, Loss: 3.3387107849121094\n",
            "Training Iteration 2873, Loss: 9.56533432006836\n",
            "Training Iteration 2874, Loss: 3.8640878200531006\n",
            "Training Iteration 2875, Loss: 5.7603654861450195\n",
            "Training Iteration 2876, Loss: 7.0794172286987305\n",
            "Training Iteration 2877, Loss: 1.9755712747573853\n",
            "Training Iteration 2878, Loss: 8.913176536560059\n",
            "Training Iteration 2879, Loss: 7.610160827636719\n",
            "Training Iteration 2880, Loss: 5.057852745056152\n",
            "Training Iteration 2881, Loss: 7.737057685852051\n",
            "Training Iteration 2882, Loss: 3.9221675395965576\n",
            "Training Iteration 2883, Loss: 5.548238277435303\n",
            "Training Iteration 2884, Loss: 6.275444030761719\n",
            "Training Iteration 2885, Loss: 3.913252353668213\n",
            "Training Iteration 2886, Loss: 7.186176300048828\n",
            "Training Iteration 2887, Loss: 6.020847797393799\n",
            "Training Iteration 2888, Loss: 3.168395519256592\n",
            "Training Iteration 2889, Loss: 3.0372719764709473\n",
            "Training Iteration 2890, Loss: 6.836503982543945\n",
            "Training Iteration 2891, Loss: 5.880384922027588\n",
            "Training Iteration 2892, Loss: 4.0198211669921875\n",
            "Training Iteration 2893, Loss: 3.1853904724121094\n",
            "Training Iteration 2894, Loss: 5.12824821472168\n",
            "Training Iteration 2895, Loss: 5.149876117706299\n",
            "Training Iteration 2896, Loss: 5.319709777832031\n",
            "Training Iteration 2897, Loss: 2.9101123809814453\n",
            "Training Iteration 2898, Loss: 6.544978618621826\n",
            "Training Iteration 2899, Loss: 3.049772024154663\n",
            "Training Iteration 2900, Loss: 2.793116331100464\n",
            "Training Iteration 2901, Loss: 2.9571120738983154\n",
            "Training Iteration 2902, Loss: 2.5037264823913574\n",
            "Training Iteration 2903, Loss: 3.797614097595215\n",
            "Training Iteration 2904, Loss: 3.9995245933532715\n",
            "Training Iteration 2905, Loss: 2.887568950653076\n",
            "Training Iteration 2906, Loss: 2.787964344024658\n",
            "Training Iteration 2907, Loss: 2.3451364040374756\n",
            "Training Iteration 2908, Loss: 3.1585724353790283\n",
            "Training Iteration 2909, Loss: 5.512852668762207\n",
            "Training Iteration 2910, Loss: 5.712060928344727\n",
            "Training Iteration 2911, Loss: 6.430424690246582\n",
            "Training Iteration 2912, Loss: 4.902559280395508\n",
            "Training Iteration 2913, Loss: 1.7680805921554565\n",
            "Training Iteration 2914, Loss: 5.639471054077148\n",
            "Training Iteration 2915, Loss: 5.123589038848877\n",
            "Training Iteration 2916, Loss: 3.9914186000823975\n",
            "Training Iteration 2917, Loss: 3.6774497032165527\n",
            "Training Iteration 2918, Loss: 3.0427770614624023\n",
            "Training Iteration 2919, Loss: 3.2300169467926025\n",
            "Training Iteration 2920, Loss: 4.25554895401001\n",
            "Training Iteration 2921, Loss: 2.6709022521972656\n",
            "Training Iteration 2922, Loss: 3.315941333770752\n",
            "Training Iteration 2923, Loss: 3.4101123809814453\n",
            "Training Iteration 2924, Loss: 2.9161484241485596\n",
            "Training Iteration 2925, Loss: 6.09203577041626\n",
            "Training Iteration 2926, Loss: 4.818098068237305\n",
            "Training Iteration 2927, Loss: 9.755411148071289\n",
            "Training Iteration 2928, Loss: 7.476414203643799\n",
            "Training Iteration 2929, Loss: 2.3573105335235596\n",
            "Training Iteration 2930, Loss: 4.105741024017334\n",
            "Training Iteration 2931, Loss: 5.749606609344482\n",
            "Training Iteration 2932, Loss: 4.250424861907959\n",
            "Training Iteration 2933, Loss: 6.4841413497924805\n",
            "Training Iteration 2934, Loss: 6.085504055023193\n",
            "Training Iteration 2935, Loss: 4.799814701080322\n",
            "Training Iteration 2936, Loss: 4.057448863983154\n",
            "Training Iteration 2937, Loss: 4.921955585479736\n",
            "Training Iteration 2938, Loss: 2.347543239593506\n",
            "Training Iteration 2939, Loss: 3.339163303375244\n",
            "Training Iteration 2940, Loss: 6.5215911865234375\n",
            "Training Iteration 2941, Loss: 7.918756008148193\n",
            "Training Iteration 2942, Loss: 7.895293235778809\n",
            "Training Iteration 2943, Loss: 3.583921194076538\n",
            "Training Iteration 2944, Loss: 8.677367210388184\n",
            "Training Iteration 2945, Loss: 5.497401237487793\n",
            "Training Iteration 2946, Loss: 4.763514041900635\n",
            "Training Iteration 2947, Loss: 3.27164888381958\n",
            "Training Iteration 2948, Loss: 6.5852203369140625\n",
            "Training Iteration 2949, Loss: 4.798374176025391\n",
            "Training Iteration 2950, Loss: 4.099493980407715\n",
            "Training Iteration 2951, Loss: 4.6729536056518555\n",
            "Training Iteration 2952, Loss: 4.0470757484436035\n",
            "Training Iteration 2953, Loss: 3.492607593536377\n",
            "Training Iteration 2954, Loss: 6.942734241485596\n",
            "Training Iteration 2955, Loss: 4.524367809295654\n",
            "Training Iteration 2956, Loss: 4.959593772888184\n",
            "Training Iteration 2957, Loss: 3.1349985599517822\n",
            "Training Iteration 2958, Loss: 5.916201591491699\n",
            "Training Iteration 2959, Loss: 5.788931369781494\n",
            "Training Iteration 2960, Loss: 3.291795015335083\n",
            "Training Iteration 2961, Loss: 4.715579986572266\n",
            "Training Iteration 2962, Loss: 3.924560308456421\n",
            "Training Iteration 2963, Loss: 5.4615559577941895\n",
            "Training Iteration 2964, Loss: 3.547231912612915\n",
            "Training Iteration 2965, Loss: 5.485774040222168\n",
            "Training Iteration 2966, Loss: 3.2157793045043945\n",
            "Training Iteration 2967, Loss: 2.178842067718506\n",
            "Training Iteration 2968, Loss: 3.4461052417755127\n",
            "Training Iteration 2969, Loss: 5.86182165145874\n",
            "Training Iteration 2970, Loss: 5.060559272766113\n",
            "Training Iteration 2971, Loss: 9.605698585510254\n",
            "Training Iteration 2972, Loss: 4.619600772857666\n",
            "Training Iteration 2973, Loss: 4.058371067047119\n",
            "Training Iteration 2974, Loss: 6.309563159942627\n",
            "Training Iteration 2975, Loss: 4.569602012634277\n",
            "Training Iteration 2976, Loss: 4.402333736419678\n",
            "Training Iteration 2977, Loss: 8.394227981567383\n",
            "Training Iteration 2978, Loss: 5.7546772956848145\n",
            "Training Iteration 2979, Loss: 4.978995323181152\n",
            "Training Iteration 2980, Loss: 4.940101623535156\n",
            "Training Iteration 2981, Loss: 3.576068639755249\n",
            "Training Iteration 2982, Loss: 6.202446460723877\n",
            "Training Iteration 2983, Loss: 3.4947493076324463\n",
            "Training Iteration 2984, Loss: 3.156106948852539\n",
            "Training Iteration 2985, Loss: 1.9900882244110107\n",
            "Training Iteration 2986, Loss: 8.143556594848633\n",
            "Training Iteration 2987, Loss: 3.7995731830596924\n",
            "Training Iteration 2988, Loss: 4.712217807769775\n",
            "Training Iteration 2989, Loss: 4.781187534332275\n",
            "Training Iteration 2990, Loss: 4.309473991394043\n",
            "Training Iteration 2991, Loss: 6.616060733795166\n",
            "Training Iteration 2992, Loss: 4.74412727355957\n",
            "Training Iteration 2993, Loss: 6.833932399749756\n",
            "Training Iteration 2994, Loss: 7.371537685394287\n",
            "Training Iteration 2995, Loss: 8.186537742614746\n",
            "Training Iteration 2996, Loss: 7.76271915435791\n",
            "Training Iteration 2997, Loss: 5.118216514587402\n",
            "Training Iteration 2998, Loss: 4.559635162353516\n",
            "Training Iteration 2999, Loss: 5.219618797302246\n",
            "Training Iteration 3000, Loss: 5.784755706787109\n",
            "Training Iteration 3001, Loss: 5.166656017303467\n",
            "Training Iteration 3002, Loss: 3.580660820007324\n",
            "Training Iteration 3003, Loss: 1.5695340633392334\n",
            "Training Iteration 3004, Loss: 6.218086242675781\n",
            "Training Iteration 3005, Loss: 8.03890323638916\n",
            "Training Iteration 3006, Loss: 3.7984628677368164\n",
            "Training Iteration 3007, Loss: 3.4922776222229004\n",
            "Training Iteration 3008, Loss: 6.034281253814697\n",
            "Training Iteration 3009, Loss: 5.942342281341553\n",
            "Training Iteration 3010, Loss: 8.462434768676758\n",
            "Training Iteration 3011, Loss: 6.653798580169678\n",
            "Training Iteration 3012, Loss: 2.5292747020721436\n",
            "Training Iteration 3013, Loss: 2.2704854011535645\n",
            "Training Iteration 3014, Loss: 4.424159049987793\n",
            "Training Iteration 3015, Loss: 5.692353248596191\n",
            "Training Iteration 3016, Loss: 6.008594036102295\n",
            "Training Iteration 3017, Loss: 2.781353235244751\n",
            "Training Iteration 3018, Loss: 4.180572986602783\n",
            "Training Iteration 3019, Loss: 4.714294910430908\n",
            "Training Iteration 3020, Loss: 3.6810717582702637\n",
            "Training Iteration 3021, Loss: 2.224658727645874\n",
            "Training Iteration 3022, Loss: 4.987337112426758\n",
            "Training Iteration 3023, Loss: 5.221466541290283\n",
            "Training Iteration 3024, Loss: 2.4542500972747803\n",
            "Training Iteration 3025, Loss: 2.676797866821289\n",
            "Training Iteration 3026, Loss: 3.6329903602600098\n",
            "Training Iteration 3027, Loss: 4.6640400886535645\n",
            "Training Iteration 3028, Loss: 5.575722694396973\n",
            "Training Iteration 3029, Loss: 8.462425231933594\n",
            "Training Iteration 3030, Loss: 5.283045768737793\n",
            "Training Iteration 3031, Loss: 4.313563823699951\n",
            "Training Iteration 3032, Loss: 4.8929362297058105\n",
            "Training Iteration 3033, Loss: 4.002049922943115\n",
            "Training Iteration 3034, Loss: 3.260282039642334\n",
            "Training Iteration 3035, Loss: 7.2393059730529785\n",
            "Training Iteration 3036, Loss: 3.9764766693115234\n",
            "Training Iteration 3037, Loss: 4.0071892738342285\n",
            "Training Iteration 3038, Loss: 4.859857559204102\n",
            "Training Iteration 3039, Loss: 5.739090919494629\n",
            "Training Iteration 3040, Loss: 6.812309265136719\n",
            "Training Iteration 3041, Loss: 6.8171844482421875\n",
            "Training Iteration 3042, Loss: 4.657205581665039\n",
            "Training Iteration 3043, Loss: 4.213046073913574\n",
            "Training Iteration 3044, Loss: 2.5207462310791016\n",
            "Training Iteration 3045, Loss: 4.471794128417969\n",
            "Training Iteration 3046, Loss: 6.840843200683594\n",
            "Training Iteration 3047, Loss: 4.698841094970703\n",
            "Training Iteration 3048, Loss: 5.239652156829834\n",
            "Training Iteration 3049, Loss: 6.880824089050293\n",
            "Training Iteration 3050, Loss: 3.2433948516845703\n",
            "Training Iteration 3051, Loss: 7.2033891677856445\n",
            "Training Iteration 3052, Loss: 7.665599822998047\n",
            "Training Iteration 3053, Loss: 6.723942279815674\n",
            "Training Iteration 3054, Loss: 3.040915012359619\n",
            "Training Iteration 3055, Loss: 4.292801380157471\n",
            "Training Iteration 3056, Loss: 3.7182815074920654\n",
            "Training Iteration 3057, Loss: 10.100851058959961\n",
            "Training Iteration 3058, Loss: 6.534668922424316\n",
            "Training Iteration 3059, Loss: 8.001143455505371\n",
            "Training Iteration 3060, Loss: 5.356752395629883\n",
            "Training Iteration 3061, Loss: 3.741058826446533\n",
            "Training Iteration 3062, Loss: 2.770521402359009\n",
            "Training Iteration 3063, Loss: 2.1553916931152344\n",
            "Training Iteration 3064, Loss: 5.001185894012451\n",
            "Training Iteration 3065, Loss: 4.832948207855225\n",
            "Training Iteration 3066, Loss: 3.01651668548584\n",
            "Training Iteration 3067, Loss: 3.3255209922790527\n",
            "Training Iteration 3068, Loss: 4.3058061599731445\n",
            "Training Iteration 3069, Loss: 2.7885754108428955\n",
            "Training Iteration 3070, Loss: 2.9521427154541016\n",
            "Training Iteration 3071, Loss: 5.4591593742370605\n",
            "Training Iteration 3072, Loss: 4.701321125030518\n",
            "Training Iteration 3073, Loss: 4.386634349822998\n",
            "Training Iteration 3074, Loss: 7.9739227294921875\n",
            "Training Iteration 3075, Loss: 8.334857940673828\n",
            "Training Iteration 3076, Loss: 2.688074827194214\n",
            "Training Iteration 3077, Loss: 4.492596626281738\n",
            "Training Iteration 3078, Loss: 3.9448180198669434\n",
            "Training Iteration 3079, Loss: 6.493407249450684\n",
            "Training Iteration 3080, Loss: 6.4828386306762695\n",
            "Training Iteration 3081, Loss: 3.3028578758239746\n",
            "Training Iteration 3082, Loss: 5.7581658363342285\n",
            "Training Iteration 3083, Loss: 3.8357009887695312\n",
            "Training Iteration 3084, Loss: 5.306850433349609\n",
            "Training Iteration 3085, Loss: 4.6304473876953125\n",
            "Training Iteration 3086, Loss: 8.195544242858887\n",
            "Training Iteration 3087, Loss: 3.948111057281494\n",
            "Training Iteration 3088, Loss: 9.05247974395752\n",
            "Training Iteration 3089, Loss: 6.554001331329346\n",
            "Training Iteration 3090, Loss: 7.48172664642334\n",
            "Training Iteration 3091, Loss: 6.687833309173584\n",
            "Training Iteration 3092, Loss: 4.794994831085205\n",
            "Training Iteration 3093, Loss: 1.8913278579711914\n",
            "Training Iteration 3094, Loss: 1.9121402502059937\n",
            "Training Iteration 3095, Loss: 5.659414291381836\n",
            "Training Iteration 3096, Loss: 3.122499942779541\n",
            "Training Iteration 3097, Loss: 4.373684406280518\n",
            "Training Iteration 3098, Loss: 3.252512216567993\n",
            "Training Iteration 3099, Loss: 5.458750247955322\n",
            "Training Iteration 3100, Loss: 4.784806251525879\n",
            "Training Iteration 3101, Loss: 3.488304615020752\n",
            "Training Iteration 3102, Loss: 10.18528938293457\n",
            "Training Iteration 3103, Loss: 9.351659774780273\n",
            "Training Iteration 3104, Loss: 4.286863327026367\n",
            "Training Iteration 3105, Loss: 4.642970561981201\n",
            "Training Iteration 3106, Loss: 4.808837890625\n",
            "Training Iteration 3107, Loss: 4.815401554107666\n",
            "Training Iteration 3108, Loss: 2.7796289920806885\n",
            "Training Iteration 3109, Loss: 12.139013290405273\n",
            "Training Iteration 3110, Loss: 10.988537788391113\n",
            "Training Iteration 3111, Loss: 5.047867774963379\n",
            "Training Iteration 3112, Loss: 3.5076451301574707\n",
            "Training Iteration 3113, Loss: 2.135624885559082\n",
            "Training Iteration 3114, Loss: 5.085780620574951\n",
            "Training Iteration 3115, Loss: 3.6538243293762207\n",
            "Training Iteration 3116, Loss: 3.1628525257110596\n",
            "Training Iteration 3117, Loss: 2.2767419815063477\n",
            "Training Iteration 3118, Loss: 11.661972045898438\n",
            "Training Iteration 3119, Loss: 5.066182613372803\n",
            "Training Iteration 3120, Loss: 2.096027374267578\n",
            "Training Iteration 3121, Loss: 8.71357250213623\n",
            "Training Iteration 3122, Loss: 7.4641923904418945\n",
            "Training Iteration 3123, Loss: 5.043178558349609\n",
            "Training Iteration 3124, Loss: 4.07446813583374\n",
            "Training Iteration 3125, Loss: 3.6068434715270996\n",
            "Training Iteration 3126, Loss: 7.640328407287598\n",
            "Training Iteration 3127, Loss: 3.2178356647491455\n",
            "Training Iteration 3128, Loss: 3.883579730987549\n",
            "Training Iteration 3129, Loss: 3.40069317817688\n",
            "Training Iteration 3130, Loss: 6.6709675788879395\n",
            "Training Iteration 3131, Loss: 3.4202535152435303\n",
            "Training Iteration 3132, Loss: 4.695684432983398\n",
            "Training Iteration 3133, Loss: 3.928020477294922\n",
            "Training Iteration 3134, Loss: 5.585659980773926\n",
            "Training Iteration 3135, Loss: 1.6433212757110596\n",
            "Training Iteration 3136, Loss: 10.165993690490723\n",
            "Training Iteration 3137, Loss: 8.502572059631348\n",
            "Training Iteration 3138, Loss: 7.5081281661987305\n",
            "Training Iteration 3139, Loss: 2.6640262603759766\n",
            "Training Iteration 3140, Loss: 4.972470283508301\n",
            "Training Iteration 3141, Loss: 5.855242729187012\n",
            "Training Iteration 3142, Loss: 4.431675910949707\n",
            "Training Iteration 3143, Loss: 3.934419631958008\n",
            "Training Iteration 3144, Loss: 3.0187788009643555\n",
            "Training Iteration 3145, Loss: 8.20434856414795\n",
            "Training Iteration 3146, Loss: 4.991568565368652\n",
            "Training Iteration 3147, Loss: 2.9755609035491943\n",
            "Training Iteration 3148, Loss: 3.851175308227539\n",
            "Training Iteration 3149, Loss: 2.1878886222839355\n",
            "Training Iteration 3150, Loss: 7.7172465324401855\n",
            "Training Iteration 3151, Loss: 7.400842189788818\n",
            "Training Iteration 3152, Loss: 3.487971782684326\n",
            "Training Iteration 3153, Loss: 7.380366325378418\n",
            "Training Iteration 3154, Loss: 5.35059118270874\n",
            "Training Iteration 3155, Loss: 5.093755722045898\n",
            "Training Iteration 3156, Loss: 5.521962642669678\n",
            "Training Iteration 3157, Loss: 6.108681678771973\n",
            "Training Iteration 3158, Loss: 4.617796897888184\n",
            "Training Iteration 3159, Loss: 5.987936019897461\n",
            "Training Iteration 3160, Loss: 3.9488649368286133\n",
            "Training Iteration 3161, Loss: 2.474199056625366\n",
            "Training Iteration 3162, Loss: 3.1686034202575684\n",
            "Training Iteration 3163, Loss: 2.909359931945801\n",
            "Training Iteration 3164, Loss: 8.630524635314941\n",
            "Training Iteration 3165, Loss: 3.4109880924224854\n",
            "Training Iteration 3166, Loss: 1.2897393703460693\n",
            "Training Iteration 3167, Loss: 2.7418150901794434\n",
            "Training Iteration 3168, Loss: 4.686893463134766\n",
            "Training Iteration 3169, Loss: 5.831453323364258\n",
            "Training Iteration 3170, Loss: 3.9425575733184814\n",
            "Training Iteration 3171, Loss: 3.1398019790649414\n",
            "Training Iteration 3172, Loss: 3.442294120788574\n",
            "Training Iteration 3173, Loss: 2.147449016571045\n",
            "Training Iteration 3174, Loss: 4.495041847229004\n",
            "Training Iteration 3175, Loss: 4.749245643615723\n",
            "Training Iteration 3176, Loss: 4.536519527435303\n",
            "Training Iteration 3177, Loss: 4.323992729187012\n",
            "Training Iteration 3178, Loss: 3.3323121070861816\n",
            "Training Iteration 3179, Loss: 5.170063495635986\n",
            "Training Iteration 3180, Loss: 2.511080026626587\n",
            "Training Iteration 3181, Loss: 9.208324432373047\n",
            "Training Iteration 3182, Loss: 3.6674442291259766\n",
            "Training Iteration 3183, Loss: 7.247640132904053\n",
            "Training Iteration 3184, Loss: 3.7093887329101562\n",
            "Training Iteration 3185, Loss: 6.241419792175293\n",
            "Training Iteration 3186, Loss: 5.21358585357666\n",
            "Training Iteration 3187, Loss: 8.942416191101074\n",
            "Training Iteration 3188, Loss: 4.258056640625\n",
            "Training Iteration 3189, Loss: 3.341543197631836\n",
            "Training Iteration 3190, Loss: 7.065314769744873\n",
            "Training Iteration 3191, Loss: 3.548146963119507\n",
            "Training Iteration 3192, Loss: 5.573657035827637\n",
            "Training Iteration 3193, Loss: 5.195066452026367\n",
            "Training Iteration 3194, Loss: 11.2935791015625\n",
            "Training Iteration 3195, Loss: 5.130492210388184\n",
            "Training Iteration 3196, Loss: 6.197070121765137\n",
            "Training Iteration 3197, Loss: 2.199864387512207\n",
            "Training Iteration 3198, Loss: 9.175154685974121\n",
            "Training Iteration 3199, Loss: 2.7640933990478516\n",
            "Training Iteration 3200, Loss: 3.7543110847473145\n",
            "Training Iteration 3201, Loss: 6.034960746765137\n",
            "Training Iteration 3202, Loss: 6.001852035522461\n",
            "Training Iteration 3203, Loss: 6.29071044921875\n",
            "Training Iteration 3204, Loss: 3.914437770843506\n",
            "Training Iteration 3205, Loss: 4.910387992858887\n",
            "Training Iteration 3206, Loss: 3.825599193572998\n",
            "Training Iteration 3207, Loss: 3.205247402191162\n",
            "Training Iteration 3208, Loss: 5.583662986755371\n",
            "Training Iteration 3209, Loss: 5.837785720825195\n",
            "Training Iteration 3210, Loss: 4.716442108154297\n",
            "Training Iteration 3211, Loss: 5.862251281738281\n",
            "Training Iteration 3212, Loss: 5.6801042556762695\n",
            "Training Iteration 3213, Loss: 1.998291254043579\n",
            "Training Iteration 3214, Loss: 3.0610156059265137\n",
            "Training Iteration 3215, Loss: 5.058443546295166\n",
            "Training Iteration 3216, Loss: 5.1158552169799805\n",
            "Training Iteration 3217, Loss: 4.246345520019531\n",
            "Training Iteration 3218, Loss: 4.806251049041748\n",
            "Training Iteration 3219, Loss: 3.738800048828125\n",
            "Training Iteration 3220, Loss: 3.3438169956207275\n",
            "Training Iteration 3221, Loss: 3.2079193592071533\n",
            "Training Iteration 3222, Loss: 3.3691117763519287\n",
            "Training Iteration 3223, Loss: 3.6432127952575684\n",
            "Training Iteration 3224, Loss: 2.213198184967041\n",
            "Training Iteration 3225, Loss: 3.4429092407226562\n",
            "Training Iteration 3226, Loss: 6.656200885772705\n",
            "Training Iteration 3227, Loss: 4.6949076652526855\n",
            "Training Iteration 3228, Loss: 2.6287546157836914\n",
            "Training Iteration 3229, Loss: 5.417079925537109\n",
            "Training Iteration 3230, Loss: 4.119480133056641\n",
            "Training Iteration 3231, Loss: 4.173858165740967\n",
            "Training Iteration 3232, Loss: 6.7859649658203125\n",
            "Training Iteration 3233, Loss: 5.187621116638184\n",
            "Training Iteration 3234, Loss: 2.369382381439209\n",
            "Training Iteration 3235, Loss: 4.479294300079346\n",
            "Training Iteration 3236, Loss: 5.62209415435791\n",
            "Training Iteration 3237, Loss: 3.529951572418213\n",
            "Training Iteration 3238, Loss: 2.8761579990386963\n",
            "Training Iteration 3239, Loss: 4.62778902053833\n",
            "Training Iteration 3240, Loss: 6.055431365966797\n",
            "Training Iteration 3241, Loss: 4.418651580810547\n",
            "Training Iteration 3242, Loss: 2.9650323390960693\n",
            "Training Iteration 3243, Loss: 2.3076255321502686\n",
            "Training Iteration 3244, Loss: 4.422019004821777\n",
            "Training Iteration 3245, Loss: 5.828983783721924\n",
            "Training Iteration 3246, Loss: 5.130491256713867\n",
            "Training Iteration 3247, Loss: 4.648577690124512\n",
            "Training Iteration 3248, Loss: 3.5950546264648438\n",
            "Training Iteration 3249, Loss: 2.0967812538146973\n",
            "Training Iteration 3250, Loss: 5.4293341636657715\n",
            "Training Iteration 3251, Loss: 4.989649295806885\n",
            "Training Iteration 3252, Loss: 3.482794761657715\n",
            "Training Iteration 3253, Loss: 4.818240642547607\n",
            "Training Iteration 3254, Loss: 3.2622947692871094\n",
            "Training Iteration 3255, Loss: 1.9548438787460327\n",
            "Training Iteration 3256, Loss: 5.485342502593994\n",
            "Training Iteration 3257, Loss: 3.9661498069763184\n",
            "Training Iteration 3258, Loss: 3.281723976135254\n",
            "Training Iteration 3259, Loss: 5.153465747833252\n",
            "Training Iteration 3260, Loss: 7.593637943267822\n",
            "Training Iteration 3261, Loss: 4.906702518463135\n",
            "Training Iteration 3262, Loss: 4.2414422035217285\n",
            "Training Iteration 3263, Loss: 4.419281482696533\n",
            "Training Iteration 3264, Loss: 4.151463985443115\n",
            "Training Iteration 3265, Loss: 5.073122501373291\n",
            "Training Iteration 3266, Loss: 4.035930633544922\n",
            "Training Iteration 3267, Loss: 5.072517395019531\n",
            "Training Iteration 3268, Loss: 9.58723258972168\n",
            "Training Iteration 3269, Loss: 4.972882270812988\n",
            "Training Iteration 3270, Loss: 4.055788516998291\n",
            "Training Iteration 3271, Loss: 2.9254117012023926\n",
            "Training Iteration 3272, Loss: 2.2078397274017334\n",
            "Training Iteration 3273, Loss: 6.143693447113037\n",
            "Training Iteration 3274, Loss: 4.0955400466918945\n",
            "Training Iteration 3275, Loss: 3.2694504261016846\n",
            "Training Iteration 3276, Loss: 4.930028915405273\n",
            "Training Iteration 3277, Loss: 4.667220592498779\n",
            "Training Iteration 3278, Loss: 5.3150410652160645\n",
            "Training Iteration 3279, Loss: 6.1761627197265625\n",
            "Training Iteration 3280, Loss: 6.470727443695068\n",
            "Training Iteration 3281, Loss: 6.562873363494873\n",
            "Training Iteration 3282, Loss: 3.0537407398223877\n",
            "Training Iteration 3283, Loss: 4.7936506271362305\n",
            "Training Iteration 3284, Loss: 6.436666011810303\n",
            "Training Iteration 3285, Loss: 3.603506565093994\n",
            "Training Iteration 3286, Loss: 4.024336814880371\n",
            "Training Iteration 3287, Loss: 4.43714714050293\n",
            "Training Iteration 3288, Loss: 4.613220691680908\n",
            "Training Iteration 3289, Loss: 2.5131216049194336\n",
            "Training Iteration 3290, Loss: 3.4200775623321533\n",
            "Training Iteration 3291, Loss: 5.379769802093506\n",
            "Training Iteration 3292, Loss: 6.491003513336182\n",
            "Training Iteration 3293, Loss: 10.774439811706543\n",
            "Training Iteration 3294, Loss: 3.4830164909362793\n",
            "Training Iteration 3295, Loss: 5.455148696899414\n",
            "Training Iteration 3296, Loss: 1.9744446277618408\n",
            "Training Iteration 3297, Loss: 3.9270565509796143\n",
            "Training Iteration 3298, Loss: 5.9019036293029785\n",
            "Training Iteration 3299, Loss: 6.062324523925781\n",
            "Training Iteration 3300, Loss: 6.005600929260254\n",
            "Training Iteration 3301, Loss: 6.221734523773193\n",
            "Training Iteration 3302, Loss: 5.06437873840332\n",
            "Training Iteration 3303, Loss: 4.718994140625\n",
            "Training Iteration 3304, Loss: 6.982546806335449\n",
            "Training Iteration 3305, Loss: 1.8207807540893555\n",
            "Training Iteration 3306, Loss: 3.0957398414611816\n",
            "Training Iteration 3307, Loss: 3.8297858238220215\n",
            "Training Iteration 3308, Loss: 6.694436073303223\n",
            "Training Iteration 3309, Loss: 5.411487579345703\n",
            "Training Iteration 3310, Loss: 4.596874713897705\n",
            "Training Iteration 3311, Loss: 3.0600035190582275\n",
            "Training Iteration 3312, Loss: 4.119393348693848\n",
            "Training Iteration 3313, Loss: 3.906219720840454\n",
            "Training Iteration 3314, Loss: 2.166131019592285\n",
            "Training Iteration 3315, Loss: 4.780124664306641\n",
            "Training Iteration 3316, Loss: 6.64260196685791\n",
            "Training Iteration 3317, Loss: 3.4044408798217773\n",
            "Training Iteration 3318, Loss: 3.7780582904815674\n",
            "Training Iteration 3319, Loss: 3.6906423568725586\n",
            "Training Iteration 3320, Loss: 5.070064067840576\n",
            "Training Iteration 3321, Loss: 3.33126163482666\n",
            "Training Iteration 3322, Loss: 5.550621509552002\n",
            "Training Iteration 3323, Loss: 3.836341381072998\n",
            "Training Iteration 3324, Loss: 7.783238410949707\n",
            "Training Iteration 3325, Loss: 6.470075607299805\n",
            "Training Iteration 3326, Loss: 5.470592975616455\n",
            "Training Iteration 3327, Loss: 5.791301727294922\n",
            "Training Iteration 3328, Loss: 4.787466049194336\n",
            "Training Iteration 3329, Loss: 7.741776943206787\n",
            "Training Iteration 3330, Loss: 4.208096027374268\n",
            "Training Iteration 3331, Loss: 2.6591358184814453\n",
            "Training Iteration 3332, Loss: 6.970803260803223\n",
            "Training Iteration 3333, Loss: 4.294116020202637\n",
            "Training Iteration 3334, Loss: 2.514943838119507\n",
            "Training Iteration 3335, Loss: 3.346312999725342\n",
            "Training Iteration 3336, Loss: 4.044578552246094\n",
            "Training Iteration 3337, Loss: 5.786154270172119\n",
            "Training Iteration 3338, Loss: 7.164821147918701\n",
            "Training Iteration 3339, Loss: 3.6816744804382324\n",
            "Training Iteration 3340, Loss: 3.31752610206604\n",
            "Training Iteration 3341, Loss: 4.697072982788086\n",
            "Training Iteration 3342, Loss: 4.755187511444092\n",
            "Training Iteration 3343, Loss: 3.081035614013672\n",
            "Training Iteration 3344, Loss: 3.981450080871582\n",
            "Training Iteration 3345, Loss: 5.270503044128418\n",
            "Training Iteration 3346, Loss: 6.316872596740723\n",
            "Training Iteration 3347, Loss: 4.558286190032959\n",
            "Training Iteration 3348, Loss: 8.875479698181152\n",
            "Training Iteration 3349, Loss: 6.799554824829102\n",
            "Training Iteration 3350, Loss: 2.405606508255005\n",
            "Training Iteration 3351, Loss: 3.3479723930358887\n",
            "Training Iteration 3352, Loss: 3.6185295581817627\n",
            "Training Iteration 3353, Loss: 6.566106796264648\n",
            "Training Iteration 3354, Loss: 4.8034138679504395\n",
            "Training Iteration 3355, Loss: 4.309641361236572\n",
            "Training Iteration 3356, Loss: 4.275002956390381\n",
            "Training Iteration 3357, Loss: 5.287529945373535\n",
            "Training Iteration 3358, Loss: 3.7908363342285156\n",
            "Training Iteration 3359, Loss: 5.38941764831543\n",
            "Training Iteration 3360, Loss: 3.7451462745666504\n",
            "Training Iteration 3361, Loss: 4.001823425292969\n",
            "Training Iteration 3362, Loss: 8.041669845581055\n",
            "Training Iteration 3363, Loss: 5.663735866546631\n",
            "Training Iteration 3364, Loss: 5.067143440246582\n",
            "Training Iteration 3365, Loss: 7.9459099769592285\n",
            "Training Iteration 3366, Loss: 3.8238165378570557\n",
            "Training Iteration 3367, Loss: 6.61053991317749\n",
            "Training Iteration 3368, Loss: 4.985846519470215\n",
            "Training Iteration 3369, Loss: 3.938272476196289\n",
            "Training Iteration 3370, Loss: 1.5270944833755493\n",
            "Training Iteration 3371, Loss: 6.825090408325195\n",
            "Training Iteration 3372, Loss: 5.876767158508301\n",
            "Training Iteration 3373, Loss: 2.961714744567871\n",
            "Training Iteration 3374, Loss: 3.9633071422576904\n",
            "Training Iteration 3375, Loss: 7.7867302894592285\n",
            "Training Iteration 3376, Loss: 2.853968858718872\n",
            "Training Iteration 3377, Loss: 4.262274742126465\n",
            "Training Iteration 3378, Loss: 2.9120240211486816\n",
            "Training Iteration 3379, Loss: 9.20625114440918\n",
            "Training Iteration 3380, Loss: 11.010477066040039\n",
            "Training Iteration 3381, Loss: 7.737785339355469\n",
            "Training Iteration 3382, Loss: 4.479435443878174\n",
            "Training Iteration 3383, Loss: 4.781785011291504\n",
            "Training Iteration 3384, Loss: 5.536576271057129\n",
            "Training Iteration 3385, Loss: 4.191485404968262\n",
            "Training Iteration 3386, Loss: 7.687044143676758\n",
            "Training Iteration 3387, Loss: 3.1086158752441406\n",
            "Training Iteration 3388, Loss: 3.763690948486328\n",
            "Training Iteration 3389, Loss: 7.212128639221191\n",
            "Training Iteration 3390, Loss: 4.720207691192627\n",
            "Training Iteration 3391, Loss: 4.170390605926514\n",
            "Training Iteration 3392, Loss: 4.231619358062744\n",
            "Training Iteration 3393, Loss: 4.538411617279053\n",
            "Training Iteration 3394, Loss: 2.397369146347046\n",
            "Training Iteration 3395, Loss: 3.5335395336151123\n",
            "Training Iteration 3396, Loss: 7.021178722381592\n",
            "Training Iteration 3397, Loss: 10.401467323303223\n",
            "Training Iteration 3398, Loss: 5.183345794677734\n",
            "Training Iteration 3399, Loss: 5.589454650878906\n",
            "Training Iteration 3400, Loss: 7.442855358123779\n",
            "Training Iteration 3401, Loss: 6.179866313934326\n",
            "Training Iteration 3402, Loss: 3.184276819229126\n",
            "Training Iteration 3403, Loss: 8.097013473510742\n",
            "Training Iteration 3404, Loss: 5.034514427185059\n",
            "Training Iteration 3405, Loss: 4.14116907119751\n",
            "Training Iteration 3406, Loss: 4.803652763366699\n",
            "Training Iteration 3407, Loss: 6.841770172119141\n",
            "Training Iteration 3408, Loss: 4.07642936706543\n",
            "Training Iteration 3409, Loss: 8.339639663696289\n",
            "Training Iteration 3410, Loss: 3.4986789226531982\n",
            "Training Iteration 3411, Loss: 2.2382113933563232\n",
            "Training Iteration 3412, Loss: 3.5983588695526123\n",
            "Training Iteration 3413, Loss: 4.121849060058594\n",
            "Training Iteration 3414, Loss: 7.417929649353027\n",
            "Training Iteration 3415, Loss: 4.721794605255127\n",
            "Training Iteration 3416, Loss: 4.7224884033203125\n",
            "Training Iteration 3417, Loss: 4.577860355377197\n",
            "Training Iteration 3418, Loss: 2.0431134700775146\n",
            "Training Iteration 3419, Loss: 8.05409049987793\n",
            "Training Iteration 3420, Loss: 3.0183708667755127\n",
            "Training Iteration 3421, Loss: 5.482288837432861\n",
            "Training Iteration 3422, Loss: 10.662222862243652\n",
            "Training Iteration 3423, Loss: 2.850141763687134\n",
            "Training Iteration 3424, Loss: 4.0804362297058105\n",
            "Training Iteration 3425, Loss: 5.0812530517578125\n",
            "Training Iteration 3426, Loss: 6.4865546226501465\n",
            "Training Iteration 3427, Loss: 4.644789218902588\n",
            "Training Iteration 3428, Loss: 2.7263166904449463\n",
            "Training Iteration 3429, Loss: 4.951993465423584\n",
            "Training Iteration 3430, Loss: 3.626748561859131\n",
            "Training Iteration 3431, Loss: 7.012984752655029\n",
            "Training Iteration 3432, Loss: 6.130737781524658\n",
            "Training Iteration 3433, Loss: 4.069851398468018\n",
            "Training Iteration 3434, Loss: 3.851316213607788\n",
            "Training Iteration 3435, Loss: 2.7350358963012695\n",
            "Training Iteration 3436, Loss: 3.4014651775360107\n",
            "Training Iteration 3437, Loss: 2.8629143238067627\n",
            "Training Iteration 3438, Loss: 5.21013879776001\n",
            "Training Iteration 3439, Loss: 3.053816795349121\n",
            "Training Iteration 3440, Loss: 3.224947452545166\n",
            "Training Iteration 3441, Loss: 3.273995876312256\n",
            "Training Iteration 3442, Loss: 5.717445373535156\n",
            "Training Iteration 3443, Loss: 4.15480899810791\n",
            "Training Iteration 3444, Loss: 2.081749677658081\n",
            "Training Iteration 3445, Loss: 6.108428001403809\n",
            "Training Iteration 3446, Loss: 2.7500572204589844\n",
            "Training Iteration 3447, Loss: 4.938939094543457\n",
            "Training Iteration 3448, Loss: 5.798167705535889\n",
            "Training Iteration 3449, Loss: 3.334045886993408\n",
            "Training Iteration 3450, Loss: 3.144197940826416\n",
            "Training Iteration 3451, Loss: 8.422953605651855\n",
            "Training Iteration 3452, Loss: 5.785120487213135\n",
            "Training Iteration 3453, Loss: 2.079782724380493\n",
            "Training Iteration 3454, Loss: 3.3575949668884277\n",
            "Training Iteration 3455, Loss: 4.2645158767700195\n",
            "Training Iteration 3456, Loss: 8.113450050354004\n",
            "Training Iteration 3457, Loss: 9.250581741333008\n",
            "Training Iteration 3458, Loss: 5.225144863128662\n",
            "Training Iteration 3459, Loss: 3.695199728012085\n",
            "Training Iteration 3460, Loss: 4.778221130371094\n",
            "Training Iteration 3461, Loss: 3.6037423610687256\n",
            "Training Iteration 3462, Loss: 9.013440132141113\n",
            "Training Iteration 3463, Loss: 4.846240520477295\n",
            "Training Iteration 3464, Loss: 7.537549018859863\n",
            "Training Iteration 3465, Loss: 6.631566047668457\n",
            "Training Iteration 3466, Loss: 5.846792697906494\n",
            "Training Iteration 3467, Loss: 4.945187091827393\n",
            "Training Iteration 3468, Loss: 8.215890884399414\n",
            "Training Iteration 3469, Loss: 5.601747035980225\n",
            "Training Iteration 3470, Loss: 3.879323959350586\n",
            "Training Iteration 3471, Loss: 5.8008036613464355\n",
            "Training Iteration 3472, Loss: 8.638970375061035\n",
            "Training Iteration 3473, Loss: 7.347287654876709\n",
            "Training Iteration 3474, Loss: 5.769430637359619\n",
            "Training Iteration 3475, Loss: 2.9743103981018066\n",
            "Training Iteration 3476, Loss: 5.345938205718994\n",
            "Training Iteration 3477, Loss: 3.5453357696533203\n",
            "Training Iteration 3478, Loss: 7.5649895668029785\n",
            "Training Iteration 3479, Loss: 7.818868160247803\n",
            "Training Iteration 3480, Loss: 5.997034549713135\n",
            "Training Iteration 3481, Loss: 4.612372398376465\n",
            "Training Iteration 3482, Loss: 5.7559661865234375\n",
            "Training Iteration 3483, Loss: 4.31294059753418\n",
            "Training Iteration 3484, Loss: 4.681758403778076\n",
            "Training Iteration 3485, Loss: 2.256124973297119\n",
            "Training Iteration 3486, Loss: 6.060398101806641\n",
            "Training Iteration 3487, Loss: 3.8790855407714844\n",
            "Training Iteration 3488, Loss: 4.5063676834106445\n",
            "Training Iteration 3489, Loss: 3.991774082183838\n",
            "Training Iteration 3490, Loss: 5.7369818687438965\n",
            "Training Iteration 3491, Loss: 5.78546142578125\n",
            "Training Iteration 3492, Loss: 5.632318019866943\n",
            "Training Iteration 3493, Loss: 3.6762094497680664\n",
            "Training Iteration 3494, Loss: 4.39386510848999\n",
            "Training Iteration 3495, Loss: 5.357680320739746\n",
            "Training Iteration 3496, Loss: 6.724854946136475\n",
            "Training Iteration 3497, Loss: 6.818456649780273\n",
            "Training Iteration 3498, Loss: 5.843763828277588\n",
            "Training Iteration 3499, Loss: 7.196465015411377\n",
            "Training Iteration 3500, Loss: 6.350034236907959\n",
            "Training Iteration 3501, Loss: 4.85711669921875\n",
            "Training Iteration 3502, Loss: 5.860939979553223\n",
            "Training Iteration 3503, Loss: 5.856843948364258\n",
            "Training Iteration 3504, Loss: 5.043130874633789\n",
            "Training Iteration 3505, Loss: 7.893889904022217\n",
            "Training Iteration 3506, Loss: 8.298126220703125\n",
            "Training Iteration 3507, Loss: 5.390847206115723\n",
            "Training Iteration 3508, Loss: 3.755802869796753\n",
            "Training Iteration 3509, Loss: 8.035391807556152\n",
            "Training Iteration 3510, Loss: 6.9629693031311035\n",
            "Training Iteration 3511, Loss: 6.551589012145996\n",
            "Training Iteration 3512, Loss: 3.113612651824951\n",
            "Training Iteration 3513, Loss: 4.137609958648682\n",
            "Training Iteration 3514, Loss: 5.072087287902832\n",
            "Training Iteration 3515, Loss: 4.686697959899902\n",
            "Training Iteration 3516, Loss: 4.766210079193115\n",
            "Training Iteration 3517, Loss: 3.794480323791504\n",
            "Training Iteration 3518, Loss: 2.0376546382904053\n",
            "Training Iteration 3519, Loss: 5.985167026519775\n",
            "Training Iteration 3520, Loss: 8.008877754211426\n",
            "Training Iteration 3521, Loss: 8.95451545715332\n",
            "Training Iteration 3522, Loss: 7.570431709289551\n",
            "Training Iteration 3523, Loss: 6.046758651733398\n",
            "Training Iteration 3524, Loss: 5.333475112915039\n",
            "Training Iteration 3525, Loss: 6.083404064178467\n",
            "Training Iteration 3526, Loss: 6.538821220397949\n",
            "Training Iteration 3527, Loss: 4.969286918640137\n",
            "Training Iteration 3528, Loss: 4.074016571044922\n",
            "Training Iteration 3529, Loss: 4.5168962478637695\n",
            "Training Iteration 3530, Loss: 7.426977634429932\n",
            "Training Iteration 3531, Loss: 4.661314487457275\n",
            "Training Iteration 3532, Loss: 4.664697647094727\n",
            "Training Iteration 3533, Loss: 4.7493109703063965\n",
            "Training Iteration 3534, Loss: 1.7930636405944824\n",
            "Training Iteration 3535, Loss: 6.160591125488281\n",
            "Training Iteration 3536, Loss: 11.568631172180176\n",
            "Training Iteration 3537, Loss: 5.302095890045166\n",
            "Training Iteration 3538, Loss: 3.660841703414917\n",
            "Training Iteration 3539, Loss: 7.240963459014893\n",
            "Training Iteration 3540, Loss: 2.2762019634246826\n",
            "Training Iteration 3541, Loss: 5.1289753913879395\n",
            "Training Iteration 3542, Loss: 8.319011688232422\n",
            "Training Iteration 3543, Loss: 5.222214698791504\n",
            "Training Iteration 3544, Loss: 3.773129463195801\n",
            "Training Iteration 3545, Loss: 5.193065643310547\n",
            "Training Iteration 3546, Loss: 7.646441459655762\n",
            "Training Iteration 3547, Loss: 6.146712303161621\n",
            "Training Iteration 3548, Loss: 2.7360212802886963\n",
            "Training Iteration 3549, Loss: 2.7467145919799805\n",
            "Training Iteration 3550, Loss: 3.3927762508392334\n",
            "Training Iteration 3551, Loss: 3.419735908508301\n",
            "Training Iteration 3552, Loss: 4.736371994018555\n",
            "Training Iteration 3553, Loss: 6.319755554199219\n",
            "Training Iteration 3554, Loss: 9.130248069763184\n",
            "Training Iteration 3555, Loss: 3.9070940017700195\n",
            "Training Iteration 3556, Loss: 3.3148257732391357\n",
            "Training Iteration 3557, Loss: 3.5106260776519775\n",
            "Training Iteration 3558, Loss: 3.952207565307617\n",
            "Training Iteration 3559, Loss: 1.845829963684082\n",
            "Training Iteration 3560, Loss: 4.82617712020874\n",
            "Training Iteration 3561, Loss: 2.8873789310455322\n",
            "Training Iteration 3562, Loss: 7.593850135803223\n",
            "Training Iteration 3563, Loss: 5.959521770477295\n",
            "Training Iteration 3564, Loss: 5.314236164093018\n",
            "Training Iteration 3565, Loss: 4.95584774017334\n",
            "Training Iteration 3566, Loss: 2.9278717041015625\n",
            "Training Iteration 3567, Loss: 5.069535255432129\n",
            "Training Iteration 3568, Loss: 5.876659393310547\n",
            "Training Iteration 3569, Loss: 5.426671981811523\n",
            "Training Iteration 3570, Loss: 4.058985710144043\n",
            "Training Iteration 3571, Loss: 4.586745738983154\n",
            "Training Iteration 3572, Loss: 3.3603947162628174\n",
            "Training Iteration 3573, Loss: 3.9272871017456055\n",
            "Training Iteration 3574, Loss: 3.9851555824279785\n",
            "Training Iteration 3575, Loss: 8.640995025634766\n",
            "Training Iteration 3576, Loss: 6.803438663482666\n",
            "Training Iteration 3577, Loss: 10.458930969238281\n",
            "Training Iteration 3578, Loss: 5.9709577560424805\n",
            "Training Iteration 3579, Loss: 8.5145845413208\n",
            "Training Iteration 3580, Loss: 4.900369167327881\n",
            "Training Iteration 3581, Loss: 3.5743465423583984\n",
            "Training Iteration 3582, Loss: 7.332674980163574\n",
            "Training Iteration 3583, Loss: 3.328983783721924\n",
            "Training Iteration 3584, Loss: 4.964051246643066\n",
            "Training Iteration 3585, Loss: 4.298503875732422\n",
            "Training Iteration 3586, Loss: 3.6433823108673096\n",
            "Training Iteration 3587, Loss: 4.964743137359619\n",
            "Training Iteration 3588, Loss: 4.06171989440918\n",
            "Training Iteration 3589, Loss: 5.4093403816223145\n",
            "Training Iteration 3590, Loss: 3.8958845138549805\n",
            "Training Iteration 3591, Loss: 5.793097496032715\n",
            "Training Iteration 3592, Loss: 5.713653564453125\n",
            "Training Iteration 3593, Loss: 1.2204194068908691\n",
            "Training Iteration 3594, Loss: 4.694432258605957\n",
            "Training Iteration 3595, Loss: 5.645562171936035\n",
            "Training Iteration 3596, Loss: 6.350989818572998\n",
            "Training Iteration 3597, Loss: 3.75616717338562\n",
            "Training Iteration 3598, Loss: 10.080034255981445\n",
            "Training Iteration 3599, Loss: 2.777796745300293\n",
            "Training Iteration 3600, Loss: 7.15847635269165\n",
            "Training Iteration 3601, Loss: 6.943655967712402\n",
            "Training Iteration 3602, Loss: 9.953731536865234\n",
            "Training Iteration 3603, Loss: 5.540307521820068\n",
            "Training Iteration 3604, Loss: 5.734281539916992\n",
            "Training Iteration 3605, Loss: 2.3813650608062744\n",
            "Training Iteration 3606, Loss: 4.158593654632568\n",
            "Training Iteration 3607, Loss: 3.732055187225342\n",
            "Training Iteration 3608, Loss: 7.311483383178711\n",
            "Training Iteration 3609, Loss: 7.509619235992432\n",
            "Training Iteration 3610, Loss: 2.7454288005828857\n",
            "Training Iteration 3611, Loss: 4.562783241271973\n",
            "Training Iteration 3612, Loss: 3.8290085792541504\n",
            "Training Iteration 3613, Loss: 3.2382144927978516\n",
            "Training Iteration 3614, Loss: 4.283599853515625\n",
            "Training Iteration 3615, Loss: 5.729142189025879\n",
            "Training Iteration 3616, Loss: 9.832710266113281\n",
            "Training Iteration 3617, Loss: 2.7995920181274414\n",
            "Training Iteration 3618, Loss: 5.221196174621582\n",
            "Training Iteration 3619, Loss: 3.7830193042755127\n",
            "Training Iteration 3620, Loss: 4.475782871246338\n",
            "Training Iteration 3621, Loss: 5.608495712280273\n",
            "Training Iteration 3622, Loss: 6.09296989440918\n",
            "Training Iteration 3623, Loss: 5.249508380889893\n",
            "Training Iteration 3624, Loss: 6.108940601348877\n",
            "Training Iteration 3625, Loss: 4.63411283493042\n",
            "Training Iteration 3626, Loss: 5.477391242980957\n",
            "Training Iteration 3627, Loss: 5.243247032165527\n",
            "Training Iteration 3628, Loss: 7.106905460357666\n",
            "Training Iteration 3629, Loss: 3.774074077606201\n",
            "Training Iteration 3630, Loss: 4.032352924346924\n",
            "Training Iteration 3631, Loss: 3.014051914215088\n",
            "Training Iteration 3632, Loss: 3.738417148590088\n",
            "Training Iteration 3633, Loss: 5.671385765075684\n",
            "Training Iteration 3634, Loss: 4.6593918800354\n",
            "Training Iteration 3635, Loss: 4.120815753936768\n",
            "Training Iteration 3636, Loss: 3.2281837463378906\n",
            "Training Iteration 3637, Loss: 4.418506145477295\n",
            "Training Iteration 3638, Loss: 5.450281143188477\n",
            "Training Iteration 3639, Loss: 3.5197556018829346\n",
            "Training Iteration 3640, Loss: 4.317571640014648\n",
            "Training Iteration 3641, Loss: 8.424691200256348\n",
            "Training Iteration 3642, Loss: 5.240368843078613\n",
            "Training Iteration 3643, Loss: 4.389819145202637\n",
            "Training Iteration 3644, Loss: 4.290778636932373\n",
            "Training Iteration 3645, Loss: 5.260351181030273\n",
            "Training Iteration 3646, Loss: 4.364678382873535\n",
            "Training Iteration 3647, Loss: 5.506025791168213\n",
            "Training Iteration 3648, Loss: 3.229901075363159\n",
            "Training Iteration 3649, Loss: 4.4565324783325195\n",
            "Training Iteration 3650, Loss: 6.539122104644775\n",
            "Training Iteration 3651, Loss: 5.565210342407227\n",
            "Training Iteration 3652, Loss: 3.7787139415740967\n",
            "Training Iteration 3653, Loss: 5.889400005340576\n",
            "Training Iteration 3654, Loss: 3.932417631149292\n",
            "Training Iteration 3655, Loss: 5.270296573638916\n",
            "Training Iteration 3656, Loss: 4.34996223449707\n",
            "Training Iteration 3657, Loss: 6.942410469055176\n",
            "Training Iteration 3658, Loss: 4.175478935241699\n",
            "Training Iteration 3659, Loss: 3.973051071166992\n",
            "Training Iteration 3660, Loss: 12.043856620788574\n",
            "Training Iteration 3661, Loss: 7.263362407684326\n",
            "Training Iteration 3662, Loss: 2.591958999633789\n",
            "Training Iteration 3663, Loss: 9.12967300415039\n",
            "Training Iteration 3664, Loss: 6.426907539367676\n",
            "Training Iteration 3665, Loss: 2.168396472930908\n",
            "Training Iteration 3666, Loss: 4.9098429679870605\n",
            "Training Iteration 3667, Loss: 4.195123672485352\n",
            "Training Iteration 3668, Loss: 9.264551162719727\n",
            "Training Iteration 3669, Loss: 7.153707027435303\n",
            "Training Iteration 3670, Loss: 8.135810852050781\n",
            "Training Iteration 3671, Loss: 2.008023738861084\n",
            "Training Iteration 3672, Loss: 5.511551856994629\n",
            "Training Iteration 3673, Loss: 3.547969102859497\n",
            "Training Iteration 3674, Loss: 8.983811378479004\n",
            "Training Iteration 3675, Loss: 7.147607326507568\n",
            "Training Iteration 3676, Loss: 7.633071422576904\n",
            "Training Iteration 3677, Loss: 4.086893558502197\n",
            "Training Iteration 3678, Loss: 4.752248287200928\n",
            "Training Iteration 3679, Loss: 6.963831901550293\n",
            "Training Iteration 3680, Loss: 8.020764350891113\n",
            "Training Iteration 3681, Loss: 8.369575500488281\n",
            "Training Iteration 3682, Loss: 3.168762445449829\n",
            "Training Iteration 3683, Loss: 6.084386825561523\n",
            "Training Iteration 3684, Loss: 3.27817964553833\n",
            "Training Iteration 3685, Loss: 5.437829494476318\n",
            "Training Iteration 3686, Loss: 5.964978218078613\n",
            "Training Iteration 3687, Loss: 4.009974002838135\n",
            "Training Iteration 3688, Loss: 3.087294578552246\n",
            "Training Iteration 3689, Loss: 5.061486721038818\n",
            "Training Iteration 3690, Loss: 3.184738874435425\n",
            "Training Iteration 3691, Loss: 1.9824185371398926\n",
            "Training Iteration 3692, Loss: 3.6899161338806152\n",
            "Training Iteration 3693, Loss: 2.318598747253418\n",
            "Training Iteration 3694, Loss: 3.879915714263916\n",
            "Training Iteration 3695, Loss: 6.52390193939209\n",
            "Training Iteration 3696, Loss: 3.7928805351257324\n",
            "Training Iteration 3697, Loss: 3.2526743412017822\n",
            "Training Iteration 3698, Loss: 4.4102582931518555\n",
            "Training Iteration 3699, Loss: 2.508939027786255\n",
            "Training Iteration 3700, Loss: 6.624059677124023\n",
            "Training Iteration 3701, Loss: 5.581661224365234\n",
            "Training Iteration 3702, Loss: 4.82442569732666\n",
            "Training Iteration 3703, Loss: 3.2070021629333496\n",
            "Training Iteration 3704, Loss: 8.993592262268066\n",
            "Training Iteration 3705, Loss: 4.033977508544922\n",
            "Training Iteration 3706, Loss: 3.8914217948913574\n",
            "Training Iteration 3707, Loss: 5.004742622375488\n",
            "Training Iteration 3708, Loss: 5.302034854888916\n",
            "Training Iteration 3709, Loss: 6.621750354766846\n",
            "Training Iteration 3710, Loss: 5.834100246429443\n",
            "Training Iteration 3711, Loss: 4.659316062927246\n",
            "Training Iteration 3712, Loss: 3.8995232582092285\n",
            "Training Iteration 3713, Loss: 7.253981590270996\n",
            "Training Iteration 3714, Loss: 6.574180603027344\n",
            "Training Iteration 3715, Loss: 6.43580961227417\n",
            "Training Iteration 3716, Loss: 3.1071739196777344\n",
            "Training Iteration 3717, Loss: 3.996730327606201\n",
            "Training Iteration 3718, Loss: 4.464881896972656\n",
            "Training Iteration 3719, Loss: 6.554132461547852\n",
            "Training Iteration 3720, Loss: 2.936988353729248\n",
            "Training Iteration 3721, Loss: 6.074671268463135\n",
            "Training Iteration 3722, Loss: 6.924930095672607\n",
            "Training Iteration 3723, Loss: 5.7885565757751465\n",
            "Training Iteration 3724, Loss: 6.6255292892456055\n",
            "Training Iteration 3725, Loss: 4.339443683624268\n",
            "Training Iteration 3726, Loss: 5.344500541687012\n",
            "Training Iteration 3727, Loss: 4.786566257476807\n",
            "Training Iteration 3728, Loss: 5.714967250823975\n",
            "Training Iteration 3729, Loss: 3.49784255027771\n",
            "Training Iteration 3730, Loss: 7.824341297149658\n",
            "Training Iteration 3731, Loss: 8.22012710571289\n",
            "Training Iteration 3732, Loss: 3.8786370754241943\n",
            "Training Iteration 3733, Loss: 6.535029411315918\n",
            "Training Iteration 3734, Loss: 3.652463912963867\n",
            "Training Iteration 3735, Loss: 5.213183403015137\n",
            "Training Iteration 3736, Loss: 4.384781837463379\n",
            "Training Iteration 3737, Loss: 4.90557861328125\n",
            "Training Iteration 3738, Loss: 4.292485237121582\n",
            "Training Iteration 3739, Loss: 4.399246692657471\n",
            "Training Iteration 3740, Loss: 4.831413269042969\n",
            "Training Iteration 3741, Loss: 4.317176818847656\n",
            "Training Iteration 3742, Loss: 6.819055080413818\n",
            "Training Iteration 3743, Loss: 5.086681365966797\n",
            "Training Iteration 3744, Loss: 4.3773674964904785\n",
            "Training Iteration 3745, Loss: 3.938765525817871\n",
            "Training Iteration 3746, Loss: 3.5166776180267334\n",
            "Training Iteration 3747, Loss: 2.4828286170959473\n",
            "Training Iteration 3748, Loss: 3.4180831909179688\n",
            "Training Iteration 3749, Loss: 3.1496012210845947\n",
            "Training Iteration 3750, Loss: 6.18548583984375\n",
            "Training Iteration 3751, Loss: 3.2193360328674316\n",
            "Training Iteration 3752, Loss: 5.52239990234375\n",
            "Training Iteration 3753, Loss: 5.272290229797363\n",
            "Training Iteration 3754, Loss: 4.534063816070557\n",
            "Training Iteration 3755, Loss: 7.247989177703857\n",
            "Training Iteration 3756, Loss: 6.020453453063965\n",
            "Training Iteration 3757, Loss: 3.61248517036438\n",
            "Training Iteration 3758, Loss: 6.0395636558532715\n",
            "Training Iteration 3759, Loss: 3.7102065086364746\n",
            "Training Iteration 3760, Loss: 4.504918098449707\n",
            "Training Iteration 3761, Loss: 6.531732082366943\n",
            "Training Iteration 3762, Loss: 6.148319244384766\n",
            "Training Iteration 3763, Loss: 6.67824649810791\n",
            "Training Iteration 3764, Loss: 7.349785804748535\n",
            "Training Iteration 3765, Loss: 4.793862819671631\n",
            "Training Iteration 3766, Loss: 4.678184986114502\n",
            "Training Iteration 3767, Loss: 6.7106170654296875\n",
            "Training Iteration 3768, Loss: 5.143486976623535\n",
            "Training Iteration 3769, Loss: 6.670219898223877\n",
            "Training Iteration 3770, Loss: 4.307368278503418\n",
            "Training Iteration 3771, Loss: 6.1177167892456055\n",
            "Training Iteration 3772, Loss: 4.115921497344971\n",
            "Training Iteration 3773, Loss: 4.19625997543335\n",
            "Training Iteration 3774, Loss: 2.468135356903076\n",
            "Training Iteration 3775, Loss: 4.190537452697754\n",
            "Training Iteration 3776, Loss: 3.810413122177124\n",
            "Training Iteration 3777, Loss: 4.925724029541016\n",
            "Training Iteration 3778, Loss: 2.895153045654297\n",
            "Training Iteration 3779, Loss: 4.166968822479248\n",
            "Training Iteration 3780, Loss: 2.248136281967163\n",
            "Training Iteration 3781, Loss: 10.46192741394043\n",
            "Training Iteration 3782, Loss: 2.787900924682617\n",
            "Training Iteration 3783, Loss: 4.792013168334961\n",
            "Training Iteration 3784, Loss: 3.264164924621582\n",
            "Training Iteration 3785, Loss: 3.61665678024292\n",
            "Training Iteration 3786, Loss: 3.4547412395477295\n",
            "Training Iteration 3787, Loss: 5.480352401733398\n",
            "Training Iteration 3788, Loss: 5.14950704574585\n",
            "Training Iteration 3789, Loss: 3.6624040603637695\n",
            "Training Iteration 3790, Loss: 4.791507244110107\n",
            "Training Iteration 3791, Loss: 3.486293315887451\n",
            "Training Iteration 3792, Loss: 4.745532512664795\n",
            "Training Iteration 3793, Loss: 4.297004699707031\n",
            "Training Iteration 3794, Loss: 3.768649101257324\n",
            "Training Iteration 3795, Loss: 6.249790668487549\n",
            "Training Iteration 3796, Loss: 4.134462356567383\n",
            "Training Iteration 3797, Loss: 6.862244129180908\n",
            "Training Iteration 3798, Loss: 2.6483750343322754\n",
            "Training Iteration 3799, Loss: 3.9626083374023438\n",
            "Training Iteration 3800, Loss: 6.341404914855957\n",
            "Training Iteration 3801, Loss: 5.647489070892334\n",
            "Training Iteration 3802, Loss: 3.5474300384521484\n",
            "Training Iteration 3803, Loss: 4.998283863067627\n",
            "Training Iteration 3804, Loss: 2.342824697494507\n",
            "Training Iteration 3805, Loss: 4.128165245056152\n",
            "Training Iteration 3806, Loss: 4.285947322845459\n",
            "Training Iteration 3807, Loss: 8.589664459228516\n",
            "Training Iteration 3808, Loss: 1.413108229637146\n",
            "Training Iteration 3809, Loss: 4.028604030609131\n",
            "Training Iteration 3810, Loss: 4.874065399169922\n",
            "Training Iteration 3811, Loss: 6.782654762268066\n",
            "Training Iteration 3812, Loss: 2.5798730850219727\n",
            "Training Iteration 3813, Loss: 4.373229503631592\n",
            "Training Iteration 3814, Loss: 4.529036521911621\n",
            "Training Iteration 3815, Loss: 5.903243064880371\n",
            "Training Iteration 3816, Loss: 2.053938627243042\n",
            "Training Iteration 3817, Loss: 4.761308670043945\n",
            "Training Iteration 3818, Loss: 2.482314109802246\n",
            "Training Iteration 3819, Loss: 2.9248416423797607\n",
            "Training Iteration 3820, Loss: 2.850708484649658\n",
            "Training Iteration 3821, Loss: 2.9879963397979736\n",
            "Training Iteration 3822, Loss: 2.9523584842681885\n",
            "Training Iteration 3823, Loss: 3.1328182220458984\n",
            "Training Iteration 3824, Loss: 3.24625825881958\n",
            "Training Iteration 3825, Loss: 4.784736633300781\n",
            "Training Iteration 3826, Loss: 4.1020894050598145\n",
            "Training Iteration 3827, Loss: 5.421493053436279\n",
            "Training Iteration 3828, Loss: 1.8659749031066895\n",
            "Training Iteration 3829, Loss: 2.7296814918518066\n",
            "Training Iteration 3830, Loss: 2.101102590560913\n",
            "Training Iteration 3831, Loss: 3.052009105682373\n",
            "Training Iteration 3832, Loss: 3.668792724609375\n",
            "Training Iteration 3833, Loss: 3.066913366317749\n",
            "Training Iteration 3834, Loss: 6.614656448364258\n",
            "Training Iteration 3835, Loss: 2.8549764156341553\n",
            "Training Iteration 3836, Loss: 1.8435372114181519\n",
            "Training Iteration 3837, Loss: 6.946000099182129\n",
            "Training Iteration 3838, Loss: 6.532973289489746\n",
            "Training Iteration 3839, Loss: 2.266265630722046\n",
            "Training Iteration 3840, Loss: 3.0384392738342285\n",
            "Training Iteration 3841, Loss: 1.5266642570495605\n",
            "Training Iteration 3842, Loss: 4.438776016235352\n",
            "Training Iteration 3843, Loss: 6.100525856018066\n",
            "Training Iteration 3844, Loss: 6.3884758949279785\n",
            "Training Iteration 3845, Loss: 7.4716477394104\n",
            "Training Iteration 3846, Loss: 3.777162551879883\n",
            "Training Iteration 3847, Loss: 2.9169702529907227\n",
            "Training Iteration 3848, Loss: 4.572037696838379\n",
            "Training Iteration 3849, Loss: 6.685549736022949\n",
            "Training Iteration 3850, Loss: 5.389869689941406\n",
            "Training Iteration 3851, Loss: 3.040055751800537\n",
            "Training Iteration 3852, Loss: 2.4959826469421387\n",
            "Training Iteration 3853, Loss: 4.3366169929504395\n",
            "Training Iteration 3854, Loss: 5.752931594848633\n",
            "Training Iteration 3855, Loss: 6.283149242401123\n",
            "Training Iteration 3856, Loss: 8.017358779907227\n",
            "Training Iteration 3857, Loss: 4.98154354095459\n",
            "Training Iteration 3858, Loss: 5.909048557281494\n",
            "Training Iteration 3859, Loss: 3.2445409297943115\n",
            "Training Iteration 3860, Loss: 5.6392822265625\n",
            "Training Iteration 3861, Loss: 3.681011915206909\n",
            "Training Iteration 3862, Loss: 4.430343151092529\n",
            "Training Iteration 3863, Loss: 3.1302242279052734\n",
            "Training Iteration 3864, Loss: 6.885349750518799\n",
            "Training Iteration 3865, Loss: 2.8194408416748047\n",
            "Training Iteration 3866, Loss: 6.888139247894287\n",
            "Training Iteration 3867, Loss: 8.6295804977417\n",
            "Training Iteration 3868, Loss: 4.818490028381348\n",
            "Training Iteration 3869, Loss: 3.4392871856689453\n",
            "Training Iteration 3870, Loss: 4.5813469886779785\n",
            "Training Iteration 3871, Loss: 4.125496864318848\n",
            "Training Iteration 3872, Loss: 5.327823162078857\n",
            "Training Iteration 3873, Loss: 4.333847522735596\n",
            "Training Iteration 3874, Loss: 3.795776605606079\n",
            "Training Iteration 3875, Loss: 5.389670372009277\n",
            "Training Iteration 3876, Loss: 3.5909018516540527\n",
            "Training Iteration 3877, Loss: 2.016906261444092\n",
            "Training Iteration 3878, Loss: 5.314757347106934\n",
            "Training Iteration 3879, Loss: 5.270185947418213\n",
            "Training Iteration 3880, Loss: 7.073146820068359\n",
            "Training Iteration 3881, Loss: 4.903343200683594\n",
            "Training Iteration 3882, Loss: 3.0589325428009033\n",
            "Training Iteration 3883, Loss: 4.131917476654053\n",
            "Training Iteration 3884, Loss: 5.074202537536621\n",
            "Training Iteration 3885, Loss: 5.2742695808410645\n",
            "Training Iteration 3886, Loss: 2.808459758758545\n",
            "Training Iteration 3887, Loss: 2.271791458129883\n",
            "Training Iteration 3888, Loss: 4.873308181762695\n",
            "Training Iteration 3889, Loss: 5.163917064666748\n",
            "Training Iteration 3890, Loss: 4.469464302062988\n",
            "Training Iteration 3891, Loss: 5.515019416809082\n",
            "Training Iteration 3892, Loss: 6.756856441497803\n",
            "Training Iteration 3893, Loss: 3.3097147941589355\n",
            "Training Iteration 3894, Loss: 5.921411514282227\n",
            "Training Iteration 3895, Loss: 5.414432525634766\n",
            "Training Iteration 3896, Loss: 4.472622394561768\n",
            "Training Iteration 3897, Loss: 5.136627674102783\n",
            "Training Iteration 3898, Loss: 4.160764217376709\n",
            "Training Iteration 3899, Loss: 4.898433208465576\n",
            "Training Iteration 3900, Loss: 1.5806442499160767\n",
            "Training Iteration 3901, Loss: 6.06487512588501\n",
            "Training Iteration 3902, Loss: 1.4451712369918823\n",
            "Training Iteration 3903, Loss: 7.621026515960693\n",
            "Training Iteration 3904, Loss: 5.223883628845215\n",
            "Training Iteration 3905, Loss: 2.852104663848877\n",
            "Training Iteration 3906, Loss: 4.162211894989014\n",
            "Training Iteration 3907, Loss: 5.2677998542785645\n",
            "Training Iteration 3908, Loss: 4.975668430328369\n",
            "Training Iteration 3909, Loss: 5.088474273681641\n",
            "Training Iteration 3910, Loss: 5.811959266662598\n",
            "Training Iteration 3911, Loss: 4.27136754989624\n",
            "Training Iteration 3912, Loss: 6.014651298522949\n",
            "Training Iteration 3913, Loss: 3.5042471885681152\n",
            "Training Iteration 3914, Loss: 3.0215837955474854\n",
            "Training Iteration 3915, Loss: 5.445943832397461\n",
            "Training Iteration 3916, Loss: 2.4735522270202637\n",
            "Training Iteration 3917, Loss: 3.7832584381103516\n",
            "Training Iteration 3918, Loss: 3.974630355834961\n",
            "Training Iteration 3919, Loss: 2.4890761375427246\n",
            "Training Iteration 3920, Loss: 1.9965370893478394\n",
            "Training Iteration 3921, Loss: 4.902730941772461\n",
            "Training Iteration 3922, Loss: 9.427447319030762\n",
            "Training Iteration 3923, Loss: 3.541473388671875\n",
            "Training Iteration 3924, Loss: 2.5981013774871826\n",
            "Training Iteration 3925, Loss: 3.681213140487671\n",
            "Training Iteration 3926, Loss: 3.488360643386841\n",
            "Training Iteration 3927, Loss: 4.846991539001465\n",
            "Training Iteration 3928, Loss: 2.760667085647583\n",
            "Training Iteration 3929, Loss: 3.978334426879883\n",
            "Training Iteration 3930, Loss: 3.751965045928955\n",
            "Training Iteration 3931, Loss: 3.6710033416748047\n",
            "Training Iteration 3932, Loss: 4.528081893920898\n",
            "Training Iteration 3933, Loss: 2.9962401390075684\n",
            "Training Iteration 3934, Loss: 3.3674182891845703\n",
            "Training Iteration 3935, Loss: 5.624687194824219\n",
            "Training Iteration 3936, Loss: 2.6661274433135986\n",
            "Training Iteration 3937, Loss: 2.8962159156799316\n",
            "Training Iteration 3938, Loss: 2.03767991065979\n",
            "Training Iteration 3939, Loss: 4.017893314361572\n",
            "Training Iteration 3940, Loss: 3.6411828994750977\n",
            "Training Iteration 3941, Loss: 3.6117522716522217\n",
            "Training Iteration 3942, Loss: 4.125861167907715\n",
            "Training Iteration 3943, Loss: 3.197143077850342\n",
            "Training Iteration 3944, Loss: 3.469480276107788\n",
            "Training Iteration 3945, Loss: 6.655730724334717\n",
            "Training Iteration 3946, Loss: 4.422301292419434\n",
            "Training Iteration 3947, Loss: 3.7838656902313232\n",
            "Training Iteration 3948, Loss: 4.197329998016357\n",
            "Training Iteration 3949, Loss: 2.6166932582855225\n",
            "Training Iteration 3950, Loss: 5.329516410827637\n",
            "Training Iteration 3951, Loss: 2.3602850437164307\n",
            "Training Iteration 3952, Loss: 2.0048930644989014\n",
            "Training Iteration 3953, Loss: 3.086085081100464\n",
            "Training Iteration 3954, Loss: 4.8933820724487305\n",
            "Training Iteration 3955, Loss: 6.339700698852539\n",
            "Training Iteration 3956, Loss: 7.016782760620117\n",
            "Training Iteration 3957, Loss: 3.4390063285827637\n",
            "Training Iteration 3958, Loss: 1.8354398012161255\n",
            "Training Iteration 3959, Loss: 5.4249138832092285\n",
            "Training Iteration 3960, Loss: 3.739508867263794\n",
            "Training Iteration 3961, Loss: 4.909682750701904\n",
            "Training Iteration 3962, Loss: 7.638570785522461\n",
            "Training Iteration 3963, Loss: 3.8230481147766113\n",
            "Training Iteration 3964, Loss: 3.4449291229248047\n",
            "Training Iteration 3965, Loss: 3.869534969329834\n",
            "Training Iteration 3966, Loss: 5.133705139160156\n",
            "Training Iteration 3967, Loss: 4.171252727508545\n",
            "Training Iteration 3968, Loss: 3.8426318168640137\n",
            "Training Iteration 3969, Loss: 3.3244006633758545\n",
            "Training Iteration 3970, Loss: 1.5583733320236206\n",
            "Training Iteration 3971, Loss: 3.122093677520752\n",
            "Training Iteration 3972, Loss: 3.9216318130493164\n",
            "Training Iteration 3973, Loss: 4.011660575866699\n",
            "Training Iteration 3974, Loss: 6.120425701141357\n",
            "Training Iteration 3975, Loss: 3.096632480621338\n",
            "Training Iteration 3976, Loss: 6.743740558624268\n",
            "Training Iteration 3977, Loss: 5.559087753295898\n",
            "Training Iteration 3978, Loss: 4.748691082000732\n",
            "Training Iteration 3979, Loss: 6.559690475463867\n",
            "Training Iteration 3980, Loss: 3.484846830368042\n",
            "Training Iteration 3981, Loss: 5.069230079650879\n",
            "Training Iteration 3982, Loss: 5.00628662109375\n",
            "Training Iteration 3983, Loss: 5.483619213104248\n",
            "Training Iteration 3984, Loss: 4.400829792022705\n",
            "Training Iteration 3985, Loss: 1.8168668746948242\n",
            "Training Iteration 3986, Loss: 5.542821884155273\n",
            "Training Iteration 3987, Loss: 5.013116836547852\n",
            "Training Iteration 3988, Loss: 4.233867168426514\n",
            "Training Iteration 3989, Loss: 7.332813739776611\n",
            "Training Iteration 3990, Loss: 4.1191582679748535\n",
            "Training Iteration 3991, Loss: 7.749242782592773\n",
            "Training Iteration 3992, Loss: 7.745432376861572\n",
            "Training Iteration 3993, Loss: 7.209076881408691\n",
            "Training Iteration 3994, Loss: 4.054174423217773\n",
            "Training Iteration 3995, Loss: 4.239466667175293\n",
            "Training Iteration 3996, Loss: 4.4644060134887695\n",
            "Training Iteration 3997, Loss: 5.090850830078125\n",
            "Training Iteration 3998, Loss: 2.3301899433135986\n",
            "Training Iteration 3999, Loss: 2.7198448181152344\n",
            "Training Iteration 4000, Loss: 5.308629512786865\n",
            "Training Iteration 4001, Loss: 5.428663730621338\n",
            "Training Iteration 4002, Loss: 3.8122825622558594\n",
            "Training Iteration 4003, Loss: 5.290578365325928\n",
            "Training Iteration 4004, Loss: 4.378402233123779\n",
            "Training Iteration 4005, Loss: 3.4509801864624023\n",
            "Training Iteration 4006, Loss: 3.579890251159668\n",
            "Training Iteration 4007, Loss: 2.4115421772003174\n",
            "Training Iteration 4008, Loss: 5.143062591552734\n",
            "Training Iteration 4009, Loss: 5.219964504241943\n",
            "Training Iteration 4010, Loss: 4.477143287658691\n",
            "Training Iteration 4011, Loss: 6.069087505340576\n",
            "Training Iteration 4012, Loss: 4.769360542297363\n",
            "Training Iteration 4013, Loss: 2.8049917221069336\n",
            "Training Iteration 4014, Loss: 4.723944187164307\n",
            "Training Iteration 4015, Loss: 3.615065336227417\n",
            "Training Iteration 4016, Loss: 4.385617733001709\n",
            "Training Iteration 4017, Loss: 3.7163147926330566\n",
            "Training Iteration 4018, Loss: 1.4974581003189087\n",
            "Training Iteration 4019, Loss: 5.763495445251465\n",
            "Training Iteration 4020, Loss: 2.334897994995117\n",
            "Training Iteration 4021, Loss: 1.8345646858215332\n",
            "Training Iteration 4022, Loss: 7.808755874633789\n",
            "Training Iteration 4023, Loss: 6.1174187660217285\n",
            "Training Iteration 4024, Loss: 7.218842029571533\n",
            "Training Iteration 4025, Loss: 4.202688694000244\n",
            "Training Iteration 4026, Loss: 7.3048601150512695\n",
            "Training Iteration 4027, Loss: 6.056889533996582\n",
            "Training Iteration 4028, Loss: 4.449193000793457\n",
            "Training Iteration 4029, Loss: 4.436490058898926\n",
            "Training Iteration 4030, Loss: 3.978119134902954\n",
            "Training Iteration 4031, Loss: 4.058684825897217\n",
            "Training Iteration 4032, Loss: 3.7690634727478027\n",
            "Training Iteration 4033, Loss: 4.0595808029174805\n",
            "Training Iteration 4034, Loss: 3.8743371963500977\n",
            "Training Iteration 4035, Loss: 4.192196369171143\n",
            "Training Iteration 4036, Loss: 2.6440579891204834\n",
            "Training Iteration 4037, Loss: 2.5762486457824707\n",
            "Training Iteration 4038, Loss: 4.834110260009766\n",
            "Training Iteration 4039, Loss: 4.917462348937988\n",
            "Training Iteration 4040, Loss: 5.389133930206299\n",
            "Training Iteration 4041, Loss: 4.689846038818359\n",
            "Training Iteration 4042, Loss: 3.223719596862793\n",
            "Training Iteration 4043, Loss: 3.555603504180908\n",
            "Training Iteration 4044, Loss: 2.4049248695373535\n",
            "Training Iteration 4045, Loss: 3.335771322250366\n",
            "Training Iteration 4046, Loss: 2.775082588195801\n",
            "Training Iteration 4047, Loss: 7.770002841949463\n",
            "Training Iteration 4048, Loss: 4.582900047302246\n",
            "Training Iteration 4049, Loss: 9.202615737915039\n",
            "Training Iteration 4050, Loss: 2.354238748550415\n",
            "Training Iteration 4051, Loss: 4.991678714752197\n",
            "Training Iteration 4052, Loss: 3.106311321258545\n",
            "Training Iteration 4053, Loss: 6.073603630065918\n",
            "Training Iteration 4054, Loss: 4.855854034423828\n",
            "Training Iteration 4055, Loss: 6.1346235275268555\n",
            "Training Iteration 4056, Loss: 2.951815366744995\n",
            "Training Iteration 4057, Loss: 4.217503547668457\n",
            "Training Iteration 4058, Loss: 4.484001159667969\n",
            "Training Iteration 4059, Loss: 4.499007225036621\n",
            "Training Iteration 4060, Loss: 1.8258130550384521\n",
            "Training Iteration 4061, Loss: 2.6524155139923096\n",
            "Training Iteration 4062, Loss: 3.853043556213379\n",
            "Training Iteration 4063, Loss: 2.920480251312256\n",
            "Training Iteration 4064, Loss: 2.853445053100586\n",
            "Training Iteration 4065, Loss: 6.7314653396606445\n",
            "Training Iteration 4066, Loss: 3.658867359161377\n",
            "Training Iteration 4067, Loss: 5.649726390838623\n",
            "Training Iteration 4068, Loss: 1.0413968563079834\n",
            "Training Iteration 4069, Loss: 6.153115272521973\n",
            "Training Iteration 4070, Loss: 5.506147861480713\n",
            "Training Iteration 4071, Loss: 4.925005912780762\n",
            "Training Iteration 4072, Loss: 8.368670463562012\n",
            "Training Iteration 4073, Loss: 1.8911762237548828\n",
            "Training Iteration 4074, Loss: 2.15323805809021\n",
            "Training Iteration 4075, Loss: 2.294355869293213\n",
            "Training Iteration 4076, Loss: 2.0495529174804688\n",
            "Training Iteration 4077, Loss: 6.6006693840026855\n",
            "Training Iteration 4078, Loss: 3.9028141498565674\n",
            "Training Iteration 4079, Loss: 4.777936935424805\n",
            "Training Iteration 4080, Loss: 5.617266654968262\n",
            "Training Iteration 4081, Loss: 3.3085968494415283\n",
            "Training Iteration 4082, Loss: 6.291823863983154\n",
            "Training Iteration 4083, Loss: 7.610060214996338\n",
            "Training Iteration 4084, Loss: 3.9399073123931885\n",
            "Training Iteration 4085, Loss: 5.362370014190674\n",
            "Training Iteration 4086, Loss: 4.5664896965026855\n",
            "Training Iteration 4087, Loss: 6.174196720123291\n",
            "Training Iteration 4088, Loss: 8.400693893432617\n",
            "Training Iteration 4089, Loss: 3.1991565227508545\n",
            "Training Iteration 4090, Loss: 4.762823104858398\n",
            "Training Iteration 4091, Loss: 3.6458358764648438\n",
            "Training Iteration 4092, Loss: 5.392844200134277\n",
            "Training Iteration 4093, Loss: 5.288249969482422\n",
            "Training Iteration 4094, Loss: 7.750695705413818\n",
            "Training Iteration 4095, Loss: 4.051730155944824\n",
            "Training Iteration 4096, Loss: 4.611385345458984\n",
            "Training Iteration 4097, Loss: 2.8106765747070312\n",
            "Training Iteration 4098, Loss: 7.438994407653809\n",
            "Training Iteration 4099, Loss: 4.374072074890137\n",
            "Training Iteration 4100, Loss: 4.703891277313232\n",
            "Training Iteration 4101, Loss: 6.674278259277344\n",
            "Training Iteration 4102, Loss: 3.3037045001983643\n",
            "Training Iteration 4103, Loss: 4.121956825256348\n",
            "Training Iteration 4104, Loss: 3.76366925239563\n",
            "Training Iteration 4105, Loss: 4.3223114013671875\n",
            "Training Iteration 4106, Loss: 4.087167263031006\n",
            "Training Iteration 4107, Loss: 2.720076084136963\n",
            "Training Iteration 4108, Loss: 2.8261775970458984\n",
            "Training Iteration 4109, Loss: 6.300436973571777\n",
            "Training Iteration 4110, Loss: 4.484524726867676\n",
            "Training Iteration 4111, Loss: 3.3647048473358154\n",
            "Training Iteration 4112, Loss: 3.355654239654541\n",
            "Training Iteration 4113, Loss: 4.793145179748535\n",
            "Training Iteration 4114, Loss: 6.519937515258789\n",
            "Training Iteration 4115, Loss: 2.91409969329834\n",
            "Training Iteration 4116, Loss: 4.639153957366943\n",
            "Training Iteration 4117, Loss: 3.7886085510253906\n",
            "Training Iteration 4118, Loss: 4.973918914794922\n",
            "Training Iteration 4119, Loss: 5.97743034362793\n",
            "Training Iteration 4120, Loss: 4.885103225708008\n",
            "Training Iteration 4121, Loss: 3.6937594413757324\n",
            "Training Iteration 4122, Loss: 6.015514850616455\n",
            "Training Iteration 4123, Loss: 4.602973937988281\n",
            "Training Iteration 4124, Loss: 3.834362030029297\n",
            "Training Iteration 4125, Loss: 4.393291473388672\n",
            "Training Iteration 4126, Loss: 5.815230369567871\n",
            "Training Iteration 4127, Loss: 1.7315454483032227\n",
            "Training Iteration 4128, Loss: 8.081940650939941\n",
            "Training Iteration 4129, Loss: 6.325042724609375\n",
            "Training Iteration 4130, Loss: 6.627413272857666\n",
            "Training Iteration 4131, Loss: 6.388103008270264\n",
            "Training Iteration 4132, Loss: 3.1062710285186768\n",
            "Training Iteration 4133, Loss: 3.5430538654327393\n",
            "Training Iteration 4134, Loss: 3.5280070304870605\n",
            "Training Iteration 4135, Loss: 4.402746677398682\n",
            "Training Iteration 4136, Loss: 4.233022689819336\n",
            "Training Iteration 4137, Loss: 2.5176570415496826\n",
            "Training Iteration 4138, Loss: 5.378033638000488\n",
            "Training Iteration 4139, Loss: 4.354917526245117\n",
            "Training Iteration 4140, Loss: 5.166134357452393\n",
            "Training Iteration 4141, Loss: 4.092820644378662\n",
            "Training Iteration 4142, Loss: 3.929638385772705\n",
            "Training Iteration 4143, Loss: 2.8929529190063477\n",
            "Training Iteration 4144, Loss: 5.635412216186523\n",
            "Training Iteration 4145, Loss: 6.832131385803223\n",
            "Training Iteration 4146, Loss: 6.2272796630859375\n",
            "Training Iteration 4147, Loss: 2.106431722640991\n",
            "Training Iteration 4148, Loss: 3.5022854804992676\n",
            "Training Iteration 4149, Loss: 4.016668319702148\n",
            "Training Iteration 4150, Loss: 4.20467472076416\n",
            "Training Iteration 4151, Loss: 4.325236797332764\n",
            "Training Iteration 4152, Loss: 4.477032661437988\n",
            "Training Iteration 4153, Loss: 3.7658660411834717\n",
            "Training Iteration 4154, Loss: 2.6681971549987793\n",
            "Training Iteration 4155, Loss: 2.8481550216674805\n",
            "Training Iteration 4156, Loss: 4.573301315307617\n",
            "Training Iteration 4157, Loss: 3.8771047592163086\n",
            "Training Iteration 4158, Loss: 2.829066276550293\n",
            "Training Iteration 4159, Loss: 5.268202781677246\n",
            "Training Iteration 4160, Loss: 4.4124250411987305\n",
            "Training Iteration 4161, Loss: 4.832215785980225\n",
            "Training Iteration 4162, Loss: 1.6549546718597412\n",
            "Training Iteration 4163, Loss: 3.757078170776367\n",
            "Training Iteration 4164, Loss: 4.785945892333984\n",
            "Training Iteration 4165, Loss: 4.618441581726074\n",
            "Training Iteration 4166, Loss: 2.082484483718872\n",
            "Training Iteration 4167, Loss: 1.6006861925125122\n",
            "Training Iteration 4168, Loss: 3.3371689319610596\n",
            "Training Iteration 4169, Loss: 3.3060665130615234\n",
            "Training Iteration 4170, Loss: 2.42341947555542\n",
            "Training Iteration 4171, Loss: 3.6861188411712646\n",
            "Training Iteration 4172, Loss: 2.4328224658966064\n",
            "Training Iteration 4173, Loss: 2.408514976501465\n",
            "Training Iteration 4174, Loss: 2.300844430923462\n",
            "Training Iteration 4175, Loss: 3.7993202209472656\n",
            "Training Iteration 4176, Loss: 2.883578300476074\n",
            "Training Iteration 4177, Loss: 2.2716026306152344\n",
            "Training Iteration 4178, Loss: 3.8663225173950195\n",
            "Training Iteration 4179, Loss: 2.5250720977783203\n",
            "Training Iteration 4180, Loss: 3.594755172729492\n",
            "Training Iteration 4181, Loss: 2.609722137451172\n",
            "Training Iteration 4182, Loss: 5.911076068878174\n",
            "Training Iteration 4183, Loss: 4.143275260925293\n",
            "Training Iteration 4184, Loss: 4.856465816497803\n",
            "Training Iteration 4185, Loss: 2.4121174812316895\n",
            "Training Iteration 4186, Loss: 3.3843894004821777\n",
            "Training Iteration 4187, Loss: 5.7382683753967285\n",
            "Training Iteration 4188, Loss: 7.952332496643066\n",
            "Training Iteration 4189, Loss: 3.7910380363464355\n",
            "Training Iteration 4190, Loss: 1.9803091287612915\n",
            "Training Iteration 4191, Loss: 6.451051235198975\n",
            "Training Iteration 4192, Loss: 3.4466183185577393\n",
            "Training Iteration 4193, Loss: 3.955217123031616\n",
            "Training Iteration 4194, Loss: 3.9222731590270996\n",
            "Training Iteration 4195, Loss: 2.783716917037964\n",
            "Training Iteration 4196, Loss: 4.287554740905762\n",
            "Training Iteration 4197, Loss: 5.743319988250732\n",
            "Training Iteration 4198, Loss: 6.223176002502441\n",
            "Training Iteration 4199, Loss: 7.138305187225342\n",
            "Training Iteration 4200, Loss: 3.3721086978912354\n",
            "Training Iteration 4201, Loss: 3.589911460876465\n",
            "Training Iteration 4202, Loss: 3.4858806133270264\n",
            "Training Iteration 4203, Loss: 6.152907848358154\n",
            "Training Iteration 4204, Loss: 3.507394790649414\n",
            "Training Iteration 4205, Loss: 3.282982587814331\n",
            "Training Iteration 4206, Loss: 3.6360480785369873\n",
            "Training Iteration 4207, Loss: 6.29241418838501\n",
            "Training Iteration 4208, Loss: 3.591736078262329\n",
            "Training Iteration 4209, Loss: 2.905832529067993\n",
            "Training Iteration 4210, Loss: 2.5325887203216553\n",
            "Training Iteration 4211, Loss: 3.872692584991455\n",
            "Training Iteration 4212, Loss: 4.0092692375183105\n",
            "Training Iteration 4213, Loss: 4.740756988525391\n",
            "Training Iteration 4214, Loss: 4.990960597991943\n",
            "Training Iteration 4215, Loss: 1.7965610027313232\n",
            "Training Iteration 4216, Loss: 3.180241584777832\n",
            "Training Iteration 4217, Loss: 6.054845333099365\n",
            "Training Iteration 4218, Loss: 5.668766498565674\n",
            "Training Iteration 4219, Loss: 5.210975646972656\n",
            "Training Iteration 4220, Loss: 5.049945831298828\n",
            "Training Iteration 4221, Loss: 3.701420307159424\n",
            "Training Iteration 4222, Loss: 5.615268230438232\n",
            "Training Iteration 4223, Loss: 4.109561920166016\n",
            "Training Iteration 4224, Loss: 4.219381332397461\n",
            "Training Iteration 4225, Loss: 2.5756676197052\n",
            "Training Iteration 4226, Loss: 4.366080284118652\n",
            "Training Iteration 4227, Loss: 4.625114440917969\n",
            "Training Iteration 4228, Loss: 2.8815107345581055\n",
            "Training Iteration 4229, Loss: 1.9795172214508057\n",
            "Training Iteration 4230, Loss: 4.108379364013672\n",
            "Training Iteration 4231, Loss: 4.097977161407471\n",
            "Training Iteration 4232, Loss: 2.9591643810272217\n",
            "Training Iteration 4233, Loss: 2.0186305046081543\n",
            "Training Iteration 4234, Loss: 3.553642988204956\n",
            "Training Iteration 4235, Loss: 4.378864288330078\n",
            "Training Iteration 4236, Loss: 3.7775204181671143\n",
            "Training Iteration 4237, Loss: 4.48146390914917\n",
            "Training Iteration 4238, Loss: 4.375356197357178\n",
            "Training Iteration 4239, Loss: 4.946249485015869\n",
            "Training Iteration 4240, Loss: 3.5551137924194336\n",
            "Training Iteration 4241, Loss: 3.4345948696136475\n",
            "Training Iteration 4242, Loss: 3.3652639389038086\n",
            "Training Iteration 4243, Loss: 3.021350383758545\n",
            "Training Iteration 4244, Loss: 3.003084421157837\n",
            "Training Iteration 4245, Loss: 2.2205958366394043\n",
            "Training Iteration 4246, Loss: 2.6519885063171387\n",
            "Training Iteration 4247, Loss: 3.437471389770508\n",
            "Training Iteration 4248, Loss: 5.382900238037109\n",
            "Training Iteration 4249, Loss: 3.267192840576172\n",
            "Training Iteration 4250, Loss: 4.465882301330566\n",
            "Training Iteration 4251, Loss: 3.1809678077697754\n",
            "Training Iteration 4252, Loss: 4.464919090270996\n",
            "Training Iteration 4253, Loss: 1.7282177209854126\n",
            "Training Iteration 4254, Loss: 5.615975379943848\n",
            "Training Iteration 4255, Loss: 3.421238422393799\n",
            "Training Iteration 4256, Loss: 4.947575569152832\n",
            "Training Iteration 4257, Loss: 2.049488067626953\n",
            "Training Iteration 4258, Loss: 4.2390947341918945\n",
            "Training Iteration 4259, Loss: 4.373082160949707\n",
            "Training Iteration 4260, Loss: 7.217879772186279\n",
            "Training Iteration 4261, Loss: 4.1233601570129395\n",
            "Training Iteration 4262, Loss: 2.702988624572754\n",
            "Training Iteration 4263, Loss: 5.151689529418945\n",
            "Training Iteration 4264, Loss: 5.079328536987305\n",
            "Training Iteration 4265, Loss: 5.892993927001953\n",
            "Training Iteration 4266, Loss: 3.037501096725464\n",
            "Training Iteration 4267, Loss: 4.416043281555176\n",
            "Training Iteration 4268, Loss: 2.7137274742126465\n",
            "Training Iteration 4269, Loss: 2.934086561203003\n",
            "Training Iteration 4270, Loss: 5.601585865020752\n",
            "Training Iteration 4271, Loss: 5.513444423675537\n",
            "Training Iteration 4272, Loss: 4.49799919128418\n",
            "Training Iteration 4273, Loss: 4.122024059295654\n",
            "Training Iteration 4274, Loss: 2.809098243713379\n",
            "Training Iteration 4275, Loss: 6.104063987731934\n",
            "Training Iteration 4276, Loss: 1.9492615461349487\n",
            "Training Iteration 4277, Loss: 8.831825256347656\n",
            "Training Iteration 4278, Loss: 7.748282432556152\n",
            "Training Iteration 4279, Loss: 2.67811918258667\n",
            "Training Iteration 4280, Loss: 3.6418368816375732\n",
            "Training Iteration 4281, Loss: 5.4012451171875\n",
            "Training Iteration 4282, Loss: 4.442868709564209\n",
            "Training Iteration 4283, Loss: 4.555946350097656\n",
            "Training Iteration 4284, Loss: 6.268031597137451\n",
            "Training Iteration 4285, Loss: 5.751175880432129\n",
            "Training Iteration 4286, Loss: 3.6076619625091553\n",
            "Training Iteration 4287, Loss: 3.4236903190612793\n",
            "Training Iteration 4288, Loss: 3.6272876262664795\n",
            "Training Iteration 4289, Loss: 5.251846790313721\n",
            "Training Iteration 4290, Loss: 5.159254550933838\n",
            "Training Iteration 4291, Loss: 3.5922296047210693\n",
            "Training Iteration 4292, Loss: 4.40891170501709\n",
            "Training Iteration 4293, Loss: 3.771852493286133\n",
            "Training Iteration 4294, Loss: 4.406825542449951\n",
            "Training Iteration 4295, Loss: 5.991175174713135\n",
            "Training Iteration 4296, Loss: 6.946082592010498\n",
            "Training Iteration 4297, Loss: 6.46301794052124\n",
            "Training Iteration 4298, Loss: 4.835122108459473\n",
            "Training Iteration 4299, Loss: 4.1201558113098145\n",
            "Training Iteration 4300, Loss: 5.944109916687012\n",
            "Training Iteration 4301, Loss: 4.8698577880859375\n",
            "Training Iteration 4302, Loss: 5.520810127258301\n",
            "Training Iteration 4303, Loss: 3.654191017150879\n",
            "Training Iteration 4304, Loss: 7.238306522369385\n",
            "Training Iteration 4305, Loss: 6.124757289886475\n",
            "Training Iteration 4306, Loss: 6.1549391746521\n",
            "Training Iteration 4307, Loss: 2.3911564350128174\n",
            "Training Iteration 4308, Loss: 10.62730598449707\n",
            "Training Iteration 4309, Loss: 5.208376407623291\n",
            "Training Iteration 4310, Loss: 5.355400085449219\n",
            "Training Iteration 4311, Loss: 3.3077006340026855\n",
            "Training Iteration 4312, Loss: 10.360724449157715\n",
            "Training Iteration 4313, Loss: 4.775908470153809\n",
            "Training Iteration 4314, Loss: 6.939221382141113\n",
            "Training Iteration 4315, Loss: 4.370704174041748\n",
            "Training Iteration 4316, Loss: 2.8726158142089844\n",
            "Training Iteration 4317, Loss: 6.888199806213379\n",
            "Training Iteration 4318, Loss: 6.043791770935059\n",
            "Training Iteration 4319, Loss: 3.7622766494750977\n",
            "Training Iteration 4320, Loss: 3.8735358715057373\n",
            "Training Iteration 4321, Loss: 8.197334289550781\n",
            "Training Iteration 4322, Loss: 6.248005390167236\n",
            "Training Iteration 4323, Loss: 4.617696285247803\n",
            "Training Iteration 4324, Loss: 4.335458755493164\n",
            "Training Iteration 4325, Loss: 4.319014072418213\n",
            "Training Iteration 4326, Loss: 6.795770645141602\n",
            "Training Iteration 4327, Loss: 2.535498857498169\n",
            "Training Iteration 4328, Loss: 5.528561592102051\n",
            "Training Iteration 4329, Loss: 4.273916721343994\n",
            "Training Iteration 4330, Loss: 5.523332595825195\n",
            "Training Iteration 4331, Loss: 6.73397970199585\n",
            "Training Iteration 4332, Loss: 6.373808860778809\n",
            "Training Iteration 4333, Loss: 0.7820141911506653\n",
            "Training Iteration 4334, Loss: 4.25302791595459\n",
            "Training Iteration 4335, Loss: 8.029500961303711\n",
            "Training Iteration 4336, Loss: 6.991670608520508\n",
            "Training Iteration 4337, Loss: 5.017789840698242\n",
            "Training Iteration 4338, Loss: 2.6358776092529297\n",
            "Training Iteration 4339, Loss: 2.610413074493408\n",
            "Training Iteration 4340, Loss: 5.2266621589660645\n",
            "Training Iteration 4341, Loss: 6.452330589294434\n",
            "Training Iteration 4342, Loss: 5.162234306335449\n",
            "Training Iteration 4343, Loss: 5.745388984680176\n",
            "Training Iteration 4344, Loss: 4.907477378845215\n",
            "Training Iteration 4345, Loss: 3.559032440185547\n",
            "Training Iteration 4346, Loss: 3.72029709815979\n",
            "Training Iteration 4347, Loss: 2.4770166873931885\n",
            "Training Iteration 4348, Loss: 2.2804741859436035\n",
            "Training Iteration 4349, Loss: 5.969313144683838\n",
            "Training Iteration 4350, Loss: 7.354857444763184\n",
            "Training Iteration 4351, Loss: 4.691508769989014\n",
            "Training Iteration 4352, Loss: 4.567553520202637\n",
            "Training Iteration 4353, Loss: 5.136480808258057\n",
            "Training Iteration 4354, Loss: 3.969466209411621\n",
            "Training Iteration 4355, Loss: 5.3159356117248535\n",
            "Training Iteration 4356, Loss: 5.958221435546875\n",
            "Training Iteration 4357, Loss: 4.3443145751953125\n",
            "Training Iteration 4358, Loss: 3.982126235961914\n",
            "Training Iteration 4359, Loss: 1.2458752393722534\n",
            "Training Iteration 4360, Loss: 7.326091766357422\n",
            "Training Iteration 4361, Loss: 9.309365272521973\n",
            "Training Iteration 4362, Loss: 3.998060464859009\n",
            "Training Iteration 4363, Loss: 6.238871097564697\n",
            "Training Iteration 4364, Loss: 1.3342289924621582\n",
            "Training Iteration 4365, Loss: 6.106420040130615\n",
            "Training Iteration 4366, Loss: 6.192815780639648\n",
            "Training Iteration 4367, Loss: 8.303659439086914\n",
            "Training Iteration 4368, Loss: 3.7536239624023438\n",
            "Training Iteration 4369, Loss: 4.008685111999512\n",
            "Training Iteration 4370, Loss: 6.470944881439209\n",
            "Training Iteration 4371, Loss: 3.7155637741088867\n",
            "Training Iteration 4372, Loss: 7.097024440765381\n",
            "Training Iteration 4373, Loss: 8.165066719055176\n",
            "Training Iteration 4374, Loss: 6.677911758422852\n",
            "Training Iteration 4375, Loss: 3.419018507003784\n",
            "Training Iteration 4376, Loss: 3.4339544773101807\n",
            "Training Iteration 4377, Loss: 4.9526262283325195\n",
            "Training Iteration 4378, Loss: 5.392341613769531\n",
            "Training Iteration 4379, Loss: 2.453345775604248\n",
            "Training Iteration 4380, Loss: 5.385679244995117\n",
            "Training Iteration 4381, Loss: 9.613722801208496\n",
            "Training Iteration 4382, Loss: 6.965744972229004\n",
            "Training Iteration 4383, Loss: 4.104193687438965\n",
            "Training Iteration 4384, Loss: 4.972587585449219\n",
            "Training Iteration 4385, Loss: 3.4686272144317627\n",
            "Training Iteration 4386, Loss: 6.572164535522461\n",
            "Training Iteration 4387, Loss: 3.3704655170440674\n",
            "Training Iteration 4388, Loss: 4.110815525054932\n",
            "Training Iteration 4389, Loss: 6.203304290771484\n",
            "Training Iteration 4390, Loss: 8.361215591430664\n",
            "Training Iteration 4391, Loss: 8.28227424621582\n",
            "Training Iteration 4392, Loss: 2.1322741508483887\n",
            "Training Iteration 4393, Loss: 3.6645569801330566\n",
            "Training Iteration 4394, Loss: 5.772524833679199\n",
            "Training Iteration 4395, Loss: 5.666357517242432\n",
            "Training Iteration 4396, Loss: 5.530984401702881\n",
            "Training Iteration 4397, Loss: 3.69817852973938\n",
            "Training Iteration 4398, Loss: 10.199655532836914\n",
            "Training Iteration 4399, Loss: 6.076765537261963\n",
            "Training Iteration 4400, Loss: 3.226419687271118\n",
            "Training Iteration 4401, Loss: 6.950615406036377\n",
            "Training Iteration 4402, Loss: 2.231156587600708\n",
            "Training Iteration 4403, Loss: 6.145927906036377\n",
            "Training Iteration 4404, Loss: 4.40516996383667\n",
            "Training Iteration 4405, Loss: 5.421268463134766\n",
            "Training Iteration 4406, Loss: 2.9634108543395996\n",
            "Training Iteration 4407, Loss: 3.2664453983306885\n",
            "Training Iteration 4408, Loss: 5.550109386444092\n",
            "Training Iteration 4409, Loss: 4.140038967132568\n",
            "Training Iteration 4410, Loss: 7.838684558868408\n",
            "Training Iteration 4411, Loss: 7.530871868133545\n",
            "Training Iteration 4412, Loss: 1.9112244844436646\n",
            "Training Iteration 4413, Loss: 2.8854291439056396\n",
            "Training Iteration 4414, Loss: 1.1376402378082275\n",
            "Training Iteration 4415, Loss: 2.053969383239746\n",
            "Training Iteration 4416, Loss: 6.856124401092529\n",
            "Training Iteration 4417, Loss: 3.9099910259246826\n",
            "Training Iteration 4418, Loss: 4.056513786315918\n",
            "Training Iteration 4419, Loss: 4.983563423156738\n",
            "Training Iteration 4420, Loss: 2.828744649887085\n",
            "Training Iteration 4421, Loss: 5.577785968780518\n",
            "Training Iteration 4422, Loss: 5.784375190734863\n",
            "Training Iteration 4423, Loss: 6.264335632324219\n",
            "Training Iteration 4424, Loss: 1.9018747806549072\n",
            "Training Iteration 4425, Loss: 3.033719062805176\n",
            "Training Iteration 4426, Loss: 4.130550384521484\n",
            "Training Iteration 4427, Loss: 3.0251710414886475\n",
            "Training Iteration 4428, Loss: 2.3286571502685547\n",
            "Training Iteration 4429, Loss: 5.332986831665039\n",
            "Training Iteration 4430, Loss: 6.776447772979736\n",
            "Training Iteration 4431, Loss: 2.9859371185302734\n",
            "Training Iteration 4432, Loss: 2.4406838417053223\n",
            "Training Iteration 4433, Loss: 2.998687505722046\n",
            "Training Iteration 4434, Loss: 6.853548049926758\n",
            "Training Iteration 4435, Loss: 7.056338310241699\n",
            "Training Iteration 4436, Loss: 8.337728500366211\n",
            "Training Iteration 4437, Loss: 2.568868637084961\n",
            "Training Iteration 4438, Loss: 3.8553614616394043\n",
            "Training Iteration 4439, Loss: 3.7290549278259277\n",
            "Training Iteration 4440, Loss: 3.7989096641540527\n",
            "Training Iteration 4441, Loss: 3.8675947189331055\n",
            "Training Iteration 4442, Loss: 2.5410306453704834\n",
            "Training Iteration 4443, Loss: 2.8448681831359863\n",
            "Training Iteration 4444, Loss: 6.078180313110352\n",
            "Training Iteration 4445, Loss: 2.903684616088867\n",
            "Training Iteration 4446, Loss: 4.6786088943481445\n",
            "Training Iteration 4447, Loss: 2.3759710788726807\n",
            "Training Iteration 4448, Loss: 3.731623649597168\n",
            "Training Iteration 4449, Loss: 2.585160732269287\n",
            "Training Iteration 4450, Loss: 5.2344651222229\n",
            "Training Iteration 4451, Loss: 3.3784821033477783\n",
            "Training Iteration 4452, Loss: 4.930192470550537\n",
            "Training Iteration 4453, Loss: 4.864181041717529\n",
            "Training Iteration 4454, Loss: 4.346374034881592\n",
            "Training Iteration 4455, Loss: 2.8698954582214355\n",
            "Training Iteration 4456, Loss: 6.192399978637695\n",
            "Training Iteration 4457, Loss: 3.9776408672332764\n",
            "Training Iteration 4458, Loss: 7.211822986602783\n",
            "Training Iteration 4459, Loss: 4.200623035430908\n",
            "Training Iteration 4460, Loss: 4.730223655700684\n",
            "Training Iteration 4461, Loss: 4.3760199546813965\n",
            "Training Iteration 4462, Loss: 5.976012229919434\n",
            "Training Iteration 4463, Loss: 5.669535160064697\n",
            "Training Iteration 4464, Loss: 3.0009822845458984\n",
            "Training Iteration 4465, Loss: 2.891963005065918\n",
            "Training Iteration 4466, Loss: 7.7068963050842285\n",
            "Training Iteration 4467, Loss: 5.561454772949219\n",
            "Training Iteration 4468, Loss: 4.739223003387451\n",
            "Training Iteration 4469, Loss: 3.9421753883361816\n",
            "Training Iteration 4470, Loss: 3.0460386276245117\n",
            "Training Iteration 4471, Loss: 3.1708106994628906\n",
            "Training Iteration 4472, Loss: 5.919678688049316\n",
            "Training Iteration 4473, Loss: 5.758656024932861\n",
            "Training Iteration 4474, Loss: 4.442455768585205\n",
            "Training Iteration 4475, Loss: 4.6265339851379395\n",
            "Training Iteration 4476, Loss: 5.2855939865112305\n",
            "Training Iteration 4477, Loss: 3.932223081588745\n",
            "Training Iteration 4478, Loss: 3.45788836479187\n",
            "Training Iteration 4479, Loss: 2.3977091312408447\n",
            "Training Iteration 4480, Loss: 5.01632022857666\n",
            "Training Iteration 4481, Loss: 2.894474506378174\n",
            "Training Iteration 4482, Loss: 10.855635643005371\n",
            "Training Iteration 4483, Loss: 7.839351177215576\n",
            "Training Iteration 4484, Loss: 4.684842109680176\n",
            "Training Iteration 4485, Loss: 2.7621054649353027\n",
            "Training Iteration 4486, Loss: 4.361533164978027\n",
            "Training Iteration 4487, Loss: 3.5056560039520264\n",
            "Training Iteration 4488, Loss: 3.168076515197754\n",
            "Training Iteration 4489, Loss: 3.346372604370117\n",
            "Training Iteration 4490, Loss: 1.9110782146453857\n",
            "Training Iteration 4491, Loss: 4.059361934661865\n",
            "Training Iteration 4492, Loss: 4.478952884674072\n",
            "Training Iteration 4493, Loss: 3.1237423419952393\n",
            "Training Iteration 4494, Loss: 3.2219786643981934\n",
            "Training Iteration 4495, Loss: 5.791618824005127\n",
            "Training Iteration 4496, Loss: 3.7980825901031494\n",
            "Training Iteration 4497, Loss: 6.974177837371826\n",
            "Training Iteration 4498, Loss: 3.747553825378418\n",
            "Training Iteration 4499, Loss: 4.3255295753479\n",
            "Training Iteration 4500, Loss: 5.570613861083984\n",
            "Training Iteration 4501, Loss: 2.4741454124450684\n",
            "Training Iteration 4502, Loss: 3.3235275745391846\n",
            "Training Iteration 4503, Loss: 5.315795421600342\n",
            "Training Iteration 4504, Loss: 6.963445663452148\n",
            "Training Iteration 4505, Loss: 4.952243804931641\n",
            "Training Iteration 4506, Loss: 3.610414505004883\n",
            "Training Iteration 4507, Loss: 10.957975387573242\n",
            "Training Iteration 4508, Loss: 7.358047962188721\n",
            "Training Iteration 4509, Loss: 4.254147052764893\n",
            "Training Iteration 4510, Loss: 1.0739731788635254\n",
            "Training Iteration 4511, Loss: 4.313783168792725\n",
            "Training Iteration 4512, Loss: 6.408402919769287\n",
            "Training Iteration 4513, Loss: 4.780637741088867\n",
            "Training Iteration 4514, Loss: 4.1387481689453125\n",
            "Training Iteration 4515, Loss: 5.995488166809082\n",
            "Training Iteration 4516, Loss: 3.9684441089630127\n",
            "Training Iteration 4517, Loss: 2.3634326457977295\n",
            "Training Iteration 4518, Loss: 4.291031360626221\n",
            "Training Iteration 4519, Loss: 4.838287830352783\n",
            "Training Iteration 4520, Loss: 6.659886360168457\n",
            "Training Iteration 4521, Loss: 6.322646617889404\n",
            "Training Iteration 4522, Loss: 3.8656177520751953\n",
            "Training Iteration 4523, Loss: 4.9663801193237305\n",
            "Training Iteration 4524, Loss: 6.643031120300293\n",
            "Training Iteration 4525, Loss: 5.207679271697998\n",
            "Training Iteration 4526, Loss: 6.964666843414307\n",
            "Training Iteration 4527, Loss: 2.3065292835235596\n",
            "Training Iteration 4528, Loss: 2.470956802368164\n",
            "Training Iteration 4529, Loss: 4.062203884124756\n",
            "Training Iteration 4530, Loss: 5.3045220375061035\n",
            "Training Iteration 4531, Loss: 1.6441212892532349\n",
            "Training Iteration 4532, Loss: 4.132023811340332\n",
            "Training Iteration 4533, Loss: 5.638747692108154\n",
            "Training Iteration 4534, Loss: 4.0478949546813965\n",
            "Training Iteration 4535, Loss: 3.6323657035827637\n",
            "Training Iteration 4536, Loss: 7.006445407867432\n",
            "Training Iteration 4537, Loss: 7.816333293914795\n",
            "Training Iteration 4538, Loss: 4.628729820251465\n",
            "Training Iteration 4539, Loss: 3.0767369270324707\n",
            "Training Iteration 4540, Loss: 2.663954973220825\n",
            "Training Iteration 4541, Loss: 7.038599967956543\n",
            "Training Iteration 4542, Loss: 4.556309700012207\n",
            "Training Iteration 4543, Loss: 2.861990213394165\n",
            "Training Iteration 4544, Loss: 5.562647342681885\n",
            "Training Iteration 4545, Loss: 3.216266632080078\n",
            "Training Iteration 4546, Loss: 4.423876762390137\n",
            "Training Iteration 4547, Loss: 3.5520248413085938\n",
            "Training Iteration 4548, Loss: 3.8591763973236084\n",
            "Training Iteration 4549, Loss: 2.029306650161743\n",
            "Training Iteration 4550, Loss: 4.32786226272583\n",
            "Training Iteration 4551, Loss: 4.0019683837890625\n",
            "Training Iteration 4552, Loss: 6.6248321533203125\n",
            "Training Iteration 4553, Loss: 3.7243218421936035\n",
            "Training Iteration 4554, Loss: 5.7101263999938965\n",
            "Training Iteration 4555, Loss: 4.625133514404297\n",
            "Training Iteration 4556, Loss: 4.258787631988525\n",
            "Training Iteration 4557, Loss: 5.234381675720215\n",
            "Training Iteration 4558, Loss: 3.543220043182373\n",
            "Training Iteration 4559, Loss: 4.637472629547119\n",
            "Training Iteration 4560, Loss: 3.9669113159179688\n",
            "Training Iteration 4561, Loss: 2.8670265674591064\n",
            "Training Iteration 4562, Loss: 6.800838947296143\n",
            "Training Iteration 4563, Loss: 3.8303816318511963\n",
            "Training Iteration 4564, Loss: 5.741411209106445\n",
            "Training Iteration 4565, Loss: 5.66956901550293\n",
            "Training Iteration 4566, Loss: 5.457284927368164\n",
            "Training Iteration 4567, Loss: 9.982956886291504\n",
            "Training Iteration 4568, Loss: 2.5853841304779053\n",
            "Training Iteration 4569, Loss: 8.228462219238281\n",
            "Training Iteration 4570, Loss: 8.063706398010254\n",
            "Training Iteration 4571, Loss: 3.108207941055298\n",
            "Training Iteration 4572, Loss: 3.5360682010650635\n",
            "Training Iteration 4573, Loss: 4.257811069488525\n",
            "Training Iteration 4574, Loss: 3.2116007804870605\n",
            "Training Iteration 4575, Loss: 3.375734806060791\n",
            "Training Iteration 4576, Loss: 4.55672550201416\n",
            "Training Iteration 4577, Loss: 7.4111008644104\n",
            "Training Iteration 4578, Loss: 5.165968418121338\n",
            "Training Iteration 4579, Loss: 5.217410087585449\n",
            "Training Iteration 4580, Loss: 6.177708625793457\n",
            "Training Iteration 4581, Loss: 2.6633522510528564\n",
            "Training Iteration 4582, Loss: 2.4495060443878174\n",
            "Training Iteration 4583, Loss: 3.297301769256592\n",
            "Training Iteration 4584, Loss: 5.449982166290283\n",
            "Training Iteration 4585, Loss: 3.7089099884033203\n",
            "Training Iteration 4586, Loss: 2.833472490310669\n",
            "Training Iteration 4587, Loss: 4.91831636428833\n",
            "Training Iteration 4588, Loss: 0.9942656755447388\n",
            "Training Iteration 4589, Loss: 7.450624465942383\n",
            "Training Iteration 4590, Loss: 5.646965980529785\n",
            "Training Iteration 4591, Loss: 3.3719711303710938\n",
            "Training Iteration 4592, Loss: 3.647928476333618\n",
            "Training Iteration 4593, Loss: 4.104426383972168\n",
            "Training Iteration 4594, Loss: 3.0334150791168213\n",
            "Training Iteration 4595, Loss: 5.688403606414795\n",
            "Training Iteration 4596, Loss: 2.5384466648101807\n",
            "Training Iteration 4597, Loss: 4.348258018493652\n",
            "Training Iteration 4598, Loss: 3.681281089782715\n",
            "Training Iteration 4599, Loss: 2.4524121284484863\n",
            "Training Iteration 4600, Loss: 3.376526355743408\n",
            "Training Iteration 4601, Loss: 6.375699996948242\n",
            "Training Iteration 4602, Loss: 4.146243095397949\n",
            "Training Iteration 4603, Loss: 2.328441858291626\n",
            "Training Iteration 4604, Loss: 2.05318021774292\n",
            "Training Iteration 4605, Loss: 5.232307434082031\n",
            "Training Iteration 4606, Loss: 3.436235189437866\n",
            "Training Iteration 4607, Loss: 4.994412899017334\n",
            "Training Iteration 4608, Loss: 7.5782036781311035\n",
            "Training Iteration 4609, Loss: 2.584233283996582\n",
            "Training Iteration 4610, Loss: 0.7609952688217163\n",
            "Training Iteration 4611, Loss: 2.2375521659851074\n",
            "Training Iteration 4612, Loss: 5.469145774841309\n",
            "Training Iteration 4613, Loss: 4.054805278778076\n",
            "Training Iteration 4614, Loss: 3.0386056900024414\n",
            "Training Iteration 4615, Loss: 2.8928911685943604\n",
            "Training Iteration 4616, Loss: 3.0878729820251465\n",
            "Training Iteration 4617, Loss: 4.598282337188721\n",
            "Training Iteration 4618, Loss: 3.0968515872955322\n",
            "Training Iteration 4619, Loss: 3.50744366645813\n",
            "Training Iteration 4620, Loss: 6.47747802734375\n",
            "Training Iteration 4621, Loss: 4.983643531799316\n",
            "Training Iteration 4622, Loss: 3.4359631538391113\n",
            "Training Iteration 4623, Loss: 4.889007568359375\n",
            "Training Iteration 4624, Loss: 6.536147117614746\n",
            "Training Iteration 4625, Loss: 7.583873271942139\n",
            "Training Iteration 4626, Loss: 4.982395172119141\n",
            "Training Iteration 4627, Loss: 5.334277153015137\n",
            "Training Iteration 4628, Loss: 2.398196220397949\n",
            "Training Iteration 4629, Loss: 5.75086784362793\n",
            "Training Iteration 4630, Loss: 3.8117246627807617\n",
            "Training Iteration 4631, Loss: 8.998429298400879\n",
            "Training Iteration 4632, Loss: 4.058798313140869\n",
            "Training Iteration 4633, Loss: 5.013556003570557\n",
            "Training Iteration 4634, Loss: 2.1698455810546875\n",
            "Training Iteration 4635, Loss: 6.033511161804199\n",
            "Training Iteration 4636, Loss: 4.713364601135254\n",
            "Training Iteration 4637, Loss: 3.476874351501465\n",
            "Training Iteration 4638, Loss: 2.7535009384155273\n",
            "Training Iteration 4639, Loss: 5.472365856170654\n",
            "Training Iteration 4640, Loss: 2.391946792602539\n",
            "Training Iteration 4641, Loss: 4.958613872528076\n",
            "Training Iteration 4642, Loss: 3.2757651805877686\n",
            "Training Iteration 4643, Loss: 2.470937728881836\n",
            "Training Iteration 4644, Loss: 4.581614017486572\n",
            "Training Iteration 4645, Loss: 8.568516731262207\n",
            "Training Iteration 4646, Loss: 7.811309814453125\n",
            "Training Iteration 4647, Loss: 10.02277946472168\n",
            "Training Iteration 4648, Loss: 3.5953729152679443\n",
            "Training Iteration 4649, Loss: 5.203110218048096\n",
            "Training Iteration 4650, Loss: 8.717775344848633\n",
            "Training Iteration 4651, Loss: 7.399259567260742\n",
            "Training Iteration 4652, Loss: 7.3383049964904785\n",
            "Training Iteration 4653, Loss: 4.378793239593506\n",
            "Training Iteration 4654, Loss: 3.382138729095459\n",
            "Training Iteration 4655, Loss: 4.380210876464844\n",
            "Training Iteration 4656, Loss: 4.112522602081299\n",
            "Training Iteration 4657, Loss: 4.736539840698242\n",
            "Training Iteration 4658, Loss: 4.155767917633057\n",
            "Training Iteration 4659, Loss: 10.593514442443848\n",
            "Training Iteration 4660, Loss: 4.3778767585754395\n",
            "Training Iteration 4661, Loss: 6.08817195892334\n",
            "Training Iteration 4662, Loss: 3.5497066974639893\n",
            "Training Iteration 4663, Loss: 7.336239337921143\n",
            "Training Iteration 4664, Loss: 4.288083553314209\n",
            "Training Iteration 4665, Loss: 3.3223371505737305\n",
            "Training Iteration 4666, Loss: 5.272019386291504\n",
            "Training Iteration 4667, Loss: 4.352258205413818\n",
            "Training Iteration 4668, Loss: 3.8251161575317383\n",
            "Training Iteration 4669, Loss: 4.632882595062256\n",
            "Training Iteration 4670, Loss: 6.4237213134765625\n",
            "Training Iteration 4671, Loss: 5.814204692840576\n",
            "Training Iteration 4672, Loss: 7.454512596130371\n",
            "Training Iteration 4673, Loss: 7.985177516937256\n",
            "Training Iteration 4674, Loss: 5.557417869567871\n",
            "Training Iteration 4675, Loss: 3.301602363586426\n",
            "Training Iteration 4676, Loss: 5.0678887367248535\n",
            "Training Iteration 4677, Loss: 2.6474320888519287\n",
            "Training Iteration 4678, Loss: 4.219860553741455\n",
            "Training Iteration 4679, Loss: 3.5952162742614746\n",
            "Training Iteration 4680, Loss: 4.705826759338379\n",
            "Training Iteration 4681, Loss: 5.745151042938232\n",
            "Training Iteration 4682, Loss: 7.07912015914917\n",
            "Training Iteration 4683, Loss: 3.137025833129883\n",
            "Training Iteration 4684, Loss: 3.49013614654541\n",
            "Training Iteration 4685, Loss: 1.884516954421997\n",
            "Training Iteration 4686, Loss: 2.5925981998443604\n",
            "Training Iteration 4687, Loss: 8.091771125793457\n",
            "Training Iteration 4688, Loss: 4.37371826171875\n",
            "Training Iteration 4689, Loss: 2.7950148582458496\n",
            "Training Iteration 4690, Loss: 6.712193489074707\n",
            "Training Iteration 4691, Loss: 5.378071308135986\n",
            "Training Iteration 4692, Loss: 5.600644588470459\n",
            "Training Iteration 4693, Loss: 4.891129016876221\n",
            "Training Iteration 4694, Loss: 3.3672304153442383\n",
            "Training Iteration 4695, Loss: 3.683548927307129\n",
            "Training Iteration 4696, Loss: 5.2746992111206055\n",
            "Training Iteration 4697, Loss: 5.611511707305908\n",
            "Training Iteration 4698, Loss: 3.2382729053497314\n",
            "Training Iteration 4699, Loss: 3.5114853382110596\n",
            "Training Iteration 4700, Loss: 4.7640581130981445\n",
            "Training Iteration 4701, Loss: 2.195941686630249\n",
            "Training Iteration 4702, Loss: 3.826575517654419\n",
            "Training Iteration 4703, Loss: 2.037184715270996\n",
            "Training Iteration 4704, Loss: 4.401700496673584\n",
            "Training Iteration 4705, Loss: 4.879807949066162\n",
            "Training Iteration 4706, Loss: 4.03179931640625\n",
            "Training Iteration 4707, Loss: 8.68612289428711\n",
            "Training Iteration 4708, Loss: 3.9729199409484863\n",
            "Training Iteration 4709, Loss: 3.989821434020996\n",
            "Training Iteration 4710, Loss: 3.8851377964019775\n",
            "Training Iteration 4711, Loss: 6.314349174499512\n",
            "Training Iteration 4712, Loss: 7.6472907066345215\n",
            "Training Iteration 4713, Loss: 4.653149127960205\n",
            "Training Iteration 4714, Loss: 3.6555333137512207\n",
            "Training Iteration 4715, Loss: 2.5735833644866943\n",
            "Training Iteration 4716, Loss: 2.8242034912109375\n",
            "Training Iteration 4717, Loss: 7.547943592071533\n",
            "Training Iteration 4718, Loss: 2.416566848754883\n",
            "Training Iteration 4719, Loss: 2.6057629585266113\n",
            "Training Iteration 4720, Loss: 6.754554748535156\n",
            "Training Iteration 4721, Loss: 4.084172248840332\n",
            "Training Iteration 4722, Loss: 4.217146873474121\n",
            "Training Iteration 4723, Loss: 8.178872108459473\n",
            "Training Iteration 4724, Loss: 5.4873480796813965\n",
            "Training Iteration 4725, Loss: 4.797112464904785\n",
            "Training Iteration 4726, Loss: 6.408933639526367\n",
            "Training Iteration 4727, Loss: 3.5915799140930176\n",
            "Training Iteration 4728, Loss: 3.6417531967163086\n",
            "Training Iteration 4729, Loss: 3.0983078479766846\n",
            "Training Iteration 4730, Loss: 3.163740634918213\n",
            "Training Iteration 4731, Loss: 4.917426109313965\n",
            "Training Iteration 4732, Loss: 1.8920637369155884\n",
            "Training Iteration 4733, Loss: 3.8077807426452637\n",
            "Training Iteration 4734, Loss: 4.819596767425537\n",
            "Training Iteration 4735, Loss: 3.540286064147949\n",
            "Training Iteration 4736, Loss: 4.6032209396362305\n",
            "Training Iteration 4737, Loss: 4.398255348205566\n",
            "Training Iteration 4738, Loss: 7.099292755126953\n",
            "Training Iteration 4739, Loss: 5.16253662109375\n",
            "Training Iteration 4740, Loss: 3.7748968601226807\n",
            "Training Iteration 4741, Loss: 4.741182804107666\n",
            "Training Iteration 4742, Loss: 5.3363728523254395\n",
            "Training Iteration 4743, Loss: 6.193345546722412\n",
            "Training Iteration 4744, Loss: 3.31685209274292\n",
            "Training Iteration 4745, Loss: 5.913775444030762\n",
            "Training Iteration 4746, Loss: 2.643843412399292\n",
            "Training Iteration 4747, Loss: 3.8239057064056396\n",
            "Training Iteration 4748, Loss: 5.003561019897461\n",
            "Training Iteration 4749, Loss: 8.821918487548828\n",
            "Training Iteration 4750, Loss: 5.655847549438477\n",
            "Training Iteration 4751, Loss: 5.477228164672852\n",
            "Training Iteration 4752, Loss: 4.8453369140625\n",
            "Training Iteration 4753, Loss: 6.087449550628662\n",
            "Training Iteration 4754, Loss: 5.501306056976318\n",
            "Training Iteration 4755, Loss: 7.233519077301025\n",
            "Training Iteration 4756, Loss: 5.580214500427246\n",
            "Training Iteration 4757, Loss: 4.561330318450928\n",
            "Training Iteration 4758, Loss: 5.261904716491699\n",
            "Training Iteration 4759, Loss: 8.229758262634277\n",
            "Training Iteration 4760, Loss: 5.632513999938965\n",
            "Training Iteration 4761, Loss: 6.162325382232666\n",
            "Training Iteration 4762, Loss: 3.8730175495147705\n",
            "Training Iteration 4763, Loss: 6.384535312652588\n",
            "Training Iteration 4764, Loss: 3.351083755493164\n",
            "Training Iteration 4765, Loss: 4.738344192504883\n",
            "Training Iteration 4766, Loss: 5.487343788146973\n",
            "Training Iteration 4767, Loss: 5.125970840454102\n",
            "Training Iteration 4768, Loss: 7.499022483825684\n",
            "Training Iteration 4769, Loss: 5.765501976013184\n",
            "Training Iteration 4770, Loss: 2.654782772064209\n",
            "Training Iteration 4771, Loss: 1.4983948469161987\n",
            "Training Iteration 4772, Loss: 3.3243203163146973\n",
            "Training Iteration 4773, Loss: 2.73816180229187\n",
            "Training Iteration 4774, Loss: 3.633103847503662\n",
            "Training Iteration 4775, Loss: 4.556294918060303\n",
            "Training Iteration 4776, Loss: 4.081951141357422\n",
            "Training Iteration 4777, Loss: 2.309093952178955\n",
            "Training Iteration 4778, Loss: 4.915037631988525\n",
            "Training Iteration 4779, Loss: 4.589679718017578\n",
            "Training Iteration 4780, Loss: 3.814872980117798\n",
            "Training Iteration 4781, Loss: 2.6759328842163086\n",
            "Training Iteration 4782, Loss: 1.9714558124542236\n",
            "Training Iteration 4783, Loss: 3.958369016647339\n",
            "Training Iteration 4784, Loss: 4.8470001220703125\n",
            "Training Iteration 4785, Loss: 3.48937726020813\n",
            "Training Iteration 4786, Loss: 4.519352436065674\n",
            "Training Iteration 4787, Loss: 3.608154296875\n",
            "Training Iteration 4788, Loss: 3.8530569076538086\n",
            "Training Iteration 4789, Loss: 6.215703964233398\n",
            "Training Iteration 4790, Loss: 6.535373687744141\n",
            "Training Iteration 4791, Loss: 4.180909156799316\n",
            "Training Iteration 4792, Loss: 4.104345321655273\n",
            "Training Iteration 4793, Loss: 2.3219966888427734\n",
            "Training Iteration 4794, Loss: 4.861575126647949\n",
            "Training Iteration 4795, Loss: 4.9823384284973145\n",
            "Training Iteration 4796, Loss: 4.3037800788879395\n",
            "Training Iteration 4797, Loss: 3.8217616081237793\n",
            "Training Iteration 4798, Loss: 5.525152683258057\n",
            "Training Iteration 4799, Loss: 5.169629096984863\n",
            "Training Iteration 4800, Loss: 2.9109368324279785\n",
            "Training Iteration 4801, Loss: 1.7590198516845703\n",
            "Training Iteration 4802, Loss: 2.0412662029266357\n",
            "Training Iteration 4803, Loss: 4.7485671043396\n",
            "Training Iteration 4804, Loss: 3.5918068885803223\n",
            "Training Iteration 4805, Loss: 2.686826229095459\n",
            "Training Iteration 4806, Loss: 2.7128257751464844\n",
            "Training Iteration 4807, Loss: 3.4987192153930664\n",
            "Training Iteration 4808, Loss: 2.6064929962158203\n",
            "Training Iteration 4809, Loss: 2.9597907066345215\n",
            "Training Iteration 4810, Loss: 6.634034156799316\n",
            "Training Iteration 4811, Loss: 8.297466278076172\n",
            "Training Iteration 4812, Loss: 2.0183911323547363\n",
            "Training Iteration 4813, Loss: 3.739840507507324\n",
            "Training Iteration 4814, Loss: 2.466090679168701\n",
            "Training Iteration 4815, Loss: 4.208622932434082\n",
            "Training Iteration 4816, Loss: 5.764019012451172\n",
            "Training Iteration 4817, Loss: 4.262701034545898\n",
            "Training Iteration 4818, Loss: 1.3493905067443848\n",
            "Training Iteration 4819, Loss: 5.403573036193848\n",
            "Training Iteration 4820, Loss: 9.61987590789795\n",
            "Training Iteration 4821, Loss: 4.31753396987915\n",
            "Training Iteration 4822, Loss: 3.578172206878662\n",
            "Training Iteration 4823, Loss: 1.5490484237670898\n",
            "Training Iteration 4824, Loss: 4.301758289337158\n",
            "Training Iteration 4825, Loss: 4.152599811553955\n",
            "Training Iteration 4826, Loss: 4.555017471313477\n",
            "Training Iteration 4827, Loss: 4.351004123687744\n",
            "Training Iteration 4828, Loss: 3.698108434677124\n",
            "Training Iteration 4829, Loss: 3.7635202407836914\n",
            "Training Iteration 4830, Loss: 6.170182228088379\n",
            "Training Iteration 4831, Loss: 4.6467108726501465\n",
            "Training Iteration 4832, Loss: 3.6722424030303955\n",
            "Training Iteration 4833, Loss: 6.871286392211914\n",
            "Training Iteration 4834, Loss: 4.4678568840026855\n",
            "Training Iteration 4835, Loss: 1.9481910467147827\n",
            "Training Iteration 4836, Loss: 5.025940895080566\n",
            "Training Iteration 4837, Loss: 3.807116985321045\n",
            "Training Iteration 4838, Loss: 6.637745380401611\n",
            "Training Iteration 4839, Loss: 1.5322372913360596\n",
            "Training Iteration 4840, Loss: 4.535495758056641\n",
            "Training Iteration 4841, Loss: 2.722231864929199\n",
            "Training Iteration 4842, Loss: 2.77152156829834\n",
            "Training Iteration 4843, Loss: 5.361265659332275\n",
            "Training Iteration 4844, Loss: 4.415710926055908\n",
            "Training Iteration 4845, Loss: 4.80437707901001\n",
            "Training Iteration 4846, Loss: 2.0360920429229736\n",
            "Training Iteration 4847, Loss: 4.2160539627075195\n",
            "Training Iteration 4848, Loss: 4.55513334274292\n",
            "Training Iteration 4849, Loss: 3.9510080814361572\n",
            "Training Iteration 4850, Loss: 3.1406478881835938\n",
            "Training Iteration 4851, Loss: 8.13914966583252\n",
            "Training Iteration 4852, Loss: 3.3656039237976074\n",
            "Training Iteration 4853, Loss: 5.2705841064453125\n",
            "Training Iteration 4854, Loss: 5.432502746582031\n",
            "Training Iteration 4855, Loss: 3.048570156097412\n",
            "Training Iteration 4856, Loss: 3.067669153213501\n",
            "Training Iteration 4857, Loss: 5.808526515960693\n",
            "Training Iteration 4858, Loss: 5.563681602478027\n",
            "Training Iteration 4859, Loss: 2.9879820346832275\n",
            "Training Iteration 4860, Loss: 2.650221347808838\n",
            "Training Iteration 4861, Loss: 7.079254627227783\n",
            "Training Iteration 4862, Loss: 4.696230888366699\n",
            "Training Iteration 4863, Loss: 8.150991439819336\n",
            "Training Iteration 4864, Loss: 7.595118999481201\n",
            "Training Iteration 4865, Loss: 5.821747303009033\n",
            "Training Iteration 4866, Loss: 3.5042362213134766\n",
            "Training Iteration 4867, Loss: 2.641977310180664\n",
            "Training Iteration 4868, Loss: 5.192356586456299\n",
            "Training Iteration 4869, Loss: 5.17326545715332\n",
            "Training Iteration 4870, Loss: 3.4825093746185303\n",
            "Training Iteration 4871, Loss: 4.886919021606445\n",
            "Training Iteration 4872, Loss: 3.290510654449463\n",
            "Training Iteration 4873, Loss: 1.9632904529571533\n",
            "Training Iteration 4874, Loss: 1.6204789876937866\n",
            "Training Iteration 4875, Loss: 4.039684295654297\n",
            "Training Iteration 4876, Loss: 6.227234363555908\n",
            "Training Iteration 4877, Loss: 3.936722755432129\n",
            "Training Iteration 4878, Loss: 3.6625595092773438\n",
            "Training Iteration 4879, Loss: 2.2051241397857666\n",
            "Training Iteration 4880, Loss: 2.6319780349731445\n",
            "Training Iteration 4881, Loss: 3.7531869411468506\n",
            "Training Iteration 4882, Loss: 8.756546974182129\n",
            "Training Iteration 4883, Loss: 4.304985046386719\n",
            "Training Iteration 4884, Loss: 3.242612600326538\n",
            "Training Iteration 4885, Loss: 3.323587656021118\n",
            "Training Iteration 4886, Loss: 2.8154962062835693\n",
            "Training Iteration 4887, Loss: 3.3885645866394043\n",
            "Training Iteration 4888, Loss: 4.276518821716309\n",
            "Training Iteration 4889, Loss: 3.2054696083068848\n",
            "Training Iteration 4890, Loss: 4.147135257720947\n",
            "Training Iteration 4891, Loss: 3.2527332305908203\n",
            "Training Iteration 4892, Loss: 3.7463715076446533\n",
            "Training Iteration 4893, Loss: 1.7729676961898804\n",
            "Training Iteration 4894, Loss: 6.517387390136719\n",
            "Training Iteration 4895, Loss: 5.515473365783691\n",
            "Training Iteration 4896, Loss: 3.4668149948120117\n",
            "Training Iteration 4897, Loss: 7.573083400726318\n",
            "Training Iteration 4898, Loss: 5.328106880187988\n",
            "Training Iteration 4899, Loss: 4.359673023223877\n",
            "Training Iteration 4900, Loss: 6.603968620300293\n",
            "Training Iteration 4901, Loss: 5.077243328094482\n",
            "Training Iteration 4902, Loss: 6.731187343597412\n",
            "Training Iteration 4903, Loss: 4.98490571975708\n",
            "Training Iteration 4904, Loss: 3.4861512184143066\n",
            "Training Iteration 4905, Loss: 3.884615182876587\n",
            "Training Iteration 4906, Loss: 3.9154553413391113\n",
            "Training Iteration 4907, Loss: 7.170392036437988\n",
            "Training Iteration 4908, Loss: 5.098940849304199\n",
            "Training Iteration 4909, Loss: 5.000503063201904\n",
            "Training Iteration 4910, Loss: 4.602029323577881\n",
            "Training Iteration 4911, Loss: 3.9612624645233154\n",
            "Training Iteration 4912, Loss: 5.65559720993042\n",
            "Training Iteration 4913, Loss: 4.35905122756958\n",
            "Training Iteration 4914, Loss: 5.214792728424072\n",
            "Training Iteration 4915, Loss: 3.7513840198516846\n",
            "Training Iteration 4916, Loss: 3.546247959136963\n",
            "Training Iteration 4917, Loss: 3.0609278678894043\n",
            "Training Iteration 4918, Loss: 2.667006254196167\n",
            "Training Iteration 4919, Loss: 1.9207701683044434\n",
            "Training Iteration 4920, Loss: 6.0047712326049805\n",
            "Training Iteration 4921, Loss: 4.138946056365967\n",
            "Training Iteration 4922, Loss: 7.653255462646484\n",
            "Training Iteration 4923, Loss: 3.4278860092163086\n",
            "Training Iteration 4924, Loss: 1.905941128730774\n",
            "Training Iteration 4925, Loss: 4.949456214904785\n",
            "Training Iteration 4926, Loss: 4.306253433227539\n",
            "Training Iteration 4927, Loss: 4.870926380157471\n",
            "Training Iteration 4928, Loss: 4.369290351867676\n",
            "Training Iteration 4929, Loss: 2.363426685333252\n",
            "Training Iteration 4930, Loss: 2.56532883644104\n",
            "Training Iteration 4931, Loss: 3.785891532897949\n",
            "Training Iteration 4932, Loss: 4.5266337394714355\n",
            "Training Iteration 4933, Loss: 7.101016521453857\n",
            "Training Iteration 4934, Loss: 4.54902458190918\n",
            "Training Iteration 4935, Loss: 7.693521499633789\n",
            "Training Iteration 4936, Loss: 4.850902080535889\n",
            "Training Iteration 4937, Loss: 5.342949867248535\n",
            "Training Iteration 4938, Loss: 4.018731117248535\n",
            "Training Iteration 4939, Loss: 5.564124584197998\n",
            "Training Iteration 4940, Loss: 6.0751447677612305\n",
            "Training Iteration 4941, Loss: 4.92498779296875\n",
            "Training Iteration 4942, Loss: 4.410033226013184\n",
            "Training Iteration 4943, Loss: 4.691499710083008\n",
            "Training Iteration 4944, Loss: 5.843132972717285\n",
            "Training Iteration 4945, Loss: 1.9497207403182983\n",
            "Training Iteration 4946, Loss: 4.37899923324585\n",
            "Training Iteration 4947, Loss: 4.821049690246582\n",
            "Training Iteration 4948, Loss: 4.343630313873291\n",
            "Training Iteration 4949, Loss: 6.206799030303955\n",
            "Training Iteration 4950, Loss: 5.057837009429932\n",
            "Training Iteration 4951, Loss: 9.404898643493652\n",
            "Training Iteration 4952, Loss: 3.8765370845794678\n",
            "Training Iteration 4953, Loss: 5.558363914489746\n",
            "Training Iteration 4954, Loss: 6.61599063873291\n",
            "Training Iteration 4955, Loss: 5.460357189178467\n",
            "Training Iteration 4956, Loss: 3.811074733734131\n",
            "Training Iteration 4957, Loss: 2.373112440109253\n",
            "Training Iteration 4958, Loss: 3.6544201374053955\n",
            "Training Iteration 4959, Loss: 6.921394348144531\n",
            "Training Iteration 4960, Loss: 2.9283359050750732\n",
            "Training Iteration 4961, Loss: 10.143790245056152\n",
            "Training Iteration 4962, Loss: 7.171071529388428\n",
            "Training Iteration 4963, Loss: 5.882071018218994\n",
            "Training Iteration 4964, Loss: 3.221987247467041\n",
            "Training Iteration 4965, Loss: 4.483736515045166\n",
            "Training Iteration 4966, Loss: 4.743655681610107\n",
            "Training Iteration 4967, Loss: 4.822177886962891\n",
            "Training Iteration 4968, Loss: 5.351655006408691\n",
            "Training Iteration 4969, Loss: 4.566028118133545\n",
            "Training Iteration 4970, Loss: 4.443233013153076\n",
            "Training Iteration 4971, Loss: 7.560593605041504\n",
            "Training Iteration 4972, Loss: 3.289078950881958\n",
            "Training Iteration 4973, Loss: 3.2532992362976074\n",
            "Training Iteration 4974, Loss: 8.371688842773438\n",
            "Training Iteration 4975, Loss: 12.366287231445312\n",
            "Training Iteration 4976, Loss: 6.4497599601745605\n",
            "Training Iteration 4977, Loss: 2.638720989227295\n",
            "Training Iteration 4978, Loss: 2.423428773880005\n",
            "Training Iteration 4979, Loss: 6.406539440155029\n",
            "Training Iteration 4980, Loss: 3.484126091003418\n",
            "Training Iteration 4981, Loss: 6.818390846252441\n",
            "Training Iteration 4982, Loss: 5.8925957679748535\n",
            "Training Iteration 4983, Loss: 1.922346830368042\n",
            "Training Iteration 4984, Loss: 7.505736351013184\n",
            "Training Iteration 4985, Loss: 8.921548843383789\n",
            "Training Iteration 4986, Loss: 6.8371710777282715\n",
            "Training Iteration 4987, Loss: 3.5895767211914062\n",
            "Training Iteration 4988, Loss: 3.0855553150177\n",
            "Training Iteration 4989, Loss: 2.6494221687316895\n",
            "Training Iteration 4990, Loss: 3.9151558876037598\n",
            "Training Iteration 4991, Loss: 6.562050819396973\n",
            "Training Iteration 4992, Loss: 4.088781833648682\n",
            "Training Iteration 4993, Loss: 2.0394034385681152\n",
            "Training Iteration 4994, Loss: 6.0687150955200195\n",
            "Training Iteration 4995, Loss: 4.60822057723999\n",
            "Training Iteration 4996, Loss: 5.429133892059326\n",
            "Training Iteration 4997, Loss: 4.613800048828125\n",
            "Training Iteration 4998, Loss: 4.1000471115112305\n",
            "Training Iteration 4999, Loss: 3.8213953971862793\n",
            "Training Iteration 5000, Loss: 3.782747268676758\n",
            "Training Iteration 5001, Loss: 4.304640769958496\n",
            "Training Iteration 5002, Loss: 7.955042839050293\n",
            "Training Iteration 5003, Loss: 3.1948282718658447\n",
            "Training Iteration 5004, Loss: 4.362708568572998\n",
            "Training Iteration 5005, Loss: 4.519449710845947\n",
            "Training Iteration 5006, Loss: 4.847734451293945\n",
            "Training Iteration 5007, Loss: 3.2978267669677734\n",
            "Training Iteration 5008, Loss: 3.7796990871429443\n",
            "Training Iteration 5009, Loss: 5.877228736877441\n",
            "Training Iteration 5010, Loss: 3.1213603019714355\n",
            "Training Iteration 5011, Loss: 2.7279298305511475\n",
            "Training Iteration 5012, Loss: 4.605692386627197\n",
            "Training Iteration 5013, Loss: 5.607791900634766\n",
            "Training Iteration 5014, Loss: 8.442703247070312\n",
            "Training Iteration 5015, Loss: 5.215031623840332\n",
            "Training Iteration 5016, Loss: 6.866783142089844\n",
            "Training Iteration 5017, Loss: 2.5549139976501465\n",
            "Training Iteration 5018, Loss: 4.6221513748168945\n",
            "Training Iteration 5019, Loss: 4.86350679397583\n",
            "Training Iteration 5020, Loss: 3.369973659515381\n",
            "Training Iteration 5021, Loss: 5.642373085021973\n",
            "Training Iteration 5022, Loss: 3.7049801349639893\n",
            "Training Iteration 5023, Loss: 6.028379917144775\n",
            "Training Iteration 5024, Loss: 5.055047035217285\n",
            "Training Iteration 5025, Loss: 1.3014492988586426\n",
            "Training Iteration 5026, Loss: 5.2907795906066895\n",
            "Training Iteration 5027, Loss: 4.314327716827393\n",
            "Training Iteration 5028, Loss: 6.300086975097656\n",
            "Training Iteration 5029, Loss: 4.186888217926025\n",
            "Training Iteration 5030, Loss: 6.9389328956604\n",
            "Training Iteration 5031, Loss: 3.128650665283203\n",
            "Training Iteration 5032, Loss: 3.9045639038085938\n",
            "Training Iteration 5033, Loss: 4.048055648803711\n",
            "Training Iteration 5034, Loss: 2.4999444484710693\n",
            "Training Iteration 5035, Loss: 6.012334823608398\n",
            "Training Iteration 5036, Loss: 5.747344017028809\n",
            "Training Iteration 5037, Loss: 3.639848232269287\n",
            "Training Iteration 5038, Loss: 6.561313629150391\n",
            "Training Iteration 5039, Loss: 3.544477939605713\n",
            "Training Iteration 5040, Loss: 3.708918333053589\n",
            "Training Iteration 5041, Loss: 5.920317649841309\n",
            "Training Iteration 5042, Loss: 1.748514175415039\n",
            "Training Iteration 5043, Loss: 10.562885284423828\n",
            "Training Iteration 5044, Loss: 4.598062038421631\n",
            "Training Iteration 5045, Loss: 4.380688190460205\n",
            "Training Iteration 5046, Loss: 4.074385166168213\n",
            "Training Iteration 5047, Loss: 4.12274169921875\n",
            "Training Iteration 5048, Loss: 5.305435657501221\n",
            "Training Iteration 5049, Loss: 4.307551383972168\n",
            "Training Iteration 5050, Loss: 2.340878963470459\n",
            "Training Iteration 5051, Loss: 4.513393878936768\n",
            "Training Iteration 5052, Loss: 4.392338752746582\n",
            "Training Iteration 5053, Loss: 2.7744247913360596\n",
            "Training Iteration 5054, Loss: 4.644703388214111\n",
            "Training Iteration 5055, Loss: 4.058028697967529\n",
            "Training Iteration 5056, Loss: 3.040191650390625\n",
            "Training Iteration 5057, Loss: 2.70975923538208\n",
            "Training Iteration 5058, Loss: 3.7638633251190186\n",
            "Training Iteration 5059, Loss: 4.81823205947876\n",
            "Training Iteration 5060, Loss: 4.4922261238098145\n",
            "Training Iteration 5061, Loss: 3.575451612472534\n",
            "Training Iteration 5062, Loss: 4.809868812561035\n",
            "Training Iteration 5063, Loss: 3.867673397064209\n",
            "Training Iteration 5064, Loss: 6.267809867858887\n",
            "Training Iteration 5065, Loss: 3.9967002868652344\n",
            "Training Iteration 5066, Loss: 2.7420990467071533\n",
            "Training Iteration 5067, Loss: 1.4168980121612549\n",
            "Training Iteration 5068, Loss: 6.590326309204102\n",
            "Training Iteration 5069, Loss: 3.291553020477295\n",
            "Training Iteration 5070, Loss: 3.2197539806365967\n",
            "Training Iteration 5071, Loss: 3.1188552379608154\n",
            "Training Iteration 5072, Loss: 4.377932548522949\n",
            "Training Iteration 5073, Loss: 6.082347869873047\n",
            "Training Iteration 5074, Loss: 5.09181547164917\n",
            "Training Iteration 5075, Loss: 9.57471752166748\n",
            "Training Iteration 5076, Loss: 5.2510576248168945\n",
            "Training Iteration 5077, Loss: 3.8239707946777344\n",
            "Training Iteration 5078, Loss: 7.264612674713135\n",
            "Training Iteration 5079, Loss: 7.826501369476318\n",
            "Training Iteration 5080, Loss: 4.454562664031982\n",
            "Training Iteration 5081, Loss: 3.75740647315979\n",
            "Training Iteration 5082, Loss: 3.069676637649536\n",
            "Training Iteration 5083, Loss: 4.337527275085449\n",
            "Training Iteration 5084, Loss: 6.356021881103516\n",
            "Training Iteration 5085, Loss: 4.594174385070801\n",
            "Training Iteration 5086, Loss: 3.3614747524261475\n",
            "Training Iteration 5087, Loss: 7.7302093505859375\n",
            "Training Iteration 5088, Loss: 4.699570178985596\n",
            "Training Iteration 5089, Loss: 5.980632781982422\n",
            "Training Iteration 5090, Loss: 3.0734939575195312\n",
            "Training Iteration 5091, Loss: 2.3404064178466797\n",
            "Training Iteration 5092, Loss: 5.166962623596191\n",
            "Training Iteration 5093, Loss: 3.9246912002563477\n",
            "Training Iteration 5094, Loss: 2.9048404693603516\n",
            "Training Iteration 5095, Loss: 2.774214029312134\n",
            "Training Iteration 5096, Loss: 4.803976535797119\n",
            "Training Iteration 5097, Loss: 2.876851797103882\n",
            "Training Iteration 5098, Loss: 5.802509784698486\n",
            "Training Iteration 5099, Loss: 4.730443000793457\n",
            "Training Iteration 5100, Loss: 4.79829216003418\n",
            "Training Iteration 5101, Loss: 4.929996013641357\n",
            "Training Iteration 5102, Loss: 4.099142074584961\n",
            "Training Iteration 5103, Loss: 5.734421253204346\n",
            "Training Iteration 5104, Loss: 4.15447998046875\n",
            "Training Iteration 5105, Loss: 4.79833984375\n",
            "Training Iteration 5106, Loss: 2.014458656311035\n",
            "Training Iteration 5107, Loss: 2.7053756713867188\n",
            "Training Iteration 5108, Loss: 6.457571983337402\n",
            "Training Iteration 5109, Loss: 3.1753978729248047\n",
            "Training Iteration 5110, Loss: 2.9090514183044434\n",
            "Training Iteration 5111, Loss: 3.611051559448242\n",
            "Training Iteration 5112, Loss: 2.0835471153259277\n",
            "Training Iteration 5113, Loss: 3.593944549560547\n",
            "Training Iteration 5114, Loss: 4.388801574707031\n",
            "Training Iteration 5115, Loss: 2.2691280841827393\n",
            "Training Iteration 5116, Loss: 6.3666887283325195\n",
            "Training Iteration 5117, Loss: 3.1463348865509033\n",
            "Training Iteration 5118, Loss: 4.2187371253967285\n",
            "Training Iteration 5119, Loss: 5.23181676864624\n",
            "Training Iteration 5120, Loss: 3.76210880279541\n",
            "Training Iteration 5121, Loss: 4.377030372619629\n",
            "Training Iteration 5122, Loss: 7.95975399017334\n",
            "Training Iteration 5123, Loss: 4.5858235359191895\n",
            "Training Iteration 5124, Loss: 3.142026901245117\n",
            "Training Iteration 5125, Loss: 8.099237442016602\n",
            "Training Iteration 5126, Loss: 5.118649005889893\n",
            "Training Iteration 5127, Loss: 6.343276023864746\n",
            "Training Iteration 5128, Loss: 7.92714786529541\n",
            "Training Iteration 5129, Loss: 1.2809569835662842\n",
            "Training Iteration 5130, Loss: 7.1660966873168945\n",
            "Training Iteration 5131, Loss: 4.69901180267334\n",
            "Training Iteration 5132, Loss: 1.6280717849731445\n",
            "Training Iteration 5133, Loss: 5.655191421508789\n",
            "Training Iteration 5134, Loss: 3.0811023712158203\n",
            "Training Iteration 5135, Loss: 4.740672588348389\n",
            "Training Iteration 5136, Loss: 2.668278932571411\n",
            "Training Iteration 5137, Loss: 3.9113821983337402\n",
            "Training Iteration 5138, Loss: 8.735696792602539\n",
            "Training Iteration 5139, Loss: 7.247939586639404\n",
            "Training Iteration 5140, Loss: 4.764806270599365\n",
            "Training Iteration 5141, Loss: 4.16623592376709\n",
            "Training Iteration 5142, Loss: 3.3612418174743652\n",
            "Training Iteration 5143, Loss: 2.4736175537109375\n",
            "Training Iteration 5144, Loss: 5.870629787445068\n",
            "Training Iteration 5145, Loss: 2.883850574493408\n",
            "Training Iteration 5146, Loss: 5.444070816040039\n",
            "Training Iteration 5147, Loss: 3.9482498168945312\n",
            "Training Iteration 5148, Loss: 2.368917226791382\n",
            "Training Iteration 5149, Loss: 3.538027763366699\n",
            "Training Iteration 5150, Loss: 7.38109016418457\n",
            "Training Iteration 5151, Loss: 5.076396942138672\n",
            "Training Iteration 5152, Loss: 1.1862444877624512\n",
            "Training Iteration 5153, Loss: 3.427184820175171\n",
            "Training Iteration 5154, Loss: 3.9135780334472656\n",
            "Training Iteration 5155, Loss: 5.301249980926514\n",
            "Training Iteration 5156, Loss: 4.608329772949219\n",
            "Training Iteration 5157, Loss: 2.996894598007202\n",
            "Training Iteration 5158, Loss: 6.072702884674072\n",
            "Training Iteration 5159, Loss: 4.233800411224365\n",
            "Training Iteration 5160, Loss: 3.5273380279541016\n",
            "Training Iteration 5161, Loss: 3.6313135623931885\n",
            "Training Iteration 5162, Loss: 4.374493598937988\n",
            "Training Iteration 5163, Loss: 3.7397756576538086\n",
            "Training Iteration 5164, Loss: 2.8493242263793945\n",
            "Training Iteration 5165, Loss: 1.3156108856201172\n",
            "Training Iteration 5166, Loss: 7.681744575500488\n",
            "Training Iteration 5167, Loss: 3.729318141937256\n",
            "Training Iteration 5168, Loss: 5.595572471618652\n",
            "Training Iteration 5169, Loss: 3.482617139816284\n",
            "Training Iteration 5170, Loss: 3.5442757606506348\n",
            "Training Iteration 5171, Loss: 4.512111186981201\n",
            "Training Iteration 5172, Loss: 6.098196029663086\n",
            "Training Iteration 5173, Loss: 6.013200759887695\n",
            "Training Iteration 5174, Loss: 3.6912126541137695\n",
            "Training Iteration 5175, Loss: 4.066812038421631\n",
            "Training Iteration 5176, Loss: 2.9663631916046143\n",
            "Training Iteration 5177, Loss: 4.803644180297852\n",
            "Training Iteration 5178, Loss: 4.850085735321045\n",
            "Training Iteration 5179, Loss: 3.1245932579040527\n",
            "Training Iteration 5180, Loss: 3.1221182346343994\n",
            "Training Iteration 5181, Loss: 3.987974166870117\n",
            "Training Iteration 5182, Loss: 5.505177974700928\n",
            "Training Iteration 5183, Loss: 5.139881134033203\n",
            "Training Iteration 5184, Loss: 2.1892027854919434\n",
            "Training Iteration 5185, Loss: 3.19063138961792\n",
            "Training Iteration 5186, Loss: 5.146294593811035\n",
            "Training Iteration 5187, Loss: 4.028613567352295\n",
            "Training Iteration 5188, Loss: 4.015901565551758\n",
            "Training Iteration 5189, Loss: 4.534830093383789\n",
            "Training Iteration 5190, Loss: 3.442889451980591\n",
            "Training Iteration 5191, Loss: 4.134673595428467\n",
            "Training Iteration 5192, Loss: 5.5878496170043945\n",
            "Training Iteration 5193, Loss: 3.4012694358825684\n",
            "Training Iteration 5194, Loss: 2.4190526008605957\n",
            "Training Iteration 5195, Loss: 2.4821646213531494\n",
            "Training Iteration 5196, Loss: 5.227310657501221\n",
            "Training Iteration 5197, Loss: 3.378979444503784\n",
            "Training Iteration 5198, Loss: 1.7550787925720215\n",
            "Training Iteration 5199, Loss: 3.148499011993408\n",
            "Training Iteration 5200, Loss: 3.1291000843048096\n",
            "Training Iteration 5201, Loss: 3.0051910877227783\n",
            "Training Iteration 5202, Loss: 2.4406657218933105\n",
            "Training Iteration 5203, Loss: 6.19394588470459\n",
            "Training Iteration 5204, Loss: 6.757369518280029\n",
            "Training Iteration 5205, Loss: 2.4716358184814453\n",
            "Training Iteration 5206, Loss: 4.441678047180176\n",
            "Training Iteration 5207, Loss: 3.8891913890838623\n",
            "Training Iteration 5208, Loss: 5.182458877563477\n",
            "Training Iteration 5209, Loss: 2.973451852798462\n",
            "Training Iteration 5210, Loss: 5.94917106628418\n",
            "Training Iteration 5211, Loss: 4.554125785827637\n",
            "Training Iteration 5212, Loss: 5.368978500366211\n",
            "Training Iteration 5213, Loss: 6.391358375549316\n",
            "Training Iteration 5214, Loss: 4.222177982330322\n",
            "Training Iteration 5215, Loss: 5.107614040374756\n",
            "Training Iteration 5216, Loss: 4.8876166343688965\n",
            "Training Iteration 5217, Loss: 3.3784737586975098\n",
            "Training Iteration 5218, Loss: 1.280592679977417\n",
            "Training Iteration 5219, Loss: 4.542449474334717\n",
            "Training Iteration 5220, Loss: 5.260900974273682\n",
            "Training Iteration 5221, Loss: 6.196133613586426\n",
            "Training Iteration 5222, Loss: 3.3186395168304443\n",
            "Training Iteration 5223, Loss: 3.101202964782715\n",
            "Training Iteration 5224, Loss: 6.671746730804443\n",
            "Training Iteration 5225, Loss: 5.786893367767334\n",
            "Training Iteration 5226, Loss: 5.301350116729736\n",
            "Training Iteration 5227, Loss: 4.546026229858398\n",
            "Training Iteration 5228, Loss: 4.909418106079102\n",
            "Training Iteration 5229, Loss: 5.574020862579346\n",
            "Training Iteration 5230, Loss: 4.472286224365234\n",
            "Training Iteration 5231, Loss: 5.17183256149292\n",
            "Training Iteration 5232, Loss: 6.581695556640625\n",
            "Training Iteration 5233, Loss: 3.827380657196045\n",
            "Training Iteration 5234, Loss: 9.703895568847656\n",
            "Training Iteration 5235, Loss: 6.842322826385498\n",
            "Training Iteration 5236, Loss: 4.984712600708008\n",
            "Training Iteration 5237, Loss: 4.345826148986816\n",
            "Training Iteration 5238, Loss: 5.558329105377197\n",
            "Training Iteration 5239, Loss: 4.241388320922852\n",
            "Training Iteration 5240, Loss: 3.6676220893859863\n",
            "Training Iteration 5241, Loss: 8.049757957458496\n",
            "Training Iteration 5242, Loss: 5.8919997215271\n",
            "Training Iteration 5243, Loss: 6.8441162109375\n",
            "Training Iteration 5244, Loss: 4.4953083992004395\n",
            "Training Iteration 5245, Loss: 3.6055712699890137\n",
            "Training Iteration 5246, Loss: 4.553370952606201\n",
            "Training Iteration 5247, Loss: 6.595094680786133\n",
            "Training Iteration 5248, Loss: 3.8392627239227295\n",
            "Training Iteration 5249, Loss: 7.108259201049805\n",
            "Training Iteration 5250, Loss: 5.584702968597412\n",
            "Training Iteration 5251, Loss: 2.508953332901001\n",
            "Training Iteration 5252, Loss: 3.4300830364227295\n",
            "Training Iteration 5253, Loss: 3.8666694164276123\n",
            "Training Iteration 5254, Loss: 5.925945281982422\n",
            "Training Iteration 5255, Loss: 3.6262922286987305\n",
            "Training Iteration 5256, Loss: 4.228081226348877\n",
            "Training Iteration 5257, Loss: 1.6225125789642334\n",
            "Training Iteration 5258, Loss: 2.4156646728515625\n",
            "Training Iteration 5259, Loss: 4.477150917053223\n",
            "Training Iteration 5260, Loss: 2.321500539779663\n",
            "Training Iteration 5261, Loss: 5.307452201843262\n",
            "Training Iteration 5262, Loss: 5.491559028625488\n",
            "Training Iteration 5263, Loss: 4.902033805847168\n",
            "Training Iteration 5264, Loss: 4.875696659088135\n",
            "Training Iteration 5265, Loss: 5.167377471923828\n",
            "Training Iteration 5266, Loss: 6.106184959411621\n",
            "Training Iteration 5267, Loss: 5.167726039886475\n",
            "Training Iteration 5268, Loss: 3.498704433441162\n",
            "Training Iteration 5269, Loss: 4.959479808807373\n",
            "Training Iteration 5270, Loss: 5.604310512542725\n",
            "Training Iteration 5271, Loss: 3.3509390354156494\n",
            "Training Iteration 5272, Loss: 3.2984259128570557\n",
            "Training Iteration 5273, Loss: 6.506515026092529\n",
            "Training Iteration 5274, Loss: 9.421760559082031\n",
            "Training Iteration 5275, Loss: 6.558774948120117\n",
            "Training Iteration 5276, Loss: 7.551036834716797\n",
            "Training Iteration 5277, Loss: 3.9822654724121094\n",
            "Training Iteration 5278, Loss: 5.611539840698242\n",
            "Training Iteration 5279, Loss: 7.676336288452148\n",
            "Training Iteration 5280, Loss: 5.27667236328125\n",
            "Training Iteration 5281, Loss: 2.4793193340301514\n",
            "Training Iteration 5282, Loss: 1.543444275856018\n",
            "Training Iteration 5283, Loss: 6.2379326820373535\n",
            "Training Iteration 5284, Loss: 3.043612003326416\n",
            "Training Iteration 5285, Loss: 7.0490827560424805\n",
            "Training Iteration 5286, Loss: 5.481974124908447\n",
            "Training Iteration 5287, Loss: 4.916311740875244\n",
            "Training Iteration 5288, Loss: 6.941566467285156\n",
            "Training Iteration 5289, Loss: 4.234220504760742\n",
            "Training Iteration 5290, Loss: 8.55614185333252\n",
            "Training Iteration 5291, Loss: 5.464531421661377\n",
            "Training Iteration 5292, Loss: 7.266580104827881\n",
            "Training Iteration 5293, Loss: 5.277287006378174\n",
            "Training Iteration 5294, Loss: 6.986576557159424\n",
            "Training Iteration 5295, Loss: 6.385064125061035\n",
            "Training Iteration 5296, Loss: 6.645910739898682\n",
            "Training Iteration 5297, Loss: 5.780854225158691\n",
            "Training Iteration 5298, Loss: 4.8192009925842285\n",
            "Training Iteration 5299, Loss: 6.088685512542725\n",
            "Training Iteration 5300, Loss: 4.826518535614014\n",
            "Training Iteration 5301, Loss: 7.338597297668457\n",
            "Training Iteration 5302, Loss: 2.125978946685791\n",
            "Training Iteration 5303, Loss: 5.92820405960083\n",
            "Training Iteration 5304, Loss: 4.4043145179748535\n",
            "Training Iteration 5305, Loss: 4.352047443389893\n",
            "Training Iteration 5306, Loss: 1.7019346952438354\n",
            "Training Iteration 5307, Loss: 5.652998924255371\n",
            "Training Iteration 5308, Loss: 3.290487051010132\n",
            "Training Iteration 5309, Loss: 5.768281936645508\n",
            "Training Iteration 5310, Loss: 5.260002136230469\n",
            "Training Iteration 5311, Loss: 7.918395042419434\n",
            "Training Iteration 5312, Loss: 7.184576988220215\n",
            "Training Iteration 5313, Loss: 3.8622140884399414\n",
            "Training Iteration 5314, Loss: 8.162418365478516\n",
            "Training Iteration 5315, Loss: 3.7388975620269775\n",
            "Training Iteration 5316, Loss: 4.242747783660889\n",
            "Training Iteration 5317, Loss: 6.948789119720459\n",
            "Training Iteration 5318, Loss: 4.734218597412109\n",
            "Training Iteration 5319, Loss: 2.3922672271728516\n",
            "Training Iteration 5320, Loss: 5.380687236785889\n",
            "Training Iteration 5321, Loss: 5.779892921447754\n",
            "Training Iteration 5322, Loss: 5.462815761566162\n",
            "Training Iteration 5323, Loss: 4.878157138824463\n",
            "Training Iteration 5324, Loss: 6.989344596862793\n",
            "Training Iteration 5325, Loss: 6.28164005279541\n",
            "Training Iteration 5326, Loss: 4.418845176696777\n",
            "Training Iteration 5327, Loss: 8.027820587158203\n",
            "Training Iteration 5328, Loss: 5.162042617797852\n",
            "Training Iteration 5329, Loss: 4.037508487701416\n",
            "Training Iteration 5330, Loss: 5.135812759399414\n",
            "Training Iteration 5331, Loss: 4.089231967926025\n",
            "Training Iteration 5332, Loss: 7.841446399688721\n",
            "Training Iteration 5333, Loss: 0.8326025009155273\n",
            "Training Iteration 5334, Loss: 2.529268741607666\n",
            "Training Iteration 5335, Loss: 2.572147846221924\n",
            "Training Iteration 5336, Loss: 9.810770988464355\n",
            "Training Iteration 5337, Loss: 5.18100118637085\n",
            "Training Iteration 5338, Loss: 3.5648300647735596\n",
            "Training Iteration 5339, Loss: 4.426656246185303\n",
            "Training Iteration 5340, Loss: 3.6255946159362793\n",
            "Training Iteration 5341, Loss: 6.594365119934082\n",
            "Training Iteration 5342, Loss: 9.835753440856934\n",
            "Training Iteration 5343, Loss: 5.544761657714844\n",
            "Training Iteration 5344, Loss: 4.67950439453125\n",
            "Training Iteration 5345, Loss: 4.361745834350586\n",
            "Training Iteration 5346, Loss: 4.049481391906738\n",
            "Training Iteration 5347, Loss: 6.003264427185059\n",
            "Training Iteration 5348, Loss: 2.9793336391448975\n",
            "Training Iteration 5349, Loss: 4.602426528930664\n",
            "Training Iteration 5350, Loss: 2.0318477153778076\n",
            "Training Iteration 5351, Loss: 5.812320709228516\n",
            "Training Iteration 5352, Loss: 4.23964262008667\n",
            "Training Iteration 5353, Loss: 3.923415422439575\n",
            "Training Iteration 5354, Loss: 2.919509172439575\n",
            "Training Iteration 5355, Loss: 3.9413840770721436\n",
            "Training Iteration 5356, Loss: 2.13834810256958\n",
            "Training Iteration 5357, Loss: 3.0427048206329346\n",
            "Training Iteration 5358, Loss: 5.759520053863525\n",
            "Training Iteration 5359, Loss: 3.624307870864868\n",
            "Training Iteration 5360, Loss: 4.915127754211426\n",
            "Training Iteration 5361, Loss: 6.622082710266113\n",
            "Training Iteration 5362, Loss: 3.4474363327026367\n",
            "Training Iteration 5363, Loss: 5.747262954711914\n",
            "Training Iteration 5364, Loss: 3.6953864097595215\n",
            "Training Iteration 5365, Loss: 2.703037977218628\n",
            "Training Iteration 5366, Loss: 2.30493426322937\n",
            "Training Iteration 5367, Loss: 3.5348589420318604\n",
            "Training Iteration 5368, Loss: 3.4535250663757324\n",
            "Training Iteration 5369, Loss: 3.741476535797119\n",
            "Training Iteration 5370, Loss: 3.6675941944122314\n",
            "Training Iteration 5371, Loss: 2.8432488441467285\n",
            "Training Iteration 5372, Loss: 3.5909676551818848\n",
            "Training Iteration 5373, Loss: 3.0630574226379395\n",
            "Training Iteration 5374, Loss: 4.4105329513549805\n",
            "Training Iteration 5375, Loss: 3.176990032196045\n",
            "Training Iteration 5376, Loss: 5.691737174987793\n",
            "Training Iteration 5377, Loss: 2.2560999393463135\n",
            "Training Iteration 5378, Loss: 3.5596060752868652\n",
            "Training Iteration 5379, Loss: 4.450952053070068\n",
            "Training Iteration 5380, Loss: 1.1726628541946411\n",
            "Training Iteration 5381, Loss: 1.9338473081588745\n",
            "Training Iteration 5382, Loss: 5.139612674713135\n",
            "Training Iteration 5383, Loss: 10.712751388549805\n",
            "Training Iteration 5384, Loss: 8.656415939331055\n",
            "Training Iteration 5385, Loss: 2.9342949390411377\n",
            "Training Iteration 5386, Loss: 4.619508266448975\n",
            "Training Iteration 5387, Loss: 4.571622848510742\n",
            "Training Iteration 5388, Loss: 4.719499588012695\n",
            "Training Iteration 5389, Loss: 3.439997673034668\n",
            "Training Iteration 5390, Loss: 3.5888466835021973\n",
            "Training Iteration 5391, Loss: 3.3566784858703613\n",
            "Training Iteration 5392, Loss: 3.5604305267333984\n",
            "Training Iteration 5393, Loss: 3.494858980178833\n",
            "Training Iteration 5394, Loss: 4.953256607055664\n",
            "Training Iteration 5395, Loss: 6.746066570281982\n",
            "Training Iteration 5396, Loss: 3.9668021202087402\n",
            "Training Iteration 5397, Loss: 5.025917053222656\n",
            "Training Iteration 5398, Loss: 2.344888210296631\n",
            "Training Iteration 5399, Loss: 3.577059268951416\n",
            "Training Iteration 5400, Loss: 5.6107330322265625\n",
            "Training Iteration 5401, Loss: 3.7918660640716553\n",
            "Training Iteration 5402, Loss: 5.552990436553955\n",
            "Training Iteration 5403, Loss: 6.186375141143799\n",
            "Training Iteration 5404, Loss: 2.7172367572784424\n",
            "Training Iteration 5405, Loss: 4.291805267333984\n",
            "Training Iteration 5406, Loss: 5.552149295806885\n",
            "Training Iteration 5407, Loss: 4.694301605224609\n",
            "Training Iteration 5408, Loss: 4.799075126647949\n",
            "Training Iteration 5409, Loss: 3.385993480682373\n",
            "Training Iteration 5410, Loss: 3.555727481842041\n",
            "Training Iteration 5411, Loss: 3.2978832721710205\n",
            "Training Iteration 5412, Loss: 5.073287010192871\n",
            "Training Iteration 5413, Loss: 5.058732032775879\n",
            "Training Iteration 5414, Loss: 3.899991989135742\n",
            "Training Iteration 5415, Loss: 5.5741729736328125\n",
            "Training Iteration 5416, Loss: 5.267856121063232\n",
            "Training Iteration 5417, Loss: 9.361366271972656\n",
            "Training Iteration 5418, Loss: 4.523349761962891\n",
            "Training Iteration 5419, Loss: 10.976880073547363\n",
            "Training Iteration 5420, Loss: 2.025113821029663\n",
            "Training Iteration 5421, Loss: 3.9874420166015625\n",
            "Training Iteration 5422, Loss: 3.086470365524292\n",
            "Training Iteration 5423, Loss: 2.995250701904297\n",
            "Training Iteration 5424, Loss: 3.9800915718078613\n",
            "Training Iteration 5425, Loss: 5.0192551612854\n",
            "Training Iteration 5426, Loss: 4.221327781677246\n",
            "Training Iteration 5427, Loss: 4.756302356719971\n",
            "Training Iteration 5428, Loss: 4.358156681060791\n",
            "Training Iteration 5429, Loss: 4.503274917602539\n",
            "Training Iteration 5430, Loss: 3.1577181816101074\n",
            "Training Iteration 5431, Loss: 5.075161457061768\n",
            "Training Iteration 5432, Loss: 6.890584468841553\n",
            "Training Iteration 5433, Loss: 4.342018127441406\n",
            "Training Iteration 5434, Loss: 4.48948860168457\n",
            "Training Iteration 5435, Loss: 5.196389198303223\n",
            "Training Iteration 5436, Loss: 6.506308078765869\n",
            "Training Iteration 5437, Loss: 9.237077713012695\n",
            "Training Iteration 5438, Loss: 4.682493686676025\n",
            "Training Iteration 5439, Loss: 8.156064987182617\n",
            "Training Iteration 5440, Loss: 5.354961395263672\n",
            "Training Iteration 5441, Loss: 4.600639343261719\n",
            "Training Iteration 5442, Loss: 3.1831822395324707\n",
            "Training Iteration 5443, Loss: 8.651803016662598\n",
            "Training Iteration 5444, Loss: 2.3568532466888428\n",
            "Training Iteration 5445, Loss: 3.166226387023926\n",
            "Training Iteration 5446, Loss: 3.045670509338379\n",
            "Training Iteration 5447, Loss: 8.309471130371094\n",
            "Training Iteration 5448, Loss: 3.607569932937622\n",
            "Training Iteration 5449, Loss: 5.422244071960449\n",
            "Training Iteration 5450, Loss: 5.850216865539551\n",
            "Training Iteration 5451, Loss: 9.344956398010254\n",
            "Training Iteration 5452, Loss: 5.724984645843506\n",
            "Training Iteration 5453, Loss: 6.507702350616455\n",
            "Training Iteration 5454, Loss: 2.3296353816986084\n",
            "Training Iteration 5455, Loss: 6.38865327835083\n",
            "Training Iteration 5456, Loss: 4.119004249572754\n",
            "Training Iteration 5457, Loss: 2.8842499256134033\n",
            "Training Iteration 5458, Loss: 3.177910327911377\n",
            "Training Iteration 5459, Loss: 3.061739683151245\n",
            "Training Iteration 5460, Loss: 4.044112682342529\n",
            "Training Iteration 5461, Loss: 2.0168826580047607\n",
            "Training Iteration 5462, Loss: 3.264059066772461\n",
            "Training Iteration 5463, Loss: 3.746716022491455\n",
            "Training Iteration 5464, Loss: 2.2116947174072266\n",
            "Training Iteration 5465, Loss: 2.828413963317871\n",
            "Training Iteration 5466, Loss: 4.81657600402832\n",
            "Training Iteration 5467, Loss: 3.3007068634033203\n",
            "Training Iteration 5468, Loss: 3.4975178241729736\n",
            "Training Iteration 5469, Loss: 6.749858856201172\n",
            "Training Iteration 5470, Loss: 4.949718952178955\n",
            "Training Iteration 5471, Loss: 4.234144687652588\n",
            "Training Iteration 5472, Loss: 6.780945777893066\n",
            "Training Iteration 5473, Loss: 5.951680660247803\n",
            "Training Iteration 5474, Loss: 4.237144947052002\n",
            "Training Iteration 5475, Loss: 3.2556467056274414\n",
            "Training Iteration 5476, Loss: 5.446332931518555\n",
            "Training Iteration 5477, Loss: 5.408051013946533\n",
            "Training Iteration 5478, Loss: 5.391984939575195\n",
            "Training Iteration 5479, Loss: 3.4862780570983887\n",
            "Training Iteration 5480, Loss: 2.0091915130615234\n",
            "Training Iteration 5481, Loss: 4.679678916931152\n",
            "Training Iteration 5482, Loss: 3.7965331077575684\n",
            "Training Iteration 5483, Loss: 1.9239400625228882\n",
            "Training Iteration 5484, Loss: 4.307620048522949\n",
            "Training Iteration 5485, Loss: 2.45912766456604\n",
            "Training Iteration 5486, Loss: 3.648725748062134\n",
            "Training Iteration 5487, Loss: 6.28291130065918\n",
            "Training Iteration 5488, Loss: 3.099740982055664\n",
            "Training Iteration 5489, Loss: 4.623551845550537\n",
            "Training Iteration 5490, Loss: 5.059358596801758\n",
            "Training Iteration 5491, Loss: 3.4821338653564453\n",
            "Training Iteration 5492, Loss: 3.5395092964172363\n",
            "Training Iteration 5493, Loss: 4.191372394561768\n",
            "Training Iteration 5494, Loss: 5.470617294311523\n",
            "Training Iteration 5495, Loss: 4.1832427978515625\n",
            "Training Iteration 5496, Loss: 4.446902275085449\n",
            "Training Iteration 5497, Loss: 3.81240177154541\n",
            "Training Iteration 5498, Loss: 2.716028928756714\n",
            "Training Iteration 5499, Loss: 7.640552043914795\n",
            "Training Iteration 5500, Loss: 5.416092872619629\n",
            "Training Iteration 5501, Loss: 7.268874645233154\n",
            "Training Iteration 5502, Loss: 2.8320932388305664\n",
            "Training Iteration 5503, Loss: 5.393730163574219\n",
            "Training Iteration 5504, Loss: 3.5171053409576416\n",
            "Training Iteration 5505, Loss: 3.7183496952056885\n",
            "Training Iteration 5506, Loss: 6.879313945770264\n",
            "Training Iteration 5507, Loss: 3.1831107139587402\n",
            "Training Iteration 5508, Loss: 3.3795006275177\n",
            "Training Iteration 5509, Loss: 2.262888193130493\n",
            "Training Iteration 5510, Loss: 2.9547853469848633\n",
            "Training Iteration 5511, Loss: 3.4347469806671143\n",
            "Training Iteration 5512, Loss: 0.4757765531539917\n",
            "Training Iteration 5513, Loss: 1.470999002456665\n",
            "Training Iteration 5514, Loss: 4.606320381164551\n",
            "Training Iteration 5515, Loss: 5.583906650543213\n",
            "Training Iteration 5516, Loss: 4.223813056945801\n",
            "Training Iteration 5517, Loss: 5.415657043457031\n",
            "Training Iteration 5518, Loss: 4.266852378845215\n",
            "Training Iteration 5519, Loss: 3.104635715484619\n",
            "Training Iteration 5520, Loss: 2.6762003898620605\n",
            "Training Iteration 5521, Loss: 4.644466400146484\n",
            "Training Iteration 5522, Loss: 4.338440895080566\n",
            "Training Iteration 5523, Loss: 2.765018939971924\n",
            "Training Iteration 5524, Loss: 5.093851089477539\n",
            "Training Iteration 5525, Loss: 5.163794040679932\n",
            "Training Iteration 5526, Loss: 3.8638482093811035\n",
            "Training Iteration 5527, Loss: 4.794344425201416\n",
            "Training Iteration 5528, Loss: 4.649790287017822\n",
            "Training Iteration 5529, Loss: 5.131958961486816\n",
            "Training Iteration 5530, Loss: 6.0678815841674805\n",
            "Training Iteration 5531, Loss: 2.714658737182617\n",
            "Training Iteration 5532, Loss: 8.447172164916992\n",
            "Training Iteration 5533, Loss: 6.52367639541626\n",
            "Training Iteration 5534, Loss: 3.405834913253784\n",
            "Training Iteration 5535, Loss: 4.07538366317749\n",
            "Training Iteration 5536, Loss: 3.6535251140594482\n",
            "Training Iteration 5537, Loss: 2.7775802612304688\n",
            "Training Iteration 5538, Loss: 2.982929229736328\n",
            "Training Iteration 5539, Loss: 5.6599016189575195\n",
            "Training Iteration 5540, Loss: 3.230900287628174\n",
            "Training Iteration 5541, Loss: 2.259805917739868\n",
            "Training Iteration 5542, Loss: 5.115527153015137\n",
            "Training Iteration 5543, Loss: 6.480776309967041\n",
            "Training Iteration 5544, Loss: 3.447194814682007\n",
            "Training Iteration 5545, Loss: 3.9987094402313232\n",
            "Training Iteration 5546, Loss: 5.371299743652344\n",
            "Training Iteration 5547, Loss: 3.396880626678467\n",
            "Training Iteration 5548, Loss: 4.714681625366211\n",
            "Training Iteration 5549, Loss: 5.061769008636475\n",
            "Training Iteration 5550, Loss: 3.815666437149048\n",
            "Training Iteration 5551, Loss: 4.0702290534973145\n",
            "Training Iteration 5552, Loss: 6.5067853927612305\n",
            "Training Iteration 5553, Loss: 2.3175394535064697\n",
            "Training Iteration 5554, Loss: 3.320361614227295\n",
            "Training Iteration 5555, Loss: 3.4558064937591553\n",
            "Training Iteration 5556, Loss: 4.531148433685303\n",
            "Training Iteration 5557, Loss: 4.961054801940918\n",
            "Training Iteration 5558, Loss: 5.372254848480225\n",
            "Training Iteration 5559, Loss: 3.134819984436035\n",
            "Training Iteration 5560, Loss: 3.605440616607666\n",
            "Training Iteration 5561, Loss: 4.665040016174316\n",
            "Training Iteration 5562, Loss: 2.7882158756256104\n",
            "Training Iteration 5563, Loss: 7.66080904006958\n",
            "Training Iteration 5564, Loss: 3.5470569133758545\n",
            "Training Iteration 5565, Loss: 4.263974666595459\n",
            "Training Iteration 5566, Loss: 3.0060982704162598\n",
            "Training Iteration 5567, Loss: 4.237285137176514\n",
            "Training Iteration 5568, Loss: 3.106809377670288\n",
            "Training Iteration 5569, Loss: 5.169349670410156\n",
            "Training Iteration 5570, Loss: 6.729313373565674\n",
            "Training Iteration 5571, Loss: 4.3980536460876465\n",
            "Training Iteration 5572, Loss: 4.96382999420166\n",
            "Training Iteration 5573, Loss: 4.2543230056762695\n",
            "Training Iteration 5574, Loss: 5.411559104919434\n",
            "Training Iteration 5575, Loss: 5.451582908630371\n",
            "Training Iteration 5576, Loss: 3.3569397926330566\n",
            "Training Iteration 5577, Loss: 6.546349048614502\n",
            "Training Iteration 5578, Loss: 2.8505783081054688\n",
            "Training Iteration 5579, Loss: 3.4184279441833496\n",
            "Training Iteration 5580, Loss: 4.7830586433410645\n",
            "Training Iteration 5581, Loss: 4.9011030197143555\n",
            "Training Iteration 5582, Loss: 4.652837753295898\n",
            "Training Iteration 5583, Loss: 4.522937297821045\n",
            "Training Iteration 5584, Loss: 3.493557929992676\n",
            "Training Iteration 5585, Loss: 6.2052226066589355\n",
            "Training Iteration 5586, Loss: 2.4323365688323975\n",
            "Training Iteration 5587, Loss: 4.792011737823486\n",
            "Training Iteration 5588, Loss: 6.302348613739014\n",
            "Training Iteration 5589, Loss: 5.36271333694458\n",
            "Training Iteration 5590, Loss: 2.321277618408203\n",
            "Training Iteration 5591, Loss: 10.894173622131348\n",
            "Training Iteration 5592, Loss: 2.8000237941741943\n",
            "Training Iteration 5593, Loss: 4.377582550048828\n",
            "Training Iteration 5594, Loss: 7.051839828491211\n",
            "Training Iteration 5595, Loss: 3.285358428955078\n",
            "Training Iteration 5596, Loss: 5.811724662780762\n",
            "Training Iteration 5597, Loss: 10.665995597839355\n",
            "Training Iteration 5598, Loss: 2.7796194553375244\n",
            "Training Iteration 5599, Loss: 3.9747393131256104\n",
            "Training Iteration 5600, Loss: 3.4673755168914795\n",
            "Training Iteration 5601, Loss: 4.142609596252441\n",
            "Training Iteration 5602, Loss: 3.522933006286621\n",
            "Training Iteration 5603, Loss: 4.983998775482178\n",
            "Training Iteration 5604, Loss: 5.757277011871338\n",
            "Training Iteration 5605, Loss: 2.1322455406188965\n",
            "Training Iteration 5606, Loss: 4.293647766113281\n",
            "Training Iteration 5607, Loss: 4.791409492492676\n",
            "Training Iteration 5608, Loss: 3.388484001159668\n",
            "Training Iteration 5609, Loss: 2.5671238899230957\n",
            "Training Iteration 5610, Loss: 7.541383266448975\n",
            "Training Iteration 5611, Loss: 3.439359188079834\n",
            "Training Iteration 5612, Loss: 3.49957537651062\n",
            "Training Iteration 5613, Loss: 7.432510852813721\n",
            "Training Iteration 5614, Loss: 6.432806491851807\n",
            "Training Iteration 5615, Loss: 2.46272873878479\n",
            "Training Iteration 5616, Loss: 3.4980030059814453\n",
            "Training Iteration 5617, Loss: 5.523597240447998\n",
            "Training Iteration 5618, Loss: 6.2644758224487305\n",
            "Training Iteration 5619, Loss: 7.736419200897217\n",
            "Training Iteration 5620, Loss: 3.092031240463257\n",
            "Training Iteration 5621, Loss: 4.803401470184326\n",
            "Training Iteration 5622, Loss: 4.579214572906494\n",
            "Training Iteration 5623, Loss: 4.625442028045654\n",
            "Training Iteration 5624, Loss: 4.041281700134277\n",
            "Training Iteration 5625, Loss: 5.509980201721191\n",
            "Training Iteration 5626, Loss: 1.8059617280960083\n",
            "Training Iteration 5627, Loss: 2.9179563522338867\n",
            "Training Iteration 5628, Loss: 1.9534976482391357\n",
            "Training Iteration 5629, Loss: 2.269871473312378\n",
            "Training Iteration 5630, Loss: 7.233579635620117\n",
            "Training Iteration 5631, Loss: 3.5765841007232666\n",
            "Training Iteration 5632, Loss: 4.821847915649414\n",
            "Training Iteration 5633, Loss: 3.8700854778289795\n",
            "Training Iteration 5634, Loss: 5.4918718338012695\n",
            "Training Iteration 5635, Loss: 5.442902088165283\n",
            "Training Iteration 5636, Loss: 5.317704200744629\n",
            "Training Iteration 5637, Loss: 6.054757595062256\n",
            "Training Iteration 5638, Loss: 4.169309139251709\n",
            "Training Iteration 5639, Loss: 3.4638445377349854\n",
            "Training Iteration 5640, Loss: 5.300044059753418\n",
            "Training Iteration 5641, Loss: 3.5422616004943848\n",
            "Training Iteration 5642, Loss: 4.5828166007995605\n",
            "Training Iteration 5643, Loss: 4.461370468139648\n",
            "Training Iteration 5644, Loss: 6.644721508026123\n",
            "Training Iteration 5645, Loss: 3.4943594932556152\n",
            "Training Iteration 5646, Loss: 5.6451544761657715\n",
            "Training Iteration 5647, Loss: 5.412232875823975\n",
            "Training Iteration 5648, Loss: 7.718916416168213\n",
            "Training Iteration 5649, Loss: 2.2660651206970215\n",
            "Training Iteration 5650, Loss: 6.938066482543945\n",
            "Training Iteration 5651, Loss: 5.638388633728027\n",
            "Training Iteration 5652, Loss: 4.8569722175598145\n",
            "Training Iteration 5653, Loss: 4.876249313354492\n",
            "Training Iteration 5654, Loss: 2.5134851932525635\n",
            "Training Iteration 5655, Loss: 4.6694769859313965\n",
            "Training Iteration 5656, Loss: 3.335338830947876\n",
            "Training Iteration 5657, Loss: 3.9291129112243652\n",
            "Training Iteration 5658, Loss: 5.563427925109863\n",
            "Training Iteration 5659, Loss: 3.025949478149414\n",
            "Training Iteration 5660, Loss: 3.6901087760925293\n",
            "Training Iteration 5661, Loss: 3.972158908843994\n",
            "Training Iteration 5662, Loss: 4.672091007232666\n",
            "Training Iteration 5663, Loss: 2.154873847961426\n",
            "Training Iteration 5664, Loss: 5.002682209014893\n",
            "Training Iteration 5665, Loss: 4.883760929107666\n",
            "Training Iteration 5666, Loss: 1.0950244665145874\n",
            "Training Iteration 5667, Loss: 3.469411849975586\n",
            "Training Iteration 5668, Loss: 5.919887542724609\n",
            "Training Iteration 5669, Loss: 4.649151802062988\n",
            "Training Iteration 5670, Loss: 2.9699172973632812\n",
            "Training Iteration 5671, Loss: 2.209859609603882\n",
            "Training Iteration 5672, Loss: 4.38138484954834\n",
            "Training Iteration 5673, Loss: 5.063394546508789\n",
            "Training Iteration 5674, Loss: 3.8268699645996094\n",
            "Training Iteration 5675, Loss: 5.1641645431518555\n",
            "Training Iteration 5676, Loss: 5.34149694442749\n",
            "Training Iteration 5677, Loss: 2.493978261947632\n",
            "Training Iteration 5678, Loss: 2.475493907928467\n",
            "Training Iteration 5679, Loss: 5.847174644470215\n",
            "Training Iteration 5680, Loss: 6.166756629943848\n",
            "Training Iteration 5681, Loss: 3.585434913635254\n",
            "Training Iteration 5682, Loss: 2.656008243560791\n",
            "Training Iteration 5683, Loss: 7.33465051651001\n",
            "Training Iteration 5684, Loss: 2.271439790725708\n",
            "Training Iteration 5685, Loss: 3.526223659515381\n",
            "Training Iteration 5686, Loss: 3.009467601776123\n",
            "Training Iteration 5687, Loss: 4.348169326782227\n",
            "Training Iteration 5688, Loss: 8.269639015197754\n",
            "Training Iteration 5689, Loss: 5.823840141296387\n",
            "Training Iteration 5690, Loss: 5.105855941772461\n",
            "Training Iteration 5691, Loss: 4.531135559082031\n",
            "Training Iteration 5692, Loss: 3.206482410430908\n",
            "Training Iteration 5693, Loss: 2.1243467330932617\n",
            "Training Iteration 5694, Loss: 4.462466239929199\n",
            "Training Iteration 5695, Loss: 5.805661678314209\n",
            "Training Iteration 5696, Loss: 5.625675201416016\n",
            "Training Iteration 5697, Loss: 5.457068920135498\n",
            "Training Iteration 5698, Loss: 3.512479066848755\n",
            "Training Iteration 5699, Loss: 6.421203136444092\n",
            "Training Iteration 5700, Loss: 2.2625176906585693\n",
            "Training Iteration 5701, Loss: 7.293735027313232\n",
            "Training Iteration 5702, Loss: 3.1344544887542725\n",
            "Training Iteration 5703, Loss: 5.729741096496582\n",
            "Training Iteration 5704, Loss: 3.83272123336792\n",
            "Training Iteration 5705, Loss: 4.249286651611328\n",
            "Training Iteration 5706, Loss: 1.6407569646835327\n",
            "Training Iteration 5707, Loss: 3.2168211936950684\n",
            "Training Iteration 5708, Loss: 3.5036892890930176\n",
            "Training Iteration 5709, Loss: 3.3444766998291016\n",
            "Training Iteration 5710, Loss: 3.1866378784179688\n",
            "Training Iteration 5711, Loss: 8.293756484985352\n",
            "Training Iteration 5712, Loss: 2.8493075370788574\n",
            "Training Iteration 5713, Loss: 4.217000484466553\n",
            "Training Iteration 5714, Loss: 2.7141199111938477\n",
            "Training Iteration 5715, Loss: 3.018378973007202\n",
            "Training Iteration 5716, Loss: 3.2802391052246094\n",
            "Training Iteration 5717, Loss: 4.3511223793029785\n",
            "Training Iteration 5718, Loss: 5.774688720703125\n",
            "Training Iteration 5719, Loss: 3.668113946914673\n",
            "Training Iteration 5720, Loss: 4.64750337600708\n",
            "Training Iteration 5721, Loss: 4.309722900390625\n",
            "Training Iteration 5722, Loss: 4.203926086425781\n",
            "Training Iteration 5723, Loss: 4.545665264129639\n",
            "Training Iteration 5724, Loss: 3.422584295272827\n",
            "Training Iteration 5725, Loss: 4.658251762390137\n",
            "Training Iteration 5726, Loss: 3.891756057739258\n",
            "Training Iteration 5727, Loss: 4.18256139755249\n",
            "Training Iteration 5728, Loss: 3.2404608726501465\n",
            "Training Iteration 5729, Loss: 3.956937313079834\n",
            "Training Iteration 5730, Loss: 3.6461098194122314\n",
            "Training Iteration 5731, Loss: 2.406207323074341\n",
            "Training Iteration 5732, Loss: 3.477600336074829\n",
            "Training Iteration 5733, Loss: 3.296748638153076\n",
            "Training Iteration 5734, Loss: 2.94636869430542\n",
            "Training Iteration 5735, Loss: 4.226815223693848\n",
            "Training Iteration 5736, Loss: 0.6530407667160034\n",
            "Training Iteration 5737, Loss: 2.5014023780822754\n",
            "Training Iteration 5738, Loss: 5.858376502990723\n",
            "Training Iteration 5739, Loss: 7.820555210113525\n",
            "Training Iteration 5740, Loss: 3.128974437713623\n",
            "Training Iteration 5741, Loss: 1.9330207109451294\n",
            "Training Iteration 5742, Loss: 3.8720531463623047\n",
            "Training Iteration 5743, Loss: 3.2079851627349854\n",
            "Training Iteration 5744, Loss: 2.7616560459136963\n",
            "Training Iteration 5745, Loss: 4.472164154052734\n",
            "Training Iteration 5746, Loss: 3.4712789058685303\n",
            "Training Iteration 5747, Loss: 5.364910125732422\n",
            "Training Iteration 5748, Loss: 1.8514899015426636\n",
            "Training Iteration 5749, Loss: 1.4308063983917236\n",
            "Training Iteration 5750, Loss: 6.329047203063965\n",
            "Training Iteration 5751, Loss: 3.303128242492676\n",
            "Training Iteration 5752, Loss: 5.255505561828613\n",
            "Training Iteration 5753, Loss: 5.158314228057861\n",
            "Training Iteration 5754, Loss: 4.8209991455078125\n",
            "Training Iteration 5755, Loss: 6.568771839141846\n",
            "Training Iteration 5756, Loss: 3.153439998626709\n",
            "Training Iteration 5757, Loss: 1.9214134216308594\n",
            "Training Iteration 5758, Loss: 4.676465034484863\n",
            "Training Iteration 5759, Loss: 2.608142137527466\n",
            "Training Iteration 5760, Loss: 4.95635986328125\n",
            "Training Iteration 5761, Loss: 3.7805633544921875\n",
            "Training Iteration 5762, Loss: 4.739621162414551\n",
            "Training Iteration 5763, Loss: 6.352483749389648\n",
            "Training Iteration 5764, Loss: 2.0292563438415527\n",
            "Training Iteration 5765, Loss: 4.010727405548096\n",
            "Training Iteration 5766, Loss: 5.48382568359375\n",
            "Training Iteration 5767, Loss: 2.5400102138519287\n",
            "Training Iteration 5768, Loss: 4.15962028503418\n",
            "Training Iteration 5769, Loss: 3.695103883743286\n",
            "Training Iteration 5770, Loss: 5.190640449523926\n",
            "Training Iteration 5771, Loss: 9.110697746276855\n",
            "Training Iteration 5772, Loss: 4.414672374725342\n",
            "Training Iteration 5773, Loss: 4.066864967346191\n",
            "Training Iteration 5774, Loss: 2.61198353767395\n",
            "Training Iteration 5775, Loss: 2.9300193786621094\n",
            "Training Iteration 5776, Loss: 5.4701433181762695\n",
            "Training Iteration 5777, Loss: 4.814642429351807\n",
            "Training Iteration 5778, Loss: 6.405714988708496\n",
            "Training Iteration 5779, Loss: 3.4723126888275146\n",
            "Training Iteration 5780, Loss: 3.8399293422698975\n",
            "Training Iteration 5781, Loss: 5.6312255859375\n",
            "Training Iteration 5782, Loss: 4.689589500427246\n",
            "Training Iteration 5783, Loss: 5.63255500793457\n",
            "Training Iteration 5784, Loss: 4.8119378089904785\n",
            "Training Iteration 5785, Loss: 4.87063455581665\n",
            "Training Iteration 5786, Loss: 3.0422439575195312\n",
            "Training Iteration 5787, Loss: 8.097210884094238\n",
            "Training Iteration 5788, Loss: 7.411965370178223\n",
            "Training Iteration 5789, Loss: 6.6876068115234375\n",
            "Training Iteration 5790, Loss: 8.020772933959961\n",
            "Training Iteration 5791, Loss: 5.570015907287598\n",
            "Training Iteration 5792, Loss: 5.063560962677002\n",
            "Training Iteration 5793, Loss: 2.359039068222046\n",
            "Training Iteration 5794, Loss: 8.375772476196289\n",
            "Training Iteration 5795, Loss: 8.339118003845215\n",
            "Training Iteration 5796, Loss: 6.206784248352051\n",
            "Training Iteration 5797, Loss: 6.027235507965088\n",
            "Training Iteration 5798, Loss: 5.849338054656982\n",
            "Training Iteration 5799, Loss: 9.752824783325195\n",
            "Training Iteration 5800, Loss: 7.81182861328125\n",
            "Training Iteration 5801, Loss: 3.980564594268799\n",
            "Training Iteration 5802, Loss: 4.836862564086914\n",
            "Training Iteration 5803, Loss: 7.463066101074219\n",
            "Training Iteration 5804, Loss: 3.9729232788085938\n",
            "Training Iteration 5805, Loss: 3.05574107170105\n",
            "Training Iteration 5806, Loss: 3.8832240104675293\n",
            "Training Iteration 5807, Loss: 4.693648338317871\n",
            "Training Iteration 5808, Loss: 3.3536150455474854\n",
            "Training Iteration 5809, Loss: 3.8162600994110107\n",
            "Training Iteration 5810, Loss: 4.2075114250183105\n",
            "Training Iteration 5811, Loss: 4.350015163421631\n",
            "Training Iteration 5812, Loss: 4.536374092102051\n",
            "Training Iteration 5813, Loss: 2.8020553588867188\n",
            "Training Iteration 5814, Loss: 3.9997806549072266\n",
            "Training Iteration 5815, Loss: 2.8342559337615967\n",
            "Training Iteration 5816, Loss: 5.515329360961914\n",
            "Training Iteration 5817, Loss: 8.366765022277832\n",
            "Training Iteration 5818, Loss: 3.9407758712768555\n",
            "Training Iteration 5819, Loss: 5.8440022468566895\n",
            "Training Iteration 5820, Loss: 3.2418344020843506\n",
            "Training Iteration 5821, Loss: 6.583364963531494\n",
            "Training Iteration 5822, Loss: 3.402921199798584\n",
            "Training Iteration 5823, Loss: 4.534854888916016\n",
            "Training Iteration 5824, Loss: 4.025416374206543\n",
            "Training Iteration 5825, Loss: 4.031710624694824\n",
            "Training Iteration 5826, Loss: 4.202138900756836\n",
            "Training Iteration 5827, Loss: 7.173061370849609\n",
            "Training Iteration 5828, Loss: 3.5222373008728027\n",
            "Training Iteration 5829, Loss: 8.072243690490723\n",
            "Training Iteration 5830, Loss: 2.871140956878662\n",
            "Training Iteration 5831, Loss: 4.205428123474121\n",
            "Training Iteration 5832, Loss: 4.7524919509887695\n",
            "Training Iteration 5833, Loss: 4.920284748077393\n",
            "Training Iteration 5834, Loss: 4.269649982452393\n",
            "Training Iteration 5835, Loss: 7.22749137878418\n",
            "Training Iteration 5836, Loss: 8.603402137756348\n",
            "Training Iteration 5837, Loss: 6.316071033477783\n",
            "Training Iteration 5838, Loss: 7.78128719329834\n",
            "Training Iteration 5839, Loss: 4.52594518661499\n",
            "Training Iteration 5840, Loss: 5.213902950286865\n",
            "Training Iteration 5841, Loss: 3.853590488433838\n",
            "Training Iteration 5842, Loss: 4.2504048347473145\n",
            "Training Iteration 5843, Loss: 3.60809326171875\n",
            "Training Iteration 5844, Loss: 2.8559277057647705\n",
            "Training Iteration 5845, Loss: 4.619049549102783\n",
            "Training Iteration 5846, Loss: 6.23432731628418\n",
            "Training Iteration 5847, Loss: 2.615797281265259\n",
            "Training Iteration 5848, Loss: 3.314307689666748\n",
            "Training Iteration 5849, Loss: 2.929363965988159\n",
            "Training Iteration 5850, Loss: 2.379756450653076\n",
            "Training Iteration 5851, Loss: 3.3950254917144775\n",
            "Training Iteration 5852, Loss: 3.395209312438965\n",
            "Training Iteration 5853, Loss: 4.24246883392334\n",
            "Training Iteration 5854, Loss: 3.5368247032165527\n",
            "Training Iteration 5855, Loss: 11.532369613647461\n",
            "Training Iteration 5856, Loss: 5.424768447875977\n",
            "Training Iteration 5857, Loss: 6.030517578125\n",
            "Training Iteration 5858, Loss: 2.6332740783691406\n",
            "Training Iteration 5859, Loss: 4.51652717590332\n",
            "Training Iteration 5860, Loss: 3.4033524990081787\n",
            "Training Iteration 5861, Loss: 4.908604145050049\n",
            "Training Iteration 5862, Loss: 2.5074450969696045\n",
            "Training Iteration 5863, Loss: 3.5650839805603027\n",
            "Training Iteration 5864, Loss: 2.290834903717041\n",
            "Training Iteration 5865, Loss: 5.361981391906738\n",
            "Training Iteration 5866, Loss: 5.357107162475586\n",
            "Training Iteration 5867, Loss: 5.78798246383667\n",
            "Training Iteration 5868, Loss: 7.2924394607543945\n",
            "Training Iteration 5869, Loss: 3.694546699523926\n",
            "Training Iteration 5870, Loss: 4.341174602508545\n",
            "Training Iteration 5871, Loss: 2.9704768657684326\n",
            "Training Iteration 5872, Loss: 8.394264221191406\n",
            "Training Iteration 5873, Loss: 3.8428354263305664\n",
            "Training Iteration 5874, Loss: 5.5953145027160645\n",
            "Training Iteration 5875, Loss: 2.3278777599334717\n",
            "Training Iteration 5876, Loss: 4.746598243713379\n",
            "Training Iteration 5877, Loss: 3.3646292686462402\n",
            "Training Iteration 5878, Loss: 4.2616143226623535\n",
            "Training Iteration 5879, Loss: 4.014269828796387\n",
            "Training Iteration 5880, Loss: 4.8797287940979\n",
            "Training Iteration 5881, Loss: 4.869011402130127\n",
            "Training Iteration 5882, Loss: 4.703125\n",
            "Training Iteration 5883, Loss: 3.9175846576690674\n",
            "Training Iteration 5884, Loss: 5.245874404907227\n",
            "Training Iteration 5885, Loss: 4.163109302520752\n",
            "Training Iteration 5886, Loss: 3.3678770065307617\n",
            "Training Iteration 5887, Loss: 4.458396911621094\n",
            "Training Iteration 5888, Loss: 1.8393564224243164\n",
            "Training Iteration 5889, Loss: 2.5892367362976074\n",
            "Training Iteration 5890, Loss: 4.03990364074707\n",
            "Training Iteration 5891, Loss: 3.5034313201904297\n",
            "Training Iteration 5892, Loss: 3.2377212047576904\n",
            "Training Iteration 5893, Loss: 4.130363941192627\n",
            "Training Iteration 5894, Loss: 4.714821815490723\n",
            "Training Iteration 5895, Loss: 4.5887627601623535\n",
            "Training Iteration 5896, Loss: 5.171056747436523\n",
            "Training Iteration 5897, Loss: 6.042003631591797\n",
            "Training Iteration 5898, Loss: 5.28472900390625\n",
            "Training Iteration 5899, Loss: 2.77182674407959\n",
            "Training Iteration 5900, Loss: 3.628969669342041\n",
            "Training Iteration 5901, Loss: 1.6727644205093384\n",
            "Training Iteration 5902, Loss: 6.075951099395752\n",
            "Training Iteration 5903, Loss: 6.154546737670898\n",
            "Training Iteration 5904, Loss: 5.40018892288208\n",
            "Training Iteration 5905, Loss: 3.399357318878174\n",
            "Training Iteration 5906, Loss: 8.429146766662598\n",
            "Training Iteration 5907, Loss: 3.412970781326294\n",
            "Training Iteration 5908, Loss: 3.8298544883728027\n",
            "Training Iteration 5909, Loss: 3.168536424636841\n",
            "Training Iteration 5910, Loss: 3.003636360168457\n",
            "Training Iteration 5911, Loss: 6.175895690917969\n",
            "Training Iteration 5912, Loss: 2.7929627895355225\n",
            "Training Iteration 5913, Loss: 7.247245788574219\n",
            "Training Iteration 5914, Loss: 6.475376129150391\n",
            "Training Iteration 5915, Loss: 7.006126880645752\n",
            "Training Iteration 5916, Loss: 5.431032657623291\n",
            "Training Iteration 5917, Loss: 4.180146217346191\n",
            "Training Iteration 5918, Loss: 5.724007606506348\n",
            "Training Iteration 5919, Loss: 3.9797892570495605\n",
            "Training Iteration 5920, Loss: 3.222830295562744\n",
            "Training Iteration 5921, Loss: 6.739346027374268\n",
            "Training Iteration 5922, Loss: 2.5485680103302\n",
            "Training Iteration 5923, Loss: 4.462917327880859\n",
            "Training Iteration 5924, Loss: 6.859833717346191\n",
            "Training Iteration 5925, Loss: 3.337888240814209\n",
            "Training Iteration 5926, Loss: 5.634426116943359\n",
            "Training Iteration 5927, Loss: 5.1395583152771\n",
            "Training Iteration 5928, Loss: 2.9394068717956543\n",
            "Training Iteration 5929, Loss: 3.098186731338501\n",
            "Training Iteration 5930, Loss: 3.7082865238189697\n",
            "Training Iteration 5931, Loss: 2.873516321182251\n",
            "Training Iteration 5932, Loss: 4.354426860809326\n",
            "Training Iteration 5933, Loss: 3.7436623573303223\n",
            "Training Iteration 5934, Loss: 3.9270741939544678\n",
            "Training Iteration 5935, Loss: 2.7651145458221436\n",
            "Training Iteration 5936, Loss: 3.9614152908325195\n",
            "Training Iteration 5937, Loss: 3.758702039718628\n",
            "Training Iteration 5938, Loss: 2.8270201683044434\n",
            "Training Iteration 5939, Loss: 3.821378231048584\n",
            "Training Iteration 5940, Loss: 4.343567848205566\n",
            "Training Iteration 5941, Loss: 3.4155330657958984\n",
            "Training Iteration 5942, Loss: 5.267161846160889\n",
            "Training Iteration 5943, Loss: 4.084543228149414\n",
            "Training Iteration 5944, Loss: 5.136408805847168\n",
            "Training Iteration 5945, Loss: 3.704223871231079\n",
            "Training Iteration 5946, Loss: 8.769739151000977\n",
            "Training Iteration 5947, Loss: 2.8415842056274414\n",
            "Training Iteration 5948, Loss: 3.3934130668640137\n",
            "Training Iteration 5949, Loss: 2.6168124675750732\n",
            "Training Iteration 5950, Loss: 4.9550700187683105\n",
            "Training Iteration 5951, Loss: 2.801832914352417\n",
            "Training Iteration 5952, Loss: 3.634951591491699\n",
            "Training Iteration 5953, Loss: 3.625565767288208\n",
            "Training Iteration 5954, Loss: 3.7666831016540527\n",
            "Training Iteration 5955, Loss: 5.947083950042725\n",
            "Training Iteration 5956, Loss: 3.9316272735595703\n",
            "Training Iteration 5957, Loss: 4.266099452972412\n",
            "Training Iteration 5958, Loss: 2.6161715984344482\n",
            "Training Iteration 5959, Loss: 1.8049862384796143\n",
            "Training Iteration 5960, Loss: 5.376856327056885\n",
            "Training Iteration 5961, Loss: 4.511031627655029\n",
            "Training Iteration 5962, Loss: 3.2324271202087402\n",
            "Training Iteration 5963, Loss: 3.9275665283203125\n",
            "Training Iteration 5964, Loss: 4.366766452789307\n",
            "Training Iteration 5965, Loss: 6.024182319641113\n",
            "Training Iteration 5966, Loss: 8.568816184997559\n",
            "Training Iteration 5967, Loss: 2.804374933242798\n",
            "Training Iteration 5968, Loss: 5.507691383361816\n",
            "Training Iteration 5969, Loss: 1.5429883003234863\n",
            "Training Iteration 5970, Loss: 3.290100336074829\n",
            "Training Iteration 5971, Loss: 5.366687774658203\n",
            "Training Iteration 5972, Loss: 3.1006853580474854\n",
            "Training Iteration 5973, Loss: 6.807131290435791\n",
            "Training Iteration 5974, Loss: 4.546236038208008\n",
            "Training Iteration 5975, Loss: 1.9484810829162598\n",
            "Training Iteration 5976, Loss: 4.0355305671691895\n",
            "Training Iteration 5977, Loss: 2.3065481185913086\n",
            "Training Iteration 5978, Loss: 9.805259704589844\n",
            "Training Iteration 5979, Loss: 5.568140506744385\n",
            "Training Iteration 5980, Loss: 5.4337382316589355\n",
            "Training Iteration 5981, Loss: 4.671499252319336\n",
            "Training Iteration 5982, Loss: 2.820138692855835\n",
            "Training Iteration 5983, Loss: 5.54788875579834\n",
            "Training Iteration 5984, Loss: 4.338556289672852\n",
            "Training Iteration 5985, Loss: 7.093962669372559\n",
            "Training Iteration 5986, Loss: 8.73620891571045\n",
            "Training Iteration 5987, Loss: 4.240018844604492\n",
            "Training Iteration 5988, Loss: 2.7032577991485596\n",
            "Training Iteration 5989, Loss: 2.61820912361145\n",
            "Training Iteration 5990, Loss: 3.597226142883301\n",
            "Training Iteration 5991, Loss: 3.7021071910858154\n",
            "Training Iteration 5992, Loss: 1.386005163192749\n",
            "Training Iteration 5993, Loss: 8.008073806762695\n",
            "Training Iteration 5994, Loss: 4.784173488616943\n",
            "Training Iteration 5995, Loss: 6.977031230926514\n",
            "Training Iteration 5996, Loss: 7.110821723937988\n",
            "Training Iteration 5997, Loss: 5.2738237380981445\n",
            "Training Iteration 5998, Loss: 3.157383918762207\n",
            "Training Iteration 5999, Loss: 3.8606767654418945\n",
            "Training Iteration 6000, Loss: 2.3577282428741455\n",
            "Training Iteration 6001, Loss: 2.7820751667022705\n",
            "Training Iteration 6002, Loss: 7.675238609313965\n",
            "Training Iteration 6003, Loss: 6.171513557434082\n",
            "Training Iteration 6004, Loss: 4.322237968444824\n",
            "Training Iteration 6005, Loss: 4.516057968139648\n",
            "Training Iteration 6006, Loss: 4.737187385559082\n",
            "Training Iteration 6007, Loss: 4.089498519897461\n",
            "Training Iteration 6008, Loss: 3.9374587535858154\n",
            "Training Iteration 6009, Loss: 7.426722049713135\n",
            "Training Iteration 6010, Loss: 4.309100151062012\n",
            "Training Iteration 6011, Loss: 3.177034378051758\n",
            "Training Iteration 6012, Loss: 4.75079345703125\n",
            "Training Iteration 6013, Loss: 3.6449787616729736\n",
            "Training Iteration 6014, Loss: 4.662703990936279\n",
            "Training Iteration 6015, Loss: 4.033286094665527\n",
            "Training Iteration 6016, Loss: 5.931779384613037\n",
            "Training Iteration 6017, Loss: 4.263628959655762\n",
            "Training Iteration 6018, Loss: 5.154239654541016\n",
            "Training Iteration 6019, Loss: 4.340145587921143\n",
            "Training Iteration 6020, Loss: 3.6422011852264404\n",
            "Training Iteration 6021, Loss: 3.332667589187622\n",
            "Training Iteration 6022, Loss: 3.7817931175231934\n",
            "Training Iteration 6023, Loss: 8.360623359680176\n",
            "Training Iteration 6024, Loss: 6.617545127868652\n",
            "Training Iteration 6025, Loss: 3.3041281700134277\n",
            "Training Iteration 6026, Loss: 5.034115791320801\n",
            "Training Iteration 6027, Loss: 4.916025161743164\n",
            "Training Iteration 6028, Loss: 3.410341262817383\n",
            "Training Iteration 6029, Loss: 2.3391339778900146\n",
            "Training Iteration 6030, Loss: 4.35479736328125\n",
            "Training Iteration 6031, Loss: 4.788332939147949\n",
            "Training Iteration 6032, Loss: 4.376645088195801\n",
            "Training Iteration 6033, Loss: 3.5066094398498535\n",
            "Training Iteration 6034, Loss: 3.380601167678833\n",
            "Training Iteration 6035, Loss: 4.1409125328063965\n",
            "Training Iteration 6036, Loss: 4.275274276733398\n",
            "Training Iteration 6037, Loss: 4.5113420486450195\n",
            "Training Iteration 6038, Loss: 4.243423938751221\n",
            "Training Iteration 6039, Loss: 2.876896381378174\n",
            "Training Iteration 6040, Loss: 3.580636501312256\n",
            "Training Iteration 6041, Loss: 2.6980504989624023\n",
            "Training Iteration 6042, Loss: 4.504095077514648\n",
            "Training Iteration 6043, Loss: 3.418715238571167\n",
            "Training Iteration 6044, Loss: 3.3630383014678955\n",
            "Training Iteration 6045, Loss: 3.656007766723633\n",
            "Training Iteration 6046, Loss: 3.9345335960388184\n",
            "Training Iteration 6047, Loss: 4.821628570556641\n",
            "Training Iteration 6048, Loss: 3.967135190963745\n",
            "Training Iteration 6049, Loss: 5.219799518585205\n",
            "Training Iteration 6050, Loss: 3.3073456287384033\n",
            "Training Iteration 6051, Loss: 3.6506752967834473\n",
            "Training Iteration 6052, Loss: 3.9067881107330322\n",
            "Training Iteration 6053, Loss: 3.901270866394043\n",
            "Training Iteration 6054, Loss: 3.924736499786377\n",
            "Training Iteration 6055, Loss: 4.802267074584961\n",
            "Training Iteration 6056, Loss: 3.483818292617798\n",
            "Training Iteration 6057, Loss: 4.721372604370117\n",
            "Training Iteration 6058, Loss: 3.5238852500915527\n",
            "Training Iteration 6059, Loss: 3.2821218967437744\n",
            "Training Iteration 6060, Loss: 2.294900894165039\n",
            "Training Iteration 6061, Loss: 4.17485237121582\n",
            "Training Iteration 6062, Loss: 4.468973159790039\n",
            "Training Iteration 6063, Loss: 3.057081460952759\n",
            "Training Iteration 6064, Loss: 2.2298789024353027\n",
            "Training Iteration 6065, Loss: 4.021927356719971\n",
            "Training Iteration 6066, Loss: 5.883824348449707\n",
            "Training Iteration 6067, Loss: 1.7502026557922363\n",
            "Training Iteration 6068, Loss: 2.857884407043457\n",
            "Training Iteration 6069, Loss: 7.288232326507568\n",
            "Training Iteration 6070, Loss: 5.573328495025635\n",
            "Training Iteration 6071, Loss: 3.217639207839966\n",
            "Training Iteration 6072, Loss: 3.5072999000549316\n",
            "Training Iteration 6073, Loss: 3.403754234313965\n",
            "Training Iteration 6074, Loss: 2.8799726963043213\n",
            "Training Iteration 6075, Loss: 4.891041278839111\n",
            "Training Iteration 6076, Loss: 2.262321949005127\n",
            "Training Iteration 6077, Loss: 2.9618992805480957\n",
            "Training Iteration 6078, Loss: 5.530592918395996\n",
            "Training Iteration 6079, Loss: 3.7456722259521484\n",
            "Training Iteration 6080, Loss: 1.7487003803253174\n",
            "Training Iteration 6081, Loss: 4.729719161987305\n",
            "Training Iteration 6082, Loss: 5.450308322906494\n",
            "Training Iteration 6083, Loss: 3.6409120559692383\n",
            "Training Iteration 6084, Loss: 4.145885944366455\n",
            "Training Iteration 6085, Loss: 3.93229079246521\n",
            "Training Iteration 6086, Loss: 4.325521469116211\n",
            "Training Iteration 6087, Loss: 4.047738075256348\n",
            "Training Iteration 6088, Loss: 3.448903799057007\n",
            "Training Iteration 6089, Loss: 4.468836307525635\n",
            "Training Iteration 6090, Loss: 2.6066370010375977\n",
            "Training Iteration 6091, Loss: 4.697620868682861\n",
            "Training Iteration 6092, Loss: 4.341322422027588\n",
            "Training Iteration 6093, Loss: 7.05294132232666\n",
            "Training Iteration 6094, Loss: 1.4737948179244995\n",
            "Training Iteration 6095, Loss: 2.965752363204956\n",
            "Training Iteration 6096, Loss: 4.692719459533691\n",
            "Training Iteration 6097, Loss: 2.5254976749420166\n",
            "Training Iteration 6098, Loss: 3.493490695953369\n",
            "Training Iteration 6099, Loss: 5.532832622528076\n",
            "Training Iteration 6100, Loss: 4.446317672729492\n",
            "Training Iteration 6101, Loss: 5.988563060760498\n",
            "Training Iteration 6102, Loss: 4.412982940673828\n",
            "Training Iteration 6103, Loss: 5.046319007873535\n",
            "Training Iteration 6104, Loss: 5.926895618438721\n",
            "Training Iteration 6105, Loss: 2.6853744983673096\n",
            "Training Iteration 6106, Loss: 5.084760665893555\n",
            "Training Iteration 6107, Loss: 2.3788907527923584\n",
            "Training Iteration 6108, Loss: 2.3121871948242188\n",
            "Training Iteration 6109, Loss: 3.8162078857421875\n",
            "Training Iteration 6110, Loss: 8.460972785949707\n",
            "Training Iteration 6111, Loss: 3.833814859390259\n",
            "Training Iteration 6112, Loss: 10.178791046142578\n",
            "Training Iteration 6113, Loss: 5.435598850250244\n",
            "Training Iteration 6114, Loss: 3.6085357666015625\n",
            "Training Iteration 6115, Loss: 8.285425186157227\n",
            "Training Iteration 6116, Loss: 3.773812770843506\n",
            "Training Iteration 6117, Loss: 4.014476299285889\n",
            "Training Iteration 6118, Loss: 7.112922668457031\n",
            "Training Iteration 6119, Loss: 10.446310997009277\n",
            "Training Iteration 6120, Loss: 3.492806911468506\n",
            "Training Iteration 6121, Loss: 4.174606800079346\n",
            "Training Iteration 6122, Loss: 5.904068946838379\n",
            "Training Iteration 6123, Loss: 5.072327613830566\n",
            "Training Iteration 6124, Loss: 4.437732219696045\n",
            "Training Iteration 6125, Loss: 5.011271953582764\n",
            "Training Iteration 6126, Loss: 5.9229912757873535\n",
            "Training Iteration 6127, Loss: 6.29681396484375\n",
            "Training Iteration 6128, Loss: 3.3032338619232178\n",
            "Training Iteration 6129, Loss: 2.6429712772369385\n",
            "Training Iteration 6130, Loss: 5.676201343536377\n",
            "Training Iteration 6131, Loss: 3.0698392391204834\n",
            "Training Iteration 6132, Loss: 3.9960310459136963\n",
            "Training Iteration 6133, Loss: 4.519418239593506\n",
            "Training Iteration 6134, Loss: 5.069453239440918\n",
            "Training Iteration 6135, Loss: 4.919265270233154\n",
            "Training Iteration 6136, Loss: 2.6103579998016357\n",
            "Training Iteration 6137, Loss: 2.340010166168213\n",
            "Training Iteration 6138, Loss: 3.0744690895080566\n",
            "Training Iteration 6139, Loss: 3.797316312789917\n",
            "Training Iteration 6140, Loss: 4.923686981201172\n",
            "Training Iteration 6141, Loss: 5.619563102722168\n",
            "Training Iteration 6142, Loss: 2.180732250213623\n",
            "Training Iteration 6143, Loss: 4.537808418273926\n",
            "Training Iteration 6144, Loss: 4.433877944946289\n",
            "Training Iteration 6145, Loss: 4.167924880981445\n",
            "Training Iteration 6146, Loss: 2.764045238494873\n",
            "Training Iteration 6147, Loss: 5.198162078857422\n",
            "Training Iteration 6148, Loss: 4.327505111694336\n",
            "Training Iteration 6149, Loss: 3.686471700668335\n",
            "Training Iteration 6150, Loss: 3.0324296951293945\n",
            "Training Iteration 6151, Loss: 5.930433750152588\n",
            "Training Iteration 6152, Loss: 3.2479705810546875\n",
            "Training Iteration 6153, Loss: 3.9301042556762695\n",
            "Training Iteration 6154, Loss: 5.840670585632324\n",
            "Training Iteration 6155, Loss: 3.029249429702759\n",
            "Training Iteration 6156, Loss: 6.46364688873291\n",
            "Training Iteration 6157, Loss: 4.8294806480407715\n",
            "Training Iteration 6158, Loss: 2.6088356971740723\n",
            "Training Iteration 6159, Loss: 3.155553102493286\n",
            "Training Iteration 6160, Loss: 4.816843509674072\n",
            "Training Iteration 6161, Loss: 4.2880401611328125\n",
            "Training Iteration 6162, Loss: 6.316190242767334\n",
            "Training Iteration 6163, Loss: 5.5217742919921875\n",
            "Training Iteration 6164, Loss: 3.7599151134490967\n",
            "Training Iteration 6165, Loss: 3.229491949081421\n",
            "Training Iteration 6166, Loss: 4.958662986755371\n",
            "Training Iteration 6167, Loss: 5.716118812561035\n",
            "Training Iteration 6168, Loss: 3.8026156425476074\n",
            "Training Iteration 6169, Loss: 5.879419803619385\n",
            "Training Iteration 6170, Loss: 4.971546649932861\n",
            "Training Iteration 6171, Loss: 3.07600474357605\n",
            "Training Iteration 6172, Loss: 5.373545169830322\n",
            "Training Iteration 6173, Loss: 5.823904991149902\n",
            "Training Iteration 6174, Loss: 3.7464871406555176\n",
            "Training Iteration 6175, Loss: 3.575402021408081\n",
            "Training Iteration 6176, Loss: 4.04380464553833\n",
            "Training Iteration 6177, Loss: 3.5804147720336914\n",
            "Training Iteration 6178, Loss: 3.585142135620117\n",
            "Training Iteration 6179, Loss: 6.187867641448975\n",
            "Training Iteration 6180, Loss: 5.994628429412842\n",
            "Training Iteration 6181, Loss: 4.559586048126221\n",
            "Training Iteration 6182, Loss: 4.215018272399902\n",
            "Training Iteration 6183, Loss: 2.21760630607605\n",
            "Training Iteration 6184, Loss: 7.31078577041626\n",
            "Training Iteration 6185, Loss: 5.075878620147705\n",
            "Training Iteration 6186, Loss: 4.793997764587402\n",
            "Training Iteration 6187, Loss: 4.182680130004883\n",
            "Training Iteration 6188, Loss: 3.5458409786224365\n",
            "Training Iteration 6189, Loss: 2.959819793701172\n",
            "Training Iteration 6190, Loss: 6.926542282104492\n",
            "Training Iteration 6191, Loss: 5.36259651184082\n",
            "Training Iteration 6192, Loss: 5.035823345184326\n",
            "Training Iteration 6193, Loss: 4.064554691314697\n",
            "Training Iteration 6194, Loss: 5.956943511962891\n",
            "Training Iteration 6195, Loss: 3.965872049331665\n",
            "Training Iteration 6196, Loss: 2.744540214538574\n",
            "Training Iteration 6197, Loss: 3.7663962841033936\n",
            "Training Iteration 6198, Loss: 5.811670303344727\n",
            "Training Iteration 6199, Loss: 5.871748447418213\n",
            "Training Iteration 6200, Loss: 4.265422821044922\n",
            "Training Iteration 6201, Loss: 3.874418258666992\n",
            "Training Iteration 6202, Loss: 7.629174709320068\n",
            "Training Iteration 6203, Loss: 7.55705451965332\n",
            "Training Iteration 6204, Loss: 2.8757076263427734\n",
            "Training Iteration 6205, Loss: 4.915318489074707\n",
            "Training Iteration 6206, Loss: 5.73627233505249\n",
            "Training Iteration 6207, Loss: 5.775206565856934\n",
            "Training Iteration 6208, Loss: 1.832865595817566\n",
            "Training Iteration 6209, Loss: 4.64409875869751\n",
            "Training Iteration 6210, Loss: 5.36417293548584\n",
            "Training Iteration 6211, Loss: 4.039991855621338\n",
            "Training Iteration 6212, Loss: 2.2057950496673584\n",
            "Training Iteration 6213, Loss: 6.071753978729248\n",
            "Training Iteration 6214, Loss: 4.576806545257568\n",
            "Training Iteration 6215, Loss: 3.989084005355835\n",
            "Training Iteration 6216, Loss: 6.77006196975708\n",
            "Training Iteration 6217, Loss: 9.443819046020508\n",
            "Training Iteration 6218, Loss: 5.743760108947754\n",
            "Training Iteration 6219, Loss: 5.017151832580566\n",
            "Training Iteration 6220, Loss: 2.796611785888672\n",
            "Training Iteration 6221, Loss: 2.1864914894104004\n",
            "Training Iteration 6222, Loss: 3.5772905349731445\n",
            "Training Iteration 6223, Loss: 3.877183198928833\n",
            "Training Iteration 6224, Loss: 6.465771675109863\n",
            "Training Iteration 6225, Loss: 3.7653651237487793\n",
            "Training Iteration 6226, Loss: 2.9224979877471924\n",
            "Training Iteration 6227, Loss: 4.087116718292236\n",
            "Training Iteration 6228, Loss: 3.432152509689331\n",
            "Training Iteration 6229, Loss: 3.057772636413574\n",
            "Training Iteration 6230, Loss: 5.46368932723999\n",
            "Training Iteration 6231, Loss: 7.997992992401123\n",
            "Training Iteration 6232, Loss: 4.639325141906738\n",
            "Training Iteration 6233, Loss: 4.8870649337768555\n",
            "Training Iteration 6234, Loss: 1.5793393850326538\n",
            "Training Iteration 6235, Loss: 5.083041667938232\n",
            "Training Iteration 6236, Loss: 4.303614616394043\n",
            "Training Iteration 6237, Loss: 5.899401664733887\n",
            "Training Iteration 6238, Loss: 3.331883430480957\n",
            "Training Iteration 6239, Loss: 3.9620981216430664\n",
            "Training Iteration 6240, Loss: 5.215627193450928\n",
            "Training Iteration 6241, Loss: 3.7421960830688477\n",
            "Training Iteration 6242, Loss: 4.769123077392578\n",
            "Training Iteration 6243, Loss: 5.172029495239258\n",
            "Training Iteration 6244, Loss: 5.405107021331787\n",
            "Training Iteration 6245, Loss: 5.792788505554199\n",
            "Training Iteration 6246, Loss: 6.357912540435791\n",
            "Training Iteration 6247, Loss: 4.358150005340576\n",
            "Training Iteration 6248, Loss: 7.280799865722656\n",
            "Training Iteration 6249, Loss: 2.5265157222747803\n",
            "Training Iteration 6250, Loss: 6.218533039093018\n",
            "Training Iteration 6251, Loss: 6.525547027587891\n",
            "Training Iteration 6252, Loss: 5.61628532409668\n",
            "Training Iteration 6253, Loss: 9.154037475585938\n",
            "Training Iteration 6254, Loss: 5.531017780303955\n",
            "Training Iteration 6255, Loss: 8.01430892944336\n",
            "Training Iteration 6256, Loss: 5.403014659881592\n",
            "Training Iteration 6257, Loss: 2.873676300048828\n",
            "Training Iteration 6258, Loss: 4.083292007446289\n",
            "Training Iteration 6259, Loss: 2.1735472679138184\n",
            "Training Iteration 6260, Loss: 8.04621410369873\n",
            "Training Iteration 6261, Loss: 5.760307312011719\n",
            "Training Iteration 6262, Loss: 2.7087292671203613\n",
            "Training Iteration 6263, Loss: 4.01947021484375\n",
            "Training Iteration 6264, Loss: 4.654134273529053\n",
            "Training Iteration 6265, Loss: 6.572170257568359\n",
            "Training Iteration 6266, Loss: 5.46684455871582\n",
            "Training Iteration 6267, Loss: 5.952998161315918\n",
            "Training Iteration 6268, Loss: 5.002557277679443\n",
            "Training Iteration 6269, Loss: 8.267868041992188\n",
            "Training Iteration 6270, Loss: 4.496606826782227\n",
            "Training Iteration 6271, Loss: 5.953093528747559\n",
            "Training Iteration 6272, Loss: 8.763458251953125\n",
            "Training Iteration 6273, Loss: 6.728021621704102\n",
            "Training Iteration 6274, Loss: 3.8594610691070557\n",
            "Training Iteration 6275, Loss: 7.239583492279053\n",
            "Training Iteration 6276, Loss: 5.984444618225098\n",
            "Training Iteration 6277, Loss: 6.849013805389404\n",
            "Training Iteration 6278, Loss: 3.415874719619751\n",
            "Training Iteration 6279, Loss: 3.502992630004883\n",
            "Training Iteration 6280, Loss: 2.0526113510131836\n",
            "Training Iteration 6281, Loss: 5.926714897155762\n",
            "Training Iteration 6282, Loss: 3.065631866455078\n",
            "Training Iteration 6283, Loss: 3.525254011154175\n",
            "Training Iteration 6284, Loss: 6.860973834991455\n",
            "Training Iteration 6285, Loss: 2.8099982738494873\n",
            "Training Iteration 6286, Loss: 2.1962451934814453\n",
            "Training Iteration 6287, Loss: 3.362917423248291\n",
            "Training Iteration 6288, Loss: 2.516416072845459\n",
            "Training Iteration 6289, Loss: 2.9607043266296387\n",
            "Training Iteration 6290, Loss: 5.691424369812012\n",
            "Training Iteration 6291, Loss: 4.26496696472168\n",
            "Training Iteration 6292, Loss: 2.8439621925354004\n",
            "Training Iteration 6293, Loss: 4.036452293395996\n",
            "Training Iteration 6294, Loss: 1.8307366371154785\n",
            "Training Iteration 6295, Loss: 3.8551464080810547\n",
            "Training Iteration 6296, Loss: 3.9319510459899902\n",
            "Training Iteration 6297, Loss: 2.481875419616699\n",
            "Training Iteration 6298, Loss: 4.6513166427612305\n",
            "Training Iteration 6299, Loss: 8.076044082641602\n",
            "Training Iteration 6300, Loss: 4.741669178009033\n",
            "Training Iteration 6301, Loss: 2.837947130203247\n",
            "Training Iteration 6302, Loss: 5.2801432609558105\n",
            "Training Iteration 6303, Loss: 11.069475173950195\n",
            "Training Iteration 6304, Loss: 3.6223435401916504\n",
            "Training Iteration 6305, Loss: 4.125366687774658\n",
            "Training Iteration 6306, Loss: 2.5008625984191895\n",
            "Training Iteration 6307, Loss: 4.295004844665527\n",
            "Training Iteration 6308, Loss: 2.701909303665161\n",
            "Training Iteration 6309, Loss: 2.863724946975708\n",
            "Training Iteration 6310, Loss: 3.148817300796509\n",
            "Training Iteration 6311, Loss: 5.360241889953613\n",
            "Training Iteration 6312, Loss: 2.0278234481811523\n",
            "Training Iteration 6313, Loss: 5.070915222167969\n",
            "Training Iteration 6314, Loss: 6.330628395080566\n",
            "Training Iteration 6315, Loss: 5.9457221031188965\n",
            "Training Iteration 6316, Loss: 8.182836532592773\n",
            "Training Iteration 6317, Loss: 3.5922958850860596\n",
            "Training Iteration 6318, Loss: 4.9503092765808105\n",
            "Training Iteration 6319, Loss: 3.6717498302459717\n",
            "Training Iteration 6320, Loss: 4.1824951171875\n",
            "Training Iteration 6321, Loss: 5.020318031311035\n",
            "Training Iteration 6322, Loss: 4.899769306182861\n",
            "Training Iteration 6323, Loss: 3.409762382507324\n",
            "Training Iteration 6324, Loss: 5.419250011444092\n",
            "Training Iteration 6325, Loss: 2.9003360271453857\n",
            "Training Iteration 6326, Loss: 7.1774749755859375\n",
            "Training Iteration 6327, Loss: 5.445488452911377\n",
            "Training Iteration 6328, Loss: 2.8787076473236084\n",
            "Training Iteration 6329, Loss: 6.418565273284912\n",
            "Training Iteration 6330, Loss: 4.4730939865112305\n",
            "Training Iteration 6331, Loss: 7.53569221496582\n",
            "Training Iteration 6332, Loss: 3.435842990875244\n",
            "Training Iteration 6333, Loss: 1.6316144466400146\n",
            "Training Iteration 6334, Loss: 3.325547218322754\n",
            "Training Iteration 6335, Loss: 4.057692050933838\n",
            "Training Iteration 6336, Loss: 3.3959555625915527\n",
            "Training Iteration 6337, Loss: 2.060206651687622\n",
            "Training Iteration 6338, Loss: 3.106505870819092\n",
            "Training Iteration 6339, Loss: 4.73657751083374\n",
            "Training Iteration 6340, Loss: 1.970549464225769\n",
            "Training Iteration 6341, Loss: 3.9312963485717773\n",
            "Training Iteration 6342, Loss: 3.552388906478882\n",
            "Training Iteration 6343, Loss: 4.985485076904297\n",
            "Training Iteration 6344, Loss: 1.893870234489441\n",
            "Training Iteration 6345, Loss: 4.53067684173584\n",
            "Training Iteration 6346, Loss: 4.637711524963379\n",
            "Training Iteration 6347, Loss: 4.541782379150391\n",
            "Training Iteration 6348, Loss: 2.513378381729126\n",
            "Training Iteration 6349, Loss: 3.9747488498687744\n",
            "Training Iteration 6350, Loss: 5.864968299865723\n",
            "Training Iteration 6351, Loss: 4.925957202911377\n",
            "Training Iteration 6352, Loss: 2.574723243713379\n",
            "Training Iteration 6353, Loss: 4.798679351806641\n",
            "Training Iteration 6354, Loss: 4.287412166595459\n",
            "Training Iteration 6355, Loss: 2.6739234924316406\n",
            "Training Iteration 6356, Loss: 6.312912464141846\n",
            "Training Iteration 6357, Loss: 2.4603734016418457\n",
            "Training Iteration 6358, Loss: 2.825763463973999\n",
            "Training Iteration 6359, Loss: 2.8453621864318848\n",
            "Training Iteration 6360, Loss: 4.52726936340332\n",
            "Training Iteration 6361, Loss: 3.6892802715301514\n",
            "Training Iteration 6362, Loss: 3.10149884223938\n",
            "Training Iteration 6363, Loss: 2.2401862144470215\n",
            "Training Iteration 6364, Loss: 2.9693844318389893\n",
            "Training Iteration 6365, Loss: 2.625753402709961\n",
            "Training Iteration 6366, Loss: 4.211931228637695\n",
            "Training Iteration 6367, Loss: 6.305582046508789\n",
            "Training Iteration 6368, Loss: 6.185474872589111\n",
            "Training Iteration 6369, Loss: 9.35075569152832\n",
            "Training Iteration 6370, Loss: 4.58914852142334\n",
            "Training Iteration 6371, Loss: 5.993943214416504\n",
            "Training Iteration 6372, Loss: 3.5839390754699707\n",
            "Training Iteration 6373, Loss: 7.032909870147705\n",
            "Training Iteration 6374, Loss: 3.8054990768432617\n",
            "Training Iteration 6375, Loss: 4.426817417144775\n",
            "Training Iteration 6376, Loss: 5.585186004638672\n",
            "Training Iteration 6377, Loss: 2.931835889816284\n",
            "Training Iteration 6378, Loss: 6.211121082305908\n",
            "Training Iteration 6379, Loss: 3.5577259063720703\n",
            "Training Iteration 6380, Loss: 4.400635242462158\n",
            "Training Iteration 6381, Loss: 7.306089401245117\n",
            "Training Iteration 6382, Loss: 6.010461807250977\n",
            "Training Iteration 6383, Loss: 3.752031087875366\n",
            "Training Iteration 6384, Loss: 4.7293171882629395\n",
            "Training Iteration 6385, Loss: 5.897145748138428\n",
            "Training Iteration 6386, Loss: 5.429623126983643\n",
            "Training Iteration 6387, Loss: 2.9845352172851562\n",
            "Training Iteration 6388, Loss: 3.2813212871551514\n",
            "Training Iteration 6389, Loss: 3.887166738510132\n",
            "Training Iteration 6390, Loss: 4.1575446128845215\n",
            "Training Iteration 6391, Loss: 3.463707447052002\n",
            "Training Iteration 6392, Loss: 5.784510612487793\n",
            "Training Iteration 6393, Loss: 4.258232593536377\n",
            "Training Iteration 6394, Loss: 4.658797740936279\n",
            "Training Iteration 6395, Loss: 3.1506588459014893\n",
            "Training Iteration 6396, Loss: 3.972517490386963\n",
            "Training Iteration 6397, Loss: 3.4262537956237793\n",
            "Training Iteration 6398, Loss: 3.0835394859313965\n",
            "Training Iteration 6399, Loss: 4.265524864196777\n",
            "Training Iteration 6400, Loss: 7.199929237365723\n",
            "Training Iteration 6401, Loss: 4.370760917663574\n",
            "Training Iteration 6402, Loss: 5.431910514831543\n",
            "Training Iteration 6403, Loss: 4.186733722686768\n",
            "Training Iteration 6404, Loss: 2.7574946880340576\n",
            "Training Iteration 6405, Loss: 3.82894229888916\n",
            "Training Iteration 6406, Loss: 4.078521728515625\n",
            "Training Iteration 6407, Loss: 2.81557297706604\n",
            "Training Iteration 6408, Loss: 9.128471374511719\n",
            "Training Iteration 6409, Loss: 5.653001308441162\n",
            "Training Iteration 6410, Loss: 6.731441497802734\n",
            "Training Iteration 6411, Loss: 4.5128560066223145\n",
            "Training Iteration 6412, Loss: 3.8917739391326904\n",
            "Training Iteration 6413, Loss: 7.766970634460449\n",
            "Training Iteration 6414, Loss: 11.284623146057129\n",
            "Training Iteration 6415, Loss: 4.559125900268555\n",
            "Training Iteration 6416, Loss: 7.899849891662598\n",
            "Training Iteration 6417, Loss: 4.103771209716797\n",
            "Training Iteration 6418, Loss: 4.7687602043151855\n",
            "Training Iteration 6419, Loss: 3.16507625579834\n",
            "Training Iteration 6420, Loss: 3.2304282188415527\n",
            "Training Iteration 6421, Loss: 2.936546802520752\n",
            "Training Iteration 6422, Loss: 4.515732765197754\n",
            "Training Iteration 6423, Loss: 6.875360012054443\n",
            "Training Iteration 6424, Loss: 3.5455737113952637\n",
            "Training Iteration 6425, Loss: 6.683399200439453\n",
            "Training Iteration 6426, Loss: 8.107161521911621\n",
            "Training Iteration 6427, Loss: 3.305983066558838\n",
            "Training Iteration 6428, Loss: 5.395179271697998\n",
            "Training Iteration 6429, Loss: 6.252748489379883\n",
            "Training Iteration 6430, Loss: 5.169825553894043\n",
            "Training Iteration 6431, Loss: 5.437083721160889\n",
            "Training Iteration 6432, Loss: 7.214327812194824\n",
            "Training Iteration 6433, Loss: 4.835666656494141\n",
            "Training Iteration 6434, Loss: 2.3046770095825195\n",
            "Training Iteration 6435, Loss: 5.136153221130371\n",
            "Training Iteration 6436, Loss: 3.2003700733184814\n",
            "Training Iteration 6437, Loss: 5.730755805969238\n",
            "Training Iteration 6438, Loss: 6.7015228271484375\n",
            "Training Iteration 6439, Loss: 2.922726631164551\n",
            "Training Iteration 6440, Loss: 9.68669605255127\n",
            "Training Iteration 6441, Loss: 4.64418888092041\n",
            "Training Iteration 6442, Loss: 3.3631279468536377\n",
            "Training Iteration 6443, Loss: 3.32161545753479\n",
            "Training Iteration 6444, Loss: 5.407718658447266\n",
            "Training Iteration 6445, Loss: 5.785966396331787\n",
            "Training Iteration 6446, Loss: 5.7805399894714355\n",
            "Training Iteration 6447, Loss: 3.5144920349121094\n",
            "Training Iteration 6448, Loss: 6.255852699279785\n",
            "Training Iteration 6449, Loss: 6.52454137802124\n",
            "Training Iteration 6450, Loss: 4.790954113006592\n",
            "Training Iteration 6451, Loss: 5.885512828826904\n",
            "Training Iteration 6452, Loss: 4.569683074951172\n",
            "Training Iteration 6453, Loss: 1.7690322399139404\n",
            "Training Iteration 6454, Loss: 4.891976356506348\n",
            "Training Iteration 6455, Loss: 3.1271347999572754\n",
            "Training Iteration 6456, Loss: 3.5771946907043457\n",
            "Training Iteration 6457, Loss: 2.0812695026397705\n",
            "Training Iteration 6458, Loss: 3.391643762588501\n",
            "Training Iteration 6459, Loss: 3.356691837310791\n",
            "Training Iteration 6460, Loss: 3.7181320190429688\n",
            "Training Iteration 6461, Loss: 3.4548473358154297\n",
            "Training Iteration 6462, Loss: 3.430619478225708\n",
            "Training Iteration 6463, Loss: 4.851202011108398\n",
            "Training Iteration 6464, Loss: 3.7660505771636963\n",
            "Training Iteration 6465, Loss: 3.66536021232605\n",
            "Training Iteration 6466, Loss: 2.019226312637329\n",
            "Training Iteration 6467, Loss: 1.8897621631622314\n",
            "Training Iteration 6468, Loss: 2.6400585174560547\n",
            "Training Iteration 6469, Loss: 4.546520233154297\n",
            "Training Iteration 6470, Loss: 4.092275142669678\n",
            "Training Iteration 6471, Loss: 4.021957874298096\n",
            "Training Iteration 6472, Loss: 5.318655014038086\n",
            "Training Iteration 6473, Loss: 2.348900318145752\n",
            "Training Iteration 6474, Loss: 4.318855285644531\n",
            "Training Iteration 6475, Loss: 2.0769877433776855\n",
            "Training Iteration 6476, Loss: 4.588782787322998\n",
            "Training Iteration 6477, Loss: 2.006747007369995\n",
            "Training Iteration 6478, Loss: 3.388584613800049\n",
            "Training Iteration 6479, Loss: 2.111297130584717\n",
            "Training Iteration 6480, Loss: 2.1939358711242676\n",
            "Training Iteration 6481, Loss: 2.9699461460113525\n",
            "Training Iteration 6482, Loss: 1.8638404607772827\n",
            "Training Iteration 6483, Loss: 6.791624546051025\n",
            "Training Iteration 6484, Loss: 4.435138702392578\n",
            "Training Iteration 6485, Loss: 3.0267415046691895\n",
            "Training Iteration 6486, Loss: 4.558911323547363\n",
            "Training Iteration 6487, Loss: 2.925548553466797\n",
            "Training Iteration 6488, Loss: 3.2934255599975586\n",
            "Training Iteration 6489, Loss: 3.077300548553467\n",
            "Training Iteration 6490, Loss: 3.757139205932617\n",
            "Training Iteration 6491, Loss: 2.6586661338806152\n",
            "Training Iteration 6492, Loss: 4.904062747955322\n",
            "Training Iteration 6493, Loss: 2.559556007385254\n",
            "Training Iteration 6494, Loss: 1.3152451515197754\n",
            "Training Iteration 6495, Loss: 4.798067092895508\n",
            "Training Iteration 6496, Loss: 4.228719711303711\n",
            "Training Iteration 6497, Loss: 2.532522678375244\n",
            "Training Iteration 6498, Loss: 2.2526917457580566\n",
            "Training Iteration 6499, Loss: 3.7843706607818604\n",
            "Training Iteration 6500, Loss: 8.197592735290527\n",
            "Training Iteration 6501, Loss: 4.448121070861816\n",
            "Training Iteration 6502, Loss: 4.593539714813232\n",
            "Training Iteration 6503, Loss: 4.68949556350708\n",
            "Training Iteration 6504, Loss: 4.136711120605469\n",
            "Training Iteration 6505, Loss: 3.950246810913086\n",
            "Training Iteration 6506, Loss: 6.488433361053467\n",
            "Training Iteration 6507, Loss: 4.6369309425354\n",
            "Training Iteration 6508, Loss: 4.434143543243408\n",
            "Training Iteration 6509, Loss: 3.5207576751708984\n",
            "Training Iteration 6510, Loss: 4.95838737487793\n",
            "Training Iteration 6511, Loss: 4.059840679168701\n",
            "Training Iteration 6512, Loss: 3.0909299850463867\n",
            "Training Iteration 6513, Loss: 4.2032599449157715\n",
            "Training Iteration 6514, Loss: 6.357443809509277\n",
            "Training Iteration 6515, Loss: 6.107774257659912\n",
            "Training Iteration 6516, Loss: 3.201148509979248\n",
            "Training Iteration 6517, Loss: 5.157080173492432\n",
            "Training Iteration 6518, Loss: 9.07846736907959\n",
            "Training Iteration 6519, Loss: 3.519519805908203\n",
            "Training Iteration 6520, Loss: 5.548656463623047\n",
            "Training Iteration 6521, Loss: 4.054067611694336\n",
            "Training Iteration 6522, Loss: 4.016260147094727\n",
            "Training Iteration 6523, Loss: 13.10344409942627\n",
            "Training Iteration 6524, Loss: 3.0847368240356445\n",
            "Training Iteration 6525, Loss: 5.966909408569336\n",
            "Training Iteration 6526, Loss: 5.625991344451904\n",
            "Training Iteration 6527, Loss: 6.706752300262451\n",
            "Training Iteration 6528, Loss: 2.8531155586242676\n",
            "Training Iteration 6529, Loss: 3.5075597763061523\n",
            "Training Iteration 6530, Loss: 4.365482330322266\n",
            "Training Iteration 6531, Loss: 3.878661632537842\n",
            "Training Iteration 6532, Loss: 5.857462406158447\n",
            "Training Iteration 6533, Loss: 2.7865967750549316\n",
            "Training Iteration 6534, Loss: 3.2407429218292236\n",
            "Training Iteration 6535, Loss: 3.9847371578216553\n",
            "Training Iteration 6536, Loss: 4.632542610168457\n",
            "Training Iteration 6537, Loss: 2.2911345958709717\n",
            "Training Iteration 6538, Loss: 2.835252046585083\n",
            "Training Iteration 6539, Loss: 2.2866618633270264\n",
            "Training Iteration 6540, Loss: 1.3338282108306885\n",
            "Training Iteration 6541, Loss: 3.9916622638702393\n",
            "Training Iteration 6542, Loss: 3.9175875186920166\n",
            "Training Iteration 6543, Loss: 3.3531486988067627\n",
            "Training Iteration 6544, Loss: 2.030707597732544\n",
            "Training Iteration 6545, Loss: 5.084667205810547\n",
            "Training Iteration 6546, Loss: 6.5352325439453125\n",
            "Training Iteration 6547, Loss: 2.878021001815796\n",
            "Training Iteration 6548, Loss: 2.6614279747009277\n",
            "Training Iteration 6549, Loss: 4.562858581542969\n",
            "Training Iteration 6550, Loss: 2.8418667316436768\n",
            "Training Iteration 6551, Loss: 4.59722900390625\n",
            "Training Iteration 6552, Loss: 5.181612014770508\n",
            "Training Iteration 6553, Loss: 6.615753173828125\n",
            "Training Iteration 6554, Loss: 11.787436485290527\n",
            "tensor([[2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        ...,\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09]])\n",
            "Training loss for epcoh 2: 4.7430394720951154\n",
            "Training accuracy for epoch 2: 0.2823789722656697\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        ...,\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09],\n",
            "        [2.6672e-03, 9.6276e-01, 9.5251e-04, 2.5824e-03, 3.1040e-02, 2.7052e-09]])\n",
            "Validation loss for epcoh 2: 4.789524911681805\n",
            "Test accuracy for epoch 2: 0.2780956740672923\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddfadc0336c0498f8b3338fab7e72635",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch No: 3:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Iteration 1, Loss: 3.1005685329437256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 5.157634258270264\n",
            "Training Iteration 1565, Loss: 2.6242334842681885\n",
            "Training Iteration 1566, Loss: 2.732616901397705\n",
            "Training Iteration 1567, Loss: 3.546945095062256\n",
            "Training Iteration 1568, Loss: 2.845391273498535\n",
            "Training Iteration 1569, Loss: 8.103182792663574\n",
            "Training Iteration 1570, Loss: 4.622302532196045\n",
            "Training Iteration 1571, Loss: 3.2648797035217285\n",
            "Training Iteration 1572, Loss: 5.331798076629639\n",
            "Training Iteration 1573, Loss: 4.076218128204346\n",
            "Training Iteration 1574, Loss: 2.3684897422790527\n",
            "Training Iteration 1575, Loss: 2.8101160526275635\n",
            "Training Iteration 1576, Loss: 8.587151527404785\n",
            "Training Iteration 1577, Loss: 7.538923740386963\n",
            "Training Iteration 1578, Loss: 2.804828405380249\n",
            "Training Iteration 1579, Loss: 4.743838787078857\n",
            "Training Iteration 1580, Loss: 2.4783055782318115\n",
            "Training Iteration 1581, Loss: 3.9186031818389893\n",
            "Training Iteration 1582, Loss: 1.6411851644515991\n",
            "Training Iteration 1583, Loss: 3.214083671569824\n",
            "Training Iteration 1584, Loss: 4.359800815582275\n",
            "Training Iteration 1585, Loss: 2.9240875244140625\n",
            "Training Iteration 1586, Loss: 3.7839672565460205\n",
            "Training Iteration 1587, Loss: 5.16213321685791\n",
            "Training Iteration 1588, Loss: 3.4066720008850098\n",
            "Training Iteration 1589, Loss: 4.870126247406006\n",
            "Training Iteration 1590, Loss: 5.697110176086426\n",
            "Training Iteration 1591, Loss: 4.916479587554932\n",
            "Training Iteration 1592, Loss: 4.527153015136719\n",
            "Training Iteration 1593, Loss: 6.243556022644043\n",
            "Training Iteration 1594, Loss: 4.649570941925049\n",
            "Training Iteration 1595, Loss: 2.921898365020752\n",
            "Training Iteration 1596, Loss: 6.6942901611328125\n",
            "Training Iteration 1597, Loss: 2.473234176635742\n",
            "Training Iteration 1598, Loss: 4.6253509521484375\n",
            "Training Iteration 1599, Loss: 6.145235061645508\n",
            "Training Iteration 1600, Loss: 7.415217399597168\n",
            "Training Iteration 1601, Loss: 6.883504390716553\n",
            "Training Iteration 1602, Loss: 7.245786190032959\n",
            "Training Iteration 1603, Loss: 6.7810893058776855\n",
            "Training Iteration 1604, Loss: 2.896730422973633\n",
            "Training Iteration 1605, Loss: 4.8092427253723145\n",
            "Training Iteration 1606, Loss: 4.217777729034424\n",
            "Training Iteration 1607, Loss: 4.219321250915527\n",
            "Training Iteration 1608, Loss: 5.091197490692139\n",
            "Training Iteration 1609, Loss: 4.0984296798706055\n",
            "Training Iteration 1610, Loss: 4.70607328414917\n",
            "Training Iteration 1611, Loss: 3.3346080780029297\n",
            "Training Iteration 1612, Loss: 1.9901727437973022\n",
            "Training Iteration 1613, Loss: 3.7784695625305176\n",
            "Training Iteration 1614, Loss: 4.784679412841797\n",
            "Training Iteration 1615, Loss: 6.224483013153076\n",
            "Training Iteration 1616, Loss: 5.278407096862793\n",
            "Training Iteration 1617, Loss: 5.943629264831543\n",
            "Training Iteration 1618, Loss: 4.168257236480713\n",
            "Training Iteration 1619, Loss: 3.2038300037384033\n",
            "Training Iteration 1620, Loss: 4.145720481872559\n",
            "Training Iteration 1621, Loss: 5.273541450500488\n",
            "Training Iteration 1622, Loss: 6.514644622802734\n",
            "Training Iteration 1623, Loss: 3.162952423095703\n",
            "Training Iteration 1624, Loss: 3.7984602451324463\n",
            "Training Iteration 1625, Loss: 3.334228992462158\n",
            "Training Iteration 1626, Loss: 6.919763088226318\n",
            "Training Iteration 1627, Loss: 2.9246747493743896\n",
            "Training Iteration 1628, Loss: 5.125084400177002\n",
            "Training Iteration 1629, Loss: 3.7741096019744873\n",
            "Training Iteration 1630, Loss: 5.045046806335449\n",
            "Training Iteration 1631, Loss: 4.063076019287109\n",
            "Training Iteration 1632, Loss: 2.5217268466949463\n",
            "Training Iteration 1633, Loss: 5.380510330200195\n",
            "Training Iteration 1634, Loss: 4.829814910888672\n",
            "Training Iteration 1635, Loss: 8.68415641784668\n",
            "Training Iteration 1636, Loss: 3.986696481704712\n",
            "Training Iteration 1637, Loss: 4.473751544952393\n",
            "Training Iteration 1638, Loss: 2.5206196308135986\n",
            "Training Iteration 1639, Loss: 4.663497447967529\n",
            "Training Iteration 1640, Loss: 5.376447677612305\n",
            "Training Iteration 1641, Loss: 8.58557415008545\n",
            "Training Iteration 1642, Loss: 4.773126602172852\n",
            "Training Iteration 1643, Loss: 6.035699367523193\n",
            "Training Iteration 1644, Loss: 3.4496963024139404\n",
            "Training Iteration 1645, Loss: 4.205976963043213\n",
            "Training Iteration 1646, Loss: 2.968353748321533\n",
            "Training Iteration 1647, Loss: 2.925232172012329\n",
            "Training Iteration 1648, Loss: 3.1857261657714844\n",
            "Training Iteration 1649, Loss: 2.197572946548462\n",
            "Training Iteration 1650, Loss: 4.662716865539551\n",
            "Training Iteration 1651, Loss: 4.993093490600586\n",
            "Training Iteration 1652, Loss: 6.937461853027344\n",
            "Training Iteration 1653, Loss: 5.284982681274414\n",
            "Training Iteration 1654, Loss: 4.444000720977783\n",
            "Training Iteration 1655, Loss: 6.955661773681641\n",
            "Training Iteration 1656, Loss: 3.0545108318328857\n",
            "Training Iteration 1657, Loss: 4.739384174346924\n",
            "Training Iteration 1658, Loss: 5.322612762451172\n",
            "Training Iteration 1659, Loss: 4.690067768096924\n",
            "Training Iteration 1660, Loss: 4.436535358428955\n",
            "Training Iteration 1661, Loss: 5.977994441986084\n",
            "Training Iteration 1662, Loss: 7.327848434448242\n",
            "Training Iteration 1663, Loss: 3.023913621902466\n",
            "Training Iteration 1664, Loss: 2.7463998794555664\n",
            "Training Iteration 1665, Loss: 2.5901243686676025\n",
            "Training Iteration 1666, Loss: 3.7634854316711426\n",
            "Training Iteration 1667, Loss: 5.8639678955078125\n",
            "Training Iteration 1668, Loss: 6.517532825469971\n",
            "Training Iteration 1669, Loss: 6.240473747253418\n",
            "Training Iteration 1670, Loss: 3.3460066318511963\n",
            "Training Iteration 1671, Loss: 3.070164680480957\n",
            "Training Iteration 1672, Loss: 6.202620506286621\n",
            "Training Iteration 1673, Loss: 2.6510403156280518\n",
            "Training Iteration 1674, Loss: 4.203446388244629\n",
            "Training Iteration 1675, Loss: 4.47442102432251\n",
            "Training Iteration 1676, Loss: 4.017053127288818\n",
            "Training Iteration 1677, Loss: 7.822164535522461\n",
            "Training Iteration 1678, Loss: 8.081381797790527\n",
            "Training Iteration 1679, Loss: 7.553114414215088\n",
            "Training Iteration 1680, Loss: 5.964632987976074\n",
            "Training Iteration 1681, Loss: 5.973843097686768\n",
            "Training Iteration 1682, Loss: 2.300549268722534\n",
            "Training Iteration 1683, Loss: 8.455806732177734\n",
            "Training Iteration 1684, Loss: 4.223947525024414\n",
            "Training Iteration 1685, Loss: 2.2065017223358154\n",
            "Training Iteration 1686, Loss: 10.507067680358887\n",
            "Training Iteration 1687, Loss: 7.002228736877441\n",
            "Training Iteration 1688, Loss: 5.461366176605225\n",
            "Training Iteration 1689, Loss: 6.749972820281982\n",
            "Training Iteration 1690, Loss: 4.328207015991211\n",
            "Training Iteration 1691, Loss: 4.399935722351074\n",
            "Training Iteration 1692, Loss: 5.104097843170166\n",
            "Training Iteration 1693, Loss: 6.367598533630371\n",
            "Training Iteration 1694, Loss: 3.9280147552490234\n",
            "Training Iteration 1695, Loss: 4.222668647766113\n",
            "Training Iteration 1696, Loss: 5.415511131286621\n",
            "Training Iteration 1697, Loss: 3.1430466175079346\n",
            "Training Iteration 1698, Loss: 2.916727066040039\n",
            "Training Iteration 1699, Loss: 6.03767728805542\n",
            "Training Iteration 1700, Loss: 2.938849449157715\n",
            "Training Iteration 1701, Loss: 2.378571033477783\n",
            "Training Iteration 1702, Loss: 3.84653639793396\n",
            "Training Iteration 1703, Loss: 6.5935540199279785\n",
            "Training Iteration 1704, Loss: 1.2837718725204468\n",
            "Training Iteration 1705, Loss: 4.228831768035889\n",
            "Training Iteration 1706, Loss: 3.083179235458374\n",
            "Training Iteration 1707, Loss: 7.222862243652344\n",
            "Training Iteration 1708, Loss: 4.077050685882568\n",
            "Training Iteration 1709, Loss: 5.500871658325195\n",
            "Training Iteration 1710, Loss: 5.200604438781738\n",
            "Training Iteration 1711, Loss: 3.0777249336242676\n",
            "Training Iteration 1712, Loss: 3.2307748794555664\n",
            "Training Iteration 1713, Loss: 4.304586410522461\n",
            "Training Iteration 1714, Loss: 3.9704952239990234\n",
            "Training Iteration 1715, Loss: 4.346936225891113\n",
            "Training Iteration 1716, Loss: 2.2697863578796387\n",
            "Training Iteration 1717, Loss: 3.0545403957366943\n",
            "Training Iteration 1718, Loss: 5.948413848876953\n",
            "Training Iteration 1719, Loss: 4.158884525299072\n",
            "Training Iteration 1720, Loss: 3.46494460105896\n",
            "Training Iteration 1721, Loss: 4.500974655151367\n",
            "Training Iteration 1722, Loss: 3.9788875579833984\n",
            "Training Iteration 1723, Loss: 4.334173679351807\n",
            "Training Iteration 1724, Loss: 3.89631724357605\n",
            "Training Iteration 1725, Loss: 5.472267150878906\n",
            "Training Iteration 1726, Loss: 8.041984558105469\n",
            "Training Iteration 1727, Loss: 3.5127856731414795\n",
            "Training Iteration 1728, Loss: 3.924988031387329\n",
            "Training Iteration 1729, Loss: 3.2042806148529053\n",
            "Training Iteration 1730, Loss: 2.621428966522217\n",
            "Training Iteration 1731, Loss: 3.557588815689087\n",
            "Training Iteration 1732, Loss: 7.3996734619140625\n",
            "Training Iteration 1733, Loss: 5.979587554931641\n",
            "Training Iteration 1734, Loss: 9.417102813720703\n",
            "Training Iteration 1735, Loss: 6.232515811920166\n",
            "Training Iteration 1736, Loss: 5.939021110534668\n",
            "Training Iteration 1737, Loss: 5.247060298919678\n",
            "Training Iteration 1738, Loss: 4.4331231117248535\n",
            "Training Iteration 1739, Loss: 3.7707724571228027\n",
            "Training Iteration 1740, Loss: 3.188187599182129\n",
            "Training Iteration 1741, Loss: 5.123166084289551\n",
            "Training Iteration 1742, Loss: 5.761544227600098\n",
            "Training Iteration 1743, Loss: 2.8637747764587402\n",
            "Training Iteration 1744, Loss: 3.6674957275390625\n",
            "Training Iteration 1745, Loss: 3.3384757041931152\n",
            "Training Iteration 1746, Loss: 5.13397741317749\n",
            "Training Iteration 1747, Loss: 4.702199459075928\n",
            "Training Iteration 1748, Loss: 4.229703426361084\n",
            "Training Iteration 1749, Loss: 4.398911476135254\n",
            "Training Iteration 1750, Loss: 4.897730827331543\n",
            "Training Iteration 1751, Loss: 5.203165531158447\n",
            "Training Iteration 1752, Loss: 3.860015392303467\n",
            "Training Iteration 1753, Loss: 5.299045562744141\n",
            "Training Iteration 1754, Loss: 3.4086930751800537\n",
            "Training Iteration 1755, Loss: 4.624066352844238\n",
            "Training Iteration 1756, Loss: 2.081935405731201\n",
            "Training Iteration 1757, Loss: 4.154535293579102\n",
            "Training Iteration 1758, Loss: 3.9702563285827637\n",
            "Training Iteration 1759, Loss: 7.085321426391602\n",
            "Training Iteration 1760, Loss: 4.80764102935791\n",
            "Training Iteration 1761, Loss: 9.91115951538086\n",
            "Training Iteration 1762, Loss: 4.946232795715332\n",
            "Training Iteration 1763, Loss: 2.214059352874756\n",
            "Training Iteration 1764, Loss: 5.008472442626953\n",
            "Training Iteration 1765, Loss: 4.998094081878662\n",
            "Training Iteration 1766, Loss: 6.526060581207275\n",
            "Training Iteration 1767, Loss: 3.963874101638794\n",
            "Training Iteration 1768, Loss: 6.941380500793457\n",
            "Training Iteration 1769, Loss: 5.711886405944824\n",
            "Training Iteration 1770, Loss: 1.6463260650634766\n",
            "Training Iteration 1771, Loss: 6.893373489379883\n",
            "Training Iteration 1772, Loss: 5.251516819000244\n",
            "Training Iteration 1773, Loss: 7.153988838195801\n",
            "Training Iteration 1774, Loss: 8.345524787902832\n",
            "Training Iteration 1775, Loss: 7.509800910949707\n",
            "Training Iteration 1776, Loss: 4.763547897338867\n",
            "Training Iteration 1777, Loss: 7.054696083068848\n",
            "Training Iteration 1778, Loss: 3.228705883026123\n",
            "Training Iteration 1779, Loss: 6.153901100158691\n",
            "Training Iteration 1780, Loss: 5.816880226135254\n",
            "Training Iteration 1781, Loss: 5.564244747161865\n",
            "Training Iteration 1782, Loss: 3.609832763671875\n",
            "Training Iteration 1783, Loss: 3.5759007930755615\n",
            "Training Iteration 1784, Loss: 4.639354228973389\n",
            "Training Iteration 1785, Loss: 4.068009376525879\n",
            "Training Iteration 1786, Loss: 5.946358680725098\n",
            "Training Iteration 1787, Loss: 4.59140682220459\n",
            "Training Iteration 1788, Loss: 9.099515914916992\n",
            "Training Iteration 1789, Loss: 8.084722518920898\n",
            "Training Iteration 1790, Loss: 2.601478099822998\n",
            "Training Iteration 1791, Loss: 5.045868873596191\n",
            "Training Iteration 1792, Loss: 6.447951316833496\n",
            "Training Iteration 1793, Loss: 6.566072463989258\n",
            "Training Iteration 1794, Loss: 7.302499294281006\n",
            "Training Iteration 1795, Loss: 7.202945709228516\n",
            "Training Iteration 1796, Loss: 7.277737140655518\n",
            "Training Iteration 1797, Loss: 7.8112382888793945\n",
            "Training Iteration 1798, Loss: 7.347347259521484\n",
            "Training Iteration 1799, Loss: 6.777317047119141\n",
            "Training Iteration 1800, Loss: 5.023758411407471\n",
            "Training Iteration 1801, Loss: 3.301874876022339\n",
            "Training Iteration 1802, Loss: 3.319098949432373\n",
            "Training Iteration 1803, Loss: 3.940783739089966\n",
            "Training Iteration 1804, Loss: 6.514763832092285\n",
            "Training Iteration 1805, Loss: 7.964486598968506\n",
            "Training Iteration 1806, Loss: 5.407543182373047\n",
            "Training Iteration 1807, Loss: 7.723228931427002\n",
            "Training Iteration 1808, Loss: 6.9407806396484375\n",
            "Training Iteration 1809, Loss: 4.887706279754639\n",
            "Training Iteration 1810, Loss: 4.3960771560668945\n",
            "Training Iteration 1811, Loss: 4.227367877960205\n",
            "Training Iteration 1812, Loss: 2.8489773273468018\n",
            "Training Iteration 1813, Loss: 4.646689414978027\n",
            "Training Iteration 1814, Loss: 2.8659982681274414\n",
            "Training Iteration 1815, Loss: 7.403509140014648\n",
            "Training Iteration 1816, Loss: 3.8862476348876953\n",
            "Training Iteration 1817, Loss: 2.101754665374756\n",
            "Training Iteration 1818, Loss: 3.1044116020202637\n",
            "Training Iteration 1819, Loss: 2.284627914428711\n",
            "Training Iteration 1820, Loss: 3.819946527481079\n",
            "Training Iteration 1821, Loss: 5.099194526672363\n",
            "Training Iteration 1822, Loss: 2.7600812911987305\n",
            "Training Iteration 1823, Loss: 9.562460899353027\n",
            "Training Iteration 1824, Loss: 5.060460090637207\n",
            "Training Iteration 1825, Loss: 7.807558059692383\n",
            "Training Iteration 1826, Loss: 4.373263359069824\n",
            "Training Iteration 1827, Loss: 4.41395902633667\n",
            "Training Iteration 1828, Loss: 7.697242259979248\n",
            "Training Iteration 1829, Loss: 5.878570079803467\n",
            "Training Iteration 1830, Loss: 3.734863758087158\n",
            "Training Iteration 1831, Loss: 3.775960922241211\n",
            "Training Iteration 1832, Loss: 2.7964417934417725\n",
            "Training Iteration 1833, Loss: 5.788487434387207\n",
            "Training Iteration 1834, Loss: 5.830301761627197\n",
            "Training Iteration 1835, Loss: 2.6453545093536377\n",
            "Training Iteration 1836, Loss: 3.688265800476074\n",
            "Training Iteration 1837, Loss: 3.350736141204834\n",
            "Training Iteration 1838, Loss: 3.727280378341675\n",
            "Training Iteration 1839, Loss: 5.44637393951416\n",
            "Training Iteration 1840, Loss: 5.517416477203369\n",
            "Training Iteration 1841, Loss: 4.653660297393799\n",
            "Training Iteration 1842, Loss: 8.380220413208008\n",
            "Training Iteration 1843, Loss: 2.674391269683838\n",
            "Training Iteration 1844, Loss: 5.036937713623047\n",
            "Training Iteration 1845, Loss: 3.2218985557556152\n",
            "Training Iteration 1846, Loss: 5.629518032073975\n",
            "Training Iteration 1847, Loss: 5.4572834968566895\n",
            "Training Iteration 1848, Loss: 3.0232012271881104\n",
            "Training Iteration 1849, Loss: 2.3541243076324463\n",
            "Training Iteration 1850, Loss: 3.125180959701538\n",
            "Training Iteration 1851, Loss: 6.676065444946289\n",
            "Training Iteration 1852, Loss: 3.1168456077575684\n",
            "Training Iteration 1853, Loss: 6.802975177764893\n",
            "Training Iteration 1854, Loss: 4.210541248321533\n",
            "Training Iteration 1855, Loss: 2.326660394668579\n",
            "Training Iteration 1856, Loss: 2.507784366607666\n",
            "Training Iteration 1857, Loss: 2.2728283405303955\n",
            "Training Iteration 1858, Loss: 5.016061305999756\n",
            "Training Iteration 1859, Loss: 3.6528666019439697\n",
            "Training Iteration 1860, Loss: 2.839122772216797\n",
            "Training Iteration 1861, Loss: 3.3127002716064453\n",
            "Training Iteration 1862, Loss: 3.1909074783325195\n",
            "Training Iteration 1863, Loss: 0.867075502872467\n",
            "Training Iteration 1864, Loss: 3.444039821624756\n",
            "Training Iteration 1865, Loss: 4.233619213104248\n",
            "Training Iteration 1866, Loss: 2.5782084465026855\n",
            "Training Iteration 1867, Loss: 2.103634834289551\n",
            "Training Iteration 1868, Loss: 3.070835828781128\n",
            "Training Iteration 1869, Loss: 2.0283055305480957\n",
            "Training Iteration 1870, Loss: 3.644204616546631\n",
            "Training Iteration 1871, Loss: 2.604346752166748\n",
            "Training Iteration 1872, Loss: 4.692460060119629\n",
            "Training Iteration 1873, Loss: 3.6640114784240723\n",
            "Training Iteration 1874, Loss: 6.681004524230957\n",
            "Training Iteration 1875, Loss: 3.122908115386963\n",
            "Training Iteration 1876, Loss: 2.7587201595306396\n",
            "Training Iteration 1877, Loss: 2.3598170280456543\n",
            "Training Iteration 1878, Loss: 3.46537446975708\n",
            "Training Iteration 1879, Loss: 5.166104793548584\n",
            "Training Iteration 1880, Loss: 4.064112186431885\n",
            "Training Iteration 1881, Loss: 5.505147933959961\n",
            "Training Iteration 1882, Loss: 3.942763328552246\n",
            "Training Iteration 1883, Loss: 4.05718994140625\n",
            "Training Iteration 1884, Loss: 4.617231369018555\n",
            "Training Iteration 1885, Loss: 3.941256284713745\n",
            "Training Iteration 1886, Loss: 5.316614627838135\n",
            "Training Iteration 1887, Loss: 5.358087539672852\n",
            "Training Iteration 1888, Loss: 2.9999537467956543\n",
            "Training Iteration 1889, Loss: 2.62823486328125\n",
            "Training Iteration 1890, Loss: 2.7560808658599854\n",
            "Training Iteration 1891, Loss: 3.2232842445373535\n",
            "Training Iteration 1892, Loss: 3.8743159770965576\n",
            "Training Iteration 1893, Loss: 2.842503309249878\n",
            "Training Iteration 1894, Loss: 2.966259002685547\n",
            "Training Iteration 1895, Loss: 4.706038475036621\n",
            "Training Iteration 1896, Loss: 2.6519932746887207\n",
            "Training Iteration 1897, Loss: 5.752951622009277\n",
            "Training Iteration 1898, Loss: 3.250246047973633\n",
            "Training Iteration 1899, Loss: 4.5679450035095215\n",
            "Training Iteration 1900, Loss: 3.151968479156494\n",
            "Training Iteration 1901, Loss: 4.78188419342041\n",
            "Training Iteration 1902, Loss: 3.8062620162963867\n",
            "Training Iteration 1903, Loss: 5.002594470977783\n",
            "Training Iteration 1904, Loss: 5.052811622619629\n",
            "Training Iteration 1905, Loss: 3.782533884048462\n",
            "Training Iteration 1906, Loss: 5.401923179626465\n",
            "Training Iteration 1907, Loss: 3.3664636611938477\n",
            "Training Iteration 1908, Loss: 4.123880386352539\n",
            "Training Iteration 1909, Loss: 2.7851266860961914\n",
            "Training Iteration 1910, Loss: 8.049936294555664\n",
            "Training Iteration 1911, Loss: 5.0150532722473145\n",
            "Training Iteration 1912, Loss: 6.144624710083008\n",
            "Training Iteration 1913, Loss: 2.8795244693756104\n",
            "Training Iteration 1914, Loss: 3.2966041564941406\n",
            "Training Iteration 1915, Loss: 4.660498142242432\n",
            "Training Iteration 1916, Loss: 2.916196584701538\n",
            "Training Iteration 1917, Loss: 3.529212236404419\n",
            "Training Iteration 1918, Loss: 3.305027961730957\n",
            "Training Iteration 1919, Loss: 5.623958587646484\n",
            "Training Iteration 1920, Loss: 5.795266151428223\n",
            "Training Iteration 1921, Loss: 5.7097344398498535\n",
            "Training Iteration 1922, Loss: 2.9491288661956787\n",
            "Training Iteration 1923, Loss: 1.782824158668518\n",
            "Training Iteration 1924, Loss: 5.663457870483398\n",
            "Training Iteration 1925, Loss: 5.378204822540283\n",
            "Training Iteration 1926, Loss: 4.286125183105469\n",
            "Training Iteration 1927, Loss: 4.018640041351318\n",
            "Training Iteration 1928, Loss: 6.377541542053223\n",
            "Training Iteration 1929, Loss: 3.3810372352600098\n",
            "Training Iteration 1930, Loss: 8.034000396728516\n",
            "Training Iteration 1931, Loss: 6.060617923736572\n",
            "Training Iteration 1932, Loss: 5.759402275085449\n",
            "Training Iteration 1933, Loss: 4.074411392211914\n",
            "Training Iteration 1934, Loss: 3.383106231689453\n",
            "Training Iteration 1935, Loss: 1.7846081256866455\n",
            "Training Iteration 1936, Loss: 7.1550750732421875\n",
            "Training Iteration 1937, Loss: 5.741360664367676\n",
            "Training Iteration 1938, Loss: 3.0257463455200195\n",
            "Training Iteration 1939, Loss: 3.794888973236084\n",
            "Training Iteration 1940, Loss: 3.5956313610076904\n",
            "Training Iteration 1941, Loss: 3.9022412300109863\n",
            "Training Iteration 1942, Loss: 4.040441989898682\n",
            "Training Iteration 1943, Loss: 3.854846477508545\n",
            "Training Iteration 1944, Loss: 5.036803245544434\n",
            "Training Iteration 1945, Loss: 6.224520206451416\n",
            "Training Iteration 1946, Loss: 7.143603801727295\n",
            "Training Iteration 1947, Loss: 4.094667911529541\n",
            "Training Iteration 1948, Loss: 2.426190137863159\n",
            "Training Iteration 1949, Loss: 3.7616562843322754\n",
            "Training Iteration 1950, Loss: 3.138756036758423\n",
            "Training Iteration 1951, Loss: 2.2007899284362793\n",
            "Training Iteration 1952, Loss: 5.830330848693848\n",
            "Training Iteration 1953, Loss: 4.701412200927734\n",
            "Training Iteration 1954, Loss: 2.7289209365844727\n",
            "Training Iteration 1955, Loss: 3.1355574131011963\n",
            "Training Iteration 1956, Loss: 6.080663681030273\n",
            "Training Iteration 1957, Loss: 5.1161017417907715\n",
            "Training Iteration 1958, Loss: 2.750434637069702\n",
            "Training Iteration 1959, Loss: 4.433130264282227\n",
            "Training Iteration 1960, Loss: 4.48579216003418\n",
            "Training Iteration 1961, Loss: 3.9714746475219727\n",
            "Training Iteration 1962, Loss: 3.9074862003326416\n",
            "Training Iteration 1963, Loss: 5.012134552001953\n",
            "Training Iteration 1964, Loss: 6.209447860717773\n",
            "Training Iteration 1965, Loss: 3.6789660453796387\n",
            "Training Iteration 1966, Loss: 6.354081153869629\n",
            "Training Iteration 1967, Loss: 3.2465198040008545\n",
            "Training Iteration 1968, Loss: 3.872296094894409\n",
            "Training Iteration 1969, Loss: 3.3892197608947754\n",
            "Training Iteration 1970, Loss: 2.9393675327301025\n",
            "Training Iteration 1971, Loss: 3.290095329284668\n",
            "Training Iteration 1972, Loss: 6.94270133972168\n",
            "Training Iteration 1973, Loss: 2.77972412109375\n",
            "Training Iteration 1974, Loss: 5.663501739501953\n",
            "Training Iteration 1975, Loss: 3.0707790851593018\n",
            "Training Iteration 1976, Loss: 4.325254440307617\n",
            "Training Iteration 1977, Loss: 5.810157299041748\n",
            "Training Iteration 1978, Loss: 5.102280139923096\n",
            "Training Iteration 1979, Loss: 3.588319778442383\n",
            "Training Iteration 1980, Loss: 4.45370626449585\n",
            "Training Iteration 1981, Loss: 2.373814344406128\n",
            "Training Iteration 1982, Loss: 3.958967924118042\n",
            "Training Iteration 1983, Loss: 5.5623579025268555\n",
            "Training Iteration 1984, Loss: 4.035068988800049\n",
            "Training Iteration 1985, Loss: 5.185656547546387\n",
            "Training Iteration 1986, Loss: 4.9153337478637695\n",
            "Training Iteration 1987, Loss: 4.284840106964111\n",
            "Training Iteration 1988, Loss: 4.370720863342285\n",
            "Training Iteration 1989, Loss: 3.8579554557800293\n",
            "Training Iteration 1990, Loss: 1.980197787284851\n",
            "Training Iteration 1991, Loss: 4.141463756561279\n",
            "Training Iteration 1992, Loss: 4.516170501708984\n",
            "Training Iteration 1993, Loss: 2.1183812618255615\n",
            "Training Iteration 1994, Loss: 4.614323616027832\n",
            "Training Iteration 1995, Loss: 5.735752105712891\n",
            "Training Iteration 1996, Loss: 3.534860610961914\n",
            "Training Iteration 1997, Loss: 2.99851655960083\n",
            "Training Iteration 1998, Loss: 5.089333534240723\n",
            "Training Iteration 1999, Loss: 4.924742698669434\n",
            "Training Iteration 2000, Loss: 3.3700850009918213\n",
            "Training Iteration 2001, Loss: 3.900125026702881\n",
            "Training Iteration 2002, Loss: 4.4464874267578125\n",
            "Training Iteration 2003, Loss: 5.205488681793213\n",
            "Training Iteration 2004, Loss: 3.1188101768493652\n",
            "Training Iteration 2005, Loss: 3.500171422958374\n",
            "Training Iteration 2006, Loss: 3.02398943901062\n",
            "Training Iteration 2007, Loss: 3.501739263534546\n",
            "Training Iteration 2008, Loss: 4.562793731689453\n",
            "Training Iteration 2009, Loss: 3.5031280517578125\n",
            "Training Iteration 2010, Loss: 5.234260559082031\n",
            "Training Iteration 2011, Loss: 4.466609954833984\n",
            "Training Iteration 2012, Loss: 6.428463935852051\n",
            "Training Iteration 2013, Loss: 3.8875539302825928\n",
            "Training Iteration 2014, Loss: 4.279122352600098\n",
            "Training Iteration 2015, Loss: 6.558035850524902\n",
            "Training Iteration 2016, Loss: 3.7857439517974854\n",
            "Training Iteration 2017, Loss: 5.2346649169921875\n",
            "Training Iteration 2018, Loss: 5.061748504638672\n",
            "Training Iteration 2019, Loss: 2.8055458068847656\n",
            "Training Iteration 2020, Loss: 2.9537718296051025\n",
            "Training Iteration 2021, Loss: 5.244861602783203\n",
            "Training Iteration 2022, Loss: 7.464641571044922\n",
            "Training Iteration 2023, Loss: 2.401416540145874\n",
            "Training Iteration 2024, Loss: 3.361957311630249\n",
            "Training Iteration 2025, Loss: 3.884148359298706\n",
            "Training Iteration 2026, Loss: 3.686847448348999\n",
            "Training Iteration 2027, Loss: 2.8929383754730225\n",
            "Training Iteration 2028, Loss: 2.9459638595581055\n",
            "Training Iteration 2029, Loss: 5.220082759857178\n",
            "Training Iteration 2030, Loss: 3.955233335494995\n",
            "Training Iteration 2031, Loss: 5.254139423370361\n",
            "Training Iteration 2032, Loss: 5.39760160446167\n",
            "Training Iteration 2033, Loss: 3.9974827766418457\n",
            "Training Iteration 2034, Loss: 4.450064182281494\n",
            "Training Iteration 2035, Loss: 4.733893871307373\n",
            "Training Iteration 2036, Loss: 6.041919708251953\n",
            "Training Iteration 2037, Loss: 5.411269664764404\n",
            "Training Iteration 2038, Loss: 3.8920695781707764\n",
            "Training Iteration 2039, Loss: 4.674585819244385\n",
            "Training Iteration 2040, Loss: 5.524481296539307\n",
            "Training Iteration 2041, Loss: 4.340871334075928\n",
            "Training Iteration 2042, Loss: 3.387117624282837\n",
            "Training Iteration 2043, Loss: 3.162209987640381\n",
            "Training Iteration 2044, Loss: 3.0302064418792725\n",
            "Training Iteration 2045, Loss: 3.2594969272613525\n",
            "Training Iteration 2046, Loss: 6.258897304534912\n",
            "Training Iteration 2047, Loss: 4.756773471832275\n",
            "Training Iteration 2048, Loss: 4.78645133972168\n",
            "Training Iteration 2049, Loss: 6.110396862030029\n",
            "Training Iteration 2050, Loss: 3.1337997913360596\n",
            "Training Iteration 2051, Loss: 2.4961328506469727\n",
            "Training Iteration 2052, Loss: 2.779203176498413\n",
            "Training Iteration 2053, Loss: 3.5135693550109863\n",
            "Training Iteration 2054, Loss: 2.468761682510376\n",
            "Training Iteration 2055, Loss: 5.5665082931518555\n",
            "Training Iteration 2056, Loss: 3.8601295948028564\n",
            "Training Iteration 2057, Loss: 6.1697468757629395\n",
            "Training Iteration 2058, Loss: 5.285611152648926\n",
            "Training Iteration 2059, Loss: 8.592049598693848\n",
            "Training Iteration 2060, Loss: 3.477735996246338\n",
            "Training Iteration 2061, Loss: 3.064525604248047\n",
            "Training Iteration 2062, Loss: 1.5798239707946777\n",
            "Training Iteration 2063, Loss: 6.713094711303711\n",
            "Training Iteration 2064, Loss: 4.527392387390137\n",
            "Training Iteration 2065, Loss: 5.176812648773193\n",
            "Training Iteration 2066, Loss: 6.323437690734863\n",
            "Training Iteration 2067, Loss: 4.168911457061768\n",
            "Training Iteration 2068, Loss: 2.8041155338287354\n",
            "Training Iteration 2069, Loss: 4.817800998687744\n",
            "Training Iteration 2070, Loss: 2.493828296661377\n",
            "Training Iteration 2071, Loss: 4.085819721221924\n",
            "Training Iteration 2072, Loss: 7.432076454162598\n",
            "Training Iteration 2073, Loss: 4.120639801025391\n",
            "Training Iteration 2074, Loss: 2.856315851211548\n",
            "Training Iteration 2075, Loss: 4.613731384277344\n",
            "Training Iteration 2076, Loss: 3.2822601795196533\n",
            "Training Iteration 2077, Loss: 2.164102077484131\n",
            "Training Iteration 2078, Loss: 5.061254024505615\n",
            "Training Iteration 2079, Loss: 2.8809468746185303\n",
            "Training Iteration 2080, Loss: 4.038590908050537\n",
            "Training Iteration 2081, Loss: 3.2194833755493164\n",
            "Training Iteration 2082, Loss: 4.5354156494140625\n",
            "Training Iteration 2083, Loss: 5.246307373046875\n",
            "Training Iteration 2084, Loss: 6.654590606689453\n",
            "Training Iteration 2085, Loss: 6.0124921798706055\n",
            "Training Iteration 2086, Loss: 4.11419153213501\n",
            "Training Iteration 2087, Loss: 3.3750951290130615\n",
            "Training Iteration 2088, Loss: 3.899723529815674\n",
            "Training Iteration 2089, Loss: 5.084597110748291\n",
            "Training Iteration 2090, Loss: 5.965814113616943\n",
            "Training Iteration 2091, Loss: 8.26255989074707\n",
            "Training Iteration 2092, Loss: 8.624431610107422\n",
            "Training Iteration 2093, Loss: 5.6685967445373535\n",
            "Training Iteration 2094, Loss: 3.99345064163208\n",
            "Training Iteration 2095, Loss: 3.027794599533081\n",
            "Training Iteration 2096, Loss: 3.0678606033325195\n",
            "Training Iteration 2097, Loss: 3.6925158500671387\n",
            "Training Iteration 2098, Loss: 4.109941482543945\n",
            "Training Iteration 2099, Loss: 7.14206600189209\n",
            "Training Iteration 2100, Loss: 8.10056209564209\n",
            "Training Iteration 2101, Loss: 11.0935697555542\n",
            "Training Iteration 2102, Loss: 3.214151620864868\n",
            "Training Iteration 2103, Loss: 4.003885269165039\n",
            "Training Iteration 2104, Loss: 8.966593742370605\n",
            "Training Iteration 2105, Loss: 1.899087905883789\n",
            "Training Iteration 2106, Loss: 3.7592031955718994\n",
            "Training Iteration 2107, Loss: 5.120495796203613\n",
            "Training Iteration 2108, Loss: 2.753368854522705\n",
            "Training Iteration 2109, Loss: 5.134398937225342\n",
            "Training Iteration 2110, Loss: 2.529845714569092\n",
            "Training Iteration 2111, Loss: 3.249563455581665\n",
            "Training Iteration 2112, Loss: 7.496078014373779\n",
            "Training Iteration 2113, Loss: 4.480520725250244\n",
            "Training Iteration 2114, Loss: 3.572962522506714\n",
            "Training Iteration 2115, Loss: 5.049171447753906\n",
            "Training Iteration 2116, Loss: 3.388023853302002\n",
            "Training Iteration 2117, Loss: 4.968109607696533\n",
            "Training Iteration 2118, Loss: 3.763011932373047\n",
            "Training Iteration 2119, Loss: 4.349005699157715\n",
            "Training Iteration 2120, Loss: 3.487825632095337\n",
            "Training Iteration 2121, Loss: 1.3591187000274658\n",
            "Training Iteration 2122, Loss: 10.368768692016602\n",
            "Training Iteration 2123, Loss: 4.706720352172852\n",
            "Training Iteration 2124, Loss: 4.0360636711120605\n",
            "Training Iteration 2125, Loss: 4.350614547729492\n",
            "Training Iteration 2126, Loss: 5.809389114379883\n",
            "Training Iteration 2127, Loss: 3.6953232288360596\n",
            "Training Iteration 2128, Loss: 4.224116325378418\n",
            "Training Iteration 2129, Loss: 4.434933662414551\n",
            "Training Iteration 2130, Loss: 3.633584499359131\n",
            "Training Iteration 2131, Loss: 6.241781711578369\n",
            "Training Iteration 2132, Loss: 6.840299606323242\n",
            "Training Iteration 2133, Loss: 1.5754270553588867\n",
            "Training Iteration 2134, Loss: 3.3967087268829346\n",
            "Training Iteration 2135, Loss: 5.226486682891846\n",
            "Training Iteration 2136, Loss: 3.227595329284668\n",
            "Training Iteration 2137, Loss: 3.340214252471924\n",
            "Training Iteration 2138, Loss: 4.606886863708496\n",
            "Training Iteration 2139, Loss: 3.729588508605957\n",
            "Training Iteration 2140, Loss: 7.587635040283203\n",
            "Training Iteration 2141, Loss: 2.2868173122406006\n",
            "Training Iteration 2142, Loss: 6.619609832763672\n",
            "Training Iteration 2143, Loss: 7.523002624511719\n",
            "Training Iteration 2144, Loss: 3.3723843097686768\n",
            "Training Iteration 2145, Loss: 2.4227294921875\n",
            "Training Iteration 2146, Loss: 3.736649990081787\n",
            "Training Iteration 2147, Loss: 3.2397921085357666\n",
            "Training Iteration 2148, Loss: 3.520803213119507\n",
            "Training Iteration 2149, Loss: 5.588138103485107\n",
            "Training Iteration 2150, Loss: 1.7382702827453613\n",
            "Training Iteration 2151, Loss: 5.427181243896484\n",
            "Training Iteration 2152, Loss: 3.6738696098327637\n",
            "Training Iteration 2153, Loss: 4.371878147125244\n",
            "Training Iteration 2154, Loss: 3.8385090827941895\n",
            "Training Iteration 2155, Loss: 6.9294114112854\n",
            "Training Iteration 2156, Loss: 5.728313446044922\n",
            "Training Iteration 2157, Loss: 4.595404148101807\n",
            "Training Iteration 2158, Loss: 4.455423831939697\n",
            "Training Iteration 2159, Loss: 2.6609911918640137\n",
            "Training Iteration 2160, Loss: 3.8948516845703125\n",
            "Training Iteration 2161, Loss: 3.5265021324157715\n",
            "Training Iteration 2162, Loss: 9.703953742980957\n",
            "Training Iteration 2163, Loss: 7.675129413604736\n",
            "Training Iteration 2164, Loss: 4.850597858428955\n",
            "Training Iteration 2165, Loss: 1.7981892824172974\n",
            "Training Iteration 2166, Loss: 3.220205307006836\n",
            "Training Iteration 2167, Loss: 4.563141345977783\n",
            "Training Iteration 2168, Loss: 3.790661573410034\n",
            "Training Iteration 2169, Loss: 4.175248622894287\n",
            "Training Iteration 2170, Loss: 4.430972099304199\n",
            "Training Iteration 2171, Loss: 2.9890995025634766\n",
            "Training Iteration 2172, Loss: 6.127131462097168\n",
            "Training Iteration 2173, Loss: 4.962950706481934\n",
            "Training Iteration 2174, Loss: 4.423956871032715\n",
            "Training Iteration 2175, Loss: 5.447556495666504\n",
            "Training Iteration 2176, Loss: 2.4893581867218018\n",
            "Training Iteration 2177, Loss: 3.8258023262023926\n",
            "Training Iteration 2178, Loss: 3.736635446548462\n",
            "Training Iteration 2179, Loss: 5.0891218185424805\n",
            "Training Iteration 2180, Loss: 4.337732791900635\n",
            "Training Iteration 2181, Loss: 5.234710216522217\n",
            "Training Iteration 2182, Loss: 4.290090084075928\n",
            "Training Iteration 2183, Loss: 4.202544212341309\n",
            "Training Iteration 2184, Loss: 3.319638252258301\n",
            "Training Iteration 2185, Loss: 5.158507347106934\n",
            "Training Iteration 2186, Loss: 7.764554023742676\n",
            "Training Iteration 2187, Loss: 6.366276264190674\n",
            "Training Iteration 2188, Loss: 2.2358601093292236\n",
            "Training Iteration 2189, Loss: 3.6075727939605713\n",
            "Training Iteration 2190, Loss: 2.840400218963623\n",
            "Training Iteration 2191, Loss: 3.5743777751922607\n",
            "Training Iteration 2192, Loss: 3.2701148986816406\n",
            "Training Iteration 2193, Loss: 5.882724285125732\n",
            "Training Iteration 2194, Loss: 3.392393112182617\n",
            "Training Iteration 2195, Loss: 2.1718811988830566\n",
            "Training Iteration 2196, Loss: 8.258936882019043\n",
            "Training Iteration 2197, Loss: 5.326986312866211\n",
            "Training Iteration 2198, Loss: 9.151589393615723\n",
            "Training Iteration 2199, Loss: 6.842167854309082\n",
            "Training Iteration 2200, Loss: 2.832481861114502\n",
            "Training Iteration 2201, Loss: 3.7163538932800293\n",
            "Training Iteration 2202, Loss: 2.147838592529297\n",
            "Training Iteration 2203, Loss: 4.470258712768555\n",
            "Training Iteration 2204, Loss: 4.793087005615234\n",
            "Training Iteration 2205, Loss: 9.06039810180664\n",
            "Training Iteration 2206, Loss: 3.7674927711486816\n",
            "Training Iteration 2207, Loss: 4.856507301330566\n",
            "Training Iteration 2208, Loss: 3.862340211868286\n",
            "Training Iteration 2209, Loss: 3.952336311340332\n",
            "Training Iteration 2210, Loss: 9.585758209228516\n",
            "Training Iteration 2211, Loss: 6.599468231201172\n",
            "Training Iteration 2212, Loss: 6.134792327880859\n",
            "Training Iteration 2213, Loss: 5.48919677734375\n",
            "Training Iteration 2214, Loss: 3.821148633956909\n",
            "Training Iteration 2215, Loss: 5.799756050109863\n",
            "Training Iteration 2216, Loss: 4.466252326965332\n",
            "Training Iteration 2217, Loss: 5.16627836227417\n",
            "Training Iteration 2218, Loss: 2.2328197956085205\n",
            "Training Iteration 2219, Loss: 4.634080410003662\n",
            "Training Iteration 2220, Loss: 3.677943468093872\n",
            "Training Iteration 2221, Loss: 3.568985939025879\n",
            "Training Iteration 2222, Loss: 4.3977131843566895\n",
            "Training Iteration 2223, Loss: 4.317118167877197\n",
            "Training Iteration 2224, Loss: 3.352661371231079\n",
            "Training Iteration 2225, Loss: 2.1215145587921143\n",
            "Training Iteration 2226, Loss: 4.716627597808838\n",
            "Training Iteration 2227, Loss: 2.9464550018310547\n",
            "Training Iteration 2228, Loss: 2.8551859855651855\n",
            "Training Iteration 2229, Loss: 3.6487324237823486\n",
            "Training Iteration 2230, Loss: 5.740768909454346\n",
            "Training Iteration 2231, Loss: 3.136726140975952\n",
            "Training Iteration 2232, Loss: 2.901124954223633\n",
            "Training Iteration 2233, Loss: 3.746138572692871\n",
            "Training Iteration 2234, Loss: 3.5097639560699463\n",
            "Training Iteration 2235, Loss: 4.919665813446045\n",
            "Training Iteration 2236, Loss: 2.9264705181121826\n",
            "Training Iteration 2237, Loss: 2.5439422130584717\n",
            "Training Iteration 2238, Loss: 2.39103627204895\n",
            "Training Iteration 2239, Loss: 4.466938018798828\n",
            "Training Iteration 2240, Loss: 2.9960896968841553\n",
            "Training Iteration 2241, Loss: 5.217901229858398\n",
            "Training Iteration 2242, Loss: 3.0805346965789795\n",
            "Training Iteration 2243, Loss: 4.487402439117432\n",
            "Training Iteration 2244, Loss: 3.0017974376678467\n",
            "Training Iteration 2245, Loss: 6.43638277053833\n",
            "Training Iteration 2246, Loss: 2.671205759048462\n",
            "Training Iteration 2247, Loss: 5.172695159912109\n",
            "Training Iteration 2248, Loss: 5.908051490783691\n",
            "Training Iteration 2249, Loss: 5.704624176025391\n",
            "Training Iteration 2250, Loss: 4.284970760345459\n",
            "Training Iteration 2251, Loss: 6.673431873321533\n",
            "Training Iteration 2252, Loss: 6.549566745758057\n",
            "Training Iteration 2253, Loss: 3.899038314819336\n",
            "Training Iteration 2254, Loss: 2.406931161880493\n",
            "Training Iteration 2255, Loss: 4.463815689086914\n",
            "Training Iteration 2256, Loss: 6.331529140472412\n",
            "Training Iteration 2257, Loss: 10.24826431274414\n",
            "Training Iteration 2258, Loss: 5.073941230773926\n",
            "Training Iteration 2259, Loss: 4.743073463439941\n",
            "Training Iteration 2260, Loss: 4.02933406829834\n",
            "Training Iteration 2261, Loss: 3.29361629486084\n",
            "Training Iteration 2262, Loss: 4.711021423339844\n",
            "Training Iteration 2263, Loss: 2.823672294616699\n",
            "Training Iteration 2264, Loss: 2.6600890159606934\n",
            "Training Iteration 2265, Loss: 4.266314506530762\n",
            "Training Iteration 2266, Loss: 5.277359962463379\n",
            "Training Iteration 2267, Loss: 6.556036949157715\n",
            "Training Iteration 2268, Loss: 4.594762802124023\n",
            "Training Iteration 2269, Loss: 4.2014265060424805\n",
            "Training Iteration 2270, Loss: 3.2985661029815674\n",
            "Training Iteration 2271, Loss: 4.2381134033203125\n",
            "Training Iteration 2272, Loss: 3.432037353515625\n",
            "Training Iteration 2273, Loss: 6.397742748260498\n",
            "Training Iteration 2274, Loss: 2.6785953044891357\n",
            "Training Iteration 2275, Loss: 4.217368125915527\n",
            "Training Iteration 2276, Loss: 3.2917861938476562\n",
            "Training Iteration 2277, Loss: 3.322277784347534\n",
            "Training Iteration 2278, Loss: 3.7158586978912354\n",
            "Training Iteration 2279, Loss: 3.543036460876465\n",
            "Training Iteration 2280, Loss: 3.666473865509033\n",
            "Training Iteration 2281, Loss: 2.1385929584503174\n",
            "Training Iteration 2282, Loss: 5.725212574005127\n",
            "Training Iteration 2283, Loss: 5.618147850036621\n",
            "Training Iteration 2284, Loss: 2.5139999389648438\n",
            "Training Iteration 2285, Loss: 4.108395099639893\n",
            "Training Iteration 2286, Loss: 3.2842721939086914\n",
            "Training Iteration 2287, Loss: 7.612110614776611\n",
            "Training Iteration 2288, Loss: 5.351903438568115\n",
            "Training Iteration 2289, Loss: 4.731013298034668\n",
            "Training Iteration 2290, Loss: 3.1223301887512207\n",
            "Training Iteration 2291, Loss: 3.0294811725616455\n",
            "Training Iteration 2292, Loss: 7.112390041351318\n",
            "Training Iteration 2293, Loss: 2.1417808532714844\n",
            "Training Iteration 2294, Loss: 4.612107276916504\n",
            "Training Iteration 2295, Loss: 4.356229782104492\n",
            "Training Iteration 2296, Loss: 5.850873947143555\n",
            "Training Iteration 2297, Loss: 7.0579447746276855\n",
            "Training Iteration 2298, Loss: 3.216451406478882\n",
            "Training Iteration 2299, Loss: 4.783037185668945\n",
            "Training Iteration 2300, Loss: 6.59979772567749\n",
            "Training Iteration 2301, Loss: 5.911760330200195\n",
            "Training Iteration 2302, Loss: 4.775172710418701\n",
            "Training Iteration 2303, Loss: 4.167167663574219\n",
            "Training Iteration 2304, Loss: 2.7746407985687256\n",
            "Training Iteration 2305, Loss: 4.228170871734619\n",
            "Training Iteration 2306, Loss: 4.227641582489014\n",
            "Training Iteration 2307, Loss: 3.9169607162475586\n",
            "Training Iteration 2308, Loss: 5.354487895965576\n",
            "Training Iteration 2309, Loss: 3.804867744445801\n",
            "Training Iteration 2310, Loss: 3.6992690563201904\n",
            "Training Iteration 2311, Loss: 5.633627891540527\n",
            "Training Iteration 2312, Loss: 2.5428414344787598\n",
            "Training Iteration 2313, Loss: 3.2466232776641846\n",
            "Training Iteration 2314, Loss: 3.6938042640686035\n",
            "Training Iteration 2315, Loss: 2.6156017780303955\n",
            "Training Iteration 2316, Loss: 5.414908409118652\n",
            "Training Iteration 2317, Loss: 6.726484298706055\n",
            "Training Iteration 2318, Loss: 2.9041624069213867\n",
            "Training Iteration 2319, Loss: 8.118204116821289\n",
            "Training Iteration 2320, Loss: 8.077550888061523\n",
            "Training Iteration 2321, Loss: 2.6518306732177734\n",
            "Training Iteration 2322, Loss: 6.393182754516602\n",
            "Training Iteration 2323, Loss: 6.361023426055908\n",
            "Training Iteration 2324, Loss: 5.230985164642334\n",
            "Training Iteration 2325, Loss: 5.067991256713867\n",
            "Training Iteration 2326, Loss: 4.080343246459961\n",
            "Training Iteration 2327, Loss: 3.8741743564605713\n",
            "Training Iteration 2328, Loss: 6.511082172393799\n",
            "Training Iteration 2329, Loss: 3.3942711353302\n",
            "Training Iteration 2330, Loss: 6.089282512664795\n",
            "Training Iteration 2331, Loss: 4.680431365966797\n",
            "Training Iteration 2332, Loss: 8.876575469970703\n",
            "Training Iteration 2333, Loss: 5.6666765213012695\n",
            "Training Iteration 2334, Loss: 6.752469539642334\n",
            "Training Iteration 2335, Loss: 5.918019771575928\n",
            "Training Iteration 2336, Loss: 5.709477424621582\n",
            "Training Iteration 2337, Loss: 6.54950475692749\n",
            "Training Iteration 2338, Loss: 5.084619522094727\n",
            "Training Iteration 2339, Loss: 6.123841762542725\n",
            "Training Iteration 2340, Loss: 8.324790000915527\n",
            "Training Iteration 2341, Loss: 4.243366718292236\n",
            "Training Iteration 2342, Loss: 6.031661510467529\n",
            "Training Iteration 2343, Loss: 11.792276382446289\n",
            "Training Iteration 2344, Loss: 3.6119182109832764\n",
            "Training Iteration 2345, Loss: 3.6396961212158203\n",
            "Training Iteration 2346, Loss: 3.5394039154052734\n",
            "Training Iteration 2347, Loss: 4.059939384460449\n",
            "Training Iteration 2348, Loss: 8.018767356872559\n",
            "Training Iteration 2349, Loss: 6.435283660888672\n",
            "Training Iteration 2350, Loss: 3.7559001445770264\n",
            "Training Iteration 2351, Loss: 2.7748868465423584\n",
            "Training Iteration 2352, Loss: 6.604873180389404\n",
            "Training Iteration 2353, Loss: 5.344549179077148\n",
            "Training Iteration 2354, Loss: 2.6389434337615967\n",
            "Training Iteration 2355, Loss: 5.761952877044678\n",
            "Training Iteration 2356, Loss: 2.9614639282226562\n",
            "Training Iteration 2357, Loss: 4.341859340667725\n",
            "Training Iteration 2358, Loss: 7.920835971832275\n",
            "Training Iteration 2359, Loss: 3.5090646743774414\n",
            "Training Iteration 2360, Loss: 7.470049858093262\n",
            "Training Iteration 2361, Loss: 4.122269630432129\n",
            "Training Iteration 2362, Loss: 5.52653694152832\n",
            "Training Iteration 2363, Loss: 5.664224147796631\n",
            "Training Iteration 2364, Loss: 8.102315902709961\n",
            "Training Iteration 2365, Loss: 5.286195755004883\n",
            "Training Iteration 2366, Loss: 4.72725772857666\n",
            "Training Iteration 2367, Loss: 7.5727434158325195\n",
            "Training Iteration 2368, Loss: 4.678676605224609\n",
            "Training Iteration 2369, Loss: 2.6807188987731934\n",
            "Training Iteration 2370, Loss: 3.764357328414917\n",
            "Training Iteration 2371, Loss: 11.019001007080078\n",
            "Training Iteration 2372, Loss: 9.030570983886719\n",
            "Training Iteration 2373, Loss: 5.640869140625\n",
            "Training Iteration 2374, Loss: 7.557389259338379\n",
            "Training Iteration 2375, Loss: 4.944028854370117\n",
            "Training Iteration 2376, Loss: 4.485376834869385\n",
            "Training Iteration 2377, Loss: 4.500705242156982\n",
            "Training Iteration 2378, Loss: 8.452451705932617\n",
            "Training Iteration 2379, Loss: 5.373013019561768\n",
            "Training Iteration 2380, Loss: 1.8843761682510376\n",
            "Training Iteration 2381, Loss: 6.867952823638916\n",
            "Training Iteration 2382, Loss: 5.71469783782959\n",
            "Training Iteration 2383, Loss: 6.26381778717041\n",
            "Training Iteration 2384, Loss: 4.183978080749512\n",
            "Training Iteration 2385, Loss: 5.750168800354004\n",
            "Training Iteration 2386, Loss: 2.441682815551758\n",
            "Training Iteration 2387, Loss: 5.742823600769043\n",
            "Training Iteration 2388, Loss: 5.485569477081299\n",
            "Training Iteration 2389, Loss: 4.994014739990234\n",
            "Training Iteration 2390, Loss: 7.650544166564941\n",
            "Training Iteration 2391, Loss: 3.888373374938965\n",
            "Training Iteration 2392, Loss: 10.087264060974121\n",
            "Training Iteration 2393, Loss: 5.563000679016113\n",
            "Training Iteration 2394, Loss: 3.2404043674468994\n",
            "Training Iteration 2395, Loss: 4.196282863616943\n",
            "Training Iteration 2396, Loss: 2.996823787689209\n",
            "Training Iteration 2397, Loss: 4.829376697540283\n",
            "Training Iteration 2398, Loss: 6.876116752624512\n",
            "Training Iteration 2399, Loss: 9.300352096557617\n",
            "Training Iteration 2400, Loss: 3.3780393600463867\n",
            "Training Iteration 2401, Loss: 4.143692970275879\n",
            "Training Iteration 2402, Loss: 4.793482303619385\n",
            "Training Iteration 2403, Loss: 7.031045913696289\n",
            "Training Iteration 2404, Loss: 2.597351312637329\n",
            "Training Iteration 2405, Loss: 2.7703213691711426\n",
            "Training Iteration 2406, Loss: 6.01090145111084\n",
            "Training Iteration 2407, Loss: 7.377130031585693\n",
            "Training Iteration 2408, Loss: 3.9865806102752686\n",
            "Training Iteration 2409, Loss: 3.144576072692871\n",
            "Training Iteration 2410, Loss: 4.064338684082031\n",
            "Training Iteration 2411, Loss: 5.737427711486816\n",
            "Training Iteration 2412, Loss: 4.655259132385254\n",
            "Training Iteration 2413, Loss: 5.583418369293213\n",
            "Training Iteration 2414, Loss: 2.009385585784912\n",
            "Training Iteration 2415, Loss: 3.4401276111602783\n",
            "Training Iteration 2416, Loss: 3.4107933044433594\n",
            "Training Iteration 2417, Loss: 5.481151580810547\n",
            "Training Iteration 2418, Loss: 5.908356666564941\n",
            "Training Iteration 2419, Loss: 7.208590507507324\n",
            "Training Iteration 2420, Loss: 5.826445579528809\n",
            "Training Iteration 2421, Loss: 2.224487066268921\n",
            "Training Iteration 2422, Loss: 3.822371482849121\n",
            "Training Iteration 2423, Loss: 4.467358589172363\n",
            "Training Iteration 2424, Loss: 3.59558367729187\n",
            "Training Iteration 2425, Loss: 3.995482921600342\n",
            "Training Iteration 2426, Loss: 1.7672677040100098\n",
            "Training Iteration 2427, Loss: 4.762187480926514\n",
            "Training Iteration 2428, Loss: 6.089903354644775\n",
            "Training Iteration 2429, Loss: 2.988597869873047\n",
            "Training Iteration 2430, Loss: 4.29907751083374\n",
            "Training Iteration 2431, Loss: 4.374546051025391\n",
            "Training Iteration 2432, Loss: 2.347749948501587\n",
            "Training Iteration 2433, Loss: 4.316623687744141\n",
            "Training Iteration 2434, Loss: 3.735663890838623\n",
            "Training Iteration 2435, Loss: 2.3345043659210205\n",
            "Training Iteration 2436, Loss: 4.541077613830566\n",
            "Training Iteration 2437, Loss: 5.221888542175293\n",
            "Training Iteration 2438, Loss: 8.955425262451172\n",
            "Training Iteration 2439, Loss: 4.9805498123168945\n",
            "Training Iteration 2440, Loss: 5.454458713531494\n",
            "Training Iteration 2441, Loss: 4.44822359085083\n",
            "Training Iteration 2442, Loss: 7.21918249130249\n",
            "Training Iteration 2443, Loss: 3.301095962524414\n",
            "Training Iteration 2444, Loss: 8.18031120300293\n",
            "Training Iteration 2445, Loss: 3.3713107109069824\n",
            "Training Iteration 2446, Loss: 5.86147403717041\n",
            "Training Iteration 2447, Loss: 7.148926258087158\n",
            "Training Iteration 2448, Loss: 4.10037088394165\n",
            "Training Iteration 2449, Loss: 3.2732017040252686\n",
            "Training Iteration 2450, Loss: 3.3184986114501953\n",
            "Training Iteration 2451, Loss: 6.2078857421875\n",
            "Training Iteration 2452, Loss: 5.195608139038086\n",
            "Training Iteration 2453, Loss: 2.6180055141448975\n",
            "Training Iteration 2454, Loss: 8.22965145111084\n",
            "Training Iteration 2455, Loss: 6.252856731414795\n",
            "Training Iteration 2456, Loss: 2.295750617980957\n",
            "Training Iteration 2457, Loss: 5.7043023109436035\n",
            "Training Iteration 2458, Loss: 4.566699028015137\n",
            "Training Iteration 2459, Loss: 4.53585147857666\n",
            "Training Iteration 2460, Loss: 4.152232646942139\n",
            "Training Iteration 2461, Loss: 4.6925811767578125\n",
            "Training Iteration 2462, Loss: 3.3986153602600098\n",
            "Training Iteration 2463, Loss: 7.608127117156982\n",
            "Training Iteration 2464, Loss: 3.7522473335266113\n",
            "Training Iteration 2465, Loss: 7.287625789642334\n",
            "Training Iteration 2466, Loss: 6.784297466278076\n",
            "Training Iteration 2467, Loss: 4.842189788818359\n",
            "Training Iteration 2468, Loss: 6.716310977935791\n",
            "Training Iteration 2469, Loss: 2.5018715858459473\n",
            "Training Iteration 2470, Loss: 6.698861122131348\n",
            "Training Iteration 2471, Loss: 9.872661590576172\n",
            "Training Iteration 2472, Loss: 5.600653171539307\n",
            "Training Iteration 2473, Loss: 5.930683135986328\n",
            "Training Iteration 2474, Loss: 4.685229301452637\n",
            "Training Iteration 2475, Loss: 6.484510898590088\n",
            "Training Iteration 2476, Loss: 1.9877829551696777\n",
            "Training Iteration 2477, Loss: 6.120736122131348\n",
            "Training Iteration 2478, Loss: 6.858912467956543\n",
            "Training Iteration 2479, Loss: 6.196334362030029\n",
            "Training Iteration 2480, Loss: 3.2756385803222656\n",
            "Training Iteration 2481, Loss: 5.547974109649658\n",
            "Training Iteration 2482, Loss: 5.862718105316162\n",
            "Training Iteration 2483, Loss: 3.503596067428589\n",
            "Training Iteration 2484, Loss: 3.2995951175689697\n",
            "Training Iteration 2485, Loss: 5.178587913513184\n",
            "Training Iteration 2486, Loss: 4.599837303161621\n",
            "Training Iteration 2487, Loss: 3.757195234298706\n",
            "Training Iteration 2488, Loss: 3.5490152835845947\n",
            "Training Iteration 2489, Loss: 2.387301206588745\n",
            "Training Iteration 2490, Loss: 4.790546417236328\n",
            "Training Iteration 2491, Loss: 6.529880046844482\n",
            "Training Iteration 2492, Loss: 3.07370924949646\n",
            "Training Iteration 2493, Loss: 4.730950355529785\n",
            "Training Iteration 2494, Loss: 4.878329277038574\n",
            "Training Iteration 2495, Loss: 4.17399787902832\n",
            "Training Iteration 2496, Loss: 4.160443305969238\n",
            "Training Iteration 2497, Loss: 2.8860673904418945\n",
            "Training Iteration 2498, Loss: 4.535861968994141\n",
            "Training Iteration 2499, Loss: 5.082385540008545\n",
            "Training Iteration 2500, Loss: 5.453569412231445\n",
            "Training Iteration 2501, Loss: 5.0715837478637695\n",
            "Training Iteration 2502, Loss: 7.33725118637085\n",
            "Training Iteration 2503, Loss: 5.259017467498779\n",
            "Training Iteration 2504, Loss: 3.794757127761841\n",
            "Training Iteration 2505, Loss: 3.9547207355499268\n",
            "Training Iteration 2506, Loss: 5.805983543395996\n",
            "Training Iteration 2507, Loss: 4.794609069824219\n",
            "Training Iteration 2508, Loss: 6.984166145324707\n",
            "Training Iteration 2509, Loss: 6.250825881958008\n",
            "Training Iteration 2510, Loss: 3.2651758193969727\n",
            "Training Iteration 2511, Loss: 5.706177234649658\n",
            "Training Iteration 2512, Loss: 2.5924246311187744\n",
            "Training Iteration 2513, Loss: 3.0896592140197754\n",
            "Training Iteration 2514, Loss: 4.1655802726745605\n",
            "Training Iteration 2515, Loss: 2.738412857055664\n",
            "Training Iteration 2516, Loss: 2.824394464492798\n",
            "Training Iteration 2517, Loss: 4.926981449127197\n",
            "Training Iteration 2518, Loss: 5.376999855041504\n",
            "Training Iteration 2519, Loss: 7.090602397918701\n",
            "Training Iteration 2520, Loss: 4.483549118041992\n",
            "Training Iteration 2521, Loss: 6.585043907165527\n",
            "Training Iteration 2522, Loss: 2.037943124771118\n",
            "Training Iteration 2523, Loss: 3.063826084136963\n",
            "Training Iteration 2524, Loss: 5.763531684875488\n",
            "Training Iteration 2525, Loss: 5.127551555633545\n",
            "Training Iteration 2526, Loss: 4.405789852142334\n",
            "Training Iteration 2527, Loss: 5.984592437744141\n",
            "Training Iteration 2528, Loss: 7.171780586242676\n",
            "Training Iteration 2529, Loss: 6.26326322555542\n",
            "Training Iteration 2530, Loss: 3.29771089553833\n",
            "Training Iteration 2531, Loss: 2.151606559753418\n",
            "Training Iteration 2532, Loss: 5.541629791259766\n",
            "Training Iteration 2533, Loss: 4.822574615478516\n",
            "Training Iteration 2534, Loss: 3.361445188522339\n",
            "Training Iteration 2535, Loss: 3.745032787322998\n",
            "Training Iteration 2536, Loss: 6.690946578979492\n",
            "Training Iteration 2537, Loss: 4.200469970703125\n",
            "Training Iteration 2538, Loss: 7.7235002517700195\n",
            "Training Iteration 2539, Loss: 2.020721197128296\n",
            "Training Iteration 2540, Loss: 4.5597686767578125\n",
            "Training Iteration 2541, Loss: 3.2163240909576416\n",
            "Training Iteration 2542, Loss: 4.145889759063721\n",
            "Training Iteration 2543, Loss: 4.843567848205566\n",
            "Training Iteration 2544, Loss: 4.147834300994873\n",
            "Training Iteration 2545, Loss: 3.3436508178710938\n",
            "Training Iteration 2546, Loss: 5.784453868865967\n",
            "Training Iteration 2547, Loss: 7.032689094543457\n",
            "Training Iteration 2548, Loss: 5.91655969619751\n",
            "Training Iteration 2549, Loss: 7.964868545532227\n",
            "Training Iteration 2550, Loss: 3.1015357971191406\n",
            "Training Iteration 2551, Loss: 3.9728846549987793\n",
            "Training Iteration 2552, Loss: 6.498159408569336\n",
            "Training Iteration 2553, Loss: 4.200052261352539\n",
            "Training Iteration 2554, Loss: 1.2615981101989746\n",
            "Training Iteration 2555, Loss: 4.083096027374268\n",
            "Training Iteration 2556, Loss: 3.7405753135681152\n",
            "Training Iteration 2557, Loss: 4.218559741973877\n",
            "Training Iteration 2558, Loss: 7.441298484802246\n",
            "Training Iteration 2559, Loss: 4.08038330078125\n",
            "Training Iteration 2560, Loss: 5.073681354522705\n",
            "Training Iteration 2561, Loss: 5.217270374298096\n",
            "Training Iteration 2562, Loss: 4.391364097595215\n",
            "Training Iteration 2563, Loss: 3.978097915649414\n",
            "Training Iteration 2564, Loss: 4.74513578414917\n",
            "Training Iteration 2565, Loss: 2.693777561187744\n",
            "Training Iteration 2566, Loss: 9.89547061920166\n",
            "Training Iteration 2567, Loss: 3.252480983734131\n",
            "Training Iteration 2568, Loss: 2.755690336227417\n",
            "Training Iteration 2569, Loss: 5.872446537017822\n",
            "Training Iteration 2570, Loss: 4.312464237213135\n",
            "Training Iteration 2571, Loss: 3.993041515350342\n",
            "Training Iteration 2572, Loss: 4.273260593414307\n",
            "Training Iteration 2573, Loss: 4.766383647918701\n",
            "Training Iteration 2574, Loss: 2.6024811267852783\n",
            "Training Iteration 2575, Loss: 3.253366470336914\n",
            "Training Iteration 2576, Loss: 4.666069030761719\n",
            "Training Iteration 2577, Loss: 2.569185495376587\n",
            "Training Iteration 2578, Loss: 3.838331460952759\n",
            "Training Iteration 2579, Loss: 1.4977076053619385\n",
            "Training Iteration 2580, Loss: 7.040114879608154\n",
            "Training Iteration 2581, Loss: 5.091756343841553\n",
            "Training Iteration 2582, Loss: 6.239074230194092\n",
            "Training Iteration 2583, Loss: 2.6546473503112793\n",
            "Training Iteration 2584, Loss: 4.395773887634277\n",
            "Training Iteration 2585, Loss: 7.052773952484131\n",
            "Training Iteration 2586, Loss: 4.397333145141602\n",
            "Training Iteration 2587, Loss: 6.763524055480957\n",
            "Training Iteration 2588, Loss: 3.165116548538208\n",
            "Training Iteration 2589, Loss: 2.368350028991699\n",
            "Training Iteration 2590, Loss: 5.435206890106201\n",
            "Training Iteration 2591, Loss: 8.056554794311523\n",
            "Training Iteration 2592, Loss: 4.125428199768066\n",
            "Training Iteration 2593, Loss: 6.855061054229736\n",
            "Training Iteration 2594, Loss: 3.9170703887939453\n",
            "Training Iteration 2595, Loss: 6.723325252532959\n",
            "Training Iteration 2596, Loss: 6.964277267456055\n",
            "Training Iteration 2597, Loss: 4.464839935302734\n",
            "Training Iteration 2598, Loss: 4.6514692306518555\n",
            "Training Iteration 2599, Loss: 2.0825917720794678\n",
            "Training Iteration 2600, Loss: 2.8628227710723877\n",
            "Training Iteration 2601, Loss: 5.774450302124023\n",
            "Training Iteration 2602, Loss: 4.335134506225586\n",
            "Training Iteration 2603, Loss: 7.7630767822265625\n",
            "Training Iteration 2604, Loss: 2.6614882946014404\n",
            "Training Iteration 2605, Loss: 4.907515048980713\n",
            "Training Iteration 2606, Loss: 4.118782043457031\n",
            "Training Iteration 2607, Loss: 3.9159626960754395\n",
            "Training Iteration 2608, Loss: 5.265784740447998\n",
            "Training Iteration 2609, Loss: 5.309914588928223\n",
            "Training Iteration 2610, Loss: 5.80019998550415\n",
            "Training Iteration 2611, Loss: 5.066837787628174\n",
            "Training Iteration 2612, Loss: 3.4302470684051514\n",
            "Training Iteration 2613, Loss: 3.6970138549804688\n",
            "Training Iteration 2614, Loss: 4.796478271484375\n",
            "Training Iteration 2615, Loss: 8.423155784606934\n",
            "Training Iteration 2616, Loss: 3.468318223953247\n",
            "Training Iteration 2617, Loss: 7.098679542541504\n",
            "Training Iteration 2618, Loss: 5.3470377922058105\n",
            "Training Iteration 2619, Loss: 6.227141380310059\n",
            "Training Iteration 2620, Loss: 5.509823322296143\n",
            "Training Iteration 2621, Loss: 2.241797685623169\n",
            "Training Iteration 2622, Loss: 5.237236499786377\n",
            "Training Iteration 2623, Loss: 3.889014482498169\n",
            "Training Iteration 2624, Loss: 7.469355583190918\n",
            "Training Iteration 2625, Loss: 2.6221022605895996\n",
            "Training Iteration 2626, Loss: 3.924093008041382\n",
            "Training Iteration 2627, Loss: 6.93745756149292\n",
            "Training Iteration 2628, Loss: 4.511921405792236\n",
            "Training Iteration 2629, Loss: 5.686705589294434\n",
            "Training Iteration 2630, Loss: 7.7861762046813965\n",
            "Training Iteration 2631, Loss: 4.192823886871338\n",
            "Training Iteration 2632, Loss: 6.102433204650879\n",
            "Training Iteration 2633, Loss: 3.267345905303955\n",
            "Training Iteration 2634, Loss: 3.479116201400757\n",
            "Training Iteration 2635, Loss: 3.7627124786376953\n",
            "Training Iteration 2636, Loss: 3.3506858348846436\n",
            "Training Iteration 2637, Loss: 3.1893327236175537\n",
            "Training Iteration 2638, Loss: 3.9191908836364746\n",
            "Training Iteration 2639, Loss: 4.533830165863037\n",
            "Training Iteration 2640, Loss: 3.8939402103424072\n",
            "Training Iteration 2641, Loss: 3.2472007274627686\n",
            "Training Iteration 2642, Loss: 7.054432392120361\n",
            "Training Iteration 2643, Loss: 8.005732536315918\n",
            "Training Iteration 2644, Loss: 4.8857502937316895\n",
            "Training Iteration 2645, Loss: 3.0920073986053467\n",
            "Training Iteration 2646, Loss: 5.509800910949707\n",
            "Training Iteration 2647, Loss: 2.488619327545166\n",
            "Training Iteration 2648, Loss: 4.7331953048706055\n",
            "Training Iteration 2649, Loss: 11.601873397827148\n",
            "Training Iteration 2650, Loss: 8.248210906982422\n",
            "Training Iteration 2651, Loss: 4.401877403259277\n",
            "Training Iteration 2652, Loss: 5.546645164489746\n",
            "Training Iteration 2653, Loss: 6.225579738616943\n",
            "Training Iteration 2654, Loss: 4.012717247009277\n",
            "Training Iteration 2655, Loss: 11.67849063873291\n",
            "Training Iteration 2656, Loss: 2.0761027336120605\n",
            "Training Iteration 2657, Loss: 5.012994289398193\n",
            "Training Iteration 2658, Loss: 2.182469367980957\n",
            "Training Iteration 2659, Loss: 5.673298358917236\n",
            "Training Iteration 2660, Loss: 8.37194538116455\n",
            "Training Iteration 2661, Loss: 9.947189331054688\n",
            "Training Iteration 2662, Loss: 6.353878498077393\n",
            "Training Iteration 2663, Loss: 8.351112365722656\n",
            "Training Iteration 2664, Loss: 6.307336807250977\n",
            "Training Iteration 2665, Loss: 5.172977447509766\n",
            "Training Iteration 2666, Loss: 6.169918060302734\n",
            "Training Iteration 2667, Loss: 4.746305465698242\n",
            "Training Iteration 2668, Loss: 3.854036569595337\n",
            "Training Iteration 2669, Loss: 4.627795219421387\n",
            "Training Iteration 2670, Loss: 4.4677557945251465\n",
            "Training Iteration 2671, Loss: 6.606101036071777\n",
            "Training Iteration 2672, Loss: 5.875450611114502\n",
            "Training Iteration 2673, Loss: 6.851831436157227\n",
            "Training Iteration 2674, Loss: 9.672369003295898\n",
            "Training Iteration 2675, Loss: 2.519385576248169\n",
            "Training Iteration 2676, Loss: 3.420728921890259\n",
            "Training Iteration 2677, Loss: 7.419875621795654\n",
            "Training Iteration 2678, Loss: 4.805400848388672\n",
            "Training Iteration 2679, Loss: 5.467830181121826\n",
            "Training Iteration 2680, Loss: 5.049605369567871\n",
            "Training Iteration 2681, Loss: 3.67256236076355\n",
            "Training Iteration 2682, Loss: 3.283170700073242\n",
            "Training Iteration 2683, Loss: 5.259435653686523\n",
            "Training Iteration 2684, Loss: 4.8230366706848145\n",
            "Training Iteration 2685, Loss: 6.405160427093506\n",
            "Training Iteration 2686, Loss: 5.172576427459717\n",
            "Training Iteration 2687, Loss: 7.978185176849365\n",
            "Training Iteration 2688, Loss: 4.487619400024414\n",
            "Training Iteration 2689, Loss: 3.314275026321411\n",
            "Training Iteration 2690, Loss: 5.506171226501465\n",
            "Training Iteration 2691, Loss: 3.239442825317383\n",
            "Training Iteration 2692, Loss: 2.8430778980255127\n",
            "Training Iteration 2693, Loss: 1.807183861732483\n",
            "Training Iteration 2694, Loss: 5.398279190063477\n",
            "Training Iteration 2695, Loss: 5.152667045593262\n",
            "Training Iteration 2696, Loss: 2.791266918182373\n",
            "Training Iteration 2697, Loss: 5.562290668487549\n",
            "Training Iteration 2698, Loss: 3.8962326049804688\n",
            "Training Iteration 2699, Loss: 3.310701847076416\n",
            "Training Iteration 2700, Loss: 5.6952362060546875\n",
            "Training Iteration 2701, Loss: 3.5978403091430664\n",
            "Training Iteration 2702, Loss: 3.8014378547668457\n",
            "Training Iteration 2703, Loss: 5.809588432312012\n",
            "Training Iteration 2704, Loss: 3.735764265060425\n",
            "Training Iteration 2705, Loss: 1.9679615497589111\n",
            "Training Iteration 2706, Loss: 4.903652191162109\n",
            "Training Iteration 2707, Loss: 6.627894878387451\n",
            "Training Iteration 2708, Loss: 4.636836528778076\n",
            "Training Iteration 2709, Loss: 4.353219509124756\n",
            "Training Iteration 2710, Loss: 4.486025810241699\n",
            "Training Iteration 2711, Loss: 3.742018938064575\n",
            "Training Iteration 2712, Loss: 7.833688735961914\n",
            "Training Iteration 2713, Loss: 2.887299060821533\n",
            "Training Iteration 2714, Loss: 3.890418767929077\n",
            "Training Iteration 2715, Loss: 4.246013164520264\n",
            "Training Iteration 2716, Loss: 4.605939865112305\n",
            "Training Iteration 2717, Loss: 5.08540153503418\n",
            "Training Iteration 2718, Loss: 2.582184314727783\n",
            "Training Iteration 2719, Loss: 6.6216864585876465\n",
            "Training Iteration 2720, Loss: 7.247293949127197\n",
            "Training Iteration 2721, Loss: 4.275510787963867\n",
            "Training Iteration 2722, Loss: 1.7951689958572388\n",
            "Training Iteration 2723, Loss: 3.3132247924804688\n",
            "Training Iteration 2724, Loss: 3.4008169174194336\n",
            "Training Iteration 2725, Loss: 5.1183085441589355\n",
            "Training Iteration 2726, Loss: 5.832920551300049\n",
            "Training Iteration 2727, Loss: 4.662998676300049\n",
            "Training Iteration 2728, Loss: 4.573063373565674\n",
            "Training Iteration 2729, Loss: 4.3397216796875\n",
            "Training Iteration 2730, Loss: 4.629825115203857\n",
            "Training Iteration 2731, Loss: 5.006274223327637\n",
            "Training Iteration 2732, Loss: 6.544537544250488\n",
            "Training Iteration 2733, Loss: 5.272236347198486\n",
            "Training Iteration 2734, Loss: 3.3676857948303223\n",
            "Training Iteration 2735, Loss: 3.687544822692871\n",
            "Training Iteration 2736, Loss: 6.317543029785156\n",
            "Training Iteration 2737, Loss: 1.5537946224212646\n",
            "Training Iteration 2738, Loss: 4.112020015716553\n",
            "Training Iteration 2739, Loss: 5.342556953430176\n",
            "Training Iteration 2740, Loss: 4.380446434020996\n",
            "Training Iteration 2741, Loss: 5.830386161804199\n",
            "Training Iteration 2742, Loss: 3.556094169616699\n",
            "Training Iteration 2743, Loss: 4.658411979675293\n",
            "Training Iteration 2744, Loss: 2.602574348449707\n",
            "Training Iteration 2745, Loss: 5.131337642669678\n",
            "Training Iteration 2746, Loss: 2.5388693809509277\n",
            "Training Iteration 2747, Loss: 6.988533020019531\n",
            "Training Iteration 2748, Loss: 4.494512557983398\n",
            "Training Iteration 2749, Loss: 4.435229301452637\n",
            "Training Iteration 2750, Loss: 2.07558274269104\n",
            "Training Iteration 2751, Loss: 5.955907344818115\n",
            "Training Iteration 2752, Loss: 4.769571781158447\n",
            "Training Iteration 2753, Loss: 4.749790191650391\n",
            "Training Iteration 2754, Loss: 5.430270195007324\n",
            "Training Iteration 2755, Loss: 0.9398195743560791\n",
            "Training Iteration 2756, Loss: 3.761610984802246\n",
            "Training Iteration 2757, Loss: 3.908594846725464\n",
            "Training Iteration 2758, Loss: 3.983365058898926\n",
            "Training Iteration 2759, Loss: 4.6316633224487305\n",
            "Training Iteration 2760, Loss: 4.283685207366943\n",
            "Training Iteration 2761, Loss: 3.6874186992645264\n",
            "Training Iteration 2762, Loss: 2.810006856918335\n",
            "Training Iteration 2763, Loss: 3.1558969020843506\n",
            "Training Iteration 2764, Loss: 3.7470192909240723\n",
            "Training Iteration 2765, Loss: 2.8756778240203857\n",
            "Training Iteration 2766, Loss: 5.1026763916015625\n",
            "Training Iteration 2767, Loss: 3.7447116374969482\n",
            "Training Iteration 2768, Loss: 3.5246307849884033\n",
            "Training Iteration 2769, Loss: 5.121640682220459\n",
            "Training Iteration 2770, Loss: 5.450403690338135\n",
            "Training Iteration 2771, Loss: 3.079211711883545\n",
            "Training Iteration 2772, Loss: 4.088346481323242\n",
            "Training Iteration 2773, Loss: 5.004274845123291\n",
            "Training Iteration 2774, Loss: 2.981881618499756\n",
            "Training Iteration 2775, Loss: 3.15700101852417\n",
            "Training Iteration 2776, Loss: 3.2527847290039062\n",
            "Training Iteration 2777, Loss: 4.783059120178223\n",
            "Training Iteration 2778, Loss: 5.460907936096191\n",
            "Training Iteration 2779, Loss: 3.3562464714050293\n",
            "Training Iteration 2780, Loss: 5.959412574768066\n",
            "Training Iteration 2781, Loss: 3.487353563308716\n",
            "Training Iteration 2782, Loss: 3.8281824588775635\n",
            "Training Iteration 2783, Loss: 3.526581287384033\n",
            "Training Iteration 2784, Loss: 5.603708267211914\n",
            "Training Iteration 2785, Loss: 7.681737899780273\n",
            "Training Iteration 2786, Loss: 3.316945791244507\n",
            "Training Iteration 2787, Loss: 3.533405303955078\n",
            "Training Iteration 2788, Loss: 4.691080093383789\n",
            "Training Iteration 2789, Loss: 5.940725803375244\n",
            "Training Iteration 2790, Loss: 7.078967571258545\n",
            "Training Iteration 2791, Loss: 4.926031112670898\n",
            "Training Iteration 2792, Loss: 3.5277292728424072\n",
            "Training Iteration 2793, Loss: 4.611730098724365\n",
            "Training Iteration 2794, Loss: 5.9246673583984375\n",
            "Training Iteration 2795, Loss: 4.076625823974609\n",
            "Training Iteration 2796, Loss: 4.121079444885254\n",
            "Training Iteration 2797, Loss: 5.460231781005859\n",
            "Training Iteration 2798, Loss: 3.4455807209014893\n",
            "Training Iteration 2799, Loss: 5.366610050201416\n",
            "Training Iteration 2800, Loss: 7.345185279846191\n",
            "Training Iteration 2801, Loss: 7.1228814125061035\n",
            "Training Iteration 2802, Loss: 3.804562568664551\n",
            "Training Iteration 2803, Loss: 3.7524421215057373\n",
            "Training Iteration 2804, Loss: 4.723593235015869\n",
            "Training Iteration 2805, Loss: 7.531917095184326\n",
            "Training Iteration 2806, Loss: 3.0486483573913574\n",
            "Training Iteration 2807, Loss: 5.2019243240356445\n",
            "Training Iteration 2808, Loss: 2.927658796310425\n",
            "Training Iteration 2809, Loss: 5.1039862632751465\n",
            "Training Iteration 2810, Loss: 3.2945456504821777\n",
            "Training Iteration 2811, Loss: 5.350510597229004\n",
            "Training Iteration 2812, Loss: 7.39656925201416\n",
            "Training Iteration 2813, Loss: 5.305854797363281\n",
            "Training Iteration 2814, Loss: 2.4330008029937744\n",
            "Training Iteration 2815, Loss: 7.919321060180664\n",
            "Training Iteration 2816, Loss: 4.989409446716309\n",
            "Training Iteration 2817, Loss: 4.208906650543213\n",
            "Training Iteration 2818, Loss: 5.231343746185303\n",
            "Training Iteration 2819, Loss: 1.9717462062835693\n",
            "Training Iteration 2820, Loss: 6.917727947235107\n",
            "Training Iteration 2821, Loss: 7.370530128479004\n",
            "Training Iteration 2822, Loss: 7.30510139465332\n",
            "Training Iteration 2823, Loss: 1.7306081056594849\n",
            "Training Iteration 2824, Loss: 10.315673828125\n",
            "Training Iteration 2825, Loss: 3.7138631343841553\n",
            "Training Iteration 2826, Loss: 5.773286819458008\n",
            "Training Iteration 2827, Loss: 2.324125051498413\n",
            "Training Iteration 2828, Loss: 5.6501312255859375\n",
            "Training Iteration 2829, Loss: 8.99450969696045\n",
            "Training Iteration 2830, Loss: 5.4991278648376465\n",
            "Training Iteration 2831, Loss: 7.159079551696777\n",
            "Training Iteration 2832, Loss: 2.404693126678467\n",
            "Training Iteration 2833, Loss: 3.065866470336914\n",
            "Training Iteration 2834, Loss: 3.957181215286255\n",
            "Training Iteration 2835, Loss: 4.657650947570801\n",
            "Training Iteration 2836, Loss: 6.4812493324279785\n",
            "Training Iteration 2837, Loss: 4.364438056945801\n",
            "Training Iteration 2838, Loss: 3.626173496246338\n",
            "Training Iteration 2839, Loss: 2.874077081680298\n",
            "Training Iteration 2840, Loss: 7.216243267059326\n",
            "Training Iteration 2841, Loss: 5.4536590576171875\n",
            "Training Iteration 2842, Loss: 2.8053338527679443\n",
            "Training Iteration 2843, Loss: 4.094018459320068\n",
            "Training Iteration 2844, Loss: 3.4429821968078613\n",
            "Training Iteration 2845, Loss: 6.956974029541016\n",
            "Training Iteration 2846, Loss: 4.723640441894531\n",
            "Training Iteration 2847, Loss: 5.162590026855469\n",
            "Training Iteration 2848, Loss: 5.319559097290039\n",
            "Training Iteration 2849, Loss: 3.0668559074401855\n",
            "Training Iteration 2850, Loss: 4.261521816253662\n",
            "Training Iteration 2851, Loss: 2.9758715629577637\n",
            "Training Iteration 2852, Loss: 3.9192399978637695\n",
            "Training Iteration 2853, Loss: 2.4871315956115723\n",
            "Training Iteration 2854, Loss: 8.647546768188477\n",
            "Training Iteration 2855, Loss: 6.186769485473633\n",
            "Training Iteration 2856, Loss: 6.456480026245117\n",
            "Training Iteration 2857, Loss: 5.070140838623047\n",
            "Training Iteration 2858, Loss: 3.0730929374694824\n",
            "Training Iteration 2859, Loss: 4.092610836029053\n",
            "Training Iteration 2860, Loss: 3.4289703369140625\n",
            "Training Iteration 2861, Loss: 4.053662300109863\n",
            "Training Iteration 2862, Loss: 4.73599910736084\n",
            "Training Iteration 2863, Loss: 4.823990821838379\n",
            "Training Iteration 2864, Loss: 3.575666904449463\n",
            "Training Iteration 2865, Loss: 7.378868103027344\n",
            "Training Iteration 2866, Loss: 4.186871528625488\n",
            "Training Iteration 2867, Loss: 7.500909328460693\n",
            "Training Iteration 2868, Loss: 4.196253299713135\n",
            "Training Iteration 2869, Loss: 5.223574638366699\n",
            "Training Iteration 2870, Loss: 1.7408279180526733\n",
            "Training Iteration 2871, Loss: 1.73329496383667\n",
            "Training Iteration 2872, Loss: 3.714561939239502\n",
            "Training Iteration 2873, Loss: 4.842914581298828\n",
            "Training Iteration 2874, Loss: 2.9711523056030273\n",
            "Training Iteration 2875, Loss: 3.193150520324707\n",
            "Training Iteration 2876, Loss: 4.509509086608887\n",
            "Training Iteration 2877, Loss: 3.4159469604492188\n",
            "Training Iteration 2878, Loss: 3.079709529876709\n",
            "Training Iteration 2879, Loss: 7.391694068908691\n",
            "Training Iteration 2880, Loss: 3.8310775756835938\n",
            "Training Iteration 2881, Loss: 4.632351398468018\n",
            "Training Iteration 2882, Loss: 8.205657958984375\n",
            "Training Iteration 2883, Loss: 4.311286926269531\n",
            "Training Iteration 2884, Loss: 3.996778726577759\n",
            "Training Iteration 2885, Loss: 3.5373716354370117\n",
            "Training Iteration 2886, Loss: 1.135693073272705\n",
            "Training Iteration 2887, Loss: 3.599863052368164\n",
            "Training Iteration 2888, Loss: 3.5268516540527344\n",
            "Training Iteration 2889, Loss: 4.431464672088623\n",
            "Training Iteration 2890, Loss: 5.275254249572754\n",
            "Training Iteration 2891, Loss: 4.816353797912598\n",
            "Training Iteration 2892, Loss: 7.785752296447754\n",
            "Training Iteration 2893, Loss: 3.95961332321167\n",
            "Training Iteration 2894, Loss: 5.8007636070251465\n",
            "Training Iteration 2895, Loss: 4.06995964050293\n",
            "Training Iteration 2896, Loss: 4.040095806121826\n",
            "Training Iteration 2897, Loss: 4.1954169273376465\n",
            "Training Iteration 2898, Loss: 2.3505687713623047\n",
            "Training Iteration 2899, Loss: 3.4475419521331787\n",
            "Training Iteration 2900, Loss: 6.153626918792725\n",
            "Training Iteration 2901, Loss: 5.051815032958984\n",
            "Training Iteration 2902, Loss: 4.325016975402832\n",
            "Training Iteration 2903, Loss: 6.946657180786133\n",
            "Training Iteration 2904, Loss: 5.857707500457764\n",
            "Training Iteration 2905, Loss: 5.583157539367676\n",
            "Training Iteration 2906, Loss: 2.3048479557037354\n",
            "Training Iteration 2907, Loss: 4.0776238441467285\n",
            "Training Iteration 2908, Loss: 6.0593461990356445\n",
            "Training Iteration 2909, Loss: 3.8309998512268066\n",
            "Training Iteration 2910, Loss: 4.534771919250488\n",
            "Training Iteration 2911, Loss: 5.8046875\n",
            "Training Iteration 2912, Loss: 7.531520843505859\n",
            "Training Iteration 2913, Loss: 6.494531154632568\n",
            "Training Iteration 2914, Loss: 2.097996234893799\n",
            "Training Iteration 2915, Loss: 3.404646873474121\n",
            "Training Iteration 2916, Loss: 8.13567066192627\n",
            "Training Iteration 2917, Loss: 4.363945960998535\n",
            "Training Iteration 2918, Loss: 4.7508864402771\n",
            "Training Iteration 2919, Loss: 3.65018367767334\n",
            "Training Iteration 2920, Loss: 5.410188674926758\n",
            "Training Iteration 2921, Loss: 9.108622550964355\n",
            "Training Iteration 2922, Loss: 4.714485168457031\n",
            "Training Iteration 2923, Loss: 6.1215033531188965\n",
            "Training Iteration 2924, Loss: 2.957409381866455\n",
            "Training Iteration 2925, Loss: 2.8625411987304688\n",
            "Training Iteration 2926, Loss: 3.5533206462860107\n",
            "Training Iteration 2927, Loss: 2.739025354385376\n",
            "Training Iteration 2928, Loss: 4.672823429107666\n",
            "Training Iteration 2929, Loss: 4.674282550811768\n",
            "Training Iteration 2930, Loss: 3.884187936782837\n",
            "Training Iteration 2931, Loss: 4.976262092590332\n",
            "Training Iteration 2932, Loss: 3.9609858989715576\n",
            "Training Iteration 2933, Loss: 5.801126480102539\n",
            "Training Iteration 2934, Loss: 2.906183958053589\n",
            "Training Iteration 2935, Loss: 6.47998046875\n",
            "Training Iteration 2936, Loss: 1.8852094411849976\n",
            "Training Iteration 2937, Loss: 4.174489974975586\n",
            "Training Iteration 2938, Loss: 2.3782901763916016\n",
            "Training Iteration 2939, Loss: 3.1067471504211426\n",
            "Training Iteration 2940, Loss: 6.468844413757324\n",
            "Training Iteration 2941, Loss: 4.9984822273254395\n",
            "Training Iteration 2942, Loss: 5.9704084396362305\n",
            "Training Iteration 2943, Loss: 5.472682952880859\n",
            "Training Iteration 2944, Loss: 5.684033393859863\n",
            "Training Iteration 2945, Loss: 5.008311748504639\n",
            "Training Iteration 2946, Loss: 4.562450408935547\n",
            "Training Iteration 2947, Loss: 4.606153964996338\n",
            "Training Iteration 2948, Loss: 2.567474126815796\n",
            "Training Iteration 2949, Loss: 6.600585460662842\n",
            "Training Iteration 2950, Loss: 3.1843369007110596\n",
            "Training Iteration 2951, Loss: 4.834526538848877\n",
            "Training Iteration 2952, Loss: 5.3514533042907715\n",
            "Training Iteration 2953, Loss: 6.447166919708252\n",
            "Training Iteration 2954, Loss: 3.5507402420043945\n",
            "Training Iteration 2955, Loss: 4.051122665405273\n",
            "Training Iteration 2956, Loss: 4.419559478759766\n",
            "Training Iteration 2957, Loss: 4.784830093383789\n",
            "Training Iteration 2958, Loss: 7.1392011642456055\n",
            "Training Iteration 2959, Loss: 8.093093872070312\n",
            "Training Iteration 2960, Loss: 6.991946697235107\n",
            "Training Iteration 2961, Loss: 5.425028324127197\n",
            "Training Iteration 2962, Loss: 4.590125560760498\n",
            "Training Iteration 2963, Loss: 1.6487541198730469\n",
            "Training Iteration 2964, Loss: 4.368071556091309\n",
            "Training Iteration 2965, Loss: 2.4702160358428955\n",
            "Training Iteration 2966, Loss: 5.919832706451416\n",
            "Training Iteration 2967, Loss: 4.314811706542969\n",
            "Training Iteration 2968, Loss: 3.8732991218566895\n",
            "Training Iteration 2969, Loss: 3.5603418350219727\n",
            "Training Iteration 2970, Loss: 2.861565113067627\n",
            "Training Iteration 2971, Loss: 6.5554914474487305\n",
            "Training Iteration 2972, Loss: 5.133665084838867\n",
            "Training Iteration 2973, Loss: 4.04093599319458\n",
            "Training Iteration 2974, Loss: 4.582149505615234\n",
            "Training Iteration 2975, Loss: 4.715036869049072\n",
            "Training Iteration 2976, Loss: 5.648619651794434\n",
            "Training Iteration 2977, Loss: 4.264952659606934\n",
            "Training Iteration 2978, Loss: 1.8461248874664307\n",
            "Training Iteration 2979, Loss: 2.4697864055633545\n",
            "Training Iteration 2980, Loss: 2.243729591369629\n",
            "Training Iteration 2981, Loss: 1.6346845626831055\n",
            "Training Iteration 2982, Loss: 4.929625511169434\n",
            "Training Iteration 2983, Loss: 2.4710795879364014\n",
            "Training Iteration 2984, Loss: 3.379091262817383\n",
            "Training Iteration 2985, Loss: 4.490986347198486\n",
            "Training Iteration 2986, Loss: 3.9352855682373047\n",
            "Training Iteration 2987, Loss: 4.8677167892456055\n",
            "Training Iteration 2988, Loss: 3.200228691101074\n",
            "Training Iteration 2989, Loss: 3.7314841747283936\n",
            "Training Iteration 2990, Loss: 2.1496353149414062\n",
            "Training Iteration 2991, Loss: 4.014251232147217\n",
            "Training Iteration 2992, Loss: 2.5902514457702637\n",
            "Training Iteration 2993, Loss: 4.359133720397949\n",
            "Training Iteration 2994, Loss: 2.3300979137420654\n",
            "Training Iteration 2995, Loss: 5.895021915435791\n",
            "Training Iteration 2996, Loss: 2.4763453006744385\n",
            "Training Iteration 2997, Loss: 2.728271484375\n",
            "Training Iteration 2998, Loss: 2.4731855392456055\n",
            "Training Iteration 2999, Loss: 3.3266220092773438\n",
            "Training Iteration 3000, Loss: 5.564821243286133\n",
            "Training Iteration 3001, Loss: 5.466897010803223\n",
            "Training Iteration 3002, Loss: 5.182506084442139\n",
            "Training Iteration 3003, Loss: 3.9908878803253174\n",
            "Training Iteration 3004, Loss: 3.9824936389923096\n",
            "Training Iteration 3005, Loss: 4.658710479736328\n",
            "Training Iteration 3006, Loss: 3.0834100246429443\n",
            "Training Iteration 3007, Loss: 4.84674072265625\n",
            "Training Iteration 3008, Loss: 3.647076368331909\n",
            "Training Iteration 3009, Loss: 4.744793891906738\n",
            "Training Iteration 3010, Loss: 3.3018734455108643\n",
            "Training Iteration 3011, Loss: 3.087674617767334\n",
            "Training Iteration 3012, Loss: 3.6214420795440674\n",
            "Training Iteration 3013, Loss: 5.017999649047852\n",
            "Training Iteration 3014, Loss: 1.892743706703186\n",
            "Training Iteration 3015, Loss: 2.0148608684539795\n",
            "Training Iteration 3016, Loss: 4.449235439300537\n",
            "Training Iteration 3017, Loss: 2.4755327701568604\n",
            "Training Iteration 3018, Loss: 3.9562649726867676\n",
            "Training Iteration 3019, Loss: 1.1528582572937012\n",
            "Training Iteration 3020, Loss: 3.2684760093688965\n",
            "Training Iteration 3021, Loss: 6.401763439178467\n",
            "Training Iteration 3022, Loss: 6.340551853179932\n",
            "Training Iteration 3023, Loss: 4.572362899780273\n",
            "Training Iteration 3024, Loss: 4.381929874420166\n",
            "Training Iteration 3025, Loss: 5.157745361328125\n",
            "Training Iteration 3026, Loss: 5.780033588409424\n",
            "Training Iteration 3027, Loss: 2.3577096462249756\n",
            "Training Iteration 3028, Loss: 5.982212543487549\n",
            "Training Iteration 3029, Loss: 3.9988765716552734\n",
            "Training Iteration 3030, Loss: 6.004798412322998\n",
            "Training Iteration 3031, Loss: 7.568724632263184\n",
            "Training Iteration 3032, Loss: 3.142413854598999\n",
            "Training Iteration 3033, Loss: 4.0492682456970215\n",
            "Training Iteration 3034, Loss: 6.654000759124756\n",
            "Training Iteration 3035, Loss: 5.577548503875732\n",
            "Training Iteration 3036, Loss: 7.945135593414307\n",
            "Training Iteration 3037, Loss: 5.117301940917969\n",
            "Training Iteration 3038, Loss: 4.3015217781066895\n",
            "Training Iteration 3039, Loss: 5.811258316040039\n",
            "Training Iteration 3040, Loss: 2.0412745475769043\n",
            "Training Iteration 3041, Loss: 6.06303071975708\n",
            "Training Iteration 3042, Loss: 7.0379638671875\n",
            "Training Iteration 3043, Loss: 7.79232120513916\n",
            "Training Iteration 3044, Loss: 5.938120365142822\n",
            "Training Iteration 3045, Loss: 6.585648059844971\n",
            "Training Iteration 3046, Loss: 5.334291458129883\n",
            "Training Iteration 3047, Loss: 5.592959403991699\n",
            "Training Iteration 3048, Loss: 4.290660858154297\n",
            "Training Iteration 3049, Loss: 5.272184371948242\n",
            "Training Iteration 3050, Loss: 6.913157939910889\n",
            "Training Iteration 3051, Loss: 4.769187927246094\n",
            "Training Iteration 3052, Loss: 6.448863983154297\n",
            "Training Iteration 3053, Loss: 6.404203414916992\n",
            "Training Iteration 3054, Loss: 4.039294719696045\n",
            "Training Iteration 3055, Loss: 2.227633237838745\n",
            "Training Iteration 3056, Loss: 3.898918390274048\n",
            "Training Iteration 3057, Loss: 4.4714531898498535\n",
            "Training Iteration 3058, Loss: 2.0795412063598633\n",
            "Training Iteration 3059, Loss: 11.49103832244873\n",
            "Training Iteration 3060, Loss: 3.1237049102783203\n",
            "Training Iteration 3061, Loss: 6.863430500030518\n",
            "Training Iteration 3062, Loss: 4.627711772918701\n",
            "Training Iteration 3063, Loss: 2.9778475761413574\n",
            "Training Iteration 3064, Loss: 4.475793361663818\n",
            "Training Iteration 3065, Loss: 6.050816059112549\n",
            "Training Iteration 3066, Loss: 5.007955551147461\n",
            "Training Iteration 3067, Loss: 2.8030683994293213\n",
            "Training Iteration 3068, Loss: 5.0032243728637695\n",
            "Training Iteration 3069, Loss: 4.314707279205322\n",
            "Training Iteration 3070, Loss: 2.1729536056518555\n",
            "Training Iteration 3071, Loss: 4.808442115783691\n",
            "Training Iteration 3072, Loss: 8.293641090393066\n",
            "Training Iteration 3073, Loss: 4.214353084564209\n",
            "Training Iteration 3074, Loss: 4.325474262237549\n",
            "Training Iteration 3075, Loss: 3.1205577850341797\n",
            "Training Iteration 3076, Loss: 4.694347381591797\n",
            "Training Iteration 3077, Loss: 4.2363505363464355\n",
            "Training Iteration 3078, Loss: 5.804850101470947\n",
            "Training Iteration 3079, Loss: 6.634088516235352\n",
            "Training Iteration 3080, Loss: 3.3284220695495605\n",
            "Training Iteration 3081, Loss: 2.552457809448242\n",
            "Training Iteration 3082, Loss: 4.872365474700928\n",
            "Training Iteration 3083, Loss: 2.674830436706543\n",
            "Training Iteration 3084, Loss: 6.2564215660095215\n",
            "Training Iteration 3085, Loss: 2.8962607383728027\n",
            "Training Iteration 3086, Loss: 8.469902038574219\n",
            "Training Iteration 3087, Loss: 5.099955081939697\n",
            "Training Iteration 3088, Loss: 5.662253379821777\n",
            "Training Iteration 3089, Loss: 2.97686767578125\n",
            "Training Iteration 3090, Loss: 3.359304189682007\n",
            "Training Iteration 3091, Loss: 6.7874884605407715\n",
            "Training Iteration 3092, Loss: 8.360038757324219\n",
            "Training Iteration 3093, Loss: 6.335336685180664\n",
            "Training Iteration 3094, Loss: 1.907880425453186\n",
            "Training Iteration 3095, Loss: 4.3942952156066895\n",
            "Training Iteration 3096, Loss: 2.6274828910827637\n",
            "Training Iteration 3097, Loss: 3.4950013160705566\n",
            "Training Iteration 3098, Loss: 3.0179195404052734\n",
            "Training Iteration 3099, Loss: 3.3709681034088135\n",
            "Training Iteration 3100, Loss: 9.016656875610352\n",
            "Training Iteration 3101, Loss: 2.2285869121551514\n",
            "Training Iteration 3102, Loss: 3.115060567855835\n",
            "Training Iteration 3103, Loss: 2.9333386421203613\n",
            "Training Iteration 3104, Loss: 4.41877555847168\n",
            "Training Iteration 3105, Loss: 4.964355945587158\n",
            "Training Iteration 3106, Loss: 3.789548635482788\n",
            "Training Iteration 3107, Loss: 5.447511196136475\n",
            "Training Iteration 3108, Loss: 2.063730239868164\n",
            "Training Iteration 3109, Loss: 9.681802749633789\n",
            "Training Iteration 3110, Loss: 7.622889518737793\n",
            "Training Iteration 3111, Loss: 3.2075579166412354\n",
            "Training Iteration 3112, Loss: 2.902930498123169\n",
            "Training Iteration 3113, Loss: 4.472357273101807\n",
            "Training Iteration 3114, Loss: 5.148575782775879\n",
            "Training Iteration 3115, Loss: 3.5949654579162598\n",
            "Training Iteration 3116, Loss: 2.6761116981506348\n",
            "Training Iteration 3117, Loss: 5.215580463409424\n",
            "Training Iteration 3118, Loss: 5.612442970275879\n",
            "Training Iteration 3119, Loss: 2.8367667198181152\n",
            "Training Iteration 3120, Loss: 4.99831485748291\n",
            "Training Iteration 3121, Loss: 5.2820329666137695\n",
            "Training Iteration 3122, Loss: 5.605879306793213\n",
            "Training Iteration 3123, Loss: 4.7203450202941895\n",
            "Training Iteration 3124, Loss: 6.41103458404541\n",
            "Training Iteration 3125, Loss: 6.68698787689209\n",
            "Training Iteration 3126, Loss: 8.318170547485352\n",
            "Training Iteration 3127, Loss: 4.113746166229248\n",
            "Training Iteration 3128, Loss: 6.0906662940979\n",
            "Training Iteration 3129, Loss: 4.454401969909668\n",
            "Training Iteration 3130, Loss: 5.205179214477539\n",
            "Training Iteration 3131, Loss: 3.906543731689453\n",
            "Training Iteration 3132, Loss: 2.570437431335449\n",
            "Training Iteration 3133, Loss: 7.0875959396362305\n",
            "Training Iteration 3134, Loss: 5.383742809295654\n",
            "Training Iteration 3135, Loss: 3.6052231788635254\n",
            "Training Iteration 3136, Loss: 2.5566961765289307\n",
            "Training Iteration 3137, Loss: 4.23108434677124\n",
            "Training Iteration 3138, Loss: 3.772007465362549\n",
            "Training Iteration 3139, Loss: 3.6412510871887207\n",
            "Training Iteration 3140, Loss: 2.1150448322296143\n",
            "Training Iteration 3141, Loss: 2.9344310760498047\n",
            "Training Iteration 3142, Loss: 2.2687556743621826\n",
            "Training Iteration 3143, Loss: 8.681681632995605\n",
            "Training Iteration 3144, Loss: 2.9088428020477295\n",
            "Training Iteration 3145, Loss: 2.4397149085998535\n",
            "Training Iteration 3146, Loss: 7.233521461486816\n",
            "Training Iteration 3147, Loss: 3.9535984992980957\n",
            "Training Iteration 3148, Loss: 3.151441812515259\n",
            "Training Iteration 3149, Loss: 3.001312255859375\n",
            "Training Iteration 3150, Loss: 8.638635635375977\n",
            "Training Iteration 3151, Loss: 4.0068440437316895\n",
            "Training Iteration 3152, Loss: 3.650127410888672\n",
            "Training Iteration 3153, Loss: 4.066356182098389\n",
            "Training Iteration 3154, Loss: 3.1033732891082764\n",
            "Training Iteration 3155, Loss: 3.6712450981140137\n",
            "Training Iteration 3156, Loss: 4.9991559982299805\n",
            "Training Iteration 3157, Loss: 6.220249176025391\n",
            "Training Iteration 3158, Loss: 2.744966983795166\n",
            "Training Iteration 3159, Loss: 2.4963154792785645\n",
            "Training Iteration 3160, Loss: 3.6845734119415283\n",
            "Training Iteration 3161, Loss: 5.962181091308594\n",
            "Training Iteration 3162, Loss: 2.242243528366089\n",
            "Training Iteration 3163, Loss: 2.490462064743042\n",
            "Training Iteration 3164, Loss: 7.335930824279785\n",
            "Training Iteration 3165, Loss: 6.2534990310668945\n",
            "Training Iteration 3166, Loss: 2.210442304611206\n",
            "Training Iteration 3167, Loss: 5.059701919555664\n",
            "Training Iteration 3168, Loss: 8.416860580444336\n",
            "Training Iteration 3169, Loss: 3.442830801010132\n",
            "Training Iteration 3170, Loss: 5.042506217956543\n",
            "Training Iteration 3171, Loss: 6.714395523071289\n",
            "Training Iteration 3172, Loss: 5.532155513763428\n",
            "Training Iteration 3173, Loss: 4.784346103668213\n",
            "Training Iteration 3174, Loss: 3.2501308917999268\n",
            "Training Iteration 3175, Loss: 4.05019998550415\n",
            "Training Iteration 3176, Loss: 5.678707599639893\n",
            "Training Iteration 3177, Loss: 2.6025307178497314\n",
            "Training Iteration 3178, Loss: 2.870471477508545\n",
            "Training Iteration 3179, Loss: 3.6188740730285645\n",
            "Training Iteration 3180, Loss: 3.730785846710205\n",
            "Training Iteration 3181, Loss: 2.854591131210327\n",
            "Training Iteration 3182, Loss: 3.355952501296997\n",
            "Training Iteration 3183, Loss: 3.042797565460205\n",
            "Training Iteration 3184, Loss: 4.913142681121826\n",
            "Training Iteration 3185, Loss: 5.771612167358398\n",
            "Training Iteration 3186, Loss: 2.857454776763916\n",
            "Training Iteration 3187, Loss: 5.820647716522217\n",
            "Training Iteration 3188, Loss: 2.268829107284546\n",
            "Training Iteration 3189, Loss: 6.005420684814453\n",
            "Training Iteration 3190, Loss: 3.8756601810455322\n",
            "Training Iteration 3191, Loss: 4.124080657958984\n",
            "Training Iteration 3192, Loss: 2.279531955718994\n",
            "Training Iteration 3193, Loss: 2.4405124187469482\n",
            "Training Iteration 3194, Loss: 4.88485050201416\n",
            "Training Iteration 3195, Loss: 3.5740208625793457\n",
            "Training Iteration 3196, Loss: 6.920579433441162\n",
            "Training Iteration 3197, Loss: 5.012936592102051\n",
            "Training Iteration 3198, Loss: 3.821669578552246\n",
            "Training Iteration 3199, Loss: 2.3044044971466064\n",
            "Training Iteration 3200, Loss: 3.4200778007507324\n",
            "Training Iteration 3201, Loss: 4.6765899658203125\n",
            "Training Iteration 3202, Loss: 5.146134376525879\n",
            "Training Iteration 3203, Loss: 2.7810864448547363\n",
            "Training Iteration 3204, Loss: 3.353248357772827\n",
            "Training Iteration 3205, Loss: 2.6444718837738037\n",
            "Training Iteration 3206, Loss: 6.488565444946289\n",
            "Training Iteration 3207, Loss: 4.790472507476807\n",
            "Training Iteration 3208, Loss: 3.858473777770996\n",
            "Training Iteration 3209, Loss: 6.042937278747559\n",
            "Training Iteration 3210, Loss: 5.629792213439941\n",
            "Training Iteration 3211, Loss: 4.823394775390625\n",
            "Training Iteration 3212, Loss: 4.127939701080322\n",
            "Training Iteration 3213, Loss: 4.903100967407227\n",
            "Training Iteration 3214, Loss: 5.343527793884277\n",
            "Training Iteration 3215, Loss: 3.7700252532958984\n",
            "Training Iteration 3216, Loss: 4.100590705871582\n",
            "Training Iteration 3217, Loss: 5.039248466491699\n",
            "Training Iteration 3218, Loss: 3.7685775756835938\n",
            "Training Iteration 3219, Loss: 4.700645446777344\n",
            "Training Iteration 3220, Loss: 1.888595700263977\n",
            "Training Iteration 3221, Loss: 4.998272895812988\n",
            "Training Iteration 3222, Loss: 3.992912530899048\n",
            "Training Iteration 3223, Loss: 3.1317148208618164\n",
            "Training Iteration 3224, Loss: 4.171563625335693\n",
            "Training Iteration 3225, Loss: 6.170701026916504\n",
            "Training Iteration 3226, Loss: 5.051799774169922\n",
            "Training Iteration 3227, Loss: 6.954331874847412\n",
            "Training Iteration 3228, Loss: 3.831104278564453\n",
            "Training Iteration 3229, Loss: 3.1091535091400146\n",
            "Training Iteration 3230, Loss: 3.099803924560547\n",
            "Training Iteration 3231, Loss: 4.255626678466797\n",
            "Training Iteration 3232, Loss: 4.374436855316162\n",
            "Training Iteration 3233, Loss: 7.744272232055664\n",
            "Training Iteration 3234, Loss: 4.390518665313721\n",
            "Training Iteration 3235, Loss: 2.7453043460845947\n",
            "Training Iteration 3236, Loss: 4.155069351196289\n",
            "Training Iteration 3237, Loss: 5.802713871002197\n",
            "Training Iteration 3238, Loss: 5.845866680145264\n",
            "Training Iteration 3239, Loss: 2.942906618118286\n",
            "Training Iteration 3240, Loss: 4.691762924194336\n",
            "Training Iteration 3241, Loss: 4.57586669921875\n",
            "Training Iteration 3242, Loss: 4.878288269042969\n",
            "Training Iteration 3243, Loss: 8.884722709655762\n",
            "Training Iteration 3244, Loss: 4.043124198913574\n",
            "Training Iteration 3245, Loss: 3.6771130561828613\n",
            "Training Iteration 3246, Loss: 5.086527347564697\n",
            "Training Iteration 3247, Loss: 4.2791972160339355\n",
            "Training Iteration 3248, Loss: 4.124235153198242\n",
            "Training Iteration 3249, Loss: 4.6790056228637695\n",
            "Training Iteration 3250, Loss: 2.3897194862365723\n",
            "Training Iteration 3251, Loss: 7.366140365600586\n",
            "Training Iteration 3252, Loss: 5.3613810539245605\n",
            "Training Iteration 3253, Loss: 6.422903537750244\n",
            "Training Iteration 3254, Loss: 5.3567399978637695\n",
            "Training Iteration 3255, Loss: 5.675243377685547\n",
            "Training Iteration 3256, Loss: 3.2209079265594482\n",
            "Training Iteration 3257, Loss: 3.8610451221466064\n",
            "Training Iteration 3258, Loss: 2.637701988220215\n",
            "Training Iteration 3259, Loss: 2.7964377403259277\n",
            "Training Iteration 3260, Loss: 3.69040584564209\n",
            "Training Iteration 3261, Loss: 5.276874542236328\n",
            "Training Iteration 3262, Loss: 3.6501147747039795\n",
            "Training Iteration 3263, Loss: 2.0575668811798096\n",
            "Training Iteration 3264, Loss: 5.343706130981445\n",
            "Training Iteration 3265, Loss: 7.352339744567871\n",
            "Training Iteration 3266, Loss: 8.853870391845703\n",
            "Training Iteration 3267, Loss: 4.376461505889893\n",
            "Training Iteration 3268, Loss: 3.939685821533203\n",
            "Training Iteration 3269, Loss: 4.458723545074463\n",
            "Training Iteration 3270, Loss: 4.121452331542969\n",
            "Training Iteration 3271, Loss: 5.682646751403809\n",
            "Training Iteration 3272, Loss: 4.35670280456543\n",
            "Training Iteration 3273, Loss: 3.7876484394073486\n",
            "Training Iteration 3274, Loss: 5.412257194519043\n",
            "Training Iteration 3275, Loss: 2.764259099960327\n",
            "Training Iteration 3276, Loss: 2.289985418319702\n",
            "Training Iteration 3277, Loss: 5.141597747802734\n",
            "Training Iteration 3278, Loss: 2.653381109237671\n",
            "Training Iteration 3279, Loss: 3.6219120025634766\n",
            "Training Iteration 3280, Loss: 9.05022144317627\n",
            "Training Iteration 3281, Loss: 2.974181890487671\n",
            "Training Iteration 3282, Loss: 5.406076431274414\n",
            "Training Iteration 3283, Loss: 3.2725634574890137\n",
            "Training Iteration 3284, Loss: 6.459556579589844\n",
            "Training Iteration 3285, Loss: 2.241126775741577\n",
            "Training Iteration 3286, Loss: 4.840419769287109\n",
            "Training Iteration 3287, Loss: 5.584585189819336\n",
            "Training Iteration 3288, Loss: 2.8414437770843506\n",
            "Training Iteration 3289, Loss: 5.3988261222839355\n",
            "Training Iteration 3290, Loss: 3.5359652042388916\n",
            "Training Iteration 3291, Loss: 1.9351553916931152\n",
            "Training Iteration 3292, Loss: 2.1911981105804443\n",
            "Training Iteration 3293, Loss: 4.851410865783691\n",
            "Training Iteration 3294, Loss: 2.4726569652557373\n",
            "Training Iteration 3295, Loss: 3.153653621673584\n",
            "Training Iteration 3296, Loss: 4.260794639587402\n",
            "Training Iteration 3297, Loss: 4.747757911682129\n",
            "Training Iteration 3298, Loss: 4.5779948234558105\n",
            "Training Iteration 3299, Loss: 3.6904025077819824\n",
            "Training Iteration 3300, Loss: 4.240676403045654\n",
            "Training Iteration 3301, Loss: 3.503330707550049\n",
            "Training Iteration 3302, Loss: 1.6014045476913452\n",
            "Training Iteration 3303, Loss: 6.699280738830566\n",
            "Training Iteration 3304, Loss: 3.6929619312286377\n",
            "Training Iteration 3305, Loss: 6.435302257537842\n",
            "Training Iteration 3306, Loss: 3.653597831726074\n",
            "Training Iteration 3307, Loss: 5.093164443969727\n",
            "Training Iteration 3308, Loss: 2.2190451622009277\n",
            "Training Iteration 3309, Loss: 7.850623607635498\n",
            "Training Iteration 3310, Loss: 7.217286586761475\n",
            "Training Iteration 3311, Loss: 3.2672383785247803\n",
            "Training Iteration 3312, Loss: 5.600072860717773\n",
            "Training Iteration 3313, Loss: 10.51826286315918\n",
            "Training Iteration 3314, Loss: 1.9282701015472412\n",
            "Training Iteration 3315, Loss: 0.8499170541763306\n",
            "Training Iteration 3316, Loss: 2.9119808673858643\n",
            "Training Iteration 3317, Loss: 3.61639142036438\n",
            "Training Iteration 3318, Loss: 3.3854053020477295\n",
            "Training Iteration 3319, Loss: 3.6600260734558105\n",
            "Training Iteration 3320, Loss: 2.8677515983581543\n",
            "Training Iteration 3321, Loss: 4.025991439819336\n",
            "Training Iteration 3322, Loss: 7.169839859008789\n",
            "Training Iteration 3323, Loss: 3.417653799057007\n",
            "Training Iteration 3324, Loss: 2.6114578247070312\n",
            "Training Iteration 3325, Loss: 3.0008647441864014\n",
            "Training Iteration 3326, Loss: 3.934967279434204\n",
            "Training Iteration 3327, Loss: 3.2286887168884277\n",
            "Training Iteration 3328, Loss: 6.435617923736572\n",
            "Training Iteration 3329, Loss: 6.20531702041626\n",
            "Training Iteration 3330, Loss: 6.052727222442627\n",
            "Training Iteration 3331, Loss: 4.611750602722168\n",
            "Training Iteration 3332, Loss: 4.188079357147217\n",
            "Training Iteration 3333, Loss: 2.7166335582733154\n",
            "Training Iteration 3334, Loss: 6.018646240234375\n",
            "Training Iteration 3335, Loss: 4.939720153808594\n",
            "Training Iteration 3336, Loss: 4.393718242645264\n",
            "Training Iteration 3337, Loss: 3.12119197845459\n",
            "Training Iteration 3338, Loss: 8.408177375793457\n",
            "Training Iteration 3339, Loss: 4.45508337020874\n",
            "Training Iteration 3340, Loss: 7.938343048095703\n",
            "Training Iteration 3341, Loss: 3.891536235809326\n",
            "Training Iteration 3342, Loss: 5.477793216705322\n",
            "Training Iteration 3343, Loss: 2.856292486190796\n",
            "Training Iteration 3344, Loss: 6.963806629180908\n",
            "Training Iteration 3345, Loss: 2.5971293449401855\n",
            "Training Iteration 3346, Loss: 6.0413384437561035\n",
            "Training Iteration 3347, Loss: 3.878936290740967\n",
            "Training Iteration 3348, Loss: 3.5012152194976807\n",
            "Training Iteration 3349, Loss: 6.356437683105469\n",
            "Training Iteration 3350, Loss: 7.14907693862915\n",
            "Training Iteration 3351, Loss: 8.382572174072266\n",
            "Training Iteration 3352, Loss: 5.400665760040283\n",
            "Training Iteration 3353, Loss: 2.9168198108673096\n",
            "Training Iteration 3354, Loss: 4.331048488616943\n",
            "Training Iteration 3355, Loss: 4.9249372482299805\n",
            "Training Iteration 3356, Loss: 3.2018611431121826\n",
            "Training Iteration 3357, Loss: 5.072354316711426\n",
            "Training Iteration 3358, Loss: 8.60166072845459\n",
            "Training Iteration 3359, Loss: 4.385284423828125\n",
            "Training Iteration 3360, Loss: 6.299657344818115\n",
            "Training Iteration 3361, Loss: 4.2422871589660645\n",
            "Training Iteration 3362, Loss: 4.32963228225708\n",
            "Training Iteration 3363, Loss: 6.243772983551025\n",
            "Training Iteration 3364, Loss: 2.875964641571045\n",
            "Training Iteration 3365, Loss: 4.10861349105835\n",
            "Training Iteration 3366, Loss: 4.736443996429443\n",
            "Training Iteration 3367, Loss: 2.8643369674682617\n",
            "Training Iteration 3368, Loss: 5.299365997314453\n",
            "Training Iteration 3369, Loss: 2.3217930793762207\n",
            "Training Iteration 3370, Loss: 4.375577926635742\n",
            "Training Iteration 3371, Loss: 7.549233436584473\n",
            "Training Iteration 3372, Loss: 3.5001111030578613\n",
            "Training Iteration 3373, Loss: 3.0445058345794678\n",
            "Training Iteration 3374, Loss: 3.613258123397827\n",
            "Training Iteration 3375, Loss: 7.5570969581604\n",
            "Training Iteration 3376, Loss: 4.488475799560547\n",
            "Training Iteration 3377, Loss: 3.0349478721618652\n",
            "Training Iteration 3378, Loss: 6.505242824554443\n",
            "Training Iteration 3379, Loss: 2.9236207008361816\n",
            "Training Iteration 3380, Loss: 9.191032409667969\n",
            "Training Iteration 3381, Loss: 7.249110698699951\n",
            "Training Iteration 3382, Loss: 4.9492340087890625\n",
            "Training Iteration 3383, Loss: 4.7247395515441895\n",
            "Training Iteration 3384, Loss: 4.041581153869629\n",
            "Training Iteration 3385, Loss: 3.683048725128174\n",
            "Training Iteration 3386, Loss: 4.338277816772461\n",
            "Training Iteration 3387, Loss: 3.820660352706909\n",
            "Training Iteration 3388, Loss: 3.513779640197754\n",
            "Training Iteration 3389, Loss: 3.874666213989258\n",
            "Training Iteration 3390, Loss: 4.474036693572998\n",
            "Training Iteration 3391, Loss: 4.439147472381592\n",
            "Training Iteration 3392, Loss: 4.643218517303467\n",
            "Training Iteration 3393, Loss: 4.914675712585449\n",
            "Training Iteration 3394, Loss: 5.818415641784668\n",
            "Training Iteration 3395, Loss: 3.801483154296875\n",
            "Training Iteration 3396, Loss: 6.6589484214782715\n",
            "Training Iteration 3397, Loss: 3.9752514362335205\n",
            "Training Iteration 3398, Loss: 4.863091468811035\n",
            "Training Iteration 3399, Loss: 5.016561031341553\n",
            "Training Iteration 3400, Loss: 5.558490753173828\n",
            "Training Iteration 3401, Loss: 5.252157211303711\n",
            "Training Iteration 3402, Loss: 3.107419013977051\n",
            "Training Iteration 3403, Loss: 2.9890034198760986\n",
            "Training Iteration 3404, Loss: 4.199705123901367\n",
            "Training Iteration 3405, Loss: 5.262694358825684\n",
            "Training Iteration 3406, Loss: 2.4920480251312256\n",
            "Training Iteration 3407, Loss: 4.492369174957275\n",
            "Training Iteration 3408, Loss: 5.235459327697754\n",
            "Training Iteration 3409, Loss: 1.5600780248641968\n",
            "Training Iteration 3410, Loss: 6.997575759887695\n",
            "Training Iteration 3411, Loss: 5.0839338302612305\n",
            "Training Iteration 3412, Loss: 5.318018436431885\n",
            "Training Iteration 3413, Loss: 3.87338924407959\n",
            "Training Iteration 3414, Loss: 6.987486839294434\n",
            "Training Iteration 3415, Loss: 3.76208233833313\n",
            "Training Iteration 3416, Loss: 3.964139223098755\n",
            "Training Iteration 3417, Loss: 3.1162266731262207\n",
            "Training Iteration 3418, Loss: 2.183950424194336\n",
            "Training Iteration 3419, Loss: 8.029290199279785\n",
            "Training Iteration 3420, Loss: 5.264766693115234\n",
            "Training Iteration 3421, Loss: 3.864485025405884\n",
            "Training Iteration 3422, Loss: 3.9910213947296143\n",
            "Training Iteration 3423, Loss: 3.625758409500122\n",
            "Training Iteration 3424, Loss: 6.346397876739502\n",
            "Training Iteration 3425, Loss: 8.594167709350586\n",
            "Training Iteration 3426, Loss: 5.543558120727539\n",
            "Training Iteration 3427, Loss: 5.801965713500977\n",
            "Training Iteration 3428, Loss: 4.916087627410889\n",
            "Training Iteration 3429, Loss: 2.229952096939087\n",
            "Training Iteration 3430, Loss: 2.3338494300842285\n",
            "Training Iteration 3431, Loss: 5.126204967498779\n",
            "Training Iteration 3432, Loss: 2.8608851432800293\n",
            "Training Iteration 3433, Loss: 2.6915805339813232\n",
            "Training Iteration 3434, Loss: 5.4804158210754395\n",
            "Training Iteration 3435, Loss: 4.244021415710449\n",
            "Training Iteration 3436, Loss: 4.44052267074585\n",
            "Training Iteration 3437, Loss: 5.652830123901367\n",
            "Training Iteration 3438, Loss: 3.5914745330810547\n",
            "Training Iteration 3439, Loss: 5.89967155456543\n",
            "Training Iteration 3440, Loss: 2.816047430038452\n",
            "Training Iteration 3441, Loss: 3.673356056213379\n",
            "Training Iteration 3442, Loss: 3.623075246810913\n",
            "Training Iteration 3443, Loss: 6.2415266036987305\n",
            "Training Iteration 3444, Loss: 5.159061431884766\n",
            "Training Iteration 3445, Loss: 4.124630928039551\n",
            "Training Iteration 3446, Loss: 3.7134087085723877\n",
            "Training Iteration 3447, Loss: 4.7042622566223145\n",
            "Training Iteration 3448, Loss: 3.61088490486145\n",
            "Training Iteration 3449, Loss: 4.805306434631348\n",
            "Training Iteration 3450, Loss: 4.283688068389893\n",
            "Training Iteration 3451, Loss: 7.375316143035889\n",
            "Training Iteration 3452, Loss: 3.995455503463745\n",
            "Training Iteration 3453, Loss: 4.310716152191162\n",
            "Training Iteration 3454, Loss: 3.4005579948425293\n",
            "Training Iteration 3455, Loss: 4.016861915588379\n",
            "Training Iteration 3456, Loss: 1.791947603225708\n",
            "Training Iteration 3457, Loss: 3.888632297515869\n",
            "Training Iteration 3458, Loss: 7.627511024475098\n",
            "Training Iteration 3459, Loss: 3.7977499961853027\n",
            "Training Iteration 3460, Loss: 2.2215864658355713\n",
            "Training Iteration 3461, Loss: 5.3929877281188965\n",
            "Training Iteration 3462, Loss: 4.749039173126221\n",
            "Training Iteration 3463, Loss: 2.4830408096313477\n",
            "Training Iteration 3464, Loss: 4.156221866607666\n",
            "Training Iteration 3465, Loss: 4.34279203414917\n",
            "Training Iteration 3466, Loss: 2.1960041522979736\n",
            "Training Iteration 3467, Loss: 4.942239284515381\n",
            "Training Iteration 3468, Loss: 2.240219831466675\n",
            "Training Iteration 3469, Loss: 2.9600331783294678\n",
            "Training Iteration 3470, Loss: 4.675926685333252\n",
            "Training Iteration 3471, Loss: 3.1206274032592773\n",
            "Training Iteration 3472, Loss: 3.9838180541992188\n",
            "Training Iteration 3473, Loss: 1.5787742137908936\n",
            "Training Iteration 3474, Loss: 4.713865756988525\n",
            "Training Iteration 3475, Loss: 3.694082021713257\n",
            "Training Iteration 3476, Loss: 5.184329032897949\n",
            "Training Iteration 3477, Loss: 8.251686096191406\n",
            "Training Iteration 3478, Loss: 3.1608824729919434\n",
            "Training Iteration 3479, Loss: 3.9645373821258545\n",
            "Training Iteration 3480, Loss: 6.689099311828613\n",
            "Training Iteration 3481, Loss: 1.364957571029663\n",
            "Training Iteration 3482, Loss: 4.633095741271973\n",
            "Training Iteration 3483, Loss: 5.011145114898682\n",
            "Training Iteration 3484, Loss: 5.403561115264893\n",
            "Training Iteration 3485, Loss: 4.441848278045654\n",
            "Training Iteration 3486, Loss: 3.842550039291382\n",
            "Training Iteration 3487, Loss: 8.23907470703125\n",
            "Training Iteration 3488, Loss: 6.2404890060424805\n",
            "Training Iteration 3489, Loss: 6.2933197021484375\n",
            "Training Iteration 3490, Loss: 6.268692970275879\n",
            "Training Iteration 3491, Loss: 3.982110023498535\n",
            "Training Iteration 3492, Loss: 6.2775092124938965\n",
            "Training Iteration 3493, Loss: 5.015263557434082\n",
            "Training Iteration 3494, Loss: 5.813516616821289\n",
            "Training Iteration 3495, Loss: 8.362138748168945\n",
            "Training Iteration 3496, Loss: 3.0709805488586426\n",
            "Training Iteration 3497, Loss: 3.7933108806610107\n",
            "Training Iteration 3498, Loss: 2.9524741172790527\n",
            "Training Iteration 3499, Loss: 2.870399236679077\n",
            "Training Iteration 3500, Loss: 4.492293357849121\n",
            "Training Iteration 3501, Loss: 6.450634479522705\n",
            "Training Iteration 3502, Loss: 10.479997634887695\n",
            "Training Iteration 3503, Loss: 2.7179369926452637\n",
            "Training Iteration 3504, Loss: 2.042830228805542\n",
            "Training Iteration 3505, Loss: 4.813765048980713\n",
            "Training Iteration 3506, Loss: 6.465424060821533\n",
            "Training Iteration 3507, Loss: 5.5470733642578125\n",
            "Training Iteration 3508, Loss: 4.882067680358887\n",
            "Training Iteration 3509, Loss: 6.203803062438965\n",
            "Training Iteration 3510, Loss: 3.0986385345458984\n",
            "Training Iteration 3511, Loss: 1.9275751113891602\n",
            "Training Iteration 3512, Loss: 4.958527565002441\n",
            "Training Iteration 3513, Loss: 11.227670669555664\n",
            "Training Iteration 3514, Loss: 2.9605164527893066\n",
            "Training Iteration 3515, Loss: 6.542752742767334\n",
            "Training Iteration 3516, Loss: 6.002184867858887\n",
            "Training Iteration 3517, Loss: 3.5770277976989746\n",
            "Training Iteration 3518, Loss: 8.701683044433594\n",
            "Training Iteration 3519, Loss: 6.910592555999756\n",
            "Training Iteration 3520, Loss: 3.0773634910583496\n",
            "Training Iteration 3521, Loss: 3.728278160095215\n",
            "Training Iteration 3522, Loss: 5.8186140060424805\n",
            "Training Iteration 3523, Loss: 5.210505485534668\n",
            "Training Iteration 3524, Loss: 4.335614204406738\n",
            "Training Iteration 3525, Loss: 5.52354097366333\n",
            "Training Iteration 3526, Loss: 1.319879174232483\n",
            "Training Iteration 3527, Loss: 1.889299988746643\n",
            "Training Iteration 3528, Loss: 3.932748794555664\n",
            "Training Iteration 3529, Loss: 5.453948020935059\n",
            "Training Iteration 3530, Loss: 6.120939254760742\n",
            "Training Iteration 3531, Loss: 5.273264408111572\n",
            "Training Iteration 3532, Loss: 3.5373353958129883\n",
            "Training Iteration 3533, Loss: 2.7363240718841553\n",
            "Training Iteration 3534, Loss: 3.3612985610961914\n",
            "Training Iteration 3535, Loss: 4.3108439445495605\n",
            "Training Iteration 3536, Loss: 5.980830192565918\n",
            "Training Iteration 3537, Loss: 4.135443210601807\n",
            "Training Iteration 3538, Loss: 3.0814406871795654\n",
            "Training Iteration 3539, Loss: 5.035295486450195\n",
            "Training Iteration 3540, Loss: 4.1240553855896\n",
            "Training Iteration 3541, Loss: 3.4763944149017334\n",
            "Training Iteration 3542, Loss: 5.3834686279296875\n",
            "Training Iteration 3543, Loss: 5.83664608001709\n",
            "Training Iteration 3544, Loss: 6.095228672027588\n",
            "Training Iteration 3545, Loss: 2.120396137237549\n",
            "Training Iteration 3546, Loss: 5.100910663604736\n",
            "Training Iteration 3547, Loss: 4.824313163757324\n",
            "Training Iteration 3548, Loss: 4.857942581176758\n",
            "Training Iteration 3549, Loss: 8.359261512756348\n",
            "Training Iteration 3550, Loss: 8.301567077636719\n",
            "Training Iteration 3551, Loss: 4.545619010925293\n",
            "Training Iteration 3552, Loss: 5.3826470375061035\n",
            "Training Iteration 3553, Loss: 5.506475448608398\n",
            "Training Iteration 3554, Loss: 4.543795108795166\n",
            "Training Iteration 3555, Loss: 6.107318878173828\n",
            "Training Iteration 3556, Loss: 8.69292163848877\n",
            "Training Iteration 3557, Loss: 4.136532306671143\n",
            "Training Iteration 3558, Loss: 4.57288122177124\n",
            "Training Iteration 3559, Loss: 2.832214593887329\n",
            "Training Iteration 3560, Loss: 5.851707935333252\n",
            "Training Iteration 3561, Loss: 3.9663212299346924\n",
            "Training Iteration 3562, Loss: 6.186354160308838\n",
            "Training Iteration 3563, Loss: 4.855801105499268\n",
            "Training Iteration 3564, Loss: 5.39622163772583\n",
            "Training Iteration 3565, Loss: 8.071979522705078\n",
            "Training Iteration 3566, Loss: 8.314736366271973\n",
            "Training Iteration 3567, Loss: 6.985055446624756\n",
            "Training Iteration 3568, Loss: 3.1228702068328857\n",
            "Training Iteration 3569, Loss: 6.073546886444092\n",
            "Training Iteration 3570, Loss: 3.3981990814208984\n",
            "Training Iteration 3571, Loss: 1.905937671661377\n",
            "Training Iteration 3572, Loss: 5.14286994934082\n",
            "Training Iteration 3573, Loss: 4.913909912109375\n",
            "Training Iteration 3574, Loss: 9.123327255249023\n",
            "Training Iteration 3575, Loss: 2.2772340774536133\n",
            "Training Iteration 3576, Loss: 9.700048446655273\n",
            "Training Iteration 3577, Loss: 4.784924507141113\n",
            "Training Iteration 3578, Loss: 4.650641441345215\n",
            "Training Iteration 3579, Loss: 2.879077672958374\n",
            "Training Iteration 3580, Loss: 4.603222846984863\n",
            "Training Iteration 3581, Loss: 4.306958198547363\n",
            "Training Iteration 3582, Loss: 3.8688995838165283\n",
            "Training Iteration 3583, Loss: 4.915460586547852\n",
            "Training Iteration 3584, Loss: 3.3923473358154297\n",
            "Training Iteration 3585, Loss: 3.070906162261963\n",
            "Training Iteration 3586, Loss: 4.595550060272217\n",
            "Training Iteration 3587, Loss: 5.016104221343994\n",
            "Training Iteration 3588, Loss: 4.3567094802856445\n",
            "Training Iteration 3589, Loss: 5.092030048370361\n",
            "Training Iteration 3590, Loss: 1.0018965005874634\n",
            "Training Iteration 3591, Loss: 10.066349029541016\n",
            "Training Iteration 3592, Loss: 7.437375068664551\n",
            "Training Iteration 3593, Loss: 2.6509387493133545\n",
            "Training Iteration 3594, Loss: 3.7380146980285645\n",
            "Training Iteration 3595, Loss: 8.465898513793945\n",
            "Training Iteration 3596, Loss: 6.710805416107178\n",
            "Training Iteration 3597, Loss: 2.9735307693481445\n",
            "Training Iteration 3598, Loss: 5.329921722412109\n",
            "Training Iteration 3599, Loss: 5.282625198364258\n",
            "Training Iteration 3600, Loss: 5.843848705291748\n",
            "Training Iteration 3601, Loss: 7.894689559936523\n",
            "Training Iteration 3602, Loss: 5.7608819007873535\n",
            "Training Iteration 3603, Loss: 5.229788780212402\n",
            "Training Iteration 3604, Loss: 2.3170242309570312\n",
            "Training Iteration 3605, Loss: 7.574987411499023\n",
            "Training Iteration 3606, Loss: 6.7310380935668945\n",
            "Training Iteration 3607, Loss: 4.372920036315918\n",
            "Training Iteration 3608, Loss: 7.428461074829102\n",
            "Training Iteration 3609, Loss: 5.654722213745117\n",
            "Training Iteration 3610, Loss: 6.084437847137451\n",
            "Training Iteration 3611, Loss: 2.4003353118896484\n",
            "Training Iteration 3612, Loss: 4.8795366287231445\n",
            "Training Iteration 3613, Loss: 5.333766937255859\n",
            "Training Iteration 3614, Loss: 4.87584924697876\n",
            "Training Iteration 3615, Loss: 5.218454837799072\n",
            "Training Iteration 3616, Loss: 5.89910364151001\n",
            "Training Iteration 3617, Loss: 2.3254449367523193\n",
            "Training Iteration 3618, Loss: 2.8847944736480713\n",
            "Training Iteration 3619, Loss: 4.714934825897217\n",
            "Training Iteration 3620, Loss: 5.6367363929748535\n",
            "Training Iteration 3621, Loss: 3.523113250732422\n",
            "Training Iteration 3622, Loss: 3.702554702758789\n",
            "Training Iteration 3623, Loss: 7.54137659072876\n",
            "Training Iteration 3624, Loss: 6.362829685211182\n",
            "Training Iteration 3625, Loss: 5.198805809020996\n",
            "Training Iteration 3626, Loss: 4.085363864898682\n",
            "Training Iteration 3627, Loss: 6.7626166343688965\n",
            "Training Iteration 3628, Loss: 9.892740249633789\n",
            "Training Iteration 3629, Loss: 8.593131065368652\n",
            "Training Iteration 3630, Loss: 4.481109619140625\n",
            "Training Iteration 3631, Loss: 4.090470314025879\n",
            "Training Iteration 3632, Loss: 2.679659128189087\n",
            "Training Iteration 3633, Loss: 5.844210624694824\n",
            "Training Iteration 3634, Loss: 3.0909013748168945\n",
            "Training Iteration 3635, Loss: 6.492148399353027\n",
            "Training Iteration 3636, Loss: 3.4383163452148438\n",
            "Training Iteration 3637, Loss: 1.689022183418274\n",
            "Training Iteration 3638, Loss: 1.7850110530853271\n",
            "Training Iteration 3639, Loss: 7.185738563537598\n",
            "Training Iteration 3640, Loss: 5.395475387573242\n",
            "Training Iteration 3641, Loss: 5.746603012084961\n",
            "Training Iteration 3642, Loss: 5.553861618041992\n",
            "Training Iteration 3643, Loss: 4.212075233459473\n",
            "Training Iteration 3644, Loss: 4.808522701263428\n",
            "Training Iteration 3645, Loss: 7.7104363441467285\n",
            "Training Iteration 3646, Loss: 5.473719596862793\n",
            "Training Iteration 3647, Loss: 3.6941583156585693\n",
            "Training Iteration 3648, Loss: 3.6287026405334473\n",
            "Training Iteration 3649, Loss: 6.022838592529297\n",
            "Training Iteration 3650, Loss: 4.550297737121582\n",
            "Training Iteration 3651, Loss: 3.797269821166992\n",
            "Training Iteration 3652, Loss: 5.573505878448486\n",
            "Training Iteration 3653, Loss: 3.9662394523620605\n",
            "Training Iteration 3654, Loss: 3.5945730209350586\n",
            "Training Iteration 3655, Loss: 4.327882766723633\n",
            "Training Iteration 3656, Loss: 3.155076503753662\n",
            "Training Iteration 3657, Loss: 6.6219282150268555\n",
            "Training Iteration 3658, Loss: 3.369253158569336\n",
            "Training Iteration 3659, Loss: 4.787382125854492\n",
            "Training Iteration 3660, Loss: 3.5008161067962646\n",
            "Training Iteration 3661, Loss: 4.977032661437988\n",
            "Training Iteration 3662, Loss: 2.7958669662475586\n",
            "Training Iteration 3663, Loss: 3.958890199661255\n",
            "Training Iteration 3664, Loss: 5.851819038391113\n",
            "Training Iteration 3665, Loss: 3.2744152545928955\n",
            "Training Iteration 3666, Loss: 5.763855934143066\n",
            "Training Iteration 3667, Loss: 3.951826333999634\n",
            "Training Iteration 3668, Loss: 6.5375237464904785\n",
            "Training Iteration 3669, Loss: 5.9635186195373535\n",
            "Training Iteration 3670, Loss: 4.49835729598999\n",
            "Training Iteration 3671, Loss: 4.599344730377197\n",
            "Training Iteration 3672, Loss: 5.329304218292236\n",
            "Training Iteration 3673, Loss: 3.727001428604126\n",
            "Training Iteration 3674, Loss: 4.286910533905029\n",
            "Training Iteration 3675, Loss: 4.5350022315979\n",
            "Training Iteration 3676, Loss: 2.2023587226867676\n",
            "Training Iteration 3677, Loss: 3.375570774078369\n",
            "Training Iteration 3678, Loss: 6.972943305969238\n",
            "Training Iteration 3679, Loss: 4.746688365936279\n",
            "Training Iteration 3680, Loss: 6.18414831161499\n",
            "Training Iteration 3681, Loss: 6.863783836364746\n",
            "Training Iteration 3682, Loss: 3.55863881111145\n",
            "Training Iteration 3683, Loss: 2.1779918670654297\n",
            "Training Iteration 3684, Loss: 5.53410005569458\n",
            "Training Iteration 3685, Loss: 9.597078323364258\n",
            "Training Iteration 3686, Loss: 4.975927829742432\n",
            "Training Iteration 3687, Loss: 3.5738494396209717\n",
            "Training Iteration 3688, Loss: 4.439465522766113\n",
            "Training Iteration 3689, Loss: 5.691890239715576\n",
            "Training Iteration 3690, Loss: 2.722208023071289\n",
            "Training Iteration 3691, Loss: 2.029971122741699\n",
            "Training Iteration 3692, Loss: 6.098654747009277\n",
            "Training Iteration 3693, Loss: 6.945595741271973\n",
            "Training Iteration 3694, Loss: 3.7921640872955322\n",
            "Training Iteration 3695, Loss: 3.282007932662964\n",
            "Training Iteration 3696, Loss: 2.262585163116455\n",
            "Training Iteration 3697, Loss: 3.923527717590332\n",
            "Training Iteration 3698, Loss: 3.9640274047851562\n",
            "Training Iteration 3699, Loss: 4.600263595581055\n",
            "Training Iteration 3700, Loss: 5.442802429199219\n",
            "Training Iteration 3701, Loss: 3.5324349403381348\n",
            "Training Iteration 3702, Loss: 10.842137336730957\n",
            "Training Iteration 3703, Loss: 4.079369068145752\n",
            "Training Iteration 3704, Loss: 3.645644187927246\n",
            "Training Iteration 3705, Loss: 5.823578357696533\n",
            "Training Iteration 3706, Loss: 2.1613104343414307\n",
            "Training Iteration 3707, Loss: 4.081452369689941\n",
            "Training Iteration 3708, Loss: 2.77201771736145\n",
            "Training Iteration 3709, Loss: 5.727360725402832\n",
            "Training Iteration 3710, Loss: 3.0673584938049316\n",
            "Training Iteration 3711, Loss: 2.5414371490478516\n",
            "Training Iteration 3712, Loss: 3.711014747619629\n",
            "Training Iteration 3713, Loss: 5.712621688842773\n",
            "Training Iteration 3714, Loss: 4.074007987976074\n",
            "Training Iteration 3715, Loss: 2.4096484184265137\n",
            "Training Iteration 3716, Loss: 2.1539065837860107\n",
            "Training Iteration 3717, Loss: 2.9022207260131836\n",
            "Training Iteration 3718, Loss: 5.2253499031066895\n",
            "Training Iteration 3719, Loss: 3.03576922416687\n",
            "Training Iteration 3720, Loss: 1.8322243690490723\n",
            "Training Iteration 3721, Loss: 4.368659019470215\n",
            "Training Iteration 3722, Loss: 4.152540683746338\n",
            "Training Iteration 3723, Loss: 3.671098232269287\n",
            "Training Iteration 3724, Loss: 6.038173198699951\n",
            "Training Iteration 3725, Loss: 5.093850135803223\n",
            "Training Iteration 3726, Loss: 4.967897891998291\n",
            "Training Iteration 3727, Loss: 3.0323188304901123\n",
            "Training Iteration 3728, Loss: 3.3993170261383057\n",
            "Training Iteration 3729, Loss: 5.54076623916626\n",
            "Training Iteration 3730, Loss: 6.236482620239258\n",
            "Training Iteration 3731, Loss: 5.676337718963623\n",
            "Training Iteration 3732, Loss: 6.758134841918945\n",
            "Training Iteration 3733, Loss: 3.9585037231445312\n",
            "Training Iteration 3734, Loss: 4.476745128631592\n",
            "Training Iteration 3735, Loss: 2.3283302783966064\n",
            "Training Iteration 3736, Loss: 5.592606544494629\n",
            "Training Iteration 3737, Loss: 4.339848518371582\n",
            "Training Iteration 3738, Loss: 4.116628170013428\n",
            "Training Iteration 3739, Loss: 5.367506980895996\n",
            "Training Iteration 3740, Loss: 2.3964743614196777\n",
            "Training Iteration 3741, Loss: 3.9759538173675537\n",
            "Training Iteration 3742, Loss: 4.033588886260986\n",
            "Training Iteration 3743, Loss: 4.406191825866699\n",
            "Training Iteration 3744, Loss: 3.0317800045013428\n",
            "Training Iteration 3745, Loss: 5.413975715637207\n",
            "Training Iteration 3746, Loss: 3.7184553146362305\n",
            "Training Iteration 3747, Loss: 4.840462684631348\n",
            "Training Iteration 3748, Loss: 3.744454860687256\n",
            "Training Iteration 3749, Loss: 4.633432388305664\n",
            "Training Iteration 3750, Loss: 6.7794694900512695\n",
            "Training Iteration 3751, Loss: 4.656914234161377\n",
            "Training Iteration 3752, Loss: 4.259122848510742\n",
            "Training Iteration 3753, Loss: 3.1436569690704346\n",
            "Training Iteration 3754, Loss: 3.1405763626098633\n",
            "Training Iteration 3755, Loss: 8.148837089538574\n",
            "Training Iteration 3756, Loss: 3.6141228675842285\n",
            "Training Iteration 3757, Loss: 4.840356349945068\n",
            "Training Iteration 3758, Loss: 3.8168249130249023\n",
            "Training Iteration 3759, Loss: 4.5984039306640625\n",
            "Training Iteration 3760, Loss: 5.695296287536621\n",
            "Training Iteration 3761, Loss: 6.146729946136475\n",
            "Training Iteration 3762, Loss: 3.933454990386963\n",
            "Training Iteration 3763, Loss: 2.0803675651550293\n",
            "Training Iteration 3764, Loss: 3.2092950344085693\n",
            "Training Iteration 3765, Loss: 4.075760841369629\n",
            "Training Iteration 3766, Loss: 3.048604965209961\n",
            "Training Iteration 3767, Loss: 6.4268107414245605\n",
            "Training Iteration 3768, Loss: 3.7173349857330322\n",
            "Training Iteration 3769, Loss: 3.7983944416046143\n",
            "Training Iteration 3770, Loss: 2.2973201274871826\n",
            "Training Iteration 3771, Loss: 4.251873970031738\n",
            "Training Iteration 3772, Loss: 7.991267204284668\n",
            "Training Iteration 3773, Loss: 3.600785493850708\n",
            "Training Iteration 3774, Loss: 4.25078010559082\n",
            "Training Iteration 3775, Loss: 5.254001140594482\n",
            "Training Iteration 3776, Loss: 3.320626974105835\n",
            "Training Iteration 3777, Loss: 5.658435821533203\n",
            "Training Iteration 3778, Loss: 3.6013545989990234\n",
            "Training Iteration 3779, Loss: 6.467602729797363\n",
            "Training Iteration 3780, Loss: 5.554851531982422\n",
            "Training Iteration 3781, Loss: 4.628235340118408\n",
            "Training Iteration 3782, Loss: 5.8833818435668945\n",
            "Training Iteration 3783, Loss: 3.510335683822632\n",
            "Training Iteration 3784, Loss: 3.9291999340057373\n",
            "Training Iteration 3785, Loss: 4.323509216308594\n",
            "Training Iteration 3786, Loss: 2.362133264541626\n",
            "Training Iteration 3787, Loss: 4.260624408721924\n",
            "Training Iteration 3788, Loss: 3.3084557056427\n",
            "Training Iteration 3789, Loss: 3.5695576667785645\n",
            "Training Iteration 3790, Loss: 7.142539978027344\n",
            "Training Iteration 3791, Loss: 4.4964470863342285\n",
            "Training Iteration 3792, Loss: 5.108526706695557\n",
            "Training Iteration 3793, Loss: 4.329880714416504\n",
            "Training Iteration 3794, Loss: 2.971485137939453\n",
            "Training Iteration 3795, Loss: 5.852174282073975\n",
            "Training Iteration 3796, Loss: 5.096033096313477\n",
            "Training Iteration 3797, Loss: 5.370570659637451\n",
            "Training Iteration 3798, Loss: 5.3531904220581055\n",
            "Training Iteration 3799, Loss: 6.063525199890137\n",
            "Training Iteration 3800, Loss: 3.926999807357788\n",
            "Training Iteration 3801, Loss: 6.3964924812316895\n",
            "Training Iteration 3802, Loss: 10.831104278564453\n",
            "Training Iteration 3803, Loss: 6.199167728424072\n",
            "Training Iteration 3804, Loss: 3.7630455493927\n",
            "Training Iteration 3805, Loss: 2.4549434185028076\n",
            "Training Iteration 3806, Loss: 3.3731112480163574\n",
            "Training Iteration 3807, Loss: 6.742085933685303\n",
            "Training Iteration 3808, Loss: 6.6214494705200195\n",
            "Training Iteration 3809, Loss: 6.720683574676514\n",
            "Training Iteration 3810, Loss: 1.4991551637649536\n",
            "Training Iteration 3811, Loss: 1.6794801950454712\n",
            "Training Iteration 3812, Loss: 5.318633556365967\n",
            "Training Iteration 3813, Loss: 2.445936918258667\n",
            "Training Iteration 3814, Loss: 5.549625396728516\n",
            "Training Iteration 3815, Loss: 4.624034404754639\n",
            "Training Iteration 3816, Loss: 3.1240994930267334\n",
            "Training Iteration 3817, Loss: 6.221808433532715\n",
            "Training Iteration 3818, Loss: 4.287939071655273\n",
            "Training Iteration 3819, Loss: 5.033389091491699\n",
            "Training Iteration 3820, Loss: 7.7988691329956055\n",
            "Training Iteration 3821, Loss: 4.819809436798096\n",
            "Training Iteration 3822, Loss: 3.7735095024108887\n",
            "Training Iteration 3823, Loss: 3.1542022228240967\n",
            "Training Iteration 3824, Loss: 5.9153733253479\n",
            "Training Iteration 3825, Loss: 3.627479076385498\n",
            "Training Iteration 3826, Loss: 2.345146894454956\n",
            "Training Iteration 3827, Loss: 3.24783992767334\n",
            "Training Iteration 3828, Loss: 9.854667663574219\n",
            "Training Iteration 3829, Loss: 5.247265338897705\n",
            "Training Iteration 3830, Loss: 4.115333080291748\n",
            "Training Iteration 3831, Loss: 3.71348237991333\n",
            "Training Iteration 3832, Loss: 10.535621643066406\n",
            "Training Iteration 3833, Loss: 8.608697891235352\n",
            "Training Iteration 3834, Loss: 1.7802811861038208\n",
            "Training Iteration 3835, Loss: 5.686659812927246\n",
            "Training Iteration 3836, Loss: 5.644705295562744\n",
            "Training Iteration 3837, Loss: 4.0924577713012695\n",
            "Training Iteration 3838, Loss: 5.912180423736572\n",
            "Training Iteration 3839, Loss: 6.128359794616699\n",
            "Training Iteration 3840, Loss: 4.918891906738281\n",
            "Training Iteration 3841, Loss: 4.378183841705322\n",
            "Training Iteration 3842, Loss: 5.081814765930176\n",
            "Training Iteration 3843, Loss: 3.0271661281585693\n",
            "Training Iteration 3844, Loss: 5.804107189178467\n",
            "Training Iteration 3845, Loss: 2.730678081512451\n",
            "Training Iteration 3846, Loss: 4.802495002746582\n",
            "Training Iteration 3847, Loss: 5.893924713134766\n",
            "Training Iteration 3848, Loss: 4.531156063079834\n",
            "Training Iteration 3849, Loss: 3.9044406414031982\n",
            "Training Iteration 3850, Loss: 5.1919074058532715\n",
            "Training Iteration 3851, Loss: 6.845397472381592\n",
            "Training Iteration 3852, Loss: 4.815290927886963\n",
            "Training Iteration 3853, Loss: 4.207414627075195\n",
            "Training Iteration 3854, Loss: 6.116950511932373\n",
            "Training Iteration 3855, Loss: 5.424163818359375\n",
            "Training Iteration 3856, Loss: 2.266340494155884\n",
            "Training Iteration 3857, Loss: 3.729670524597168\n",
            "Training Iteration 3858, Loss: 5.241050720214844\n",
            "Training Iteration 3859, Loss: 3.1736867427825928\n",
            "Training Iteration 3860, Loss: 2.4970834255218506\n",
            "Training Iteration 3861, Loss: 6.764540672302246\n",
            "Training Iteration 3862, Loss: 2.3395004272460938\n",
            "Training Iteration 3863, Loss: 2.8470659255981445\n",
            "Training Iteration 3864, Loss: 2.173461437225342\n",
            "Training Iteration 3865, Loss: 4.308259963989258\n",
            "Training Iteration 3866, Loss: 2.0670652389526367\n",
            "Training Iteration 3867, Loss: 2.8468716144561768\n",
            "Training Iteration 3868, Loss: 3.1194334030151367\n",
            "Training Iteration 3869, Loss: 8.993175506591797\n",
            "Training Iteration 3870, Loss: 6.340156555175781\n",
            "Training Iteration 3871, Loss: 3.6651251316070557\n",
            "Training Iteration 3872, Loss: 4.353555679321289\n",
            "Training Iteration 3873, Loss: 2.4685888290405273\n",
            "Training Iteration 3874, Loss: 6.041672229766846\n",
            "Training Iteration 3875, Loss: 6.891420364379883\n",
            "Training Iteration 3876, Loss: 5.348750591278076\n",
            "Training Iteration 3877, Loss: 3.0231103897094727\n",
            "Training Iteration 3878, Loss: 2.5853188037872314\n",
            "Training Iteration 3879, Loss: 2.1589038372039795\n",
            "Training Iteration 3880, Loss: 4.105710029602051\n",
            "Training Iteration 3881, Loss: 3.42763614654541\n",
            "Training Iteration 3882, Loss: 2.8984477519989014\n",
            "Training Iteration 3883, Loss: 3.844514846801758\n",
            "Training Iteration 3884, Loss: 2.9646215438842773\n",
            "Training Iteration 3885, Loss: 4.590815544128418\n",
            "Training Iteration 3886, Loss: 3.2471580505371094\n",
            "Training Iteration 3887, Loss: 5.799456596374512\n",
            "Training Iteration 3888, Loss: 6.600184917449951\n",
            "Training Iteration 3889, Loss: 10.36583423614502\n",
            "Training Iteration 3890, Loss: 3.849581718444824\n",
            "Training Iteration 3891, Loss: 3.006909132003784\n",
            "Training Iteration 3892, Loss: 4.547433853149414\n",
            "Training Iteration 3893, Loss: 3.351580858230591\n",
            "Training Iteration 3894, Loss: 8.66010856628418\n",
            "Training Iteration 3895, Loss: 7.696134090423584\n",
            "Training Iteration 3896, Loss: 3.4302945137023926\n",
            "Training Iteration 3897, Loss: 5.719930648803711\n",
            "Training Iteration 3898, Loss: 2.541893720626831\n",
            "Training Iteration 3899, Loss: 2.234866142272949\n",
            "Training Iteration 3900, Loss: 5.9441142082214355\n",
            "Training Iteration 3901, Loss: 7.387038230895996\n",
            "Training Iteration 3902, Loss: 5.444352149963379\n",
            "Training Iteration 3903, Loss: 4.099920749664307\n",
            "Training Iteration 3904, Loss: 4.717439651489258\n",
            "Training Iteration 3905, Loss: 3.855137825012207\n",
            "Training Iteration 3906, Loss: 2.226344108581543\n",
            "Training Iteration 3907, Loss: 4.02144193649292\n",
            "Training Iteration 3908, Loss: 4.974624156951904\n",
            "Training Iteration 3909, Loss: 3.4499425888061523\n",
            "Training Iteration 3910, Loss: 3.2199172973632812\n",
            "Training Iteration 3911, Loss: 4.4143452644348145\n",
            "Training Iteration 3912, Loss: 3.1933324337005615\n",
            "Training Iteration 3913, Loss: 2.7610011100769043\n",
            "Training Iteration 3914, Loss: 7.4057416915893555\n",
            "Training Iteration 3915, Loss: 7.066638946533203\n",
            "Training Iteration 3916, Loss: 4.900308132171631\n",
            "Training Iteration 3917, Loss: 6.462721824645996\n",
            "Training Iteration 3918, Loss: 3.2396509647369385\n",
            "Training Iteration 3919, Loss: 3.297663450241089\n",
            "Training Iteration 3920, Loss: 7.112044811248779\n",
            "Training Iteration 3921, Loss: 11.359914779663086\n",
            "Training Iteration 3922, Loss: 6.031505584716797\n",
            "Training Iteration 3923, Loss: 4.148981094360352\n",
            "Training Iteration 3924, Loss: 3.861722230911255\n",
            "Training Iteration 3925, Loss: 4.449878692626953\n",
            "Training Iteration 3926, Loss: 5.42611837387085\n",
            "Training Iteration 3927, Loss: 4.390109062194824\n",
            "Training Iteration 3928, Loss: 4.211824417114258\n",
            "Training Iteration 3929, Loss: 5.885863304138184\n",
            "Training Iteration 3930, Loss: 2.2645010948181152\n",
            "Training Iteration 3931, Loss: 4.664237976074219\n",
            "Training Iteration 3932, Loss: 1.9570437669754028\n",
            "Training Iteration 3933, Loss: 3.701127529144287\n",
            "Training Iteration 3934, Loss: 5.863494396209717\n",
            "Training Iteration 3935, Loss: 6.7647705078125\n",
            "Training Iteration 3936, Loss: 4.310056209564209\n",
            "Training Iteration 3937, Loss: 5.347967624664307\n",
            "Training Iteration 3938, Loss: 3.499337911605835\n",
            "Training Iteration 3939, Loss: 4.997150421142578\n",
            "Training Iteration 3940, Loss: 3.115006685256958\n",
            "Training Iteration 3941, Loss: 3.216135025024414\n",
            "Training Iteration 3942, Loss: 4.924783706665039\n",
            "Training Iteration 3943, Loss: 4.082572937011719\n",
            "Training Iteration 3944, Loss: 5.554969310760498\n",
            "Training Iteration 3945, Loss: 7.364719867706299\n",
            "Training Iteration 3946, Loss: 5.761327266693115\n",
            "Training Iteration 3947, Loss: 5.77475643157959\n",
            "Training Iteration 3948, Loss: 9.122797012329102\n",
            "Training Iteration 3949, Loss: 3.5812790393829346\n",
            "Training Iteration 3950, Loss: 2.4305694103240967\n",
            "Training Iteration 3951, Loss: 2.840160846710205\n",
            "Training Iteration 3952, Loss: 4.3209075927734375\n",
            "Training Iteration 3953, Loss: 4.877213954925537\n",
            "Training Iteration 3954, Loss: 6.0038557052612305\n",
            "Training Iteration 3955, Loss: 6.101357936859131\n",
            "Training Iteration 3956, Loss: 4.562626361846924\n",
            "Training Iteration 3957, Loss: 4.5529561042785645\n",
            "Training Iteration 3958, Loss: 3.866239070892334\n",
            "Training Iteration 3959, Loss: 4.64717435836792\n",
            "Training Iteration 3960, Loss: 2.491030216217041\n",
            "Training Iteration 3961, Loss: 5.466086387634277\n",
            "Training Iteration 3962, Loss: 4.378818035125732\n",
            "Training Iteration 3963, Loss: 3.1438910961151123\n",
            "Training Iteration 3964, Loss: 4.966433048248291\n",
            "Training Iteration 3965, Loss: 6.049152374267578\n",
            "Training Iteration 3966, Loss: 4.250104904174805\n",
            "Training Iteration 3967, Loss: 5.147618293762207\n",
            "Training Iteration 3968, Loss: 6.696713447570801\n",
            "Training Iteration 3969, Loss: 2.838728904724121\n",
            "Training Iteration 3970, Loss: 4.256288051605225\n",
            "Training Iteration 3971, Loss: 2.8714582920074463\n",
            "Training Iteration 3972, Loss: 8.36448860168457\n",
            "Training Iteration 3973, Loss: 5.05894660949707\n",
            "Training Iteration 3974, Loss: 3.2814223766326904\n",
            "Training Iteration 3975, Loss: 4.9838385581970215\n",
            "Training Iteration 3976, Loss: 3.9650278091430664\n",
            "Training Iteration 3977, Loss: 5.505416393280029\n",
            "Training Iteration 3978, Loss: 4.272247314453125\n",
            "Training Iteration 3979, Loss: 4.922891139984131\n",
            "Training Iteration 3980, Loss: 4.4724812507629395\n",
            "Training Iteration 3981, Loss: 3.310147285461426\n",
            "Training Iteration 3982, Loss: 1.9543310403823853\n",
            "Training Iteration 3983, Loss: 3.114934206008911\n",
            "Training Iteration 3984, Loss: 5.5870466232299805\n",
            "Training Iteration 3985, Loss: 4.7209272384643555\n",
            "Training Iteration 3986, Loss: 5.983071327209473\n",
            "Training Iteration 3987, Loss: 5.010608196258545\n",
            "Training Iteration 3988, Loss: 7.918582916259766\n",
            "Training Iteration 3989, Loss: 9.543249130249023\n",
            "Training Iteration 3990, Loss: 6.7934346199035645\n",
            "Training Iteration 3991, Loss: 5.061190605163574\n",
            "Training Iteration 3992, Loss: 4.7005615234375\n",
            "Training Iteration 3993, Loss: 6.93146276473999\n",
            "Training Iteration 3994, Loss: 5.642223358154297\n",
            "Training Iteration 3995, Loss: 5.4532575607299805\n",
            "Training Iteration 3996, Loss: 4.425140857696533\n",
            "Training Iteration 3997, Loss: 8.603110313415527\n",
            "Training Iteration 3998, Loss: 6.543498992919922\n",
            "Training Iteration 3999, Loss: 4.308029651641846\n",
            "Training Iteration 4000, Loss: 3.5298571586608887\n",
            "Training Iteration 4001, Loss: 4.682650089263916\n",
            "Training Iteration 4002, Loss: 6.517158508300781\n",
            "Training Iteration 4003, Loss: 4.870092391967773\n",
            "Training Iteration 4004, Loss: 4.204008102416992\n",
            "Training Iteration 4005, Loss: 2.440413236618042\n",
            "Training Iteration 4006, Loss: 4.153717994689941\n",
            "Training Iteration 4007, Loss: 5.195157051086426\n",
            "Training Iteration 4008, Loss: 3.393699884414673\n",
            "Training Iteration 4009, Loss: 4.004312038421631\n",
            "Training Iteration 4010, Loss: 3.0592613220214844\n",
            "Training Iteration 4011, Loss: 5.832683563232422\n",
            "Training Iteration 4012, Loss: 2.8331308364868164\n",
            "Training Iteration 4013, Loss: 2.8448104858398438\n",
            "Training Iteration 4014, Loss: 2.7773776054382324\n",
            "Training Iteration 4015, Loss: 3.521562337875366\n",
            "Training Iteration 4016, Loss: 2.0325117111206055\n",
            "Training Iteration 4017, Loss: 7.005633354187012\n",
            "Training Iteration 4018, Loss: 4.179530620574951\n",
            "Training Iteration 4019, Loss: 4.396897315979004\n",
            "Training Iteration 4020, Loss: 5.437301158905029\n",
            "Training Iteration 4021, Loss: 5.655229568481445\n",
            "Training Iteration 4022, Loss: 1.828808069229126\n",
            "Training Iteration 4023, Loss: 0.8887727856636047\n",
            "Training Iteration 4024, Loss: 3.984164237976074\n",
            "Training Iteration 4025, Loss: 6.249207973480225\n",
            "Training Iteration 4026, Loss: 3.342555522918701\n",
            "Training Iteration 4027, Loss: 4.342592239379883\n",
            "Training Iteration 4028, Loss: 5.2275495529174805\n",
            "Training Iteration 4029, Loss: 5.206072807312012\n",
            "Training Iteration 4030, Loss: 4.833229064941406\n",
            "Training Iteration 4031, Loss: 8.572421073913574\n",
            "Training Iteration 4032, Loss: 4.63099479675293\n",
            "Training Iteration 4033, Loss: 3.5159809589385986\n",
            "Training Iteration 4034, Loss: 7.460025787353516\n",
            "Training Iteration 4035, Loss: 4.20552396774292\n",
            "Training Iteration 4036, Loss: 2.9863438606262207\n",
            "Training Iteration 4037, Loss: 2.4342472553253174\n",
            "Training Iteration 4038, Loss: 3.0850400924682617\n",
            "Training Iteration 4039, Loss: 3.119286298751831\n",
            "Training Iteration 4040, Loss: 4.661987781524658\n",
            "Training Iteration 4041, Loss: 2.240264892578125\n",
            "Training Iteration 4042, Loss: 3.763927936553955\n",
            "Training Iteration 4043, Loss: 4.8303542137146\n",
            "Training Iteration 4044, Loss: 1.4979223012924194\n",
            "Training Iteration 4045, Loss: 5.518465518951416\n",
            "Training Iteration 4046, Loss: 4.150464057922363\n",
            "Training Iteration 4047, Loss: 1.0749441385269165\n",
            "Training Iteration 4048, Loss: 3.1716670989990234\n",
            "Training Iteration 4049, Loss: 6.70985746383667\n",
            "Training Iteration 4050, Loss: 4.361808776855469\n",
            "Training Iteration 4051, Loss: 4.622668266296387\n",
            "Training Iteration 4052, Loss: 2.99147629737854\n",
            "Training Iteration 4053, Loss: 6.0675201416015625\n",
            "Training Iteration 4054, Loss: 2.7887628078460693\n",
            "Training Iteration 4055, Loss: 5.963160991668701\n",
            "Training Iteration 4056, Loss: 4.01937198638916\n",
            "Training Iteration 4057, Loss: 4.753287315368652\n",
            "Training Iteration 4058, Loss: 6.851036071777344\n",
            "Training Iteration 4059, Loss: 4.817204475402832\n",
            "Training Iteration 4060, Loss: 9.700264930725098\n",
            "Training Iteration 4061, Loss: 8.490099906921387\n",
            "Training Iteration 4062, Loss: 5.563441753387451\n",
            "Training Iteration 4063, Loss: 5.80634880065918\n",
            "Training Iteration 4064, Loss: 5.259089469909668\n",
            "Training Iteration 4065, Loss: 3.375607967376709\n",
            "Training Iteration 4066, Loss: 7.391909122467041\n",
            "Training Iteration 4067, Loss: 5.708240032196045\n",
            "Training Iteration 4068, Loss: 6.486786842346191\n",
            "Training Iteration 4069, Loss: 4.372575283050537\n",
            "Training Iteration 4070, Loss: 3.847715377807617\n",
            "Training Iteration 4071, Loss: 0.8064252734184265\n",
            "Training Iteration 4072, Loss: 2.8921542167663574\n",
            "Training Iteration 4073, Loss: 6.531278610229492\n",
            "Training Iteration 4074, Loss: 4.144424915313721\n",
            "Training Iteration 4075, Loss: 3.7502522468566895\n",
            "Training Iteration 4076, Loss: 5.921333312988281\n",
            "Training Iteration 4077, Loss: 7.1928534507751465\n",
            "Training Iteration 4078, Loss: 10.191859245300293\n",
            "Training Iteration 4079, Loss: 11.34371566772461\n",
            "Training Iteration 4080, Loss: 3.8292734622955322\n",
            "Training Iteration 4081, Loss: 2.774676561355591\n",
            "Training Iteration 4082, Loss: 4.4958086013793945\n",
            "Training Iteration 4083, Loss: 2.43306565284729\n",
            "Training Iteration 4084, Loss: 3.89797306060791\n",
            "Training Iteration 4085, Loss: 3.7853281497955322\n",
            "Training Iteration 4086, Loss: 4.0490522384643555\n",
            "Training Iteration 4087, Loss: 3.4757003784179688\n",
            "Training Iteration 4088, Loss: 3.838207244873047\n",
            "Training Iteration 4089, Loss: 3.323436975479126\n",
            "Training Iteration 4090, Loss: 5.402675628662109\n",
            "Training Iteration 4091, Loss: 3.676236867904663\n",
            "Training Iteration 4092, Loss: 3.91092848777771\n",
            "Training Iteration 4093, Loss: 5.086980819702148\n",
            "Training Iteration 4094, Loss: 3.3495891094207764\n",
            "Training Iteration 4095, Loss: 4.634252071380615\n",
            "Training Iteration 4096, Loss: 2.299496650695801\n",
            "Training Iteration 4097, Loss: 5.610398769378662\n",
            "Training Iteration 4098, Loss: 6.860240936279297\n",
            "Training Iteration 4099, Loss: 4.227653503417969\n",
            "Training Iteration 4100, Loss: 3.696510076522827\n",
            "Training Iteration 4101, Loss: 4.5260491371154785\n",
            "Training Iteration 4102, Loss: 3.8624796867370605\n",
            "Training Iteration 4103, Loss: 4.027858734130859\n",
            "Training Iteration 4104, Loss: 4.658596992492676\n",
            "Training Iteration 4105, Loss: 3.640780448913574\n",
            "Training Iteration 4106, Loss: 3.5039875507354736\n",
            "Training Iteration 4107, Loss: 8.594389915466309\n",
            "Training Iteration 4108, Loss: 3.0104494094848633\n",
            "Training Iteration 4109, Loss: 5.712658405303955\n",
            "Training Iteration 4110, Loss: 7.576827526092529\n",
            "Training Iteration 4111, Loss: 6.306436061859131\n",
            "Training Iteration 4112, Loss: 2.9611527919769287\n",
            "Training Iteration 4113, Loss: 5.151914596557617\n",
            "Training Iteration 4114, Loss: 4.596896171569824\n",
            "Training Iteration 4115, Loss: 4.415111064910889\n",
            "Training Iteration 4116, Loss: 1.620924711227417\n",
            "Training Iteration 4117, Loss: 4.507114887237549\n",
            "Training Iteration 4118, Loss: 4.930704593658447\n",
            "Training Iteration 4119, Loss: 2.298515796661377\n",
            "Training Iteration 4120, Loss: 2.209364414215088\n",
            "Training Iteration 4121, Loss: 5.065336227416992\n",
            "Training Iteration 4122, Loss: 2.3961803913116455\n",
            "Training Iteration 4123, Loss: 3.6885712146759033\n",
            "Training Iteration 4124, Loss: 5.447000503540039\n",
            "Training Iteration 4125, Loss: 4.373019218444824\n",
            "Training Iteration 4126, Loss: 3.7934632301330566\n",
            "Training Iteration 4127, Loss: 4.099091529846191\n",
            "Training Iteration 4128, Loss: 3.1576361656188965\n",
            "Training Iteration 4129, Loss: 4.349328517913818\n",
            "Training Iteration 4130, Loss: 3.147728443145752\n",
            "Training Iteration 4131, Loss: 5.038453102111816\n",
            "Training Iteration 4132, Loss: 4.7455644607543945\n",
            "Training Iteration 4133, Loss: 5.468215465545654\n",
            "Training Iteration 4134, Loss: 2.953958034515381\n",
            "Training Iteration 4135, Loss: 4.030483722686768\n",
            "Training Iteration 4136, Loss: 2.178473711013794\n",
            "Training Iteration 4137, Loss: 2.591249704360962\n",
            "Training Iteration 4138, Loss: 5.7954912185668945\n",
            "Training Iteration 4139, Loss: 3.300889492034912\n",
            "Training Iteration 4140, Loss: 3.577362060546875\n",
            "Training Iteration 4141, Loss: 6.258340358734131\n",
            "Training Iteration 4142, Loss: 2.9166276454925537\n",
            "Training Iteration 4143, Loss: 3.027479887008667\n",
            "Training Iteration 4144, Loss: 3.341414213180542\n",
            "Training Iteration 4145, Loss: 4.898708343505859\n",
            "Training Iteration 4146, Loss: 4.9099249839782715\n",
            "Training Iteration 4147, Loss: 4.381582736968994\n",
            "Training Iteration 4148, Loss: 4.166473388671875\n",
            "Training Iteration 4149, Loss: 3.624133348464966\n",
            "Training Iteration 4150, Loss: 2.4382240772247314\n",
            "Training Iteration 4151, Loss: 3.138620376586914\n",
            "Training Iteration 4152, Loss: 5.561917781829834\n",
            "Training Iteration 4153, Loss: 4.280415058135986\n",
            "Training Iteration 4154, Loss: 1.8730061054229736\n",
            "Training Iteration 4155, Loss: 4.231096267700195\n",
            "Training Iteration 4156, Loss: 1.873907446861267\n",
            "Training Iteration 4157, Loss: 4.491944789886475\n",
            "Training Iteration 4158, Loss: 5.378626823425293\n",
            "Training Iteration 4159, Loss: 2.590973138809204\n",
            "Training Iteration 4160, Loss: 3.7802038192749023\n",
            "Training Iteration 4161, Loss: 4.879653453826904\n",
            "Training Iteration 4162, Loss: 2.4165685176849365\n",
            "Training Iteration 4163, Loss: 3.6193108558654785\n",
            "Training Iteration 4164, Loss: 2.624532699584961\n",
            "Training Iteration 4165, Loss: 5.51369047164917\n",
            "Training Iteration 4166, Loss: 5.36815071105957\n",
            "Training Iteration 4167, Loss: 3.8234469890594482\n",
            "Training Iteration 4168, Loss: 4.755743503570557\n",
            "Training Iteration 4169, Loss: 3.378650188446045\n",
            "Training Iteration 4170, Loss: 3.0668387413024902\n",
            "Training Iteration 4171, Loss: 4.558781147003174\n",
            "Training Iteration 4172, Loss: 4.6866254806518555\n",
            "Training Iteration 4173, Loss: 6.465662956237793\n",
            "Training Iteration 4174, Loss: 0.8329194188117981\n",
            "Training Iteration 4175, Loss: 4.524609088897705\n",
            "Training Iteration 4176, Loss: 5.657257556915283\n",
            "Training Iteration 4177, Loss: 4.493192195892334\n",
            "Training Iteration 4178, Loss: 3.675962448120117\n",
            "Training Iteration 4179, Loss: 4.52329683303833\n",
            "Training Iteration 4180, Loss: 3.8904876708984375\n",
            "Training Iteration 4181, Loss: 2.548031806945801\n",
            "Training Iteration 4182, Loss: 2.5648741722106934\n",
            "Training Iteration 4183, Loss: 3.5446338653564453\n",
            "Training Iteration 4184, Loss: 5.974809646606445\n",
            "Training Iteration 4185, Loss: 3.283081531524658\n",
            "Training Iteration 4186, Loss: 1.7715463638305664\n",
            "Training Iteration 4187, Loss: 4.894852638244629\n",
            "Training Iteration 4188, Loss: 4.204901695251465\n",
            "Training Iteration 4189, Loss: 4.162456512451172\n",
            "Training Iteration 4190, Loss: 7.026378154754639\n",
            "Training Iteration 4191, Loss: 4.420185089111328\n",
            "Training Iteration 4192, Loss: 7.807714462280273\n",
            "Training Iteration 4193, Loss: 6.052344799041748\n",
            "Training Iteration 4194, Loss: 3.9127120971679688\n",
            "Training Iteration 4195, Loss: 3.8912296295166016\n",
            "Training Iteration 4196, Loss: 4.227322578430176\n",
            "Training Iteration 4197, Loss: 6.122692108154297\n",
            "Training Iteration 4198, Loss: 7.1009202003479\n",
            "Training Iteration 4199, Loss: 5.932921409606934\n",
            "Training Iteration 4200, Loss: 6.398794651031494\n",
            "Training Iteration 4201, Loss: 5.446247577667236\n",
            "Training Iteration 4202, Loss: 2.935879707336426\n",
            "Training Iteration 4203, Loss: 4.728619575500488\n",
            "Training Iteration 4204, Loss: 3.0798232555389404\n",
            "Training Iteration 4205, Loss: 4.922260761260986\n",
            "Training Iteration 4206, Loss: 3.1142804622650146\n",
            "Training Iteration 4207, Loss: 3.272250175476074\n",
            "Training Iteration 4208, Loss: 6.455350399017334\n",
            "Training Iteration 4209, Loss: 4.107499122619629\n",
            "Training Iteration 4210, Loss: 1.4257639646530151\n",
            "Training Iteration 4211, Loss: 6.792999744415283\n",
            "Training Iteration 4212, Loss: 6.187300682067871\n",
            "Training Iteration 4213, Loss: 3.6809885501861572\n",
            "Training Iteration 4214, Loss: 2.9897866249084473\n",
            "Training Iteration 4215, Loss: 2.8848116397857666\n",
            "Training Iteration 4216, Loss: 5.127846717834473\n",
            "Training Iteration 4217, Loss: 5.747971534729004\n",
            "Training Iteration 4218, Loss: 6.54512882232666\n",
            "Training Iteration 4219, Loss: 6.121463775634766\n",
            "Training Iteration 4220, Loss: 4.476353645324707\n",
            "Training Iteration 4221, Loss: 3.9140970706939697\n",
            "Training Iteration 4222, Loss: 6.603029727935791\n",
            "Training Iteration 4223, Loss: 4.548418998718262\n",
            "Training Iteration 4224, Loss: 3.981668710708618\n",
            "Training Iteration 4225, Loss: 8.673118591308594\n",
            "Training Iteration 4226, Loss: 4.656014919281006\n",
            "Training Iteration 4227, Loss: 3.6237242221832275\n",
            "Training Iteration 4228, Loss: 4.836084842681885\n",
            "Training Iteration 4229, Loss: 3.188502550125122\n",
            "Training Iteration 4230, Loss: 4.74619197845459\n",
            "Training Iteration 4231, Loss: 2.658803939819336\n",
            "Training Iteration 4232, Loss: 6.6818695068359375\n",
            "Training Iteration 4233, Loss: 3.0529580116271973\n",
            "Training Iteration 4234, Loss: 4.925340175628662\n",
            "Training Iteration 4235, Loss: 2.8342971801757812\n",
            "Training Iteration 4236, Loss: 5.1485748291015625\n",
            "Training Iteration 4237, Loss: 7.669486999511719\n",
            "Training Iteration 4238, Loss: 4.520724296569824\n",
            "Training Iteration 4239, Loss: 4.714727401733398\n",
            "Training Iteration 4240, Loss: 3.936547040939331\n",
            "Training Iteration 4241, Loss: 3.9799699783325195\n",
            "Training Iteration 4242, Loss: 3.7741432189941406\n",
            "Training Iteration 4243, Loss: 4.408970832824707\n",
            "Training Iteration 4244, Loss: 3.9791550636291504\n",
            "Training Iteration 4245, Loss: 8.182845115661621\n",
            "Training Iteration 4246, Loss: 7.149989604949951\n",
            "Training Iteration 4247, Loss: 4.226271629333496\n",
            "Training Iteration 4248, Loss: 4.499422073364258\n",
            "Training Iteration 4249, Loss: 2.7485432624816895\n",
            "Training Iteration 4250, Loss: 4.7668867111206055\n",
            "Training Iteration 4251, Loss: 3.746528387069702\n",
            "Training Iteration 4252, Loss: 5.36517333984375\n",
            "Training Iteration 4253, Loss: 6.301311016082764\n",
            "Training Iteration 4254, Loss: 3.2409462928771973\n",
            "Training Iteration 4255, Loss: 4.538008689880371\n",
            "Training Iteration 4256, Loss: 7.0432538986206055\n",
            "Training Iteration 4257, Loss: 5.758389472961426\n",
            "Training Iteration 4258, Loss: 5.642396926879883\n",
            "Training Iteration 4259, Loss: 6.087884426116943\n",
            "Training Iteration 4260, Loss: 3.370234251022339\n",
            "Training Iteration 4261, Loss: 4.2670674324035645\n",
            "Training Iteration 4262, Loss: 3.5163941383361816\n",
            "Training Iteration 4263, Loss: 7.071956157684326\n",
            "Training Iteration 4264, Loss: 3.7186267375946045\n",
            "Training Iteration 4265, Loss: 5.5099873542785645\n",
            "Training Iteration 4266, Loss: 3.6569766998291016\n",
            "Training Iteration 4267, Loss: 5.583654880523682\n",
            "Training Iteration 4268, Loss: 3.656620502471924\n",
            "Training Iteration 4269, Loss: 5.7820210456848145\n",
            "Training Iteration 4270, Loss: 2.1046738624572754\n",
            "Training Iteration 4271, Loss: 5.251148700714111\n",
            "Training Iteration 4272, Loss: 3.940579414367676\n",
            "Training Iteration 4273, Loss: 5.286019802093506\n",
            "Training Iteration 4274, Loss: 2.9373669624328613\n",
            "Training Iteration 4275, Loss: 3.7331595420837402\n",
            "Training Iteration 4276, Loss: 3.503267288208008\n",
            "Training Iteration 4277, Loss: 7.8147101402282715\n",
            "Training Iteration 4278, Loss: 1.715881109237671\n",
            "Training Iteration 4279, Loss: 6.98387336730957\n",
            "Training Iteration 4280, Loss: 5.490574836730957\n",
            "Training Iteration 4281, Loss: 3.1327974796295166\n",
            "Training Iteration 4282, Loss: 4.254212856292725\n",
            "Training Iteration 4283, Loss: 1.9178125858306885\n",
            "Training Iteration 4284, Loss: 1.0053306818008423\n",
            "Training Iteration 4285, Loss: 3.2428314685821533\n",
            "Training Iteration 4286, Loss: 3.3944790363311768\n",
            "Training Iteration 4287, Loss: 3.778944969177246\n",
            "Training Iteration 4288, Loss: 3.5834221839904785\n",
            "Training Iteration 4289, Loss: 4.836748123168945\n",
            "Training Iteration 4290, Loss: 6.029175758361816\n",
            "Training Iteration 4291, Loss: 4.411265850067139\n",
            "Training Iteration 4292, Loss: 6.431166648864746\n",
            "Training Iteration 4293, Loss: 4.212310791015625\n",
            "Training Iteration 4294, Loss: 3.684415102005005\n",
            "Training Iteration 4295, Loss: 2.3306896686553955\n",
            "Training Iteration 4296, Loss: 7.227050304412842\n",
            "Training Iteration 4297, Loss: 3.923469066619873\n",
            "Training Iteration 4298, Loss: 2.2292909622192383\n",
            "Training Iteration 4299, Loss: 3.479666233062744\n",
            "Training Iteration 4300, Loss: 2.549650192260742\n",
            "Training Iteration 4301, Loss: 4.023860931396484\n",
            "Training Iteration 4302, Loss: 1.792912483215332\n",
            "Training Iteration 4303, Loss: 4.935276985168457\n",
            "Training Iteration 4304, Loss: 1.4085050821304321\n",
            "Training Iteration 4305, Loss: 6.212574481964111\n",
            "Training Iteration 4306, Loss: 6.638224124908447\n",
            "Training Iteration 4307, Loss: 7.050832748413086\n",
            "Training Iteration 4308, Loss: 3.3039350509643555\n",
            "Training Iteration 4309, Loss: 4.088672637939453\n",
            "Training Iteration 4310, Loss: 7.534601211547852\n",
            "Training Iteration 4311, Loss: 3.850588321685791\n",
            "Training Iteration 4312, Loss: 5.042212009429932\n",
            "Training Iteration 4313, Loss: 5.338993072509766\n",
            "Training Iteration 4314, Loss: 2.4281980991363525\n",
            "Training Iteration 4315, Loss: 4.426746845245361\n",
            "Training Iteration 4316, Loss: 3.7543513774871826\n",
            "Training Iteration 4317, Loss: 3.667165517807007\n",
            "Training Iteration 4318, Loss: 4.789634704589844\n",
            "Training Iteration 4319, Loss: 2.289820671081543\n",
            "Training Iteration 4320, Loss: 4.930822372436523\n",
            "Training Iteration 4321, Loss: 3.743161678314209\n",
            "Training Iteration 4322, Loss: 5.046027660369873\n",
            "Training Iteration 4323, Loss: 4.471664905548096\n",
            "Training Iteration 4324, Loss: 6.240814208984375\n",
            "Training Iteration 4325, Loss: 7.283035755157471\n",
            "Training Iteration 4326, Loss: 3.402320623397827\n",
            "Training Iteration 4327, Loss: 7.612791538238525\n",
            "Training Iteration 4328, Loss: 7.000093460083008\n",
            "Training Iteration 4329, Loss: 4.554266929626465\n",
            "Training Iteration 4330, Loss: 5.791049957275391\n",
            "Training Iteration 4331, Loss: 4.177234649658203\n",
            "Training Iteration 4332, Loss: 4.6954264640808105\n",
            "Training Iteration 4333, Loss: 4.927524566650391\n",
            "Training Iteration 4334, Loss: 5.2125935554504395\n",
            "Training Iteration 4335, Loss: 12.47158432006836\n",
            "Training Iteration 4336, Loss: 8.309700012207031\n",
            "Training Iteration 4337, Loss: 5.900315761566162\n",
            "Training Iteration 4338, Loss: 12.403564453125\n",
            "Training Iteration 4339, Loss: 1.1899577379226685\n",
            "Training Iteration 4340, Loss: 6.729864597320557\n",
            "Training Iteration 4341, Loss: 6.239897727966309\n",
            "Training Iteration 4342, Loss: 7.125284194946289\n",
            "Training Iteration 4343, Loss: 5.766955375671387\n",
            "Training Iteration 4344, Loss: 4.066128730773926\n",
            "Training Iteration 4345, Loss: 4.323575973510742\n",
            "Training Iteration 4346, Loss: 6.039192199707031\n",
            "Training Iteration 4347, Loss: 5.25781774520874\n",
            "Training Iteration 4348, Loss: 5.087911605834961\n",
            "Training Iteration 4349, Loss: 4.8119049072265625\n",
            "Training Iteration 4350, Loss: 4.095269680023193\n",
            "Training Iteration 4351, Loss: 4.378172874450684\n",
            "Training Iteration 4352, Loss: 3.4790470600128174\n",
            "Training Iteration 4353, Loss: 7.13994026184082\n",
            "Training Iteration 4354, Loss: 3.997009515762329\n",
            "Training Iteration 4355, Loss: 3.892603874206543\n",
            "Training Iteration 4356, Loss: 2.0367608070373535\n",
            "Training Iteration 4357, Loss: 4.295228958129883\n",
            "Training Iteration 4358, Loss: 5.682018756866455\n",
            "Training Iteration 4359, Loss: 4.148293972015381\n",
            "Training Iteration 4360, Loss: 5.681657791137695\n",
            "Training Iteration 4361, Loss: 1.5063221454620361\n",
            "Training Iteration 4362, Loss: 4.299022197723389\n",
            "Training Iteration 4363, Loss: 8.69309139251709\n",
            "Training Iteration 4364, Loss: 4.49674654006958\n",
            "Training Iteration 4365, Loss: 3.841005563735962\n",
            "Training Iteration 4366, Loss: 6.66790771484375\n",
            "Training Iteration 4367, Loss: 5.331765174865723\n",
            "Training Iteration 4368, Loss: 5.575339317321777\n",
            "Training Iteration 4369, Loss: 3.3135156631469727\n",
            "Training Iteration 4370, Loss: 2.97959041595459\n",
            "Training Iteration 4371, Loss: 3.147174119949341\n",
            "Training Iteration 4372, Loss: 4.219118118286133\n",
            "Training Iteration 4373, Loss: 3.6568753719329834\n",
            "Training Iteration 4374, Loss: 2.8570468425750732\n",
            "Training Iteration 4375, Loss: 4.3717780113220215\n",
            "Training Iteration 4376, Loss: 5.445920467376709\n",
            "Training Iteration 4377, Loss: 6.3158979415893555\n",
            "Training Iteration 4378, Loss: 5.43886137008667\n",
            "Training Iteration 4379, Loss: 1.3758748769760132\n",
            "Training Iteration 4380, Loss: 3.1600558757781982\n",
            "Training Iteration 4381, Loss: 2.5357935428619385\n",
            "Training Iteration 4382, Loss: 2.981231451034546\n",
            "Training Iteration 4383, Loss: 4.576098442077637\n",
            "Training Iteration 4384, Loss: 2.1775898933410645\n",
            "Training Iteration 4385, Loss: 3.378342390060425\n",
            "Training Iteration 4386, Loss: 3.441556692123413\n",
            "Training Iteration 4387, Loss: 2.9395763874053955\n",
            "Training Iteration 4388, Loss: 4.002547740936279\n",
            "Training Iteration 4389, Loss: 2.9815783500671387\n",
            "Training Iteration 4390, Loss: 2.7360193729400635\n",
            "Training Iteration 4391, Loss: 10.459563255310059\n",
            "Training Iteration 4392, Loss: 5.080697059631348\n",
            "Training Iteration 4393, Loss: 7.790631294250488\n",
            "Training Iteration 4394, Loss: 5.427337169647217\n",
            "Training Iteration 4395, Loss: 5.365054607391357\n",
            "Training Iteration 4396, Loss: 2.802619695663452\n",
            "Training Iteration 4397, Loss: 2.86557674407959\n",
            "Training Iteration 4398, Loss: 4.151025295257568\n",
            "Training Iteration 4399, Loss: 3.820856809616089\n",
            "Training Iteration 4400, Loss: 3.8220417499542236\n",
            "Training Iteration 4401, Loss: 2.871838092803955\n",
            "Training Iteration 4402, Loss: 4.749434947967529\n",
            "Training Iteration 4403, Loss: 7.9207444190979\n",
            "Training Iteration 4404, Loss: 5.523550987243652\n",
            "Training Iteration 4405, Loss: 6.736556053161621\n",
            "Training Iteration 4406, Loss: 5.122655868530273\n",
            "Training Iteration 4407, Loss: 4.67625093460083\n",
            "Training Iteration 4408, Loss: 4.563779354095459\n",
            "Training Iteration 4409, Loss: 6.746577262878418\n",
            "Training Iteration 4410, Loss: 5.49452018737793\n",
            "Training Iteration 4411, Loss: 5.165025234222412\n",
            "Training Iteration 4412, Loss: 5.049530982971191\n",
            "Training Iteration 4413, Loss: 4.238521575927734\n",
            "Training Iteration 4414, Loss: 3.459322690963745\n",
            "Training Iteration 4415, Loss: 3.2331032752990723\n",
            "Training Iteration 4416, Loss: 4.943950176239014\n",
            "Training Iteration 4417, Loss: 5.018021583557129\n",
            "Training Iteration 4418, Loss: 7.246866226196289\n",
            "Training Iteration 4419, Loss: 3.1030707359313965\n",
            "Training Iteration 4420, Loss: 3.9674692153930664\n",
            "Training Iteration 4421, Loss: 8.363489151000977\n",
            "Training Iteration 4422, Loss: 5.3493475914001465\n",
            "Training Iteration 4423, Loss: 7.4223856925964355\n",
            "Training Iteration 4424, Loss: 8.758715629577637\n",
            "Training Iteration 4425, Loss: 4.767061233520508\n",
            "Training Iteration 4426, Loss: 5.351820945739746\n",
            "Training Iteration 4427, Loss: 5.49540901184082\n",
            "Training Iteration 4428, Loss: 4.577818870544434\n",
            "Training Iteration 4429, Loss: 6.431388854980469\n",
            "Training Iteration 4430, Loss: 2.5483226776123047\n",
            "Training Iteration 4431, Loss: 6.865697860717773\n",
            "Training Iteration 4432, Loss: 5.719461917877197\n",
            "Training Iteration 4433, Loss: 6.105575084686279\n",
            "Training Iteration 4434, Loss: 2.9172935485839844\n",
            "Training Iteration 4435, Loss: 4.46718692779541\n",
            "Training Iteration 4436, Loss: 2.993574619293213\n",
            "Training Iteration 4437, Loss: 5.3979973793029785\n",
            "Training Iteration 4438, Loss: 3.812161922454834\n",
            "Training Iteration 4439, Loss: 4.215982437133789\n",
            "Training Iteration 4440, Loss: 4.453292369842529\n",
            "Training Iteration 4441, Loss: 9.90158748626709\n",
            "Training Iteration 4442, Loss: 5.4039740562438965\n",
            "Training Iteration 4443, Loss: 6.363205909729004\n",
            "Training Iteration 4444, Loss: 4.988536834716797\n",
            "Training Iteration 4445, Loss: 4.629737854003906\n",
            "Training Iteration 4446, Loss: 5.72537899017334\n",
            "Training Iteration 4447, Loss: 5.273090362548828\n",
            "Training Iteration 4448, Loss: 4.0859198570251465\n",
            "Training Iteration 4449, Loss: 3.846769332885742\n",
            "Training Iteration 4450, Loss: 3.451592206954956\n",
            "Training Iteration 4451, Loss: 2.8562824726104736\n",
            "Training Iteration 4452, Loss: 0.5406519174575806\n",
            "Training Iteration 4453, Loss: 3.0517325401306152\n",
            "Training Iteration 4454, Loss: 6.76206636428833\n",
            "Training Iteration 4455, Loss: 5.645059585571289\n",
            "Training Iteration 4456, Loss: 6.261293411254883\n",
            "Training Iteration 4457, Loss: 4.277674198150635\n",
            "Training Iteration 4458, Loss: 4.610604286193848\n",
            "Training Iteration 4459, Loss: 6.043201446533203\n",
            "Training Iteration 4460, Loss: 2.114697217941284\n",
            "Training Iteration 4461, Loss: 4.314152240753174\n",
            "Training Iteration 4462, Loss: 6.8143134117126465\n",
            "Training Iteration 4463, Loss: 3.196133852005005\n",
            "Training Iteration 4464, Loss: 2.285104990005493\n",
            "Training Iteration 4465, Loss: 4.551543235778809\n",
            "Training Iteration 4466, Loss: 6.340679168701172\n",
            "Training Iteration 4467, Loss: 4.793825626373291\n",
            "Training Iteration 4468, Loss: 4.508374214172363\n",
            "Training Iteration 4469, Loss: 2.755455732345581\n",
            "Training Iteration 4470, Loss: 3.197728157043457\n",
            "Training Iteration 4471, Loss: 2.990330696105957\n",
            "Training Iteration 4472, Loss: 4.422905445098877\n",
            "Training Iteration 4473, Loss: 2.467785120010376\n",
            "Training Iteration 4474, Loss: 5.259981155395508\n",
            "Training Iteration 4475, Loss: 5.496048450469971\n",
            "Training Iteration 4476, Loss: 4.543044090270996\n",
            "Training Iteration 4477, Loss: 3.3673853874206543\n",
            "Training Iteration 4478, Loss: 3.2463879585266113\n",
            "Training Iteration 4479, Loss: 3.3159191608428955\n",
            "Training Iteration 4480, Loss: 2.5123400688171387\n",
            "Training Iteration 4481, Loss: 4.7892913818359375\n",
            "Training Iteration 4482, Loss: 3.0826399326324463\n",
            "Training Iteration 4483, Loss: 2.7553021907806396\n",
            "Training Iteration 4484, Loss: 3.288417100906372\n",
            "Training Iteration 4485, Loss: 3.316253900527954\n",
            "Training Iteration 4486, Loss: 4.656258583068848\n",
            "Training Iteration 4487, Loss: 4.233970642089844\n",
            "Training Iteration 4488, Loss: 2.484119415283203\n",
            "Training Iteration 4489, Loss: 5.636401653289795\n",
            "Training Iteration 4490, Loss: 2.2872743606567383\n",
            "Training Iteration 4491, Loss: 3.277008056640625\n",
            "Training Iteration 4492, Loss: 4.067922115325928\n",
            "Training Iteration 4493, Loss: 2.737992763519287\n",
            "Training Iteration 4494, Loss: 4.482369422912598\n",
            "Training Iteration 4495, Loss: 4.458178997039795\n",
            "Training Iteration 4496, Loss: 3.819756507873535\n",
            "Training Iteration 4497, Loss: 3.342730760574341\n",
            "Training Iteration 4498, Loss: 4.112890720367432\n",
            "Training Iteration 4499, Loss: 4.274125576019287\n",
            "Training Iteration 4500, Loss: 3.4085419178009033\n",
            "Training Iteration 4501, Loss: 6.296938896179199\n",
            "Training Iteration 4502, Loss: 6.050655364990234\n",
            "Training Iteration 4503, Loss: 5.888657569885254\n",
            "Training Iteration 4504, Loss: 4.12014102935791\n",
            "Training Iteration 4505, Loss: 5.007868766784668\n",
            "Training Iteration 4506, Loss: 4.765193462371826\n",
            "Training Iteration 4507, Loss: 2.617433547973633\n",
            "Training Iteration 4508, Loss: 3.5931386947631836\n",
            "Training Iteration 4509, Loss: 8.303779602050781\n",
            "Training Iteration 4510, Loss: 4.943589210510254\n",
            "Training Iteration 4511, Loss: 6.7018327713012695\n",
            "Training Iteration 4512, Loss: 3.404946804046631\n",
            "Training Iteration 4513, Loss: 7.988950252532959\n",
            "Training Iteration 4514, Loss: 6.062638759613037\n",
            "Training Iteration 4515, Loss: 3.0774190425872803\n",
            "Training Iteration 4516, Loss: 4.566974639892578\n",
            "Training Iteration 4517, Loss: 5.048588275909424\n",
            "Training Iteration 4518, Loss: 3.868781328201294\n",
            "Training Iteration 4519, Loss: 7.916142463684082\n",
            "Training Iteration 4520, Loss: 8.464935302734375\n",
            "Training Iteration 4521, Loss: 7.040115833282471\n",
            "Training Iteration 4522, Loss: 5.921029090881348\n",
            "Training Iteration 4523, Loss: 3.4433460235595703\n",
            "Training Iteration 4524, Loss: 4.522151470184326\n",
            "Training Iteration 4525, Loss: 4.370025157928467\n",
            "Training Iteration 4526, Loss: 7.580320835113525\n",
            "Training Iteration 4527, Loss: 3.751646041870117\n",
            "Training Iteration 4528, Loss: 7.653778076171875\n",
            "Training Iteration 4529, Loss: 3.893730640411377\n",
            "Training Iteration 4530, Loss: 2.3789281845092773\n",
            "Training Iteration 4531, Loss: 4.18303918838501\n",
            "Training Iteration 4532, Loss: 2.5082662105560303\n",
            "Training Iteration 4533, Loss: 4.9380927085876465\n",
            "Training Iteration 4534, Loss: 6.475409984588623\n",
            "Training Iteration 4535, Loss: 3.693153142929077\n",
            "Training Iteration 4536, Loss: 4.023357391357422\n",
            "Training Iteration 4537, Loss: 4.677707672119141\n",
            "Training Iteration 4538, Loss: 3.0101253986358643\n",
            "Training Iteration 4539, Loss: 4.1593732833862305\n",
            "Training Iteration 4540, Loss: 6.0976033210754395\n",
            "Training Iteration 4541, Loss: 3.307727336883545\n",
            "Training Iteration 4542, Loss: 4.527685165405273\n",
            "Training Iteration 4543, Loss: 5.619785785675049\n",
            "Training Iteration 4544, Loss: 4.750119209289551\n",
            "Training Iteration 4545, Loss: 5.599573135375977\n",
            "Training Iteration 4546, Loss: 3.9573752880096436\n",
            "Training Iteration 4547, Loss: 5.5565409660339355\n",
            "Training Iteration 4548, Loss: 3.978987455368042\n",
            "Training Iteration 4549, Loss: 5.222236633300781\n",
            "Training Iteration 4550, Loss: 4.270073413848877\n",
            "Training Iteration 4551, Loss: 3.431755781173706\n",
            "Training Iteration 4552, Loss: 4.004857063293457\n",
            "Training Iteration 4553, Loss: 4.247063636779785\n",
            "Training Iteration 4554, Loss: 3.40580677986145\n",
            "Training Iteration 4555, Loss: 3.9703431129455566\n",
            "Training Iteration 4556, Loss: 5.468238830566406\n",
            "Training Iteration 4557, Loss: 4.072746753692627\n",
            "Training Iteration 4558, Loss: 3.4148006439208984\n",
            "Training Iteration 4559, Loss: 5.740487575531006\n",
            "Training Iteration 4560, Loss: 3.5007290840148926\n",
            "Training Iteration 4561, Loss: 3.3197836875915527\n",
            "Training Iteration 4562, Loss: 1.8321404457092285\n",
            "Training Iteration 4563, Loss: 2.37166428565979\n",
            "Training Iteration 4564, Loss: 5.638985633850098\n",
            "Training Iteration 4565, Loss: 4.056455135345459\n",
            "Training Iteration 4566, Loss: 7.3511738777160645\n",
            "Training Iteration 4567, Loss: 1.4340391159057617\n",
            "Training Iteration 4568, Loss: 3.8700790405273438\n",
            "Training Iteration 4569, Loss: 3.5994133949279785\n",
            "Training Iteration 4570, Loss: 2.280533790588379\n",
            "Training Iteration 4571, Loss: 2.7494003772735596\n",
            "Training Iteration 4572, Loss: 4.188334941864014\n",
            "Training Iteration 4573, Loss: 4.155694484710693\n",
            "Training Iteration 4574, Loss: 2.2480931282043457\n",
            "Training Iteration 4575, Loss: 7.627134323120117\n",
            "Training Iteration 4576, Loss: 3.5165176391601562\n",
            "Training Iteration 4577, Loss: 3.6370108127593994\n",
            "Training Iteration 4578, Loss: 2.751085042953491\n",
            "Training Iteration 4579, Loss: 3.1476099491119385\n",
            "Training Iteration 4580, Loss: 5.419063568115234\n",
            "Training Iteration 4581, Loss: 3.5436503887176514\n",
            "Training Iteration 4582, Loss: 2.1070337295532227\n",
            "Training Iteration 4583, Loss: 4.0288214683532715\n",
            "Training Iteration 4584, Loss: 4.216396808624268\n",
            "Training Iteration 4585, Loss: 3.615631580352783\n",
            "Training Iteration 4586, Loss: 1.309073567390442\n",
            "Training Iteration 4587, Loss: 4.942116737365723\n",
            "Training Iteration 4588, Loss: 1.8308942317962646\n",
            "Training Iteration 4589, Loss: 6.162498474121094\n",
            "Training Iteration 4590, Loss: 5.869266986846924\n",
            "Training Iteration 4591, Loss: 2.316470146179199\n",
            "Training Iteration 4592, Loss: 2.846895694732666\n",
            "Training Iteration 4593, Loss: 2.130659341812134\n",
            "Training Iteration 4594, Loss: 4.587230205535889\n",
            "Training Iteration 4595, Loss: 5.34279203414917\n",
            "Training Iteration 4596, Loss: 2.931065082550049\n",
            "Training Iteration 4597, Loss: 2.923349142074585\n",
            "Training Iteration 4598, Loss: 3.7791943550109863\n",
            "Training Iteration 4599, Loss: 7.778398036956787\n",
            "Training Iteration 4600, Loss: 3.3316445350646973\n",
            "Training Iteration 4601, Loss: 2.785644292831421\n",
            "Training Iteration 4602, Loss: 3.2084267139434814\n",
            "Training Iteration 4603, Loss: 4.924829483032227\n",
            "Training Iteration 4604, Loss: 3.562828302383423\n",
            "Training Iteration 4605, Loss: 1.3782459497451782\n",
            "Training Iteration 4606, Loss: 3.3571321964263916\n",
            "Training Iteration 4607, Loss: 6.105871200561523\n",
            "Training Iteration 4608, Loss: 5.874417304992676\n",
            "Training Iteration 4609, Loss: 3.671734094619751\n",
            "Training Iteration 4610, Loss: 6.758397102355957\n",
            "Training Iteration 4611, Loss: 6.190066337585449\n",
            "Training Iteration 4612, Loss: 4.898129940032959\n",
            "Training Iteration 4613, Loss: 4.165889263153076\n",
            "Training Iteration 4614, Loss: 3.993192195892334\n",
            "Training Iteration 4615, Loss: 5.842911720275879\n",
            "Training Iteration 4616, Loss: 3.2456531524658203\n",
            "Training Iteration 4617, Loss: 9.138097763061523\n",
            "Training Iteration 4618, Loss: 6.242100238800049\n",
            "Training Iteration 4619, Loss: 5.513001918792725\n",
            "Training Iteration 4620, Loss: 2.2436394691467285\n",
            "Training Iteration 4621, Loss: 7.366191387176514\n",
            "Training Iteration 4622, Loss: 7.677227020263672\n",
            "Training Iteration 4623, Loss: 7.754517078399658\n",
            "Training Iteration 4624, Loss: 3.744900703430176\n",
            "Training Iteration 4625, Loss: 2.5297725200653076\n",
            "Training Iteration 4626, Loss: 6.214275360107422\n",
            "Training Iteration 4627, Loss: 5.698507308959961\n",
            "Training Iteration 4628, Loss: 4.053674221038818\n",
            "Training Iteration 4629, Loss: 2.841662645339966\n",
            "Training Iteration 4630, Loss: 7.418889045715332\n",
            "Training Iteration 4631, Loss: 4.452375411987305\n",
            "Training Iteration 4632, Loss: 2.686720848083496\n",
            "Training Iteration 4633, Loss: 7.035420894622803\n",
            "Training Iteration 4634, Loss: 2.852426528930664\n",
            "Training Iteration 4635, Loss: 2.104919910430908\n",
            "Training Iteration 4636, Loss: 3.133729934692383\n",
            "Training Iteration 4637, Loss: 6.800929546356201\n",
            "Training Iteration 4638, Loss: 3.9967238903045654\n",
            "Training Iteration 4639, Loss: 4.089211463928223\n",
            "Training Iteration 4640, Loss: 5.1050286293029785\n",
            "Training Iteration 4641, Loss: 5.996565818786621\n",
            "Training Iteration 4642, Loss: 1.871604323387146\n",
            "Training Iteration 4643, Loss: 6.046259880065918\n",
            "Training Iteration 4644, Loss: 3.573106050491333\n",
            "Training Iteration 4645, Loss: 9.743718147277832\n",
            "Training Iteration 4646, Loss: 3.570920467376709\n",
            "Training Iteration 4647, Loss: 6.037257194519043\n",
            "Training Iteration 4648, Loss: 2.866722583770752\n",
            "Training Iteration 4649, Loss: 3.848649024963379\n",
            "Training Iteration 4650, Loss: 5.883665561676025\n",
            "Training Iteration 4651, Loss: 8.247916221618652\n",
            "Training Iteration 4652, Loss: 5.941503047943115\n",
            "Training Iteration 4653, Loss: 2.8452682495117188\n",
            "Training Iteration 4654, Loss: 4.678771495819092\n",
            "Training Iteration 4655, Loss: 3.2551965713500977\n",
            "Training Iteration 4656, Loss: 3.604503631591797\n",
            "Training Iteration 4657, Loss: 4.610588550567627\n",
            "Training Iteration 4658, Loss: 6.446031093597412\n",
            "Training Iteration 4659, Loss: 5.395559787750244\n",
            "Training Iteration 4660, Loss: 7.225266933441162\n",
            "Training Iteration 4661, Loss: 7.693778991699219\n",
            "Training Iteration 4662, Loss: 8.155781745910645\n",
            "Training Iteration 4663, Loss: 3.7780280113220215\n",
            "Training Iteration 4664, Loss: 3.549391508102417\n",
            "Training Iteration 4665, Loss: 4.78367805480957\n",
            "Training Iteration 4666, Loss: 4.307530879974365\n",
            "Training Iteration 4667, Loss: 3.811901807785034\n",
            "Training Iteration 4668, Loss: 7.16215181350708\n",
            "Training Iteration 4669, Loss: 5.297781467437744\n",
            "Training Iteration 4670, Loss: 4.490082263946533\n",
            "Training Iteration 4671, Loss: 3.430394411087036\n",
            "Training Iteration 4672, Loss: 4.692027568817139\n",
            "Training Iteration 4673, Loss: 4.181263446807861\n",
            "Training Iteration 4674, Loss: 5.298226356506348\n",
            "Training Iteration 4675, Loss: 3.552969455718994\n",
            "Training Iteration 4676, Loss: 2.4638314247131348\n",
            "Training Iteration 4677, Loss: 4.196900367736816\n",
            "Training Iteration 4678, Loss: 4.869879722595215\n",
            "Training Iteration 4679, Loss: 4.540445804595947\n",
            "Training Iteration 4680, Loss: 1.760930061340332\n",
            "Training Iteration 4681, Loss: 4.7977190017700195\n",
            "Training Iteration 4682, Loss: 6.6827802658081055\n",
            "Training Iteration 4683, Loss: 5.221541404724121\n",
            "Training Iteration 4684, Loss: 5.975402355194092\n",
            "Training Iteration 4685, Loss: 5.557150840759277\n",
            "Training Iteration 4686, Loss: 5.318977355957031\n",
            "Training Iteration 4687, Loss: 4.415586471557617\n",
            "Training Iteration 4688, Loss: 3.7890138626098633\n",
            "Training Iteration 4689, Loss: 5.28077507019043\n",
            "Training Iteration 4690, Loss: 4.243069648742676\n",
            "Training Iteration 4691, Loss: 5.87044620513916\n",
            "Training Iteration 4692, Loss: 3.7321853637695312\n",
            "Training Iteration 4693, Loss: 3.5964415073394775\n",
            "Training Iteration 4694, Loss: 6.702240943908691\n",
            "Training Iteration 4695, Loss: 3.9133856296539307\n",
            "Training Iteration 4696, Loss: 6.664227485656738\n",
            "Training Iteration 4697, Loss: 6.424217224121094\n",
            "Training Iteration 4698, Loss: 8.792455673217773\n",
            "Training Iteration 4699, Loss: 7.9870429039001465\n",
            "Training Iteration 4700, Loss: 5.17974328994751\n",
            "Training Iteration 4701, Loss: 3.0288515090942383\n",
            "Training Iteration 4702, Loss: 4.562619209289551\n",
            "Training Iteration 4703, Loss: 6.017581462860107\n",
            "Training Iteration 4704, Loss: 2.4616189002990723\n",
            "Training Iteration 4705, Loss: 2.0810537338256836\n",
            "Training Iteration 4706, Loss: 3.2747340202331543\n",
            "Training Iteration 4707, Loss: 4.608024597167969\n",
            "Training Iteration 4708, Loss: 3.438920259475708\n",
            "Training Iteration 4709, Loss: 3.1648178100585938\n",
            "Training Iteration 4710, Loss: 2.841954469680786\n",
            "Training Iteration 4711, Loss: 2.4989380836486816\n",
            "Training Iteration 4712, Loss: 1.3594632148742676\n",
            "Training Iteration 4713, Loss: 7.1479573249816895\n",
            "Training Iteration 4714, Loss: 4.6779375076293945\n",
            "Training Iteration 4715, Loss: 3.746643543243408\n",
            "Training Iteration 4716, Loss: 6.560268402099609\n",
            "Training Iteration 4717, Loss: 6.165764808654785\n",
            "Training Iteration 4718, Loss: 3.8452935218811035\n",
            "Training Iteration 4719, Loss: 4.2739129066467285\n",
            "Training Iteration 4720, Loss: 5.560698986053467\n",
            "Training Iteration 4721, Loss: 6.044862747192383\n",
            "Training Iteration 4722, Loss: 6.099184989929199\n",
            "Training Iteration 4723, Loss: 5.003093719482422\n",
            "Training Iteration 4724, Loss: 2.2295238971710205\n",
            "Training Iteration 4725, Loss: 2.9412405490875244\n",
            "Training Iteration 4726, Loss: 6.111533164978027\n",
            "Training Iteration 4727, Loss: 9.05988597869873\n",
            "Training Iteration 4728, Loss: 4.307098865509033\n",
            "Training Iteration 4729, Loss: 9.025728225708008\n",
            "Training Iteration 4730, Loss: 5.138121604919434\n",
            "Training Iteration 4731, Loss: 6.928780555725098\n",
            "Training Iteration 4732, Loss: 3.9170403480529785\n",
            "Training Iteration 4733, Loss: 1.9352149963378906\n",
            "Training Iteration 4734, Loss: 4.787010669708252\n",
            "Training Iteration 4735, Loss: 4.246811866760254\n",
            "Training Iteration 4736, Loss: 4.877024173736572\n",
            "Training Iteration 4737, Loss: 2.9241251945495605\n",
            "Training Iteration 4738, Loss: 9.298022270202637\n",
            "Training Iteration 4739, Loss: 3.9550387859344482\n",
            "Training Iteration 4740, Loss: 1.927253007888794\n",
            "Training Iteration 4741, Loss: 4.5944695472717285\n",
            "Training Iteration 4742, Loss: 4.037875652313232\n",
            "Training Iteration 4743, Loss: 3.9681243896484375\n",
            "Training Iteration 4744, Loss: 2.78576922416687\n",
            "Training Iteration 4745, Loss: 5.011553764343262\n",
            "Training Iteration 4746, Loss: 5.014838218688965\n",
            "Training Iteration 4747, Loss: 4.757548809051514\n",
            "Training Iteration 4748, Loss: 7.776776313781738\n",
            "Training Iteration 4749, Loss: 3.149120807647705\n",
            "Training Iteration 4750, Loss: 3.1107258796691895\n",
            "Training Iteration 4751, Loss: 4.460138320922852\n",
            "Training Iteration 4752, Loss: 4.720485687255859\n",
            "Training Iteration 4753, Loss: 2.3734068870544434\n",
            "Training Iteration 4754, Loss: 4.674954414367676\n",
            "Training Iteration 4755, Loss: 3.787426233291626\n",
            "Training Iteration 4756, Loss: 4.356647491455078\n",
            "Training Iteration 4757, Loss: 2.921060562133789\n",
            "Training Iteration 4758, Loss: 3.674708604812622\n",
            "Training Iteration 4759, Loss: 3.942842721939087\n",
            "Training Iteration 4760, Loss: 2.9772253036499023\n",
            "Training Iteration 4761, Loss: 1.7368390560150146\n",
            "Training Iteration 4762, Loss: 2.8692054748535156\n",
            "Training Iteration 4763, Loss: 2.181330680847168\n",
            "Training Iteration 4764, Loss: 5.873807430267334\n",
            "Training Iteration 4765, Loss: 3.5421817302703857\n",
            "Training Iteration 4766, Loss: 2.6167807579040527\n",
            "Training Iteration 4767, Loss: 2.2686307430267334\n",
            "Training Iteration 4768, Loss: 4.090634346008301\n",
            "Training Iteration 4769, Loss: 4.165952205657959\n",
            "Training Iteration 4770, Loss: 3.3502418994903564\n",
            "Training Iteration 4771, Loss: 1.9413330554962158\n",
            "Training Iteration 4772, Loss: 3.746954917907715\n",
            "Training Iteration 4773, Loss: 3.0689821243286133\n",
            "Training Iteration 4774, Loss: 6.360689163208008\n",
            "Training Iteration 4775, Loss: 2.1455612182617188\n",
            "Training Iteration 4776, Loss: 3.2978737354278564\n",
            "Training Iteration 4777, Loss: 5.390715599060059\n",
            "Training Iteration 4778, Loss: 4.283689975738525\n",
            "Training Iteration 4779, Loss: 4.722761154174805\n",
            "Training Iteration 4780, Loss: 7.241812705993652\n",
            "Training Iteration 4781, Loss: 4.418820381164551\n",
            "Training Iteration 4782, Loss: 3.3114850521087646\n",
            "Training Iteration 4783, Loss: 4.731818199157715\n",
            "Training Iteration 4784, Loss: 3.58766770362854\n",
            "Training Iteration 4785, Loss: 4.7472333908081055\n",
            "Training Iteration 4786, Loss: 3.3374831676483154\n",
            "Training Iteration 4787, Loss: 2.1347081661224365\n",
            "Training Iteration 4788, Loss: 4.501766681671143\n",
            "Training Iteration 4789, Loss: 3.9519705772399902\n",
            "Training Iteration 4790, Loss: 6.707583427429199\n",
            "Training Iteration 4791, Loss: 2.8930864334106445\n",
            "Training Iteration 4792, Loss: 3.5668444633483887\n",
            "Training Iteration 4793, Loss: 6.442513465881348\n",
            "Training Iteration 4794, Loss: 4.724064826965332\n",
            "Training Iteration 4795, Loss: 5.2889018058776855\n",
            "Training Iteration 4796, Loss: 3.6553144454956055\n",
            "Training Iteration 4797, Loss: 3.551145315170288\n",
            "Training Iteration 4798, Loss: 6.145041465759277\n",
            "Training Iteration 4799, Loss: 2.0666236877441406\n",
            "Training Iteration 4800, Loss: 7.5964155197143555\n",
            "Training Iteration 4801, Loss: 3.9607858657836914\n",
            "Training Iteration 4802, Loss: 3.2100625038146973\n",
            "Training Iteration 4803, Loss: 2.3056442737579346\n",
            "Training Iteration 4804, Loss: 3.437272548675537\n",
            "Training Iteration 4805, Loss: 7.499331474304199\n",
            "Training Iteration 4806, Loss: 5.010788440704346\n",
            "Training Iteration 4807, Loss: 2.970618486404419\n",
            "Training Iteration 4808, Loss: 2.9271981716156006\n",
            "Training Iteration 4809, Loss: 8.930631637573242\n",
            "Training Iteration 4810, Loss: 3.135084867477417\n",
            "Training Iteration 4811, Loss: 6.417178630828857\n",
            "Training Iteration 4812, Loss: 3.8055057525634766\n",
            "Training Iteration 4813, Loss: 4.358448505401611\n",
            "Training Iteration 4814, Loss: 5.540915012359619\n",
            "Training Iteration 4815, Loss: 5.003200054168701\n",
            "Training Iteration 4816, Loss: 5.020581245422363\n",
            "Training Iteration 4817, Loss: 4.165454864501953\n",
            "Training Iteration 4818, Loss: 3.8665730953216553\n",
            "Training Iteration 4819, Loss: 5.7431464195251465\n",
            "Training Iteration 4820, Loss: 6.157733917236328\n",
            "Training Iteration 4821, Loss: 2.7531075477600098\n",
            "Training Iteration 4822, Loss: 3.6293258666992188\n",
            "Training Iteration 4823, Loss: 5.816330909729004\n",
            "Training Iteration 4824, Loss: 4.870465278625488\n",
            "Training Iteration 4825, Loss: 3.7091012001037598\n",
            "Training Iteration 4826, Loss: 4.40503454208374\n",
            "Training Iteration 4827, Loss: 4.649120807647705\n",
            "Training Iteration 4828, Loss: 2.7008163928985596\n",
            "Training Iteration 4829, Loss: 6.035315990447998\n",
            "Training Iteration 4830, Loss: 2.16815447807312\n",
            "Training Iteration 4831, Loss: 4.691448211669922\n",
            "Training Iteration 4832, Loss: 3.8149194717407227\n",
            "Training Iteration 4833, Loss: 3.3201842308044434\n",
            "Training Iteration 4834, Loss: 3.869122266769409\n",
            "Training Iteration 4835, Loss: 3.2459397315979004\n",
            "Training Iteration 4836, Loss: 3.5993943214416504\n",
            "Training Iteration 4837, Loss: 4.306111812591553\n",
            "Training Iteration 4838, Loss: 5.626634120941162\n",
            "Training Iteration 4839, Loss: 3.4475371837615967\n",
            "Training Iteration 4840, Loss: 4.10612154006958\n",
            "Training Iteration 4841, Loss: 3.040938138961792\n",
            "Training Iteration 4842, Loss: 5.68837308883667\n",
            "Training Iteration 4843, Loss: 4.255466938018799\n",
            "Training Iteration 4844, Loss: 5.142453670501709\n",
            "Training Iteration 4845, Loss: 5.563025951385498\n",
            "Training Iteration 4846, Loss: 3.907121181488037\n",
            "Training Iteration 4847, Loss: 5.0636372566223145\n",
            "Training Iteration 4848, Loss: 1.6412713527679443\n",
            "Training Iteration 4849, Loss: 2.153768301010132\n",
            "Training Iteration 4850, Loss: 3.6960835456848145\n",
            "Training Iteration 4851, Loss: 1.3054111003875732\n",
            "Training Iteration 4852, Loss: 2.8260467052459717\n",
            "Training Iteration 4853, Loss: 5.866641521453857\n",
            "Training Iteration 4854, Loss: 7.012733459472656\n",
            "Training Iteration 4855, Loss: 4.620400905609131\n",
            "Training Iteration 4856, Loss: 5.464948654174805\n",
            "Training Iteration 4857, Loss: 3.5680174827575684\n",
            "Training Iteration 4858, Loss: 5.388532638549805\n",
            "Training Iteration 4859, Loss: 2.560389995574951\n",
            "Training Iteration 4860, Loss: 5.1534576416015625\n",
            "Training Iteration 4861, Loss: 3.623811721801758\n",
            "Training Iteration 4862, Loss: 2.713999032974243\n",
            "Training Iteration 4863, Loss: 2.5649569034576416\n",
            "Training Iteration 4864, Loss: 4.387133598327637\n",
            "Training Iteration 4865, Loss: 4.108590126037598\n",
            "Training Iteration 4866, Loss: 2.9802305698394775\n",
            "Training Iteration 4867, Loss: 6.021389961242676\n",
            "Training Iteration 4868, Loss: 4.690120220184326\n",
            "Training Iteration 4869, Loss: 5.919086933135986\n",
            "Training Iteration 4870, Loss: 3.4599952697753906\n",
            "Training Iteration 4871, Loss: 3.8334803581237793\n",
            "Training Iteration 4872, Loss: 3.2188022136688232\n",
            "Training Iteration 4873, Loss: 1.9731050729751587\n",
            "Training Iteration 4874, Loss: 3.7808139324188232\n",
            "Training Iteration 4875, Loss: 6.526879787445068\n",
            "Training Iteration 4876, Loss: 7.854119300842285\n",
            "Training Iteration 4877, Loss: 4.7718400955200195\n",
            "Training Iteration 4878, Loss: 8.918381690979004\n",
            "Training Iteration 4879, Loss: 1.4398040771484375\n",
            "Training Iteration 4880, Loss: 5.365335464477539\n",
            "Training Iteration 4881, Loss: 5.535995006561279\n",
            "Training Iteration 4882, Loss: 4.598458290100098\n",
            "Training Iteration 4883, Loss: 5.20538854598999\n",
            "Training Iteration 4884, Loss: 2.862905740737915\n",
            "Training Iteration 4885, Loss: 3.6343069076538086\n",
            "Training Iteration 4886, Loss: 4.6977691650390625\n",
            "Training Iteration 4887, Loss: 4.959022521972656\n",
            "Training Iteration 4888, Loss: 6.034120082855225\n",
            "Training Iteration 4889, Loss: 3.9435627460479736\n",
            "Training Iteration 4890, Loss: 4.011591911315918\n",
            "Training Iteration 4891, Loss: 6.990009784698486\n",
            "Training Iteration 4892, Loss: 3.894376516342163\n",
            "Training Iteration 4893, Loss: 3.80853009223938\n",
            "Training Iteration 4894, Loss: 5.411189079284668\n",
            "Training Iteration 4895, Loss: 5.51722526550293\n",
            "Training Iteration 4896, Loss: 5.077591896057129\n",
            "Training Iteration 4897, Loss: 6.135964393615723\n",
            "Training Iteration 4898, Loss: 3.150371551513672\n",
            "Training Iteration 4899, Loss: 2.7410011291503906\n",
            "Training Iteration 4900, Loss: 4.934989929199219\n",
            "Training Iteration 4901, Loss: 7.176506996154785\n",
            "Training Iteration 4902, Loss: 6.058314323425293\n",
            "Training Iteration 4903, Loss: 4.127255916595459\n",
            "Training Iteration 4904, Loss: 3.3382644653320312\n",
            "Training Iteration 4905, Loss: 3.2842719554901123\n",
            "Training Iteration 4906, Loss: 2.337324857711792\n",
            "Training Iteration 4907, Loss: 6.881500720977783\n",
            "Training Iteration 4908, Loss: 2.37797474861145\n",
            "Training Iteration 4909, Loss: 3.183502197265625\n",
            "Training Iteration 4910, Loss: 4.7615885734558105\n",
            "Training Iteration 4911, Loss: 5.195833683013916\n",
            "Training Iteration 4912, Loss: 4.834490776062012\n",
            "Training Iteration 4913, Loss: 2.6455931663513184\n",
            "Training Iteration 4914, Loss: 9.99648380279541\n",
            "Training Iteration 4915, Loss: 8.434285163879395\n",
            "Training Iteration 4916, Loss: 4.1935529708862305\n",
            "Training Iteration 4917, Loss: 2.3517444133758545\n",
            "Training Iteration 4918, Loss: 5.1050920486450195\n",
            "Training Iteration 4919, Loss: 8.14858341217041\n",
            "Training Iteration 4920, Loss: 4.786632061004639\n",
            "Training Iteration 4921, Loss: 5.1045613288879395\n",
            "Training Iteration 4922, Loss: 5.26469612121582\n",
            "Training Iteration 4923, Loss: 6.176242351531982\n",
            "Training Iteration 4924, Loss: 5.284853935241699\n",
            "Training Iteration 4925, Loss: 6.468020915985107\n",
            "Training Iteration 4926, Loss: 4.443915843963623\n",
            "Training Iteration 4927, Loss: 5.3873677253723145\n",
            "Training Iteration 4928, Loss: 3.8353285789489746\n",
            "Training Iteration 4929, Loss: 4.491784572601318\n",
            "Training Iteration 4930, Loss: 5.345100402832031\n",
            "Training Iteration 4931, Loss: 2.2961678504943848\n",
            "Training Iteration 4932, Loss: 5.294495582580566\n",
            "Training Iteration 4933, Loss: 3.654794692993164\n",
            "Training Iteration 4934, Loss: 7.299518585205078\n",
            "Training Iteration 4935, Loss: 5.983827114105225\n",
            "Training Iteration 4936, Loss: 6.580770492553711\n",
            "Training Iteration 4937, Loss: 3.347489356994629\n",
            "Training Iteration 4938, Loss: 4.241133689880371\n",
            "Training Iteration 4939, Loss: 5.111159801483154\n",
            "Training Iteration 4940, Loss: 5.279308795928955\n",
            "Training Iteration 4941, Loss: 1.6381933689117432\n",
            "Training Iteration 4942, Loss: 4.323106288909912\n",
            "Training Iteration 4943, Loss: 5.277280807495117\n",
            "Training Iteration 4944, Loss: 3.320863723754883\n",
            "Training Iteration 4945, Loss: 4.141812324523926\n",
            "Training Iteration 4946, Loss: 4.456297874450684\n",
            "Training Iteration 4947, Loss: 3.214190721511841\n",
            "Training Iteration 4948, Loss: 3.368774652481079\n",
            "Training Iteration 4949, Loss: 5.502321720123291\n",
            "Training Iteration 4950, Loss: 3.358266592025757\n",
            "Training Iteration 4951, Loss: 5.240903854370117\n",
            "Training Iteration 4952, Loss: 1.4181095361709595\n",
            "Training Iteration 4953, Loss: 9.580734252929688\n",
            "Training Iteration 4954, Loss: 5.540410995483398\n",
            "Training Iteration 4955, Loss: 5.465052127838135\n",
            "Training Iteration 4956, Loss: 6.540099143981934\n",
            "Training Iteration 4957, Loss: 4.004606246948242\n",
            "Training Iteration 4958, Loss: 7.750358581542969\n",
            "Training Iteration 4959, Loss: 3.224343776702881\n",
            "Training Iteration 4960, Loss: 4.76616907119751\n",
            "Training Iteration 4961, Loss: 4.726145267486572\n",
            "Training Iteration 4962, Loss: 3.226391553878784\n",
            "Training Iteration 4963, Loss: 3.1324446201324463\n",
            "Training Iteration 4964, Loss: 5.0458478927612305\n",
            "Training Iteration 4965, Loss: 4.938135147094727\n",
            "Training Iteration 4966, Loss: 4.011031627655029\n",
            "Training Iteration 4967, Loss: 2.0208513736724854\n",
            "Training Iteration 4968, Loss: 2.8331410884857178\n",
            "Training Iteration 4969, Loss: 5.725226402282715\n",
            "Training Iteration 4970, Loss: 7.879077434539795\n",
            "Training Iteration 4971, Loss: 5.330287933349609\n",
            "Training Iteration 4972, Loss: 3.3485357761383057\n",
            "Training Iteration 4973, Loss: 8.260573387145996\n",
            "Training Iteration 4974, Loss: 2.77987003326416\n",
            "Training Iteration 4975, Loss: 7.566738128662109\n",
            "Training Iteration 4976, Loss: 1.7453103065490723\n",
            "Training Iteration 4977, Loss: 7.313859462738037\n",
            "Training Iteration 4978, Loss: 5.582744598388672\n",
            "Training Iteration 4979, Loss: 6.259744167327881\n",
            "Training Iteration 4980, Loss: 5.288409233093262\n",
            "Training Iteration 4981, Loss: 2.4828648567199707\n",
            "Training Iteration 4982, Loss: 3.162334442138672\n",
            "Training Iteration 4983, Loss: 7.00220251083374\n",
            "Training Iteration 4984, Loss: 6.604462623596191\n",
            "Training Iteration 4985, Loss: 6.752148151397705\n",
            "Training Iteration 4986, Loss: 3.974307060241699\n",
            "Training Iteration 4987, Loss: 5.418571472167969\n",
            "Training Iteration 4988, Loss: 4.425694942474365\n",
            "Training Iteration 4989, Loss: 4.895431041717529\n",
            "Training Iteration 4990, Loss: 3.3740062713623047\n",
            "Training Iteration 4991, Loss: 5.125934600830078\n",
            "Training Iteration 4992, Loss: 5.865031719207764\n",
            "Training Iteration 4993, Loss: 7.378317832946777\n",
            "Training Iteration 4994, Loss: 10.346006393432617\n",
            "Training Iteration 4995, Loss: 4.282776355743408\n",
            "Training Iteration 4996, Loss: 2.43795108795166\n",
            "Training Iteration 4997, Loss: 5.840853214263916\n",
            "Training Iteration 4998, Loss: 3.9506921768188477\n",
            "Training Iteration 4999, Loss: 4.994412422180176\n",
            "Training Iteration 5000, Loss: 3.3554346561431885\n",
            "Training Iteration 5001, Loss: 4.2128190994262695\n",
            "Training Iteration 5002, Loss: 5.6266303062438965\n",
            "Training Iteration 5003, Loss: 4.232304573059082\n",
            "Training Iteration 5004, Loss: 3.537692070007324\n",
            "Training Iteration 5005, Loss: 8.393840789794922\n",
            "Training Iteration 5006, Loss: 5.575854301452637\n",
            "Training Iteration 5007, Loss: 5.575405120849609\n",
            "Training Iteration 5008, Loss: 2.6711487770080566\n",
            "Training Iteration 5009, Loss: 3.580941677093506\n",
            "Training Iteration 5010, Loss: 6.565755367279053\n",
            "Training Iteration 5011, Loss: 5.17739725112915\n",
            "Training Iteration 5012, Loss: 5.839164733886719\n",
            "Training Iteration 5013, Loss: 3.6936542987823486\n",
            "Training Iteration 5014, Loss: 5.9076762199401855\n",
            "Training Iteration 5015, Loss: 4.516382217407227\n",
            "Training Iteration 5016, Loss: 5.797149181365967\n",
            "Training Iteration 5017, Loss: 4.38180685043335\n",
            "Training Iteration 5018, Loss: 3.8332128524780273\n",
            "Training Iteration 5019, Loss: 4.442237854003906\n",
            "Training Iteration 5020, Loss: 3.566944122314453\n",
            "Training Iteration 5021, Loss: 4.951167106628418\n",
            "Training Iteration 5022, Loss: 5.067025184631348\n",
            "Training Iteration 5023, Loss: 4.740051746368408\n",
            "Training Iteration 5024, Loss: 4.115667819976807\n",
            "Training Iteration 5025, Loss: 2.900054693222046\n",
            "Training Iteration 5026, Loss: 3.3983592987060547\n",
            "Training Iteration 5027, Loss: 3.7624735832214355\n",
            "Training Iteration 5028, Loss: 3.4543380737304688\n",
            "Training Iteration 5029, Loss: 2.975828170776367\n",
            "Training Iteration 5030, Loss: 5.834918022155762\n",
            "Training Iteration 5031, Loss: 5.919291973114014\n",
            "Training Iteration 5032, Loss: 3.233724594116211\n",
            "Training Iteration 5033, Loss: 4.296021938323975\n",
            "Training Iteration 5034, Loss: 3.0112266540527344\n",
            "Training Iteration 5035, Loss: 3.724327325820923\n",
            "Training Iteration 5036, Loss: 3.9859611988067627\n",
            "Training Iteration 5037, Loss: 5.053326606750488\n",
            "Training Iteration 5038, Loss: 3.3920505046844482\n",
            "Training Iteration 5039, Loss: 3.7319202423095703\n",
            "Training Iteration 5040, Loss: 4.320441246032715\n",
            "Training Iteration 5041, Loss: 4.665144443511963\n",
            "Training Iteration 5042, Loss: 2.9730336666107178\n",
            "Training Iteration 5043, Loss: 5.450448036193848\n",
            "Training Iteration 5044, Loss: 3.4718594551086426\n",
            "Training Iteration 5045, Loss: 2.0425541400909424\n",
            "Training Iteration 5046, Loss: 4.576549530029297\n",
            "Training Iteration 5047, Loss: 3.539604663848877\n",
            "Training Iteration 5048, Loss: 3.779996395111084\n",
            "Training Iteration 5049, Loss: 5.785868167877197\n",
            "Training Iteration 5050, Loss: 5.041232109069824\n",
            "Training Iteration 5051, Loss: 7.333902359008789\n",
            "Training Iteration 5052, Loss: 3.278163194656372\n",
            "Training Iteration 5053, Loss: 6.566728115081787\n",
            "Training Iteration 5054, Loss: 5.496861457824707\n",
            "Training Iteration 5055, Loss: 4.693760871887207\n",
            "Training Iteration 5056, Loss: 2.4650039672851562\n",
            "Training Iteration 5057, Loss: 4.699830532073975\n",
            "Training Iteration 5058, Loss: 6.595462322235107\n",
            "Training Iteration 5059, Loss: 7.081541061401367\n",
            "Training Iteration 5060, Loss: 2.0352656841278076\n",
            "Training Iteration 5061, Loss: 6.728036880493164\n",
            "Training Iteration 5062, Loss: 3.1962361335754395\n",
            "Training Iteration 5063, Loss: 3.5018649101257324\n",
            "Training Iteration 5064, Loss: 2.8497707843780518\n",
            "Training Iteration 5065, Loss: 4.698318004608154\n",
            "Training Iteration 5066, Loss: 6.522521018981934\n",
            "Training Iteration 5067, Loss: 3.5823261737823486\n",
            "Training Iteration 5068, Loss: 5.198328018188477\n",
            "Training Iteration 5069, Loss: 4.929100513458252\n",
            "Training Iteration 5070, Loss: 2.9368906021118164\n",
            "Training Iteration 5071, Loss: 4.996984004974365\n",
            "Training Iteration 5072, Loss: 4.757725715637207\n",
            "Training Iteration 5073, Loss: 5.6295061111450195\n",
            "Training Iteration 5074, Loss: 5.5061726570129395\n",
            "Training Iteration 5075, Loss: 3.4296257495880127\n",
            "Training Iteration 5076, Loss: 2.34356689453125\n",
            "Training Iteration 5077, Loss: 5.749329566955566\n",
            "Training Iteration 5078, Loss: 5.259806156158447\n",
            "Training Iteration 5079, Loss: 5.132410526275635\n",
            "Training Iteration 5080, Loss: 3.903815269470215\n",
            "Training Iteration 5081, Loss: 8.407524108886719\n",
            "Training Iteration 5082, Loss: 6.1044182777404785\n",
            "Training Iteration 5083, Loss: 4.319869518280029\n",
            "Training Iteration 5084, Loss: 2.812160015106201\n",
            "Training Iteration 5085, Loss: 5.3346052169799805\n",
            "Training Iteration 5086, Loss: 4.4094977378845215\n",
            "Training Iteration 5087, Loss: 3.4424593448638916\n",
            "Training Iteration 5088, Loss: 3.715447187423706\n",
            "Training Iteration 5089, Loss: 2.091571569442749\n",
            "Training Iteration 5090, Loss: 3.792599678039551\n",
            "Training Iteration 5091, Loss: 3.905898332595825\n",
            "Training Iteration 5092, Loss: 4.410996913909912\n",
            "Training Iteration 5093, Loss: 3.4397389888763428\n",
            "Training Iteration 5094, Loss: 4.876359939575195\n",
            "Training Iteration 5095, Loss: 2.130121946334839\n",
            "Training Iteration 5096, Loss: 1.8001024723052979\n",
            "Training Iteration 5097, Loss: 7.621923446655273\n",
            "Training Iteration 5098, Loss: 4.504932880401611\n",
            "Training Iteration 5099, Loss: 6.103102684020996\n",
            "Training Iteration 5100, Loss: 7.664978504180908\n",
            "Training Iteration 5101, Loss: 2.330164909362793\n",
            "Training Iteration 5102, Loss: 4.251084327697754\n",
            "Training Iteration 5103, Loss: 5.265991687774658\n",
            "Training Iteration 5104, Loss: 2.4814367294311523\n",
            "Training Iteration 5105, Loss: 3.0712037086486816\n",
            "Training Iteration 5106, Loss: 5.633609294891357\n",
            "Training Iteration 5107, Loss: 4.132627010345459\n",
            "Training Iteration 5108, Loss: 10.022148132324219\n",
            "Training Iteration 5109, Loss: 6.373112678527832\n",
            "Training Iteration 5110, Loss: 5.876392841339111\n",
            "Training Iteration 5111, Loss: 5.277184009552002\n",
            "Training Iteration 5112, Loss: 4.941472053527832\n",
            "Training Iteration 5113, Loss: 7.432477951049805\n",
            "Training Iteration 5114, Loss: 8.548836708068848\n",
            "Training Iteration 5115, Loss: 5.437106132507324\n",
            "Training Iteration 5116, Loss: 4.383871555328369\n",
            "Training Iteration 5117, Loss: 5.260958671569824\n",
            "Training Iteration 5118, Loss: 1.5876991748809814\n",
            "Training Iteration 5119, Loss: 4.27937650680542\n",
            "Training Iteration 5120, Loss: 4.025482177734375\n",
            "Training Iteration 5121, Loss: 2.7609639167785645\n",
            "Training Iteration 5122, Loss: 3.472795248031616\n",
            "Training Iteration 5123, Loss: 7.397965431213379\n",
            "Training Iteration 5124, Loss: 4.7565083503723145\n",
            "Training Iteration 5125, Loss: 4.390997886657715\n",
            "Training Iteration 5126, Loss: 2.6079163551330566\n",
            "Training Iteration 5127, Loss: 3.071653366088867\n",
            "Training Iteration 5128, Loss: 4.304119110107422\n",
            "Training Iteration 5129, Loss: 5.751801490783691\n",
            "Training Iteration 5130, Loss: 3.6509766578674316\n",
            "Training Iteration 5131, Loss: 6.493690490722656\n",
            "Training Iteration 5132, Loss: 7.375758647918701\n",
            "Training Iteration 5133, Loss: 7.474858283996582\n",
            "Training Iteration 5134, Loss: 5.538471221923828\n",
            "Training Iteration 5135, Loss: 11.596999168395996\n",
            "Training Iteration 5136, Loss: 6.127554893493652\n",
            "Training Iteration 5137, Loss: 2.910728931427002\n",
            "Training Iteration 5138, Loss: 4.262441158294678\n",
            "Training Iteration 5139, Loss: 5.76593017578125\n",
            "Training Iteration 5140, Loss: 5.7404375076293945\n",
            "Training Iteration 5141, Loss: 3.3926243782043457\n",
            "Training Iteration 5142, Loss: 3.382070541381836\n",
            "Training Iteration 5143, Loss: 5.107091426849365\n",
            "Training Iteration 5144, Loss: 6.2778778076171875\n",
            "Training Iteration 5145, Loss: 2.880443811416626\n",
            "Training Iteration 5146, Loss: 4.450613498687744\n",
            "Training Iteration 5147, Loss: 7.676248550415039\n",
            "Training Iteration 5148, Loss: 3.4570934772491455\n",
            "Training Iteration 5149, Loss: 0.6298854947090149\n",
            "Training Iteration 5150, Loss: 5.676077365875244\n",
            "Training Iteration 5151, Loss: 7.55611515045166\n",
            "Training Iteration 5152, Loss: 7.8618950843811035\n",
            "Training Iteration 5153, Loss: 7.248544216156006\n",
            "Training Iteration 5154, Loss: 2.100132465362549\n",
            "Training Iteration 5155, Loss: 3.7867560386657715\n",
            "Training Iteration 5156, Loss: 4.291506767272949\n",
            "Training Iteration 5157, Loss: 13.869468688964844\n",
            "Training Iteration 5158, Loss: 2.708202838897705\n",
            "Training Iteration 5159, Loss: 5.096738338470459\n",
            "Training Iteration 5160, Loss: 3.814129114151001\n",
            "Training Iteration 5161, Loss: 6.399136543273926\n",
            "Training Iteration 5162, Loss: 3.892003059387207\n",
            "Training Iteration 5163, Loss: 3.6827385425567627\n",
            "Training Iteration 5164, Loss: 4.145208835601807\n",
            "Training Iteration 5165, Loss: 5.5526533126831055\n",
            "Training Iteration 5166, Loss: 3.567807674407959\n",
            "Training Iteration 5167, Loss: 2.545994997024536\n",
            "Training Iteration 5168, Loss: 1.016459345817566\n",
            "Training Iteration 5169, Loss: 2.677830934524536\n",
            "Training Iteration 5170, Loss: 4.378715991973877\n",
            "Training Iteration 5171, Loss: 5.394055366516113\n",
            "Training Iteration 5172, Loss: 2.8224048614501953\n",
            "Training Iteration 5173, Loss: 4.3609395027160645\n",
            "Training Iteration 5174, Loss: 7.102633953094482\n",
            "Training Iteration 5175, Loss: 7.786481857299805\n",
            "Training Iteration 5176, Loss: 9.42849349975586\n",
            "Training Iteration 5177, Loss: 6.236738681793213\n",
            "Training Iteration 5178, Loss: 4.030529022216797\n",
            "Training Iteration 5179, Loss: 3.08530855178833\n",
            "Training Iteration 5180, Loss: 3.3762810230255127\n",
            "Training Iteration 5181, Loss: 2.2811992168426514\n",
            "Training Iteration 5182, Loss: 5.18390417098999\n",
            "Training Iteration 5183, Loss: 4.677600860595703\n",
            "Training Iteration 5184, Loss: 3.9524986743927\n",
            "Training Iteration 5185, Loss: 3.489708423614502\n",
            "Training Iteration 5186, Loss: 5.412103176116943\n",
            "Training Iteration 5187, Loss: 3.5840494632720947\n",
            "Training Iteration 5188, Loss: 2.9034459590911865\n",
            "Training Iteration 5189, Loss: 3.2790746688842773\n",
            "Training Iteration 5190, Loss: 1.4939908981323242\n",
            "Training Iteration 5191, Loss: 3.1940536499023438\n",
            "Training Iteration 5192, Loss: 3.2539000511169434\n",
            "Training Iteration 5193, Loss: 5.806694030761719\n",
            "Training Iteration 5194, Loss: 3.690600633621216\n",
            "Training Iteration 5195, Loss: 2.149136781692505\n",
            "Training Iteration 5196, Loss: 4.984787940979004\n",
            "Training Iteration 5197, Loss: 2.3789145946502686\n",
            "Training Iteration 5198, Loss: 4.2609171867370605\n",
            "Training Iteration 5199, Loss: 3.1352200508117676\n",
            "Training Iteration 5200, Loss: 2.969322681427002\n",
            "Training Iteration 5201, Loss: 2.9466352462768555\n",
            "Training Iteration 5202, Loss: 3.430098533630371\n",
            "Training Iteration 5203, Loss: 5.951203346252441\n",
            "Training Iteration 5204, Loss: 4.734106540679932\n",
            "Training Iteration 5205, Loss: 2.9029502868652344\n",
            "Training Iteration 5206, Loss: 2.6870737075805664\n",
            "Training Iteration 5207, Loss: 3.030717611312866\n",
            "Training Iteration 5208, Loss: 5.525935649871826\n",
            "Training Iteration 5209, Loss: 3.7798736095428467\n",
            "Training Iteration 5210, Loss: 4.968725204467773\n",
            "Training Iteration 5211, Loss: 3.7233290672302246\n",
            "Training Iteration 5212, Loss: 9.626282691955566\n",
            "Training Iteration 5213, Loss: 4.046844482421875\n",
            "Training Iteration 5214, Loss: 4.864316463470459\n",
            "Training Iteration 5215, Loss: 4.0403642654418945\n",
            "Training Iteration 5216, Loss: 4.1430253982543945\n",
            "Training Iteration 5217, Loss: 5.9497222900390625\n",
            "Training Iteration 5218, Loss: 5.956999778747559\n",
            "Training Iteration 5219, Loss: 4.003641605377197\n",
            "Training Iteration 5220, Loss: 3.072503089904785\n",
            "Training Iteration 5221, Loss: 4.435457229614258\n",
            "Training Iteration 5222, Loss: 3.2626593112945557\n",
            "Training Iteration 5223, Loss: 3.5608181953430176\n",
            "Training Iteration 5224, Loss: 5.061724662780762\n",
            "Training Iteration 5225, Loss: 3.5795485973358154\n",
            "Training Iteration 5226, Loss: 4.997642517089844\n",
            "Training Iteration 5227, Loss: 5.309820175170898\n",
            "Training Iteration 5228, Loss: 5.844334602355957\n",
            "Training Iteration 5229, Loss: 4.446318626403809\n",
            "Training Iteration 5230, Loss: 4.619223594665527\n",
            "Training Iteration 5231, Loss: 4.966113567352295\n",
            "Training Iteration 5232, Loss: 5.079031944274902\n",
            "Training Iteration 5233, Loss: 3.763758897781372\n",
            "Training Iteration 5234, Loss: 3.7870023250579834\n",
            "Training Iteration 5235, Loss: 2.896595001220703\n",
            "Training Iteration 5236, Loss: 1.6440421342849731\n",
            "Training Iteration 5237, Loss: 3.8143138885498047\n",
            "Training Iteration 5238, Loss: 5.937421798706055\n",
            "Training Iteration 5239, Loss: 3.949821710586548\n",
            "Training Iteration 5240, Loss: 7.065178871154785\n",
            "Training Iteration 5241, Loss: 4.806176662445068\n",
            "Training Iteration 5242, Loss: 5.566275596618652\n",
            "Training Iteration 5243, Loss: 4.536966323852539\n",
            "Training Iteration 5244, Loss: 4.347781181335449\n",
            "Training Iteration 5245, Loss: 5.077767848968506\n",
            "Training Iteration 5246, Loss: 5.483681678771973\n",
            "Training Iteration 5247, Loss: 6.199274063110352\n",
            "Training Iteration 5248, Loss: 5.534684181213379\n",
            "Training Iteration 5249, Loss: 4.528130531311035\n",
            "Training Iteration 5250, Loss: 5.571028232574463\n",
            "Training Iteration 5251, Loss: 4.701559543609619\n",
            "Training Iteration 5252, Loss: 3.3770694732666016\n",
            "Training Iteration 5253, Loss: 4.50508975982666\n",
            "Training Iteration 5254, Loss: 1.9705073833465576\n",
            "Training Iteration 5255, Loss: 5.844537734985352\n",
            "Training Iteration 5256, Loss: 4.251677513122559\n",
            "Training Iteration 5257, Loss: 3.8384666442871094\n",
            "Training Iteration 5258, Loss: 5.51727819442749\n",
            "Training Iteration 5259, Loss: 5.602245330810547\n",
            "Training Iteration 5260, Loss: 1.9262199401855469\n",
            "Training Iteration 5261, Loss: 4.55135440826416\n",
            "Training Iteration 5262, Loss: 3.0534136295318604\n",
            "Training Iteration 5263, Loss: 2.4036593437194824\n",
            "Training Iteration 5264, Loss: 4.179637432098389\n",
            "Training Iteration 5265, Loss: 2.269347667694092\n",
            "Training Iteration 5266, Loss: 5.719601631164551\n",
            "Training Iteration 5267, Loss: 5.0282464027404785\n",
            "Training Iteration 5268, Loss: 1.7871173620224\n",
            "Training Iteration 5269, Loss: 6.925773620605469\n",
            "Training Iteration 5270, Loss: 3.3355612754821777\n",
            "Training Iteration 5271, Loss: 7.285672664642334\n",
            "Training Iteration 5272, Loss: 4.4039435386657715\n",
            "Training Iteration 5273, Loss: 5.03564453125\n",
            "Training Iteration 5274, Loss: 5.70303201675415\n",
            "Training Iteration 5275, Loss: 5.515141010284424\n",
            "Training Iteration 5276, Loss: 5.7297797203063965\n",
            "Training Iteration 5277, Loss: 8.370000839233398\n",
            "Training Iteration 5278, Loss: 3.575728178024292\n",
            "Training Iteration 5279, Loss: 5.078334331512451\n",
            "Training Iteration 5280, Loss: 8.585598945617676\n",
            "Training Iteration 5281, Loss: 5.516397953033447\n",
            "Training Iteration 5282, Loss: 3.6209781169891357\n",
            "Training Iteration 5283, Loss: 2.039823293685913\n",
            "Training Iteration 5284, Loss: 4.014505863189697\n",
            "Training Iteration 5285, Loss: 11.407346725463867\n",
            "Training Iteration 5286, Loss: 7.658275127410889\n",
            "Training Iteration 5287, Loss: 6.159019947052002\n",
            "Training Iteration 5288, Loss: 5.0806050300598145\n",
            "Training Iteration 5289, Loss: 7.936502933502197\n",
            "Training Iteration 5290, Loss: 4.9608564376831055\n",
            "Training Iteration 5291, Loss: 7.643126964569092\n",
            "Training Iteration 5292, Loss: 3.625075340270996\n",
            "Training Iteration 5293, Loss: 4.196682929992676\n",
            "Training Iteration 5294, Loss: 4.431243896484375\n",
            "Training Iteration 5295, Loss: 5.956555366516113\n",
            "Training Iteration 5296, Loss: 6.837811470031738\n",
            "Training Iteration 5297, Loss: 3.986781597137451\n",
            "Training Iteration 5298, Loss: 3.538254499435425\n",
            "Training Iteration 5299, Loss: 3.7499501705169678\n",
            "Training Iteration 5300, Loss: 2.6499524116516113\n",
            "Training Iteration 5301, Loss: 2.6314010620117188\n",
            "Training Iteration 5302, Loss: 2.2101008892059326\n",
            "Training Iteration 5303, Loss: 3.832536220550537\n",
            "Training Iteration 5304, Loss: 6.196829319000244\n",
            "Training Iteration 5305, Loss: 4.522165775299072\n",
            "Training Iteration 5306, Loss: 7.312671184539795\n",
            "Training Iteration 5307, Loss: 4.221189022064209\n",
            "Training Iteration 5308, Loss: 7.368136405944824\n",
            "Training Iteration 5309, Loss: 5.3560566902160645\n",
            "Training Iteration 5310, Loss: 3.3423163890838623\n",
            "Training Iteration 5311, Loss: 5.553017616271973\n",
            "Training Iteration 5312, Loss: 9.832761764526367\n",
            "Training Iteration 5313, Loss: 6.242406845092773\n",
            "Training Iteration 5314, Loss: 7.666394233703613\n",
            "Training Iteration 5315, Loss: 6.322317123413086\n",
            "Training Iteration 5316, Loss: 6.012096405029297\n",
            "Training Iteration 5317, Loss: 3.8279974460601807\n",
            "Training Iteration 5318, Loss: 3.261281728744507\n",
            "Training Iteration 5319, Loss: 4.822869777679443\n",
            "Training Iteration 5320, Loss: 3.4117982387542725\n",
            "Training Iteration 5321, Loss: 3.219024658203125\n",
            "Training Iteration 5322, Loss: 6.246603965759277\n",
            "Training Iteration 5323, Loss: 6.061225891113281\n",
            "Training Iteration 5324, Loss: 9.587384223937988\n",
            "Training Iteration 5325, Loss: 3.0514135360717773\n",
            "Training Iteration 5326, Loss: 2.045602321624756\n",
            "Training Iteration 5327, Loss: 2.7223451137542725\n",
            "Training Iteration 5328, Loss: 4.216257572174072\n",
            "Training Iteration 5329, Loss: 5.376328945159912\n",
            "Training Iteration 5330, Loss: 4.224923610687256\n",
            "Training Iteration 5331, Loss: 3.3475804328918457\n",
            "Training Iteration 5332, Loss: 4.348240375518799\n",
            "Training Iteration 5333, Loss: 5.477700710296631\n",
            "Training Iteration 5334, Loss: 5.820423603057861\n",
            "Training Iteration 5335, Loss: 4.366941928863525\n",
            "Training Iteration 5336, Loss: 5.38547945022583\n",
            "Training Iteration 5337, Loss: 6.100088596343994\n",
            "Training Iteration 5338, Loss: 4.958234786987305\n",
            "Training Iteration 5339, Loss: 3.9708285331726074\n",
            "Training Iteration 5340, Loss: 6.781625270843506\n",
            "Training Iteration 5341, Loss: 6.4228057861328125\n",
            "Training Iteration 5342, Loss: 3.46148943901062\n",
            "Training Iteration 5343, Loss: 4.284824848175049\n",
            "Training Iteration 5344, Loss: 4.670618534088135\n",
            "Training Iteration 5345, Loss: 1.7212872505187988\n",
            "Training Iteration 5346, Loss: 2.4345932006835938\n",
            "Training Iteration 5347, Loss: 3.9440083503723145\n",
            "Training Iteration 5348, Loss: 3.427100419998169\n",
            "Training Iteration 5349, Loss: 5.3147873878479\n",
            "Training Iteration 5350, Loss: 2.5799577236175537\n",
            "Training Iteration 5351, Loss: 5.760100364685059\n",
            "Training Iteration 5352, Loss: 7.0243072509765625\n",
            "Training Iteration 5353, Loss: 4.722429275512695\n",
            "Training Iteration 5354, Loss: 4.517712593078613\n",
            "Training Iteration 5355, Loss: 3.0232417583465576\n",
            "Training Iteration 5356, Loss: 5.133416175842285\n",
            "Training Iteration 5357, Loss: 4.443966865539551\n",
            "Training Iteration 5358, Loss: 4.035116195678711\n",
            "Training Iteration 5359, Loss: 2.6717529296875\n",
            "Training Iteration 5360, Loss: 5.493451118469238\n",
            "Training Iteration 5361, Loss: 4.494162559509277\n",
            "Training Iteration 5362, Loss: 5.173305988311768\n",
            "Training Iteration 5363, Loss: 4.5983967781066895\n",
            "Training Iteration 5364, Loss: 5.936515808105469\n",
            "Training Iteration 5365, Loss: 6.456404209136963\n",
            "Training Iteration 5366, Loss: 4.074990749359131\n",
            "Training Iteration 5367, Loss: 3.4499664306640625\n",
            "Training Iteration 5368, Loss: 4.177870273590088\n",
            "Training Iteration 5369, Loss: 4.751598358154297\n",
            "Training Iteration 5370, Loss: 4.739793300628662\n",
            "Training Iteration 5371, Loss: 7.0824360847473145\n",
            "Training Iteration 5372, Loss: 6.424606800079346\n",
            "Training Iteration 5373, Loss: 4.078590393066406\n",
            "Training Iteration 5374, Loss: 3.482675552368164\n",
            "Training Iteration 5375, Loss: 3.8124051094055176\n",
            "Training Iteration 5376, Loss: 5.295104026794434\n",
            "Training Iteration 5377, Loss: 2.524838924407959\n",
            "Training Iteration 5378, Loss: 6.201115131378174\n",
            "Training Iteration 5379, Loss: 5.10076379776001\n",
            "Training Iteration 5380, Loss: 6.337129592895508\n",
            "Training Iteration 5381, Loss: 2.325194835662842\n",
            "Training Iteration 5382, Loss: 3.7052693367004395\n",
            "Training Iteration 5383, Loss: 5.5710320472717285\n",
            "Training Iteration 5384, Loss: 7.383676528930664\n",
            "Training Iteration 5385, Loss: 5.827925205230713\n",
            "Training Iteration 5386, Loss: 4.825657367706299\n",
            "Training Iteration 5387, Loss: 3.3484878540039062\n",
            "Training Iteration 5388, Loss: 1.6584413051605225\n",
            "Training Iteration 5389, Loss: 5.870296478271484\n",
            "Training Iteration 5390, Loss: 5.668147563934326\n",
            "Training Iteration 5391, Loss: 3.93682599067688\n",
            "Training Iteration 5392, Loss: 4.243600845336914\n",
            "Training Iteration 5393, Loss: 2.878588914871216\n",
            "Training Iteration 5394, Loss: 3.285701274871826\n",
            "Training Iteration 5395, Loss: 5.041872024536133\n",
            "Training Iteration 5396, Loss: 5.579558372497559\n",
            "Training Iteration 5397, Loss: 3.7284531593322754\n",
            "Training Iteration 5398, Loss: 4.8358001708984375\n",
            "Training Iteration 5399, Loss: 3.7426774501800537\n",
            "Training Iteration 5400, Loss: 4.03497314453125\n",
            "Training Iteration 5401, Loss: 2.194756031036377\n",
            "Training Iteration 5402, Loss: 1.7611902952194214\n",
            "Training Iteration 5403, Loss: 5.483685493469238\n",
            "Training Iteration 5404, Loss: 4.137205123901367\n",
            "Training Iteration 5405, Loss: 6.598482131958008\n",
            "Training Iteration 5406, Loss: 6.225683689117432\n",
            "Training Iteration 5407, Loss: 3.5624032020568848\n",
            "Training Iteration 5408, Loss: 5.653787136077881\n",
            "Training Iteration 5409, Loss: 4.099668502807617\n",
            "Training Iteration 5410, Loss: 4.915138244628906\n",
            "Training Iteration 5411, Loss: 7.1815361976623535\n",
            "Training Iteration 5412, Loss: 3.3017258644104004\n",
            "Training Iteration 5413, Loss: 4.837422847747803\n",
            "Training Iteration 5414, Loss: 5.865316867828369\n",
            "Training Iteration 5415, Loss: 2.175673484802246\n",
            "Training Iteration 5416, Loss: 6.382413864135742\n",
            "Training Iteration 5417, Loss: 3.836557626724243\n",
            "Training Iteration 5418, Loss: 6.301867485046387\n",
            "Training Iteration 5419, Loss: 3.8514938354492188\n",
            "Training Iteration 5420, Loss: 4.188869476318359\n",
            "Training Iteration 5421, Loss: 6.137878894805908\n",
            "Training Iteration 5422, Loss: 7.087331771850586\n",
            "Training Iteration 5423, Loss: 5.759316444396973\n",
            "Training Iteration 5424, Loss: 5.4816484451293945\n",
            "Training Iteration 5425, Loss: 3.853598117828369\n",
            "Training Iteration 5426, Loss: 5.679913520812988\n",
            "Training Iteration 5427, Loss: 4.887296676635742\n",
            "Training Iteration 5428, Loss: 4.938229560852051\n",
            "Training Iteration 5429, Loss: 5.5533905029296875\n",
            "Training Iteration 5430, Loss: 2.7460334300994873\n",
            "Training Iteration 5431, Loss: 3.8065972328186035\n",
            "Training Iteration 5432, Loss: 3.114022731781006\n",
            "Training Iteration 5433, Loss: 4.92602014541626\n",
            "Training Iteration 5434, Loss: 1.9998095035552979\n",
            "Training Iteration 5435, Loss: 1.9326788187026978\n",
            "Training Iteration 5436, Loss: 3.7628743648529053\n",
            "Training Iteration 5437, Loss: 6.798599720001221\n",
            "Training Iteration 5438, Loss: 6.2605791091918945\n",
            "Training Iteration 5439, Loss: 2.7431395053863525\n",
            "Training Iteration 5440, Loss: 3.3704042434692383\n",
            "Training Iteration 5441, Loss: 5.082268238067627\n",
            "Training Iteration 5442, Loss: 7.138290882110596\n",
            "Training Iteration 5443, Loss: 6.206549644470215\n",
            "Training Iteration 5444, Loss: 6.6291680335998535\n",
            "Training Iteration 5445, Loss: 3.1449813842773438\n",
            "Training Iteration 5446, Loss: 4.5798797607421875\n",
            "Training Iteration 5447, Loss: 7.397775173187256\n",
            "Training Iteration 5448, Loss: 6.684084415435791\n",
            "Training Iteration 5449, Loss: 3.5071043968200684\n",
            "Training Iteration 5450, Loss: 3.121746063232422\n",
            "Training Iteration 5451, Loss: 3.169938325881958\n",
            "Training Iteration 5452, Loss: 3.4752516746520996\n",
            "Training Iteration 5453, Loss: 4.602538585662842\n",
            "Training Iteration 5454, Loss: 1.9981460571289062\n",
            "Training Iteration 5455, Loss: 2.4257895946502686\n",
            "Training Iteration 5456, Loss: 5.902735710144043\n",
            "Training Iteration 5457, Loss: 3.857999801635742\n",
            "Training Iteration 5458, Loss: 4.459407806396484\n",
            "Training Iteration 5459, Loss: 5.895647048950195\n",
            "Training Iteration 5460, Loss: 7.345738410949707\n",
            "Training Iteration 5461, Loss: 2.7665443420410156\n",
            "Training Iteration 5462, Loss: 2.1963894367218018\n",
            "Training Iteration 5463, Loss: 5.926157474517822\n",
            "Training Iteration 5464, Loss: 5.270482540130615\n",
            "Training Iteration 5465, Loss: 4.37907600402832\n",
            "Training Iteration 5466, Loss: 4.189487934112549\n",
            "Training Iteration 5467, Loss: 2.8244729042053223\n",
            "Training Iteration 5468, Loss: 3.8400845527648926\n",
            "Training Iteration 5469, Loss: 4.0967841148376465\n",
            "Training Iteration 5470, Loss: 5.595376491546631\n",
            "Training Iteration 5471, Loss: 6.632781028747559\n",
            "Training Iteration 5472, Loss: 6.231935977935791\n",
            "Training Iteration 5473, Loss: 3.7531518936157227\n",
            "Training Iteration 5474, Loss: 4.904532432556152\n",
            "Training Iteration 5475, Loss: 2.7216694355010986\n",
            "Training Iteration 5476, Loss: 2.3029394149780273\n",
            "Training Iteration 5477, Loss: 4.616601467132568\n",
            "Training Iteration 5478, Loss: 4.4430623054504395\n",
            "Training Iteration 5479, Loss: 2.9814915657043457\n",
            "Training Iteration 5480, Loss: 3.746833324432373\n",
            "Training Iteration 5481, Loss: 2.7510361671447754\n",
            "Training Iteration 5482, Loss: 5.1722307205200195\n",
            "Training Iteration 5483, Loss: 6.2042083740234375\n",
            "Training Iteration 5484, Loss: 3.240215539932251\n",
            "Training Iteration 5485, Loss: 1.725232481956482\n",
            "Training Iteration 5486, Loss: 3.7632687091827393\n",
            "Training Iteration 5487, Loss: 3.2400760650634766\n",
            "Training Iteration 5488, Loss: 3.9477531909942627\n",
            "Training Iteration 5489, Loss: 4.992944717407227\n",
            "Training Iteration 5490, Loss: 4.449059963226318\n",
            "Training Iteration 5491, Loss: 5.749872207641602\n",
            "Training Iteration 5492, Loss: 6.1164469718933105\n",
            "Training Iteration 5493, Loss: 3.4168760776519775\n",
            "Training Iteration 5494, Loss: 5.406727313995361\n",
            "Training Iteration 5495, Loss: 6.034829616546631\n",
            "Training Iteration 5496, Loss: 3.621298313140869\n",
            "Training Iteration 5497, Loss: 4.182322978973389\n",
            "Training Iteration 5498, Loss: 8.048616409301758\n",
            "Training Iteration 5499, Loss: 10.518385887145996\n",
            "Training Iteration 5500, Loss: 6.010673522949219\n",
            "Training Iteration 5501, Loss: 8.2605562210083\n",
            "Training Iteration 5502, Loss: 5.112884044647217\n",
            "Training Iteration 5503, Loss: 2.822542905807495\n",
            "Training Iteration 5504, Loss: 2.478508472442627\n",
            "Training Iteration 5505, Loss: 6.32808256149292\n",
            "Training Iteration 5506, Loss: 4.6309814453125\n",
            "Training Iteration 5507, Loss: 5.270366668701172\n",
            "Training Iteration 5508, Loss: 4.288394927978516\n",
            "Training Iteration 5509, Loss: 9.093496322631836\n",
            "Training Iteration 5510, Loss: 2.8010029792785645\n",
            "Training Iteration 5511, Loss: 2.680332660675049\n",
            "Training Iteration 5512, Loss: 4.554447174072266\n",
            "Training Iteration 5513, Loss: 5.862792015075684\n",
            "Training Iteration 5514, Loss: 3.9534034729003906\n",
            "Training Iteration 5515, Loss: 3.1413941383361816\n",
            "Training Iteration 5516, Loss: 4.5935468673706055\n",
            "Training Iteration 5517, Loss: 3.426896333694458\n",
            "Training Iteration 5518, Loss: 5.022951126098633\n",
            "Training Iteration 5519, Loss: 4.037713527679443\n",
            "Training Iteration 5520, Loss: 3.592262029647827\n",
            "Training Iteration 5521, Loss: 7.595859527587891\n",
            "Training Iteration 5522, Loss: 5.555008411407471\n",
            "Training Iteration 5523, Loss: 4.850954055786133\n",
            "Training Iteration 5524, Loss: 3.2818562984466553\n",
            "Training Iteration 5525, Loss: 4.525527000427246\n",
            "Training Iteration 5526, Loss: 2.522130012512207\n",
            "Training Iteration 5527, Loss: 5.002194404602051\n",
            "Training Iteration 5528, Loss: 4.2348785400390625\n",
            "Training Iteration 5529, Loss: 6.6696295738220215\n",
            "Training Iteration 5530, Loss: 4.7684831619262695\n",
            "Training Iteration 5531, Loss: 3.0781185626983643\n",
            "Training Iteration 5532, Loss: 4.874591827392578\n",
            "Training Iteration 5533, Loss: 4.993491172790527\n",
            "Training Iteration 5534, Loss: 2.5560569763183594\n",
            "Training Iteration 5535, Loss: 6.488945960998535\n",
            "Training Iteration 5536, Loss: 4.842055797576904\n",
            "Training Iteration 5537, Loss: 1.975082278251648\n",
            "Training Iteration 5538, Loss: 1.4618661403656006\n",
            "Training Iteration 5539, Loss: 4.200284957885742\n",
            "Training Iteration 5540, Loss: 11.086231231689453\n",
            "Training Iteration 5541, Loss: 5.246521949768066\n",
            "Training Iteration 5542, Loss: 5.768711090087891\n",
            "Training Iteration 5543, Loss: 3.7431464195251465\n",
            "Training Iteration 5544, Loss: 7.464613437652588\n",
            "Training Iteration 5545, Loss: 7.491819858551025\n",
            "Training Iteration 5546, Loss: 3.594395160675049\n",
            "Training Iteration 5547, Loss: 4.619690418243408\n",
            "Training Iteration 5548, Loss: 4.609940528869629\n",
            "Training Iteration 5549, Loss: 4.881924629211426\n",
            "Training Iteration 5550, Loss: 5.242689609527588\n",
            "Training Iteration 5551, Loss: 8.3965425491333\n",
            "Training Iteration 5552, Loss: 6.005117416381836\n",
            "Training Iteration 5553, Loss: 9.459126472473145\n",
            "Training Iteration 5554, Loss: 3.3731820583343506\n",
            "Training Iteration 5555, Loss: 5.712833404541016\n",
            "Training Iteration 5556, Loss: 2.985111713409424\n",
            "Training Iteration 5557, Loss: 4.32861328125\n",
            "Training Iteration 5558, Loss: 6.645193099975586\n",
            "Training Iteration 5559, Loss: 5.292055606842041\n",
            "Training Iteration 5560, Loss: 5.663384914398193\n",
            "Training Iteration 5561, Loss: 4.08002233505249\n",
            "Training Iteration 5562, Loss: 5.610901355743408\n",
            "Training Iteration 5563, Loss: 8.339329719543457\n",
            "Training Iteration 5564, Loss: 3.961944580078125\n",
            "Training Iteration 5565, Loss: 5.402190208435059\n",
            "Training Iteration 5566, Loss: 3.444890022277832\n",
            "Training Iteration 5567, Loss: 3.5090980529785156\n",
            "Training Iteration 5568, Loss: 7.786462306976318\n",
            "Training Iteration 5569, Loss: 7.6259260177612305\n",
            "Training Iteration 5570, Loss: 6.180600643157959\n",
            "Training Iteration 5571, Loss: 8.603782653808594\n",
            "Training Iteration 5572, Loss: 3.704669237136841\n",
            "Training Iteration 5573, Loss: 3.7259695529937744\n",
            "Training Iteration 5574, Loss: 5.734208583831787\n",
            "Training Iteration 5575, Loss: 4.092459678649902\n",
            "Training Iteration 5576, Loss: 5.868040561676025\n",
            "Training Iteration 5577, Loss: 5.570489883422852\n",
            "Training Iteration 5578, Loss: 3.1216814517974854\n",
            "Training Iteration 5579, Loss: 4.5868330001831055\n",
            "Training Iteration 5580, Loss: 3.5509161949157715\n",
            "Training Iteration 5581, Loss: 5.1845550537109375\n",
            "Training Iteration 5582, Loss: 10.962997436523438\n",
            "Training Iteration 5583, Loss: 5.760230541229248\n",
            "Training Iteration 5584, Loss: 9.276350021362305\n",
            "Training Iteration 5585, Loss: 7.154852390289307\n",
            "Training Iteration 5586, Loss: 6.685067653656006\n",
            "Training Iteration 5587, Loss: 4.350456714630127\n",
            "Training Iteration 5588, Loss: 5.072410583496094\n",
            "Training Iteration 5589, Loss: 2.6839144229888916\n",
            "Training Iteration 5590, Loss: 6.918050765991211\n",
            "Training Iteration 5591, Loss: 3.7482264041900635\n",
            "Training Iteration 5592, Loss: 1.9557868242263794\n",
            "Training Iteration 5593, Loss: 2.825629949569702\n",
            "Training Iteration 5594, Loss: 7.832036018371582\n",
            "Training Iteration 5595, Loss: 7.81520414352417\n",
            "Training Iteration 5596, Loss: 5.193015098571777\n",
            "Training Iteration 5597, Loss: 5.139403820037842\n",
            "Training Iteration 5598, Loss: 6.43155574798584\n",
            "Training Iteration 5599, Loss: 1.4352644681930542\n",
            "Training Iteration 5600, Loss: 6.413554668426514\n",
            "Training Iteration 5601, Loss: 4.969153881072998\n",
            "Training Iteration 5602, Loss: 4.981794357299805\n",
            "Training Iteration 5603, Loss: 3.1639065742492676\n",
            "Training Iteration 5604, Loss: 5.306633472442627\n",
            "Training Iteration 5605, Loss: 4.087200164794922\n",
            "Training Iteration 5606, Loss: 5.3023834228515625\n",
            "Training Iteration 5607, Loss: 6.168329238891602\n",
            "Training Iteration 5608, Loss: 4.32028865814209\n",
            "Training Iteration 5609, Loss: 3.3011791706085205\n",
            "Training Iteration 5610, Loss: 4.069693088531494\n",
            "Training Iteration 5611, Loss: 5.3397650718688965\n",
            "Training Iteration 5612, Loss: 4.794609069824219\n",
            "Training Iteration 5613, Loss: 5.8274102210998535\n",
            "Training Iteration 5614, Loss: 5.002300262451172\n",
            "Training Iteration 5615, Loss: 5.341060638427734\n",
            "Training Iteration 5616, Loss: 3.339461088180542\n",
            "Training Iteration 5617, Loss: 3.1695570945739746\n",
            "Training Iteration 5618, Loss: 2.760301113128662\n",
            "Training Iteration 5619, Loss: 1.4269641637802124\n",
            "Training Iteration 5620, Loss: 5.034264087677002\n",
            "Training Iteration 5621, Loss: 3.7931697368621826\n",
            "Training Iteration 5622, Loss: 5.172188758850098\n",
            "Training Iteration 5623, Loss: 4.942166805267334\n",
            "Training Iteration 5624, Loss: 3.6741931438446045\n",
            "Training Iteration 5625, Loss: 3.3730289936065674\n",
            "Training Iteration 5626, Loss: 4.42654275894165\n",
            "Training Iteration 5627, Loss: 4.567592620849609\n",
            "Training Iteration 5628, Loss: 6.0620341300964355\n",
            "Training Iteration 5629, Loss: 5.360283374786377\n",
            "Training Iteration 5630, Loss: 2.9419713020324707\n",
            "Training Iteration 5631, Loss: 2.2733712196350098\n",
            "Training Iteration 5632, Loss: 5.4362592697143555\n",
            "Training Iteration 5633, Loss: 6.180856704711914\n",
            "Training Iteration 5634, Loss: 3.417767286300659\n",
            "Training Iteration 5635, Loss: 2.569159984588623\n",
            "Training Iteration 5636, Loss: 2.2264492511749268\n",
            "Training Iteration 5637, Loss: 3.0833899974823\n",
            "Training Iteration 5638, Loss: 9.226519584655762\n",
            "Training Iteration 5639, Loss: 5.557186126708984\n",
            "Training Iteration 5640, Loss: 1.4407949447631836\n",
            "Training Iteration 5641, Loss: 3.631702423095703\n",
            "Training Iteration 5642, Loss: 4.58212423324585\n",
            "Training Iteration 5643, Loss: 3.779263734817505\n",
            "Training Iteration 5644, Loss: 4.878177642822266\n",
            "Training Iteration 5645, Loss: 5.163541793823242\n",
            "Training Iteration 5646, Loss: 4.44624137878418\n",
            "Training Iteration 5647, Loss: 4.10375452041626\n",
            "Training Iteration 5648, Loss: 3.0732476711273193\n",
            "Training Iteration 5649, Loss: 3.8267452716827393\n",
            "Training Iteration 5650, Loss: 7.337601661682129\n",
            "Training Iteration 5651, Loss: 8.6917724609375\n",
            "Training Iteration 5652, Loss: 6.121035575866699\n",
            "Training Iteration 5653, Loss: 3.731816530227661\n",
            "Training Iteration 5654, Loss: 5.766437530517578\n",
            "Training Iteration 5655, Loss: 4.1249823570251465\n",
            "Training Iteration 5656, Loss: 3.2412850856781006\n",
            "Training Iteration 5657, Loss: 7.542607307434082\n",
            "Training Iteration 5658, Loss: 3.859982967376709\n",
            "Training Iteration 5659, Loss: 2.3010997772216797\n",
            "Training Iteration 5660, Loss: 2.334066867828369\n",
            "Training Iteration 5661, Loss: 4.812037467956543\n",
            "Training Iteration 5662, Loss: 5.543573379516602\n",
            "Training Iteration 5663, Loss: 4.51865291595459\n",
            "Training Iteration 5664, Loss: 3.6363887786865234\n",
            "Training Iteration 5665, Loss: 3.930337429046631\n",
            "Training Iteration 5666, Loss: 5.787144660949707\n",
            "Training Iteration 5667, Loss: 3.1253950595855713\n",
            "Training Iteration 5668, Loss: 3.6569089889526367\n",
            "Training Iteration 5669, Loss: 4.302146911621094\n",
            "Training Iteration 5670, Loss: 3.5795693397521973\n",
            "Training Iteration 5671, Loss: 4.382746696472168\n",
            "Training Iteration 5672, Loss: 5.546421527862549\n",
            "Training Iteration 5673, Loss: 4.185273170471191\n",
            "Training Iteration 5674, Loss: 2.7729310989379883\n",
            "Training Iteration 5675, Loss: 2.067014217376709\n",
            "Training Iteration 5676, Loss: 5.495541572570801\n",
            "Training Iteration 5677, Loss: 4.446341514587402\n",
            "Training Iteration 5678, Loss: 5.542749404907227\n",
            "Training Iteration 5679, Loss: 4.009581565856934\n",
            "Training Iteration 5680, Loss: 0.9933311939239502\n",
            "Training Iteration 5681, Loss: 2.7257771492004395\n",
            "Training Iteration 5682, Loss: 3.5158636569976807\n",
            "Training Iteration 5683, Loss: 6.763674736022949\n",
            "Training Iteration 5684, Loss: 4.714420795440674\n",
            "Training Iteration 5685, Loss: 2.8494348526000977\n",
            "Training Iteration 5686, Loss: 5.519523620605469\n",
            "Training Iteration 5687, Loss: 2.1297619342803955\n",
            "Training Iteration 5688, Loss: 4.481281280517578\n",
            "Training Iteration 5689, Loss: 3.6489181518554688\n",
            "Training Iteration 5690, Loss: 3.824683666229248\n",
            "Training Iteration 5691, Loss: 3.507101535797119\n",
            "Training Iteration 5692, Loss: 5.238708019256592\n",
            "Training Iteration 5693, Loss: 1.6220016479492188\n",
            "Training Iteration 5694, Loss: 3.774275302886963\n",
            "Training Iteration 5695, Loss: 3.9036295413970947\n",
            "Training Iteration 5696, Loss: 5.00373649597168\n",
            "Training Iteration 5697, Loss: 4.465173244476318\n",
            "Training Iteration 5698, Loss: 4.896018981933594\n",
            "Training Iteration 5699, Loss: 4.342144012451172\n",
            "Training Iteration 5700, Loss: 7.039356708526611\n",
            "Training Iteration 5701, Loss: 3.5102508068084717\n",
            "Training Iteration 5702, Loss: 3.3058314323425293\n",
            "Training Iteration 5703, Loss: 6.289000988006592\n",
            "Training Iteration 5704, Loss: 3.791393756866455\n",
            "Training Iteration 5705, Loss: 6.4316325187683105\n",
            "Training Iteration 5706, Loss: 3.322033166885376\n",
            "Training Iteration 5707, Loss: 8.158863067626953\n",
            "Training Iteration 5708, Loss: 5.577093124389648\n",
            "Training Iteration 5709, Loss: 3.0755510330200195\n",
            "Training Iteration 5710, Loss: 3.79896879196167\n",
            "Training Iteration 5711, Loss: 5.321409702301025\n",
            "Training Iteration 5712, Loss: 6.03812837600708\n",
            "Training Iteration 5713, Loss: 3.0705854892730713\n",
            "Training Iteration 5714, Loss: 4.254786968231201\n",
            "Training Iteration 5715, Loss: 4.083373069763184\n",
            "Training Iteration 5716, Loss: 5.570339202880859\n",
            "Training Iteration 5717, Loss: 3.401027202606201\n",
            "Training Iteration 5718, Loss: 3.555675983428955\n",
            "Training Iteration 5719, Loss: 2.9307940006256104\n",
            "Training Iteration 5720, Loss: 3.0593454837799072\n",
            "Training Iteration 5721, Loss: 4.610347747802734\n",
            "Training Iteration 5722, Loss: 3.5912132263183594\n",
            "Training Iteration 5723, Loss: 6.009593963623047\n",
            "Training Iteration 5724, Loss: 3.626253128051758\n",
            "Training Iteration 5725, Loss: 6.560611248016357\n",
            "Training Iteration 5726, Loss: 4.866671562194824\n",
            "Training Iteration 5727, Loss: 5.517832279205322\n",
            "Training Iteration 5728, Loss: 4.38832426071167\n",
            "Training Iteration 5729, Loss: 1.452416181564331\n",
            "Training Iteration 5730, Loss: 4.842948913574219\n",
            "Training Iteration 5731, Loss: 3.414450168609619\n",
            "Training Iteration 5732, Loss: 4.824405670166016\n",
            "Training Iteration 5733, Loss: 6.738326549530029\n",
            "Training Iteration 5734, Loss: 4.63559103012085\n",
            "Training Iteration 5735, Loss: 4.602385520935059\n",
            "Training Iteration 5736, Loss: 5.261085510253906\n",
            "Training Iteration 5737, Loss: 1.1377538442611694\n",
            "Training Iteration 5738, Loss: 2.712625741958618\n",
            "Training Iteration 5739, Loss: 5.403304576873779\n",
            "Training Iteration 5740, Loss: 7.095168113708496\n",
            "Training Iteration 5741, Loss: 4.986019611358643\n",
            "Training Iteration 5742, Loss: 4.218915939331055\n",
            "Training Iteration 5743, Loss: 1.766787052154541\n",
            "Training Iteration 5744, Loss: 5.893621444702148\n",
            "Training Iteration 5745, Loss: 2.566819190979004\n",
            "Training Iteration 5746, Loss: 1.513295292854309\n",
            "Training Iteration 5747, Loss: 4.614341735839844\n",
            "Training Iteration 5748, Loss: 4.083096504211426\n",
            "Training Iteration 5749, Loss: 5.349789142608643\n",
            "Training Iteration 5750, Loss: 4.03584623336792\n",
            "Training Iteration 5751, Loss: 3.450002431869507\n",
            "Training Iteration 5752, Loss: 3.94230318069458\n",
            "Training Iteration 5753, Loss: 7.644883632659912\n",
            "Training Iteration 5754, Loss: 5.849702835083008\n",
            "Training Iteration 5755, Loss: 3.543194055557251\n",
            "Training Iteration 5756, Loss: 3.8816280364990234\n",
            "Training Iteration 5757, Loss: 4.844841003417969\n",
            "Training Iteration 5758, Loss: 6.5086350440979\n",
            "Training Iteration 5759, Loss: 6.009598255157471\n",
            "Training Iteration 5760, Loss: 1.8661835193634033\n",
            "Training Iteration 5761, Loss: 4.6865925788879395\n",
            "Training Iteration 5762, Loss: 4.810949802398682\n",
            "Training Iteration 5763, Loss: 5.7822160720825195\n",
            "Training Iteration 5764, Loss: 5.48574161529541\n",
            "Training Iteration 5765, Loss: 6.234965801239014\n",
            "Training Iteration 5766, Loss: 3.1669349670410156\n",
            "Training Iteration 5767, Loss: 4.280316352844238\n",
            "Training Iteration 5768, Loss: 4.610546588897705\n",
            "Training Iteration 5769, Loss: 6.364363193511963\n",
            "Training Iteration 5770, Loss: 7.106387615203857\n",
            "Training Iteration 5771, Loss: 3.4968245029449463\n",
            "Training Iteration 5772, Loss: 8.115493774414062\n",
            "Training Iteration 5773, Loss: 5.815038681030273\n",
            "Training Iteration 5774, Loss: 4.864705562591553\n",
            "Training Iteration 5775, Loss: 6.48151969909668\n",
            "Training Iteration 5776, Loss: 4.594161033630371\n",
            "Training Iteration 5777, Loss: 2.6901283264160156\n",
            "Training Iteration 5778, Loss: 6.498236656188965\n",
            "Training Iteration 5779, Loss: 5.105399131774902\n",
            "Training Iteration 5780, Loss: 7.059631824493408\n",
            "Training Iteration 5781, Loss: 8.231059074401855\n",
            "Training Iteration 5782, Loss: 10.48642349243164\n",
            "Training Iteration 5783, Loss: 10.381364822387695\n",
            "Training Iteration 5784, Loss: 4.025805950164795\n",
            "Training Iteration 5785, Loss: 5.744132041931152\n",
            "Training Iteration 5786, Loss: 5.253028869628906\n",
            "Training Iteration 5787, Loss: 7.308955669403076\n",
            "Training Iteration 5788, Loss: 6.597531318664551\n",
            "Training Iteration 5789, Loss: 7.661262512207031\n",
            "Training Iteration 5790, Loss: 5.870730400085449\n",
            "Training Iteration 5791, Loss: 4.798087120056152\n",
            "Training Iteration 5792, Loss: 5.128201007843018\n",
            "Training Iteration 5793, Loss: 7.0438737869262695\n",
            "Training Iteration 5794, Loss: 3.2610127925872803\n",
            "Training Iteration 5795, Loss: 8.283382415771484\n",
            "Training Iteration 5796, Loss: 8.839618682861328\n",
            "Training Iteration 5797, Loss: 6.015856742858887\n",
            "Training Iteration 5798, Loss: 3.9447708129882812\n",
            "Training Iteration 5799, Loss: 7.5566301345825195\n",
            "Training Iteration 5800, Loss: 5.5273284912109375\n",
            "Training Iteration 5801, Loss: 3.2691261768341064\n",
            "Training Iteration 5802, Loss: 6.651187419891357\n",
            "Training Iteration 5803, Loss: 5.075716495513916\n",
            "Training Iteration 5804, Loss: 7.994391918182373\n",
            "Training Iteration 5805, Loss: 2.687535524368286\n",
            "Training Iteration 5806, Loss: 3.3431737422943115\n",
            "Training Iteration 5807, Loss: 5.203835487365723\n",
            "Training Iteration 5808, Loss: 6.55128288269043\n",
            "Training Iteration 5809, Loss: 4.472532749176025\n",
            "Training Iteration 5810, Loss: 3.727639675140381\n",
            "Training Iteration 5811, Loss: 6.398800849914551\n",
            "Training Iteration 5812, Loss: 4.4843292236328125\n",
            "Training Iteration 5813, Loss: 4.694600582122803\n",
            "Training Iteration 5814, Loss: 3.867011070251465\n",
            "Training Iteration 5815, Loss: 7.318685531616211\n",
            "Training Iteration 5816, Loss: 5.67667293548584\n",
            "Training Iteration 5817, Loss: 5.009866714477539\n",
            "Training Iteration 5818, Loss: 3.2543773651123047\n",
            "Training Iteration 5819, Loss: 3.3400821685791016\n",
            "Training Iteration 5820, Loss: 3.4372928142547607\n",
            "Training Iteration 5821, Loss: 8.3433837890625\n",
            "Training Iteration 5822, Loss: 3.7010927200317383\n",
            "Training Iteration 5823, Loss: 3.4106359481811523\n",
            "Training Iteration 5824, Loss: 5.5535759925842285\n",
            "Training Iteration 5825, Loss: 4.349116325378418\n",
            "Training Iteration 5826, Loss: 12.084001541137695\n",
            "Training Iteration 5827, Loss: 7.797983169555664\n",
            "Training Iteration 5828, Loss: 4.2018938064575195\n",
            "Training Iteration 5829, Loss: 2.8548941612243652\n",
            "Training Iteration 5830, Loss: 2.2125682830810547\n",
            "Training Iteration 5831, Loss: 4.0722432136535645\n",
            "Training Iteration 5832, Loss: 8.320124626159668\n",
            "Training Iteration 5833, Loss: 5.354217052459717\n",
            "Training Iteration 5834, Loss: 3.005009889602661\n",
            "Training Iteration 5835, Loss: 8.400272369384766\n",
            "Training Iteration 5836, Loss: 6.448238372802734\n",
            "Training Iteration 5837, Loss: 10.667248725891113\n",
            "Training Iteration 5838, Loss: 3.1049602031707764\n",
            "Training Iteration 5839, Loss: 0.7847715616226196\n",
            "Training Iteration 5840, Loss: 4.9359002113342285\n",
            "Training Iteration 5841, Loss: 2.3050873279571533\n",
            "Training Iteration 5842, Loss: 4.012470245361328\n",
            "Training Iteration 5843, Loss: 3.5907013416290283\n",
            "Training Iteration 5844, Loss: 4.320130348205566\n",
            "Training Iteration 5845, Loss: 5.258337497711182\n",
            "Training Iteration 5846, Loss: 2.042327404022217\n",
            "Training Iteration 5847, Loss: 3.1218061447143555\n",
            "Training Iteration 5848, Loss: 4.564901351928711\n",
            "Training Iteration 5849, Loss: 9.146171569824219\n",
            "Training Iteration 5850, Loss: 4.982280731201172\n",
            "Training Iteration 5851, Loss: 3.817568778991699\n",
            "Training Iteration 5852, Loss: 9.876885414123535\n",
            "Training Iteration 5853, Loss: 5.3749918937683105\n",
            "Training Iteration 5854, Loss: 6.032435417175293\n",
            "Training Iteration 5855, Loss: 5.13282585144043\n",
            "Training Iteration 5856, Loss: 5.0279221534729\n",
            "Training Iteration 5857, Loss: 3.3893065452575684\n",
            "Training Iteration 5858, Loss: 4.15601921081543\n",
            "Training Iteration 5859, Loss: 3.8761723041534424\n",
            "Training Iteration 5860, Loss: 5.433475494384766\n",
            "Training Iteration 5861, Loss: 4.196324348449707\n",
            "Training Iteration 5862, Loss: 5.427284240722656\n",
            "Training Iteration 5863, Loss: 3.497159004211426\n",
            "Training Iteration 5864, Loss: 3.2175207138061523\n",
            "Training Iteration 5865, Loss: 9.404829025268555\n",
            "Training Iteration 5866, Loss: 15.608643531799316\n",
            "Training Iteration 5867, Loss: 3.151261806488037\n",
            "Training Iteration 5868, Loss: 5.029420375823975\n",
            "Training Iteration 5869, Loss: 5.003831386566162\n",
            "Training Iteration 5870, Loss: 2.3945837020874023\n",
            "Training Iteration 5871, Loss: 8.041410446166992\n",
            "Training Iteration 5872, Loss: 5.853687763214111\n",
            "Training Iteration 5873, Loss: 3.9068591594696045\n",
            "Training Iteration 5874, Loss: 4.276037216186523\n",
            "Training Iteration 5875, Loss: 6.342376708984375\n",
            "Training Iteration 5876, Loss: 8.159345626831055\n",
            "Training Iteration 5877, Loss: 4.620216369628906\n",
            "Training Iteration 5878, Loss: 6.261414051055908\n",
            "Training Iteration 5879, Loss: 4.540648460388184\n",
            "Training Iteration 5880, Loss: 5.509655952453613\n",
            "Training Iteration 5881, Loss: 5.52020788192749\n",
            "Training Iteration 5882, Loss: 4.554224014282227\n",
            "Training Iteration 5883, Loss: 5.830164909362793\n",
            "Training Iteration 5884, Loss: 2.5054049491882324\n",
            "Training Iteration 5885, Loss: 5.224425315856934\n",
            "Training Iteration 5886, Loss: 3.96500301361084\n",
            "Training Iteration 5887, Loss: 5.663604736328125\n",
            "Training Iteration 5888, Loss: 5.84421443939209\n",
            "Training Iteration 5889, Loss: 5.492430686950684\n",
            "Training Iteration 5890, Loss: 3.446922779083252\n",
            "Training Iteration 5891, Loss: 6.12880277633667\n",
            "Training Iteration 5892, Loss: 3.202258586883545\n",
            "Training Iteration 5893, Loss: 5.759322643280029\n",
            "Training Iteration 5894, Loss: 4.8170928955078125\n",
            "Training Iteration 5895, Loss: 3.4813923835754395\n",
            "Training Iteration 5896, Loss: 4.737427711486816\n",
            "Training Iteration 5897, Loss: 3.2401866912841797\n",
            "Training Iteration 5898, Loss: 2.309119939804077\n",
            "Training Iteration 5899, Loss: 6.924482345581055\n",
            "Training Iteration 5900, Loss: 6.341479301452637\n",
            "Training Iteration 5901, Loss: 8.496817588806152\n",
            "Training Iteration 5902, Loss: 7.180919647216797\n",
            "Training Iteration 5903, Loss: 4.423117160797119\n",
            "Training Iteration 5904, Loss: 3.3883554935455322\n",
            "Training Iteration 5905, Loss: 5.074794292449951\n",
            "Training Iteration 5906, Loss: 7.156063556671143\n",
            "Training Iteration 5907, Loss: 3.6431097984313965\n",
            "Training Iteration 5908, Loss: 2.6680164337158203\n",
            "Training Iteration 5909, Loss: 6.141388893127441\n",
            "Training Iteration 5910, Loss: 4.6085357666015625\n",
            "Training Iteration 5911, Loss: 2.2739992141723633\n",
            "Training Iteration 5912, Loss: 3.65560245513916\n",
            "Training Iteration 5913, Loss: 3.2771854400634766\n",
            "Training Iteration 5914, Loss: 3.487755060195923\n",
            "Training Iteration 5915, Loss: 4.237226963043213\n",
            "Training Iteration 5916, Loss: 4.256908416748047\n",
            "Training Iteration 5917, Loss: 3.851778507232666\n",
            "Training Iteration 5918, Loss: 5.4661784172058105\n",
            "Training Iteration 5919, Loss: 5.432936668395996\n",
            "Training Iteration 5920, Loss: 3.38602614402771\n",
            "Training Iteration 5921, Loss: 5.4514288902282715\n",
            "Training Iteration 5922, Loss: 1.5022087097167969\n",
            "Training Iteration 5923, Loss: 2.1312873363494873\n",
            "Training Iteration 5924, Loss: 4.661884784698486\n",
            "Training Iteration 5925, Loss: 2.792196273803711\n",
            "Training Iteration 5926, Loss: 6.407022953033447\n",
            "Training Iteration 5927, Loss: 5.724138259887695\n",
            "Training Iteration 5928, Loss: 3.8529815673828125\n",
            "Training Iteration 5929, Loss: 4.180947303771973\n",
            "Training Iteration 5930, Loss: 6.626861095428467\n",
            "Training Iteration 5931, Loss: 4.417024612426758\n",
            "Training Iteration 5932, Loss: 4.121711254119873\n",
            "Training Iteration 5933, Loss: 2.0626790523529053\n",
            "Training Iteration 5934, Loss: 6.194693088531494\n",
            "Training Iteration 5935, Loss: 2.6250805854797363\n",
            "Training Iteration 5936, Loss: 4.6044087409973145\n",
            "Training Iteration 5937, Loss: 3.700629949569702\n",
            "Training Iteration 5938, Loss: 5.221609115600586\n",
            "Training Iteration 5939, Loss: 4.3334221839904785\n",
            "Training Iteration 5940, Loss: 3.924278974533081\n",
            "Training Iteration 5941, Loss: 3.5146517753601074\n",
            "Training Iteration 5942, Loss: 4.362318515777588\n",
            "Training Iteration 5943, Loss: 7.485442638397217\n",
            "Training Iteration 5944, Loss: 4.898592472076416\n",
            "Training Iteration 5945, Loss: 2.349060535430908\n",
            "Training Iteration 5946, Loss: 2.4935286045074463\n",
            "Training Iteration 5947, Loss: 3.6428275108337402\n",
            "Training Iteration 5948, Loss: 2.879795789718628\n",
            "Training Iteration 5949, Loss: 3.879237651824951\n",
            "Training Iteration 5950, Loss: 5.8461222648620605\n",
            "Training Iteration 5951, Loss: 4.1215434074401855\n",
            "Training Iteration 5952, Loss: 3.3753411769866943\n",
            "Training Iteration 5953, Loss: 3.084120512008667\n",
            "Training Iteration 5954, Loss: 2.5580272674560547\n",
            "Training Iteration 5955, Loss: 1.9088762998580933\n",
            "Training Iteration 5956, Loss: 5.15821647644043\n",
            "Training Iteration 5957, Loss: 3.0076565742492676\n",
            "Training Iteration 5958, Loss: 3.187767744064331\n",
            "Training Iteration 5959, Loss: 3.034252643585205\n",
            "Training Iteration 5960, Loss: 8.001880645751953\n",
            "Training Iteration 5961, Loss: 7.666420936584473\n",
            "Training Iteration 5962, Loss: 6.802766799926758\n",
            "Training Iteration 5963, Loss: 2.824280261993408\n",
            "Training Iteration 5964, Loss: 6.501357555389404\n",
            "Training Iteration 5965, Loss: 5.2174482345581055\n",
            "Training Iteration 5966, Loss: 4.167183876037598\n",
            "Training Iteration 5967, Loss: 4.797767162322998\n",
            "Training Iteration 5968, Loss: 5.31821346282959\n",
            "Training Iteration 5969, Loss: 1.371716856956482\n",
            "Training Iteration 5970, Loss: 4.101690769195557\n",
            "Training Iteration 5971, Loss: 6.138284206390381\n",
            "Training Iteration 5972, Loss: 8.847028732299805\n",
            "Training Iteration 5973, Loss: 4.08552360534668\n",
            "Training Iteration 5974, Loss: 3.953120470046997\n",
            "Training Iteration 5975, Loss: 4.182678699493408\n",
            "Training Iteration 5976, Loss: 3.388270378112793\n",
            "Training Iteration 5977, Loss: 4.6724534034729\n",
            "Training Iteration 5978, Loss: 3.274423122406006\n",
            "Training Iteration 5979, Loss: 3.9760379791259766\n",
            "Training Iteration 5980, Loss: 4.208657741546631\n",
            "Training Iteration 5981, Loss: 5.192880153656006\n",
            "Training Iteration 5982, Loss: 5.815101623535156\n",
            "Training Iteration 5983, Loss: 2.9665424823760986\n",
            "Training Iteration 5984, Loss: 4.456003665924072\n",
            "Training Iteration 5985, Loss: 4.530600547790527\n",
            "Training Iteration 5986, Loss: 4.531461238861084\n",
            "Training Iteration 5987, Loss: 4.4996490478515625\n",
            "Training Iteration 5988, Loss: 3.02474308013916\n",
            "Training Iteration 5989, Loss: 3.6056225299835205\n",
            "Training Iteration 5990, Loss: 1.792726993560791\n",
            "Training Iteration 5991, Loss: 4.450752258300781\n",
            "Training Iteration 5992, Loss: 6.289794921875\n",
            "Training Iteration 5993, Loss: 4.752493858337402\n",
            "Training Iteration 5994, Loss: 5.14934778213501\n",
            "Training Iteration 5995, Loss: 1.6578117609024048\n",
            "Training Iteration 5996, Loss: 1.8605002164840698\n",
            "Training Iteration 5997, Loss: 2.0260467529296875\n",
            "Training Iteration 5998, Loss: 5.2974090576171875\n",
            "Training Iteration 5999, Loss: 5.544475555419922\n",
            "Training Iteration 6000, Loss: 3.9043431282043457\n",
            "Training Iteration 6001, Loss: 6.283524513244629\n",
            "Training Iteration 6002, Loss: 7.263305187225342\n",
            "Training Iteration 6003, Loss: 6.337738037109375\n",
            "Training Iteration 6004, Loss: 5.029354572296143\n",
            "Training Iteration 6005, Loss: 4.640859603881836\n",
            "Training Iteration 6006, Loss: 2.7500574588775635\n",
            "Training Iteration 6007, Loss: 3.464656352996826\n",
            "Training Iteration 6008, Loss: 3.595123052597046\n",
            "Training Iteration 6009, Loss: 6.196337699890137\n",
            "Training Iteration 6010, Loss: 7.817605972290039\n",
            "Training Iteration 6011, Loss: 4.362987995147705\n",
            "Training Iteration 6012, Loss: 5.031546115875244\n",
            "Training Iteration 6013, Loss: 5.386149883270264\n",
            "Training Iteration 6014, Loss: 9.685318946838379\n",
            "Training Iteration 6015, Loss: 6.7739458084106445\n",
            "Training Iteration 6016, Loss: 7.035016059875488\n",
            "Training Iteration 6017, Loss: 3.5852925777435303\n",
            "Training Iteration 6018, Loss: 3.61354923248291\n",
            "Training Iteration 6019, Loss: 3.414853811264038\n",
            "Training Iteration 6020, Loss: 9.050146102905273\n",
            "Training Iteration 6021, Loss: 7.750491619110107\n",
            "Training Iteration 6022, Loss: 10.012693405151367\n",
            "Training Iteration 6023, Loss: 8.890278816223145\n",
            "Training Iteration 6024, Loss: 3.416921615600586\n",
            "Training Iteration 6025, Loss: 4.0564422607421875\n",
            "Training Iteration 6026, Loss: 4.043390274047852\n",
            "Training Iteration 6027, Loss: 3.801891326904297\n",
            "Training Iteration 6028, Loss: 8.779468536376953\n",
            "Training Iteration 6029, Loss: 5.44209623336792\n",
            "Training Iteration 6030, Loss: 5.7290472984313965\n",
            "Training Iteration 6031, Loss: 2.548929214477539\n",
            "Training Iteration 6032, Loss: 3.8300039768218994\n",
            "Training Iteration 6033, Loss: 5.305763244628906\n",
            "Training Iteration 6034, Loss: 11.536458015441895\n",
            "Training Iteration 6035, Loss: 4.833831310272217\n",
            "Training Iteration 6036, Loss: 5.159543514251709\n",
            "Training Iteration 6037, Loss: 2.5575363636016846\n",
            "Training Iteration 6038, Loss: 4.700514793395996\n",
            "Training Iteration 6039, Loss: 6.633754730224609\n",
            "Training Iteration 6040, Loss: 5.231419563293457\n",
            "Training Iteration 6041, Loss: 4.928991317749023\n",
            "Training Iteration 6042, Loss: 5.388310432434082\n",
            "Training Iteration 6043, Loss: 3.2312207221984863\n",
            "Training Iteration 6044, Loss: 5.735954284667969\n",
            "Training Iteration 6045, Loss: 4.764707088470459\n",
            "Training Iteration 6046, Loss: 7.280733108520508\n",
            "Training Iteration 6047, Loss: 4.437308311462402\n",
            "Training Iteration 6048, Loss: 3.3139920234680176\n",
            "Training Iteration 6049, Loss: 3.3826565742492676\n",
            "Training Iteration 6050, Loss: 4.184577941894531\n",
            "Training Iteration 6051, Loss: 4.4936652183532715\n",
            "Training Iteration 6052, Loss: 5.29922342300415\n",
            "Training Iteration 6053, Loss: 5.810275554656982\n",
            "Training Iteration 6054, Loss: 3.5276939868927\n",
            "Training Iteration 6055, Loss: 8.087000846862793\n",
            "Training Iteration 6056, Loss: 5.54399299621582\n",
            "Training Iteration 6057, Loss: 6.110624313354492\n",
            "Training Iteration 6058, Loss: 5.321985244750977\n",
            "Training Iteration 6059, Loss: 4.312114238739014\n",
            "Training Iteration 6060, Loss: 3.8056156635284424\n",
            "Training Iteration 6061, Loss: 3.797529458999634\n",
            "Training Iteration 6062, Loss: 3.0604088306427\n",
            "Training Iteration 6063, Loss: 5.284404754638672\n",
            "Training Iteration 6064, Loss: 4.830087184906006\n",
            "Training Iteration 6065, Loss: 5.454795837402344\n",
            "Training Iteration 6066, Loss: 6.674405097961426\n",
            "Training Iteration 6067, Loss: 4.436060905456543\n",
            "Training Iteration 6068, Loss: 2.855778455734253\n",
            "Training Iteration 6069, Loss: 4.822266101837158\n",
            "Training Iteration 6070, Loss: 4.668953895568848\n",
            "Training Iteration 6071, Loss: 5.29118013381958\n",
            "Training Iteration 6072, Loss: 5.163726329803467\n",
            "Training Iteration 6073, Loss: 3.467233896255493\n",
            "Training Iteration 6074, Loss: 3.5107316970825195\n",
            "Training Iteration 6075, Loss: 3.1288697719573975\n",
            "Training Iteration 6076, Loss: 3.887643337249756\n",
            "Training Iteration 6077, Loss: 2.048691511154175\n",
            "Training Iteration 6078, Loss: 1.4816200733184814\n",
            "Training Iteration 6079, Loss: 5.867037296295166\n",
            "Training Iteration 6080, Loss: 2.4464478492736816\n",
            "Training Iteration 6081, Loss: 4.9990129470825195\n",
            "Training Iteration 6082, Loss: 4.142801284790039\n",
            "Training Iteration 6083, Loss: 3.6245999336242676\n",
            "Training Iteration 6084, Loss: 5.76772928237915\n",
            "Training Iteration 6085, Loss: 4.574438095092773\n",
            "Training Iteration 6086, Loss: 10.222188949584961\n",
            "Training Iteration 6087, Loss: 7.379395961761475\n",
            "Training Iteration 6088, Loss: 5.696253299713135\n",
            "Training Iteration 6089, Loss: 2.060126781463623\n",
            "Training Iteration 6090, Loss: 6.090221405029297\n",
            "Training Iteration 6091, Loss: 2.813164710998535\n",
            "Training Iteration 6092, Loss: 4.438276290893555\n",
            "Training Iteration 6093, Loss: 2.8031060695648193\n",
            "Training Iteration 6094, Loss: 4.401838779449463\n",
            "Training Iteration 6095, Loss: 4.2046661376953125\n",
            "Training Iteration 6096, Loss: 3.905219793319702\n",
            "Training Iteration 6097, Loss: 4.057375907897949\n",
            "Training Iteration 6098, Loss: 5.724809169769287\n",
            "Training Iteration 6099, Loss: 7.111973285675049\n",
            "Training Iteration 6100, Loss: 4.84490442276001\n",
            "Training Iteration 6101, Loss: 4.305330276489258\n",
            "Training Iteration 6102, Loss: 3.4131546020507812\n",
            "Training Iteration 6103, Loss: 6.371358871459961\n",
            "Training Iteration 6104, Loss: 3.3704235553741455\n",
            "Training Iteration 6105, Loss: 3.937666177749634\n",
            "Training Iteration 6106, Loss: 4.0668535232543945\n",
            "Training Iteration 6107, Loss: 5.063443660736084\n",
            "Training Iteration 6108, Loss: 2.2594027519226074\n",
            "Training Iteration 6109, Loss: 3.1854748725891113\n",
            "Training Iteration 6110, Loss: 6.913376808166504\n",
            "Training Iteration 6111, Loss: 3.8612301349639893\n",
            "Training Iteration 6112, Loss: 3.037014961242676\n",
            "Training Iteration 6113, Loss: 4.343854904174805\n",
            "Training Iteration 6114, Loss: 3.963109016418457\n",
            "Training Iteration 6115, Loss: 4.458590507507324\n",
            "Training Iteration 6116, Loss: 4.135571479797363\n",
            "Training Iteration 6117, Loss: 3.846768617630005\n",
            "Training Iteration 6118, Loss: 5.5363616943359375\n",
            "Training Iteration 6119, Loss: 5.1551971435546875\n",
            "Training Iteration 6120, Loss: 4.247783660888672\n",
            "Training Iteration 6121, Loss: 2.202242374420166\n",
            "Training Iteration 6122, Loss: 4.36605167388916\n",
            "Training Iteration 6123, Loss: 4.500828266143799\n",
            "Training Iteration 6124, Loss: 4.665882110595703\n",
            "Training Iteration 6125, Loss: 3.534229278564453\n",
            "Training Iteration 6126, Loss: 3.31211519241333\n",
            "Training Iteration 6127, Loss: 4.238525390625\n",
            "Training Iteration 6128, Loss: 4.965435028076172\n",
            "Training Iteration 6129, Loss: 2.9408226013183594\n",
            "Training Iteration 6130, Loss: 3.893097162246704\n",
            "Training Iteration 6131, Loss: 8.686407089233398\n",
            "Training Iteration 6132, Loss: 3.1432437896728516\n",
            "Training Iteration 6133, Loss: 2.8579893112182617\n",
            "Training Iteration 6134, Loss: 1.6212424039840698\n",
            "Training Iteration 6135, Loss: 3.446474552154541\n",
            "Training Iteration 6136, Loss: 5.295985221862793\n",
            "Training Iteration 6137, Loss: 6.263637065887451\n",
            "Training Iteration 6138, Loss: 4.171198844909668\n",
            "Training Iteration 6139, Loss: 3.185472249984741\n",
            "Training Iteration 6140, Loss: 3.3131351470947266\n",
            "Training Iteration 6141, Loss: 1.7405112981796265\n",
            "Training Iteration 6142, Loss: 5.090906143188477\n",
            "Training Iteration 6143, Loss: 3.1578187942504883\n",
            "Training Iteration 6144, Loss: 2.6181132793426514\n",
            "Training Iteration 6145, Loss: 3.6137402057647705\n",
            "Training Iteration 6146, Loss: 6.042768955230713\n",
            "Training Iteration 6147, Loss: 3.4058144092559814\n",
            "Training Iteration 6148, Loss: 3.172165870666504\n",
            "Training Iteration 6149, Loss: 5.651691913604736\n",
            "Training Iteration 6150, Loss: 2.537814140319824\n",
            "Training Iteration 6151, Loss: 6.136953830718994\n",
            "Training Iteration 6152, Loss: 3.9013185501098633\n",
            "Training Iteration 6153, Loss: 7.679500102996826\n",
            "Training Iteration 6154, Loss: 6.706414222717285\n",
            "Training Iteration 6155, Loss: 2.5799293518066406\n",
            "Training Iteration 6156, Loss: 3.9556124210357666\n",
            "Training Iteration 6157, Loss: 3.8202199935913086\n",
            "Training Iteration 6158, Loss: 6.295254707336426\n",
            "Training Iteration 6159, Loss: 6.620520114898682\n",
            "Training Iteration 6160, Loss: 4.302622318267822\n",
            "Training Iteration 6161, Loss: 4.5523681640625\n",
            "Training Iteration 6162, Loss: 2.311580181121826\n",
            "Training Iteration 6163, Loss: 6.093691349029541\n",
            "Training Iteration 6164, Loss: 3.9224886894226074\n",
            "Training Iteration 6165, Loss: 3.1739683151245117\n",
            "Training Iteration 6166, Loss: 4.267396926879883\n",
            "Training Iteration 6167, Loss: 5.4080915451049805\n",
            "Training Iteration 6168, Loss: 5.08036470413208\n",
            "Training Iteration 6169, Loss: 4.316232681274414\n",
            "Training Iteration 6170, Loss: 6.088183879852295\n",
            "Training Iteration 6171, Loss: 3.71069598197937\n",
            "Training Iteration 6172, Loss: 2.4580130577087402\n",
            "Training Iteration 6173, Loss: 4.78741979598999\n",
            "Training Iteration 6174, Loss: 4.3422698974609375\n",
            "Training Iteration 6175, Loss: 2.6253790855407715\n",
            "Training Iteration 6176, Loss: 5.148913383483887\n",
            "Training Iteration 6177, Loss: 3.3892014026641846\n",
            "Training Iteration 6178, Loss: 4.9052252769470215\n",
            "Training Iteration 6179, Loss: 4.632765293121338\n",
            "Training Iteration 6180, Loss: 2.738069534301758\n",
            "Training Iteration 6181, Loss: 8.35404109954834\n",
            "Training Iteration 6182, Loss: 5.893048286437988\n",
            "Training Iteration 6183, Loss: 4.823392391204834\n",
            "Training Iteration 6184, Loss: 3.927525043487549\n",
            "Training Iteration 6185, Loss: 3.6516287326812744\n",
            "Training Iteration 6186, Loss: 5.810919284820557\n",
            "Training Iteration 6187, Loss: 3.4068219661712646\n",
            "Training Iteration 6188, Loss: 3.6937386989593506\n",
            "Training Iteration 6189, Loss: 4.982371807098389\n",
            "Training Iteration 6190, Loss: 3.0129752159118652\n",
            "Training Iteration 6191, Loss: 3.0636959075927734\n",
            "Training Iteration 6192, Loss: 4.003085136413574\n",
            "Training Iteration 6193, Loss: 2.89054799079895\n",
            "Training Iteration 6194, Loss: 2.6439287662506104\n",
            "Training Iteration 6195, Loss: 2.9273674488067627\n",
            "Training Iteration 6196, Loss: 3.5901520252227783\n",
            "Training Iteration 6197, Loss: 7.181295871734619\n",
            "Training Iteration 6198, Loss: 4.102240562438965\n",
            "Training Iteration 6199, Loss: 4.4883131980896\n",
            "Training Iteration 6200, Loss: 4.914921760559082\n",
            "Training Iteration 6201, Loss: 2.79803466796875\n",
            "Training Iteration 6202, Loss: 3.784996509552002\n",
            "Training Iteration 6203, Loss: 3.963202476501465\n",
            "Training Iteration 6204, Loss: 3.916821241378784\n",
            "Training Iteration 6205, Loss: 4.677107810974121\n",
            "Training Iteration 6206, Loss: 2.6000521183013916\n",
            "Training Iteration 6207, Loss: 1.6389014720916748\n",
            "Training Iteration 6208, Loss: 6.200166702270508\n",
            "Training Iteration 6209, Loss: 3.230252742767334\n",
            "Training Iteration 6210, Loss: 4.594021320343018\n",
            "Training Iteration 6211, Loss: 3.8112101554870605\n",
            "Training Iteration 6212, Loss: 4.073648929595947\n",
            "Training Iteration 6213, Loss: 2.6198973655700684\n",
            "Training Iteration 6214, Loss: 2.804216146469116\n",
            "Training Iteration 6215, Loss: 4.299842834472656\n",
            "Training Iteration 6216, Loss: 4.055209636688232\n",
            "Training Iteration 6217, Loss: 1.5035157203674316\n",
            "Training Iteration 6218, Loss: 5.763935089111328\n",
            "Training Iteration 6219, Loss: 8.180207252502441\n",
            "Training Iteration 6220, Loss: 3.510823965072632\n",
            "Training Iteration 6221, Loss: 4.643215656280518\n",
            "Training Iteration 6222, Loss: 7.154050827026367\n",
            "Training Iteration 6223, Loss: 5.776719570159912\n",
            "Training Iteration 6224, Loss: 4.18135404586792\n",
            "Training Iteration 6225, Loss: 4.302464962005615\n",
            "Training Iteration 6226, Loss: 7.602022647857666\n",
            "Training Iteration 6227, Loss: 5.658231735229492\n",
            "Training Iteration 6228, Loss: 4.951487064361572\n",
            "Training Iteration 6229, Loss: 5.25956392288208\n",
            "Training Iteration 6230, Loss: 4.537160396575928\n",
            "Training Iteration 6231, Loss: 4.041440010070801\n",
            "Training Iteration 6232, Loss: 2.9470338821411133\n",
            "Training Iteration 6233, Loss: 3.2604618072509766\n",
            "Training Iteration 6234, Loss: 4.523355960845947\n",
            "Training Iteration 6235, Loss: 4.237926006317139\n",
            "Training Iteration 6236, Loss: 6.547447681427002\n",
            "Training Iteration 6237, Loss: 0.934349775314331\n",
            "Training Iteration 6238, Loss: 4.448967933654785\n",
            "Training Iteration 6239, Loss: 5.834598064422607\n",
            "Training Iteration 6240, Loss: 5.2148637771606445\n",
            "Training Iteration 6241, Loss: 6.290247440338135\n",
            "Training Iteration 6242, Loss: 7.641012191772461\n",
            "Training Iteration 6243, Loss: 4.122755527496338\n",
            "Training Iteration 6244, Loss: 8.500621795654297\n",
            "Training Iteration 6245, Loss: 3.325465440750122\n",
            "Training Iteration 6246, Loss: 5.28286600112915\n",
            "Training Iteration 6247, Loss: 3.900691032409668\n",
            "Training Iteration 6248, Loss: 4.262912273406982\n",
            "Training Iteration 6249, Loss: 3.976470708847046\n",
            "Training Iteration 6250, Loss: 8.000875473022461\n",
            "Training Iteration 6251, Loss: 3.774029016494751\n",
            "Training Iteration 6252, Loss: 6.238324165344238\n",
            "Training Iteration 6253, Loss: 5.019658088684082\n",
            "Training Iteration 6254, Loss: 3.9499902725219727\n",
            "Training Iteration 6255, Loss: 5.748716354370117\n",
            "Training Iteration 6256, Loss: 5.612383842468262\n",
            "Training Iteration 6257, Loss: 5.687568187713623\n",
            "Training Iteration 6258, Loss: 2.9704182147979736\n",
            "Training Iteration 6259, Loss: 7.207722187042236\n",
            "Training Iteration 6260, Loss: 1.751829743385315\n",
            "Training Iteration 6261, Loss: 7.0086846351623535\n",
            "Training Iteration 6262, Loss: 5.29100227355957\n",
            "Training Iteration 6263, Loss: 5.778870105743408\n",
            "Training Iteration 6264, Loss: 4.744829177856445\n",
            "Training Iteration 6265, Loss: 5.214913368225098\n",
            "Training Iteration 6266, Loss: 4.849769592285156\n",
            "Training Iteration 6267, Loss: 4.277141094207764\n",
            "Training Iteration 6268, Loss: 5.381868839263916\n",
            "Training Iteration 6269, Loss: 4.282751083374023\n",
            "Training Iteration 6270, Loss: 6.686647891998291\n",
            "Training Iteration 6271, Loss: 10.106088638305664\n",
            "Training Iteration 6272, Loss: 7.399788856506348\n",
            "Training Iteration 6273, Loss: 6.4915971755981445\n",
            "Training Iteration 6274, Loss: 4.651155948638916\n",
            "Training Iteration 6275, Loss: 4.183853626251221\n",
            "Training Iteration 6276, Loss: 5.259698390960693\n",
            "Training Iteration 6277, Loss: 5.424614906311035\n",
            "Training Iteration 6278, Loss: 2.8851494789123535\n",
            "Training Iteration 6279, Loss: 2.145071029663086\n",
            "Training Iteration 6280, Loss: 4.289547920227051\n",
            "Training Iteration 6281, Loss: 2.759554862976074\n",
            "Training Iteration 6282, Loss: 3.228067636489868\n",
            "Training Iteration 6283, Loss: 6.773398399353027\n",
            "Training Iteration 6284, Loss: 2.512603998184204\n",
            "Training Iteration 6285, Loss: 5.507259368896484\n",
            "Training Iteration 6286, Loss: 4.731556415557861\n",
            "Training Iteration 6287, Loss: 3.888965606689453\n",
            "Training Iteration 6288, Loss: 4.547012805938721\n",
            "Training Iteration 6289, Loss: 4.343067646026611\n",
            "Training Iteration 6290, Loss: 2.6649158000946045\n",
            "Training Iteration 6291, Loss: 2.586038112640381\n",
            "Training Iteration 6292, Loss: 6.0921711921691895\n",
            "Training Iteration 6293, Loss: 2.0257177352905273\n",
            "Training Iteration 6294, Loss: 3.794623374938965\n",
            "Training Iteration 6295, Loss: 3.4351813793182373\n",
            "Training Iteration 6296, Loss: 5.081439018249512\n",
            "Training Iteration 6297, Loss: 0.9725874662399292\n",
            "Training Iteration 6298, Loss: 2.5797579288482666\n",
            "Training Iteration 6299, Loss: 3.5069127082824707\n",
            "Training Iteration 6300, Loss: 1.2360554933547974\n",
            "Training Iteration 6301, Loss: 7.711925029754639\n",
            "Training Iteration 6302, Loss: 7.299091339111328\n",
            "Training Iteration 6303, Loss: 2.8537707328796387\n",
            "Training Iteration 6304, Loss: 2.8467319011688232\n",
            "Training Iteration 6305, Loss: 7.197727680206299\n",
            "Training Iteration 6306, Loss: 7.138957977294922\n",
            "Training Iteration 6307, Loss: 4.719633102416992\n",
            "Training Iteration 6308, Loss: 3.222445249557495\n",
            "Training Iteration 6309, Loss: 3.9449386596679688\n",
            "Training Iteration 6310, Loss: 4.684876918792725\n",
            "Training Iteration 6311, Loss: 4.9249114990234375\n",
            "Training Iteration 6312, Loss: 5.186036586761475\n",
            "Training Iteration 6313, Loss: 4.659135818481445\n",
            "Training Iteration 6314, Loss: 2.819288730621338\n",
            "Training Iteration 6315, Loss: 3.43740177154541\n",
            "Training Iteration 6316, Loss: 6.0405778884887695\n",
            "Training Iteration 6317, Loss: 6.604205131530762\n",
            "Training Iteration 6318, Loss: 5.074549674987793\n",
            "Training Iteration 6319, Loss: 5.4160051345825195\n",
            "Training Iteration 6320, Loss: 4.772364616394043\n",
            "Training Iteration 6321, Loss: 4.929058074951172\n",
            "Training Iteration 6322, Loss: 0.7542076110839844\n",
            "Training Iteration 6323, Loss: 9.734448432922363\n",
            "Training Iteration 6324, Loss: 4.461785316467285\n",
            "Training Iteration 6325, Loss: 4.782552719116211\n",
            "Training Iteration 6326, Loss: 4.476422309875488\n",
            "Training Iteration 6327, Loss: 6.136873245239258\n",
            "Training Iteration 6328, Loss: 5.156487464904785\n",
            "Training Iteration 6329, Loss: 5.7863593101501465\n",
            "Training Iteration 6330, Loss: 1.7354623079299927\n",
            "Training Iteration 6331, Loss: 6.188320159912109\n",
            "Training Iteration 6332, Loss: 5.445193767547607\n",
            "Training Iteration 6333, Loss: 6.215667724609375\n",
            "Training Iteration 6334, Loss: 7.110739707946777\n",
            "Training Iteration 6335, Loss: 5.361058712005615\n",
            "Training Iteration 6336, Loss: 3.148573398590088\n",
            "Training Iteration 6337, Loss: 3.485569953918457\n",
            "Training Iteration 6338, Loss: 4.3762054443359375\n",
            "Training Iteration 6339, Loss: 5.402345180511475\n",
            "Training Iteration 6340, Loss: 4.705808639526367\n",
            "Training Iteration 6341, Loss: 3.880707025527954\n",
            "Training Iteration 6342, Loss: 5.799533367156982\n",
            "Training Iteration 6343, Loss: 2.7441210746765137\n",
            "Training Iteration 6344, Loss: 4.870161533355713\n",
            "Training Iteration 6345, Loss: 7.60333251953125\n",
            "Training Iteration 6346, Loss: 7.756383895874023\n",
            "Training Iteration 6347, Loss: 6.887928485870361\n",
            "Training Iteration 6348, Loss: 3.7626001834869385\n",
            "Training Iteration 6349, Loss: 6.168605327606201\n",
            "Training Iteration 6350, Loss: 4.485836029052734\n",
            "Training Iteration 6351, Loss: 3.499952554702759\n",
            "Training Iteration 6352, Loss: 2.686555862426758\n",
            "Training Iteration 6353, Loss: 6.926010608673096\n",
            "Training Iteration 6354, Loss: 2.864309787750244\n",
            "Training Iteration 6355, Loss: 8.299098014831543\n",
            "Training Iteration 6356, Loss: 5.543689727783203\n",
            "Training Iteration 6357, Loss: 3.3344149589538574\n",
            "Training Iteration 6358, Loss: 4.425151824951172\n",
            "Training Iteration 6359, Loss: 3.507746696472168\n",
            "Training Iteration 6360, Loss: 2.426844835281372\n",
            "Training Iteration 6361, Loss: 4.100607872009277\n",
            "Training Iteration 6362, Loss: 2.9551401138305664\n",
            "Training Iteration 6363, Loss: 2.755051374435425\n",
            "Training Iteration 6364, Loss: 5.698132038116455\n",
            "Training Iteration 6365, Loss: 4.833611965179443\n",
            "Training Iteration 6366, Loss: 5.346968650817871\n",
            "Training Iteration 6367, Loss: 4.630880832672119\n",
            "Training Iteration 6368, Loss: 4.172958850860596\n",
            "Training Iteration 6369, Loss: 2.9804117679595947\n",
            "Training Iteration 6370, Loss: 5.535493850708008\n",
            "Training Iteration 6371, Loss: 3.4999027252197266\n",
            "Training Iteration 6372, Loss: 4.2248029708862305\n",
            "Training Iteration 6373, Loss: 3.671373128890991\n",
            "Training Iteration 6374, Loss: 4.041481018066406\n",
            "Training Iteration 6375, Loss: 4.169959545135498\n",
            "Training Iteration 6376, Loss: 1.6487964391708374\n",
            "Training Iteration 6377, Loss: 6.069815635681152\n",
            "Training Iteration 6378, Loss: 5.389066696166992\n",
            "Training Iteration 6379, Loss: 1.945101022720337\n",
            "Training Iteration 6380, Loss: 3.6805636882781982\n",
            "Training Iteration 6381, Loss: 5.017580509185791\n",
            "Training Iteration 6382, Loss: 4.514002323150635\n",
            "Training Iteration 6383, Loss: 5.826504230499268\n",
            "Training Iteration 6384, Loss: 3.5456159114837646\n",
            "Training Iteration 6385, Loss: 2.6288700103759766\n",
            "Training Iteration 6386, Loss: 2.6089797019958496\n",
            "Training Iteration 6387, Loss: 4.372851848602295\n",
            "Training Iteration 6388, Loss: 2.107983112335205\n",
            "Training Iteration 6389, Loss: 3.884042501449585\n",
            "Training Iteration 6390, Loss: 5.324606418609619\n",
            "Training Iteration 6391, Loss: 3.5802953243255615\n",
            "Training Iteration 6392, Loss: 4.387894153594971\n",
            "Training Iteration 6393, Loss: 4.365289688110352\n",
            "Training Iteration 6394, Loss: 7.525691509246826\n",
            "Training Iteration 6395, Loss: 4.346339225769043\n",
            "Training Iteration 6396, Loss: 5.013301849365234\n",
            "Training Iteration 6397, Loss: 6.394443511962891\n",
            "Training Iteration 6398, Loss: 4.038609504699707\n",
            "Training Iteration 6399, Loss: 8.703658103942871\n",
            "Training Iteration 6400, Loss: 4.840426921844482\n",
            "Training Iteration 6401, Loss: 4.811625003814697\n",
            "Training Iteration 6402, Loss: 4.681330680847168\n",
            "Training Iteration 6403, Loss: 4.154952526092529\n",
            "Training Iteration 6404, Loss: 3.404311180114746\n",
            "Training Iteration 6405, Loss: 4.1667656898498535\n",
            "Training Iteration 6406, Loss: 3.7241315841674805\n",
            "Training Iteration 6407, Loss: 4.157569885253906\n",
            "Training Iteration 6408, Loss: 5.364558219909668\n",
            "Training Iteration 6409, Loss: 3.7445874214172363\n",
            "Training Iteration 6410, Loss: 3.6438212394714355\n",
            "Training Iteration 6411, Loss: 2.334197521209717\n",
            "Training Iteration 6412, Loss: 5.681838512420654\n",
            "Training Iteration 6413, Loss: 5.521190643310547\n",
            "Training Iteration 6414, Loss: 5.08029317855835\n",
            "Training Iteration 6415, Loss: 8.891973495483398\n",
            "Training Iteration 6416, Loss: 7.305294990539551\n",
            "Training Iteration 6417, Loss: 5.870882987976074\n",
            "Training Iteration 6418, Loss: 3.528644561767578\n",
            "Training Iteration 6419, Loss: 3.7517688274383545\n",
            "Training Iteration 6420, Loss: 3.525794506072998\n",
            "Training Iteration 6421, Loss: 3.9088685512542725\n",
            "Training Iteration 6422, Loss: 2.2492237091064453\n",
            "Training Iteration 6423, Loss: 3.9137156009674072\n",
            "Training Iteration 6424, Loss: 6.370984077453613\n",
            "Training Iteration 6425, Loss: 2.4990005493164062\n",
            "Training Iteration 6426, Loss: 4.637609481811523\n",
            "Training Iteration 6427, Loss: 7.036444187164307\n",
            "Training Iteration 6428, Loss: 4.706150531768799\n",
            "Training Iteration 6429, Loss: 4.048230171203613\n",
            "Training Iteration 6430, Loss: 1.9855495691299438\n",
            "Training Iteration 6431, Loss: 3.8393731117248535\n",
            "Training Iteration 6432, Loss: 3.4715847969055176\n",
            "Training Iteration 6433, Loss: 5.615618705749512\n",
            "Training Iteration 6434, Loss: 2.6133368015289307\n",
            "Training Iteration 6435, Loss: 3.997783660888672\n",
            "Training Iteration 6436, Loss: 3.5204670429229736\n",
            "Training Iteration 6437, Loss: 3.455883026123047\n",
            "Training Iteration 6438, Loss: 8.172711372375488\n",
            "Training Iteration 6439, Loss: 4.502137660980225\n",
            "Training Iteration 6440, Loss: 2.7692506313323975\n",
            "Training Iteration 6441, Loss: 3.937838077545166\n",
            "Training Iteration 6442, Loss: 2.342592239379883\n",
            "Training Iteration 6443, Loss: 4.616600036621094\n",
            "Training Iteration 6444, Loss: 6.598849296569824\n",
            "Training Iteration 6445, Loss: 5.030991077423096\n",
            "Training Iteration 6446, Loss: 4.627211570739746\n",
            "Training Iteration 6447, Loss: 4.307695388793945\n",
            "Training Iteration 6448, Loss: 4.740584850311279\n",
            "Training Iteration 6449, Loss: 2.1995863914489746\n",
            "Training Iteration 6450, Loss: 5.626702308654785\n",
            "Training Iteration 6451, Loss: 3.0598812103271484\n",
            "Training Iteration 6452, Loss: 5.5525312423706055\n",
            "Training Iteration 6453, Loss: 3.879638671875\n",
            "Training Iteration 6454, Loss: 2.4691600799560547\n",
            "Training Iteration 6455, Loss: 3.172734498977661\n",
            "Training Iteration 6456, Loss: 2.6724798679351807\n",
            "Training Iteration 6457, Loss: 4.021200656890869\n",
            "Training Iteration 6458, Loss: 2.3779006004333496\n",
            "Training Iteration 6459, Loss: 5.776578903198242\n",
            "Training Iteration 6460, Loss: 6.273669242858887\n",
            "Training Iteration 6461, Loss: 3.7435126304626465\n",
            "Training Iteration 6462, Loss: 5.56364107131958\n",
            "Training Iteration 6463, Loss: 4.81989049911499\n",
            "Training Iteration 6464, Loss: 4.674354076385498\n",
            "Training Iteration 6465, Loss: 3.2070648670196533\n",
            "Training Iteration 6466, Loss: 3.581427574157715\n",
            "Training Iteration 6467, Loss: 6.7477030754089355\n",
            "Training Iteration 6468, Loss: 5.494467258453369\n",
            "Training Iteration 6469, Loss: 5.94438362121582\n",
            "Training Iteration 6470, Loss: 3.9886701107025146\n",
            "Training Iteration 6471, Loss: 4.9186506271362305\n",
            "Training Iteration 6472, Loss: 3.242405891418457\n",
            "Training Iteration 6473, Loss: 4.277890682220459\n",
            "Training Iteration 6474, Loss: 4.994850158691406\n",
            "Training Iteration 6475, Loss: 5.101797580718994\n",
            "Training Iteration 6476, Loss: 6.1034650802612305\n",
            "Training Iteration 6477, Loss: 5.967837333679199\n",
            "Training Iteration 6478, Loss: 2.3945133686065674\n",
            "Training Iteration 6479, Loss: 3.940247058868408\n",
            "Training Iteration 6480, Loss: 3.691282033920288\n",
            "Training Iteration 6481, Loss: 2.8493096828460693\n",
            "Training Iteration 6482, Loss: 5.037956714630127\n",
            "Training Iteration 6483, Loss: 5.066110134124756\n",
            "Training Iteration 6484, Loss: 4.317721366882324\n",
            "Training Iteration 6485, Loss: 2.559401512145996\n",
            "Training Iteration 6486, Loss: 2.1346421241760254\n",
            "Training Iteration 6487, Loss: 5.069920539855957\n",
            "Training Iteration 6488, Loss: 4.279911518096924\n",
            "Training Iteration 6489, Loss: 3.7905051708221436\n",
            "Training Iteration 6490, Loss: 4.249484539031982\n",
            "Training Iteration 6491, Loss: 2.611706495285034\n",
            "Training Iteration 6492, Loss: 4.431661128997803\n",
            "Training Iteration 6493, Loss: 2.952683448791504\n",
            "Training Iteration 6494, Loss: 4.969574451446533\n",
            "Training Iteration 6495, Loss: 5.525116443634033\n",
            "Training Iteration 6496, Loss: 4.601953506469727\n",
            "Training Iteration 6497, Loss: 7.039319038391113\n",
            "Training Iteration 6498, Loss: 4.117284774780273\n",
            "Training Iteration 6499, Loss: 3.7496895790100098\n",
            "Training Iteration 6500, Loss: 4.737558841705322\n",
            "Training Iteration 6501, Loss: 5.065691947937012\n",
            "Training Iteration 6502, Loss: 6.345602035522461\n",
            "Training Iteration 6503, Loss: 3.1048736572265625\n",
            "Training Iteration 6504, Loss: 3.8857452869415283\n",
            "Training Iteration 6505, Loss: 4.064886093139648\n",
            "Training Iteration 6506, Loss: 6.550328254699707\n",
            "Training Iteration 6507, Loss: 5.739989280700684\n",
            "Training Iteration 6508, Loss: 3.9076943397521973\n",
            "Training Iteration 6509, Loss: 5.523672580718994\n",
            "Training Iteration 6510, Loss: 3.7612669467926025\n",
            "Training Iteration 6511, Loss: 5.692566871643066\n",
            "Training Iteration 6512, Loss: 3.2078325748443604\n",
            "Training Iteration 6513, Loss: 5.272222518920898\n",
            "Training Iteration 6514, Loss: 5.398979187011719\n",
            "Training Iteration 6515, Loss: 2.7813074588775635\n",
            "Training Iteration 6516, Loss: 4.418217658996582\n",
            "Training Iteration 6517, Loss: 5.707487106323242\n",
            "Training Iteration 6518, Loss: 11.448304176330566\n",
            "Training Iteration 6519, Loss: 9.672718048095703\n",
            "Training Iteration 6520, Loss: 8.88625431060791\n",
            "Training Iteration 6521, Loss: 2.5838022232055664\n",
            "Training Iteration 6522, Loss: 2.7168667316436768\n",
            "Training Iteration 6523, Loss: 4.574643611907959\n",
            "Training Iteration 6524, Loss: 5.251627445220947\n",
            "Training Iteration 6525, Loss: 6.36433744430542\n",
            "Training Iteration 6526, Loss: 4.262540817260742\n",
            "Training Iteration 6527, Loss: 3.7372963428497314\n",
            "Training Iteration 6528, Loss: 5.319629669189453\n",
            "Training Iteration 6529, Loss: 6.572787284851074\n",
            "Training Iteration 6530, Loss: 6.345810890197754\n",
            "Training Iteration 6531, Loss: 4.162937164306641\n",
            "Training Iteration 6532, Loss: 8.59317398071289\n",
            "Training Iteration 6533, Loss: 3.428715944290161\n",
            "Training Iteration 6534, Loss: 2.332340717315674\n",
            "Training Iteration 6535, Loss: 4.581082344055176\n",
            "Training Iteration 6536, Loss: 7.79555082321167\n",
            "Training Iteration 6537, Loss: 7.136369705200195\n",
            "Training Iteration 6538, Loss: 5.83125114440918\n",
            "Training Iteration 6539, Loss: 5.12705659866333\n",
            "Training Iteration 6540, Loss: 2.6927835941314697\n",
            "Training Iteration 6541, Loss: 2.846881866455078\n",
            "Training Iteration 6542, Loss: 2.503556251525879\n",
            "Training Iteration 6543, Loss: 6.639766216278076\n",
            "Training Iteration 6544, Loss: 1.1818933486938477\n",
            "Training Iteration 6545, Loss: 1.7677452564239502\n",
            "Training Iteration 6546, Loss: 1.9543818235397339\n",
            "Training Iteration 6547, Loss: 3.6699609756469727\n",
            "Training Iteration 6548, Loss: 3.857509136199951\n",
            "Training Iteration 6549, Loss: 3.8026859760284424\n",
            "Training Iteration 6550, Loss: 1.475090742111206\n",
            "Training Iteration 6551, Loss: 4.402162075042725\n",
            "Training Iteration 6552, Loss: 5.467841625213623\n",
            "Training Iteration 6553, Loss: 3.7825403213500977\n",
            "Training Iteration 6554, Loss: 3.1000349521636963\n",
            "tensor([[8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        ...,\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04]])\n",
            "Training loss for epcoh 3: 2.9125986485118216\n",
            "Training accuracy for epoch 3: 0.30667989165681153\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        ...,\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04],\n",
            "        [8.2702e-01, 1.6714e-02, 4.2490e-02, 1.6483e-03, 1.1201e-01, 1.2178e-04]])\n",
            "Validation loss for epcoh 3: 2.912689926840315\n",
            "Test accuracy for epoch 3: 0.3103685053788052\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53375d19fa8c428cb67647444fa062c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch No: 4:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Iteration 1, Loss: 2.736755847930908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 4.325657367706299\n",
            "Training Iteration 1565, Loss: 3.7345287799835205\n",
            "Training Iteration 1566, Loss: 3.738718271255493\n",
            "Training Iteration 1567, Loss: 4.82472562789917\n",
            "Training Iteration 1568, Loss: 5.795790672302246\n",
            "Training Iteration 1569, Loss: 6.878973007202148\n",
            "Training Iteration 1570, Loss: 3.9092838764190674\n",
            "Training Iteration 1571, Loss: 3.250180244445801\n",
            "Training Iteration 1572, Loss: 3.8108410835266113\n",
            "Training Iteration 1573, Loss: 4.229984283447266\n",
            "Training Iteration 1574, Loss: 7.807801246643066\n",
            "Training Iteration 1575, Loss: 2.821890354156494\n",
            "Training Iteration 1576, Loss: 4.468331336975098\n",
            "Training Iteration 1577, Loss: 5.404160499572754\n",
            "Training Iteration 1578, Loss: 8.247955322265625\n",
            "Training Iteration 1579, Loss: 6.38871431350708\n",
            "Training Iteration 1580, Loss: 1.7977235317230225\n",
            "Training Iteration 1581, Loss: 5.338288307189941\n",
            "Training Iteration 1582, Loss: 5.2716522216796875\n",
            "Training Iteration 1583, Loss: 4.839561462402344\n",
            "Training Iteration 1584, Loss: 3.3557486534118652\n",
            "Training Iteration 1585, Loss: 5.384050369262695\n",
            "Training Iteration 1586, Loss: 3.857574462890625\n",
            "Training Iteration 1587, Loss: 3.9602603912353516\n",
            "Training Iteration 1588, Loss: 3.0245203971862793\n",
            "Training Iteration 1589, Loss: 6.7678351402282715\n",
            "Training Iteration 1590, Loss: 5.3742828369140625\n",
            "Training Iteration 1591, Loss: 5.349541187286377\n",
            "Training Iteration 1592, Loss: 2.0292341709136963\n",
            "Training Iteration 1593, Loss: 4.781623840332031\n",
            "Training Iteration 1594, Loss: 3.210533618927002\n",
            "Training Iteration 1595, Loss: 4.055277347564697\n",
            "Training Iteration 1596, Loss: 4.343243598937988\n",
            "Training Iteration 1597, Loss: 4.7560625076293945\n",
            "Training Iteration 1598, Loss: 4.631206512451172\n",
            "Training Iteration 1599, Loss: 3.6807360649108887\n",
            "Training Iteration 1600, Loss: 5.644444465637207\n",
            "Training Iteration 1601, Loss: 6.136214256286621\n",
            "Training Iteration 1602, Loss: 7.531741142272949\n",
            "Training Iteration 1603, Loss: 4.868008613586426\n",
            "Training Iteration 1604, Loss: 6.59603214263916\n",
            "Training Iteration 1605, Loss: 1.5061769485473633\n",
            "Training Iteration 1606, Loss: 4.407234191894531\n",
            "Training Iteration 1607, Loss: 6.889887809753418\n",
            "Training Iteration 1608, Loss: 6.587489128112793\n",
            "Training Iteration 1609, Loss: 4.796723365783691\n",
            "Training Iteration 1610, Loss: 1.7606114149093628\n",
            "Training Iteration 1611, Loss: 5.83380126953125\n",
            "Training Iteration 1612, Loss: 1.854824423789978\n",
            "Training Iteration 1613, Loss: 6.571903228759766\n",
            "Training Iteration 1614, Loss: 5.164794445037842\n",
            "Training Iteration 1615, Loss: 4.467710494995117\n",
            "Training Iteration 1616, Loss: 2.7564849853515625\n",
            "Training Iteration 1617, Loss: 5.97998571395874\n",
            "Training Iteration 1618, Loss: 5.0488972663879395\n",
            "Training Iteration 1619, Loss: 3.088479518890381\n",
            "Training Iteration 1620, Loss: 3.612865924835205\n",
            "Training Iteration 1621, Loss: 5.155249118804932\n",
            "Training Iteration 1622, Loss: 5.755558013916016\n",
            "Training Iteration 1623, Loss: 3.553622245788574\n",
            "Training Iteration 1624, Loss: 7.560562610626221\n",
            "Training Iteration 1625, Loss: 4.849556922912598\n",
            "Training Iteration 1626, Loss: 4.0453596115112305\n",
            "Training Iteration 1627, Loss: 6.613770484924316\n",
            "Training Iteration 1628, Loss: 10.297114372253418\n",
            "Training Iteration 1629, Loss: 6.614019870758057\n",
            "Training Iteration 1630, Loss: 6.815049171447754\n",
            "Training Iteration 1631, Loss: 6.064900875091553\n",
            "Training Iteration 1632, Loss: 6.7846808433532715\n",
            "Training Iteration 1633, Loss: 5.3636794090271\n",
            "Training Iteration 1634, Loss: 3.4200544357299805\n",
            "Training Iteration 1635, Loss: 2.427809000015259\n",
            "Training Iteration 1636, Loss: 6.305105209350586\n",
            "Training Iteration 1637, Loss: 4.30885648727417\n",
            "Training Iteration 1638, Loss: 4.27109432220459\n",
            "Training Iteration 1639, Loss: 11.493037223815918\n",
            "Training Iteration 1640, Loss: 4.161153316497803\n",
            "Training Iteration 1641, Loss: 5.9705491065979\n",
            "Training Iteration 1642, Loss: 3.3336188793182373\n",
            "Training Iteration 1643, Loss: 9.59260368347168\n",
            "Training Iteration 1644, Loss: 2.6269164085388184\n",
            "Training Iteration 1645, Loss: 6.5362749099731445\n",
            "Training Iteration 1646, Loss: 4.736816883087158\n",
            "Training Iteration 1647, Loss: 5.591076374053955\n",
            "Training Iteration 1648, Loss: 6.06422758102417\n",
            "Training Iteration 1649, Loss: 4.814544677734375\n",
            "Training Iteration 1650, Loss: 4.70328950881958\n",
            "Training Iteration 1651, Loss: 3.5703372955322266\n",
            "Training Iteration 1652, Loss: 4.799243450164795\n",
            "Training Iteration 1653, Loss: 6.1640496253967285\n",
            "Training Iteration 1654, Loss: 3.859347105026245\n",
            "Training Iteration 1655, Loss: 4.494352340698242\n",
            "Training Iteration 1656, Loss: 3.4179089069366455\n",
            "Training Iteration 1657, Loss: 6.366891860961914\n",
            "Training Iteration 1658, Loss: 3.1970300674438477\n",
            "Training Iteration 1659, Loss: 6.294723033905029\n",
            "Training Iteration 1660, Loss: 5.618600368499756\n",
            "Training Iteration 1661, Loss: 2.6450397968292236\n",
            "Training Iteration 1662, Loss: 5.653383255004883\n",
            "Training Iteration 1663, Loss: 1.731550931930542\n",
            "Training Iteration 1664, Loss: 3.9366824626922607\n",
            "Training Iteration 1665, Loss: 9.232637405395508\n",
            "Training Iteration 1666, Loss: 3.6929214000701904\n",
            "Training Iteration 1667, Loss: 4.965667724609375\n",
            "Training Iteration 1668, Loss: 7.6967973709106445\n",
            "Training Iteration 1669, Loss: 6.401473045349121\n",
            "Training Iteration 1670, Loss: 7.146849632263184\n",
            "Training Iteration 1671, Loss: 5.126910209655762\n",
            "Training Iteration 1672, Loss: 6.992423057556152\n",
            "Training Iteration 1673, Loss: 5.821665287017822\n",
            "Training Iteration 1674, Loss: 5.719953536987305\n",
            "Training Iteration 1675, Loss: 4.98974084854126\n",
            "Training Iteration 1676, Loss: 3.3938138484954834\n",
            "Training Iteration 1677, Loss: 5.1665263175964355\n",
            "Training Iteration 1678, Loss: 5.109321117401123\n",
            "Training Iteration 1679, Loss: 5.635470390319824\n",
            "Training Iteration 1680, Loss: 3.515834093093872\n",
            "Training Iteration 1681, Loss: 5.08115816116333\n",
            "Training Iteration 1682, Loss: 5.100337505340576\n",
            "Training Iteration 1683, Loss: 2.4996280670166016\n",
            "Training Iteration 1684, Loss: 3.0725743770599365\n",
            "Training Iteration 1685, Loss: 2.379699230194092\n",
            "Training Iteration 1686, Loss: 4.408299446105957\n",
            "Training Iteration 1687, Loss: 3.0868420600891113\n",
            "Training Iteration 1688, Loss: 2.879154682159424\n",
            "Training Iteration 1689, Loss: 6.239531517028809\n",
            "Training Iteration 1690, Loss: 5.297121524810791\n",
            "Training Iteration 1691, Loss: 5.550937652587891\n",
            "Training Iteration 1692, Loss: 2.791768789291382\n",
            "Training Iteration 1693, Loss: 5.014740467071533\n",
            "Training Iteration 1694, Loss: 8.174686431884766\n",
            "Training Iteration 1695, Loss: 5.369798183441162\n",
            "Training Iteration 1696, Loss: 5.930112838745117\n",
            "Training Iteration 1697, Loss: 3.908078908920288\n",
            "Training Iteration 1698, Loss: 3.0503249168395996\n",
            "Training Iteration 1699, Loss: 4.158172130584717\n",
            "Training Iteration 1700, Loss: 2.9749233722686768\n",
            "Training Iteration 1701, Loss: 5.374803066253662\n",
            "Training Iteration 1702, Loss: 3.7790029048919678\n",
            "Training Iteration 1703, Loss: 6.095295429229736\n",
            "Training Iteration 1704, Loss: 4.827263355255127\n",
            "Training Iteration 1705, Loss: 3.5189619064331055\n",
            "Training Iteration 1706, Loss: 5.3731255531311035\n",
            "Training Iteration 1707, Loss: 5.617210388183594\n",
            "Training Iteration 1708, Loss: 3.0615439414978027\n",
            "Training Iteration 1709, Loss: 4.112088203430176\n",
            "Training Iteration 1710, Loss: 4.755636692047119\n",
            "Training Iteration 1711, Loss: 3.283653736114502\n",
            "Training Iteration 1712, Loss: 5.546567916870117\n",
            "Training Iteration 1713, Loss: 3.028317451477051\n",
            "Training Iteration 1714, Loss: 6.68068790435791\n",
            "Training Iteration 1715, Loss: 3.9525349140167236\n",
            "Training Iteration 1716, Loss: 4.01344108581543\n",
            "Training Iteration 1717, Loss: 6.44875431060791\n",
            "Training Iteration 1718, Loss: 3.0901806354522705\n",
            "Training Iteration 1719, Loss: 4.999167442321777\n",
            "Training Iteration 1720, Loss: 3.6655325889587402\n",
            "Training Iteration 1721, Loss: 4.968044281005859\n",
            "Training Iteration 1722, Loss: 4.2993574142456055\n",
            "Training Iteration 1723, Loss: 4.86503791809082\n",
            "Training Iteration 1724, Loss: 5.602096080780029\n",
            "Training Iteration 1725, Loss: 5.939267158508301\n",
            "Training Iteration 1726, Loss: 2.076094627380371\n",
            "Training Iteration 1727, Loss: 7.7771806716918945\n",
            "Training Iteration 1728, Loss: 5.752453804016113\n",
            "Training Iteration 1729, Loss: 3.6614394187927246\n",
            "Training Iteration 1730, Loss: 3.1454355716705322\n",
            "Training Iteration 1731, Loss: 4.139321327209473\n",
            "Training Iteration 1732, Loss: 3.5774049758911133\n",
            "Training Iteration 1733, Loss: 5.545008182525635\n",
            "Training Iteration 1734, Loss: 6.1320648193359375\n",
            "Training Iteration 1735, Loss: 4.652659893035889\n",
            "Training Iteration 1736, Loss: 4.532083511352539\n",
            "Training Iteration 1737, Loss: 4.257809162139893\n",
            "Training Iteration 1738, Loss: 3.907705545425415\n",
            "Training Iteration 1739, Loss: 3.7993552684783936\n",
            "Training Iteration 1740, Loss: 4.737392425537109\n",
            "Training Iteration 1741, Loss: 5.449166297912598\n",
            "Training Iteration 1742, Loss: 4.000631809234619\n",
            "Training Iteration 1743, Loss: 2.9253458976745605\n",
            "Training Iteration 1744, Loss: 1.6039246320724487\n",
            "Training Iteration 1745, Loss: 3.699470043182373\n",
            "Training Iteration 1746, Loss: 6.50319242477417\n",
            "Training Iteration 1747, Loss: 4.559544563293457\n",
            "Training Iteration 1748, Loss: 6.13404655456543\n",
            "Training Iteration 1749, Loss: 1.745549201965332\n",
            "Training Iteration 1750, Loss: 4.407981872558594\n",
            "Training Iteration 1751, Loss: 5.382763862609863\n",
            "Training Iteration 1752, Loss: 4.78120231628418\n",
            "Training Iteration 1753, Loss: 5.547811985015869\n",
            "Training Iteration 1754, Loss: 4.096961975097656\n",
            "Training Iteration 1755, Loss: 2.755995512008667\n",
            "Training Iteration 1756, Loss: 2.870821714401245\n",
            "Training Iteration 1757, Loss: 1.619903564453125\n",
            "Training Iteration 1758, Loss: 3.6966123580932617\n",
            "Training Iteration 1759, Loss: 4.186422348022461\n",
            "Training Iteration 1760, Loss: 4.963676452636719\n",
            "Training Iteration 1761, Loss: 3.410770893096924\n",
            "Training Iteration 1762, Loss: 3.451528787612915\n",
            "Training Iteration 1763, Loss: 3.130831241607666\n",
            "Training Iteration 1764, Loss: 2.3470964431762695\n",
            "Training Iteration 1765, Loss: 8.861238479614258\n",
            "Training Iteration 1766, Loss: 2.0124623775482178\n",
            "Training Iteration 1767, Loss: 1.440666675567627\n",
            "Training Iteration 1768, Loss: 2.546027183532715\n",
            "Training Iteration 1769, Loss: 3.8414788246154785\n",
            "Training Iteration 1770, Loss: 3.2071352005004883\n",
            "Training Iteration 1771, Loss: 4.282484531402588\n",
            "Training Iteration 1772, Loss: 1.910920262336731\n",
            "Training Iteration 1773, Loss: 3.0328893661499023\n",
            "Training Iteration 1774, Loss: 3.6626951694488525\n",
            "Training Iteration 1775, Loss: 3.2386152744293213\n",
            "Training Iteration 1776, Loss: 5.082614898681641\n",
            "Training Iteration 1777, Loss: 4.192584991455078\n",
            "Training Iteration 1778, Loss: 5.7086992263793945\n",
            "Training Iteration 1779, Loss: 4.42707633972168\n",
            "Training Iteration 1780, Loss: 3.502392530441284\n",
            "Training Iteration 1781, Loss: 5.766075611114502\n",
            "Training Iteration 1782, Loss: 2.641334056854248\n",
            "Training Iteration 1783, Loss: 3.231498956680298\n",
            "Training Iteration 1784, Loss: 2.9457387924194336\n",
            "Training Iteration 1785, Loss: 2.780313491821289\n",
            "Training Iteration 1786, Loss: 3.7777259349823\n",
            "Training Iteration 1787, Loss: 3.90132999420166\n",
            "Training Iteration 1788, Loss: 2.208129644393921\n",
            "Training Iteration 1789, Loss: 4.355959415435791\n",
            "Training Iteration 1790, Loss: 5.299057483673096\n",
            "Training Iteration 1791, Loss: 7.643101692199707\n",
            "Training Iteration 1792, Loss: 7.272747993469238\n",
            "Training Iteration 1793, Loss: 3.5912320613861084\n",
            "Training Iteration 1794, Loss: 5.1620612144470215\n",
            "Training Iteration 1795, Loss: 6.103261947631836\n",
            "Training Iteration 1796, Loss: 4.794387340545654\n",
            "Training Iteration 1797, Loss: 5.408550262451172\n",
            "Training Iteration 1798, Loss: 3.526899814605713\n",
            "Training Iteration 1799, Loss: 4.3342204093933105\n",
            "Training Iteration 1800, Loss: 3.199432373046875\n",
            "Training Iteration 1801, Loss: 6.323333263397217\n",
            "Training Iteration 1802, Loss: 6.891852378845215\n",
            "Training Iteration 1803, Loss: 3.960176944732666\n",
            "Training Iteration 1804, Loss: 6.750715255737305\n",
            "Training Iteration 1805, Loss: 8.72746753692627\n",
            "Training Iteration 1806, Loss: 5.2509660720825195\n",
            "Training Iteration 1807, Loss: 5.92496919631958\n",
            "Training Iteration 1808, Loss: 8.231111526489258\n",
            "Training Iteration 1809, Loss: 3.4652538299560547\n",
            "Training Iteration 1810, Loss: 4.884122371673584\n",
            "Training Iteration 1811, Loss: 3.5961852073669434\n",
            "Training Iteration 1812, Loss: 7.392016410827637\n",
            "Training Iteration 1813, Loss: 4.8042826652526855\n",
            "Training Iteration 1814, Loss: 4.26332426071167\n",
            "Training Iteration 1815, Loss: 7.2046098709106445\n",
            "Training Iteration 1816, Loss: 4.1755900382995605\n",
            "Training Iteration 1817, Loss: 6.396054744720459\n",
            "Training Iteration 1818, Loss: 4.201432228088379\n",
            "Training Iteration 1819, Loss: 5.153052806854248\n",
            "Training Iteration 1820, Loss: 5.219592094421387\n",
            "Training Iteration 1821, Loss: 3.5830018520355225\n",
            "Training Iteration 1822, Loss: 5.494924545288086\n",
            "Training Iteration 1823, Loss: 6.261475563049316\n",
            "Training Iteration 1824, Loss: 5.156151294708252\n",
            "Training Iteration 1825, Loss: 6.701939582824707\n",
            "Training Iteration 1826, Loss: 5.27933931350708\n",
            "Training Iteration 1827, Loss: 7.291312217712402\n",
            "Training Iteration 1828, Loss: 6.084855556488037\n",
            "Training Iteration 1829, Loss: 5.0491509437561035\n",
            "Training Iteration 1830, Loss: 3.4418797492980957\n",
            "Training Iteration 1831, Loss: 5.830397129058838\n",
            "Training Iteration 1832, Loss: 4.368147373199463\n",
            "Training Iteration 1833, Loss: 5.872996807098389\n",
            "Training Iteration 1834, Loss: 4.104681968688965\n",
            "Training Iteration 1835, Loss: 4.203852653503418\n",
            "Training Iteration 1836, Loss: 3.305044412612915\n",
            "Training Iteration 1837, Loss: 3.056946039199829\n",
            "Training Iteration 1838, Loss: 3.1559865474700928\n",
            "Training Iteration 1839, Loss: 6.3460588455200195\n",
            "Training Iteration 1840, Loss: 3.062216281890869\n",
            "Training Iteration 1841, Loss: 7.805293560028076\n",
            "Training Iteration 1842, Loss: 6.828207492828369\n",
            "Training Iteration 1843, Loss: 3.4741673469543457\n",
            "Training Iteration 1844, Loss: 7.301290512084961\n",
            "Training Iteration 1845, Loss: 4.122894763946533\n",
            "Training Iteration 1846, Loss: 4.476828098297119\n",
            "Training Iteration 1847, Loss: 4.082530498504639\n",
            "Training Iteration 1848, Loss: 1.2871873378753662\n",
            "Training Iteration 1849, Loss: 3.8215980529785156\n",
            "Training Iteration 1850, Loss: 4.7928080558776855\n",
            "Training Iteration 1851, Loss: 2.8467845916748047\n",
            "Training Iteration 1852, Loss: 5.017375946044922\n",
            "Training Iteration 1853, Loss: 4.31873083114624\n",
            "Training Iteration 1854, Loss: 3.4553966522216797\n",
            "Training Iteration 1855, Loss: 3.7290308475494385\n",
            "Training Iteration 1856, Loss: 3.4470458030700684\n",
            "Training Iteration 1857, Loss: 3.2758255004882812\n",
            "Training Iteration 1858, Loss: 2.9470102787017822\n",
            "Training Iteration 1859, Loss: 6.295808792114258\n",
            "Training Iteration 1860, Loss: 6.175180435180664\n",
            "Training Iteration 1861, Loss: 4.132392883300781\n",
            "Training Iteration 1862, Loss: 3.92146372795105\n",
            "Training Iteration 1863, Loss: 4.748425483703613\n",
            "Training Iteration 1864, Loss: 1.6328868865966797\n",
            "Training Iteration 1865, Loss: 5.889535427093506\n",
            "Training Iteration 1866, Loss: 6.367986679077148\n",
            "Training Iteration 1867, Loss: 7.268311500549316\n",
            "Training Iteration 1868, Loss: 7.125708103179932\n",
            "Training Iteration 1869, Loss: 9.26488971710205\n",
            "Training Iteration 1870, Loss: 4.08001184463501\n",
            "Training Iteration 1871, Loss: 5.3950090408325195\n",
            "Training Iteration 1872, Loss: 3.1254708766937256\n",
            "Training Iteration 1873, Loss: 5.5168304443359375\n",
            "Training Iteration 1874, Loss: 4.734951019287109\n",
            "Training Iteration 1875, Loss: 5.697350025177002\n",
            "Training Iteration 1876, Loss: 7.224839210510254\n",
            "Training Iteration 1877, Loss: 6.5251545906066895\n",
            "Training Iteration 1878, Loss: 5.3199028968811035\n",
            "Training Iteration 1879, Loss: 3.590726852416992\n",
            "Training Iteration 1880, Loss: 3.660841464996338\n",
            "Training Iteration 1881, Loss: 3.881758689880371\n",
            "Training Iteration 1882, Loss: 4.479857444763184\n",
            "Training Iteration 1883, Loss: 4.5869011878967285\n",
            "Training Iteration 1884, Loss: 5.588470935821533\n",
            "Training Iteration 1885, Loss: 4.640324115753174\n",
            "Training Iteration 1886, Loss: 5.012976169586182\n",
            "Training Iteration 1887, Loss: 3.5843076705932617\n",
            "Training Iteration 1888, Loss: 3.0677649974823\n",
            "Training Iteration 1889, Loss: 3.4249343872070312\n",
            "Training Iteration 1890, Loss: 4.53847599029541\n",
            "Training Iteration 1891, Loss: 4.079263210296631\n",
            "Training Iteration 1892, Loss: 2.562929153442383\n",
            "Training Iteration 1893, Loss: 6.937551021575928\n",
            "Training Iteration 1894, Loss: 6.047544956207275\n",
            "Training Iteration 1895, Loss: 10.891332626342773\n",
            "Training Iteration 1896, Loss: 7.721299648284912\n",
            "Training Iteration 1897, Loss: 4.786447048187256\n",
            "Training Iteration 1898, Loss: 3.42281436920166\n",
            "Training Iteration 1899, Loss: 6.3922529220581055\n",
            "Training Iteration 1900, Loss: 2.4041988849639893\n",
            "Training Iteration 1901, Loss: 4.637216091156006\n",
            "Training Iteration 1902, Loss: 2.0505900382995605\n",
            "Training Iteration 1903, Loss: 5.824537754058838\n",
            "Training Iteration 1904, Loss: 5.2857747077941895\n",
            "Training Iteration 1905, Loss: 6.1698713302612305\n",
            "Training Iteration 1906, Loss: 5.9381537437438965\n",
            "Training Iteration 1907, Loss: 7.399608612060547\n",
            "Training Iteration 1908, Loss: 8.028071403503418\n",
            "Training Iteration 1909, Loss: 3.0987625122070312\n",
            "Training Iteration 1910, Loss: 4.463818073272705\n",
            "Training Iteration 1911, Loss: 2.9902091026306152\n",
            "Training Iteration 1912, Loss: 5.297226428985596\n",
            "Training Iteration 1913, Loss: 6.9722418785095215\n",
            "Training Iteration 1914, Loss: 2.757354974746704\n",
            "Training Iteration 1915, Loss: 8.571611404418945\n",
            "Training Iteration 1916, Loss: 8.475786209106445\n",
            "Training Iteration 1917, Loss: 4.916865825653076\n",
            "Training Iteration 1918, Loss: 2.270737409591675\n",
            "Training Iteration 1919, Loss: 6.15010404586792\n",
            "Training Iteration 1920, Loss: 6.540093898773193\n",
            "Training Iteration 1921, Loss: 6.432244777679443\n",
            "Training Iteration 1922, Loss: 3.5712103843688965\n",
            "Training Iteration 1923, Loss: 9.676177024841309\n",
            "Training Iteration 1924, Loss: 3.7678403854370117\n",
            "Training Iteration 1925, Loss: 2.9713616371154785\n",
            "Training Iteration 1926, Loss: 7.881800174713135\n",
            "Training Iteration 1927, Loss: 6.570682525634766\n",
            "Training Iteration 1928, Loss: 7.380637168884277\n",
            "Training Iteration 1929, Loss: 9.79261589050293\n",
            "Training Iteration 1930, Loss: 5.8945698738098145\n",
            "Training Iteration 1931, Loss: 2.783304452896118\n",
            "Training Iteration 1932, Loss: 4.749115943908691\n",
            "Training Iteration 1933, Loss: 4.46061372756958\n",
            "Training Iteration 1934, Loss: 4.196282863616943\n",
            "Training Iteration 1935, Loss: 4.397357940673828\n",
            "Training Iteration 1936, Loss: 8.358078956604004\n",
            "Training Iteration 1937, Loss: 4.155356407165527\n",
            "Training Iteration 1938, Loss: 7.954495429992676\n",
            "Training Iteration 1939, Loss: 2.480192184448242\n",
            "Training Iteration 1940, Loss: 8.371747016906738\n",
            "Training Iteration 1941, Loss: 5.679750919342041\n",
            "Training Iteration 1942, Loss: 4.621313095092773\n",
            "Training Iteration 1943, Loss: 6.9910807609558105\n",
            "Training Iteration 1944, Loss: 5.651678085327148\n",
            "Training Iteration 1945, Loss: 6.47515344619751\n",
            "Training Iteration 1946, Loss: 4.511394500732422\n",
            "Training Iteration 1947, Loss: 4.451633930206299\n",
            "Training Iteration 1948, Loss: 2.376889944076538\n",
            "Training Iteration 1949, Loss: 5.234985828399658\n",
            "Training Iteration 1950, Loss: 2.3115313053131104\n",
            "Training Iteration 1951, Loss: 2.506446599960327\n",
            "Training Iteration 1952, Loss: 2.898597002029419\n",
            "Training Iteration 1953, Loss: 4.484676361083984\n",
            "Training Iteration 1954, Loss: 6.063830375671387\n",
            "Training Iteration 1955, Loss: 3.260241985321045\n",
            "Training Iteration 1956, Loss: 2.007279872894287\n",
            "Training Iteration 1957, Loss: 2.158358573913574\n",
            "Training Iteration 1958, Loss: 3.0621719360351562\n",
            "Training Iteration 1959, Loss: 5.305239200592041\n",
            "Training Iteration 1960, Loss: 3.867656946182251\n",
            "Training Iteration 1961, Loss: 3.2701218128204346\n",
            "Training Iteration 1962, Loss: 4.552133083343506\n",
            "Training Iteration 1963, Loss: 5.207019329071045\n",
            "Training Iteration 1964, Loss: 6.837601184844971\n",
            "Training Iteration 1965, Loss: 4.678701877593994\n",
            "Training Iteration 1966, Loss: 3.92714524269104\n",
            "Training Iteration 1967, Loss: 5.939196586608887\n",
            "Training Iteration 1968, Loss: 5.502935886383057\n",
            "Training Iteration 1969, Loss: 3.906646728515625\n",
            "Training Iteration 1970, Loss: 3.1902029514312744\n",
            "Training Iteration 1971, Loss: 3.3062691688537598\n",
            "Training Iteration 1972, Loss: 1.7851210832595825\n",
            "Training Iteration 1973, Loss: 5.4413299560546875\n",
            "Training Iteration 1974, Loss: 4.532586097717285\n",
            "Training Iteration 1975, Loss: 8.041682243347168\n",
            "Training Iteration 1976, Loss: 5.633319854736328\n",
            "Training Iteration 1977, Loss: 3.7801835536956787\n",
            "Training Iteration 1978, Loss: 7.2731804847717285\n",
            "Training Iteration 1979, Loss: 2.176602363586426\n",
            "Training Iteration 1980, Loss: 3.8363795280456543\n",
            "Training Iteration 1981, Loss: 3.1781582832336426\n",
            "Training Iteration 1982, Loss: 3.2453408241271973\n",
            "Training Iteration 1983, Loss: 5.068803787231445\n",
            "Training Iteration 1984, Loss: 3.412716865539551\n",
            "Training Iteration 1985, Loss: 3.948108673095703\n",
            "Training Iteration 1986, Loss: 4.371288299560547\n",
            "Training Iteration 1987, Loss: 7.466042518615723\n",
            "Training Iteration 1988, Loss: 5.40090274810791\n",
            "Training Iteration 1989, Loss: 3.794574022293091\n",
            "Training Iteration 1990, Loss: 4.422138690948486\n",
            "Training Iteration 1991, Loss: 3.8134658336639404\n",
            "Training Iteration 1992, Loss: 4.236268520355225\n",
            "Training Iteration 1993, Loss: 4.235328674316406\n",
            "Training Iteration 1994, Loss: 4.684657096862793\n",
            "Training Iteration 1995, Loss: 3.7138266563415527\n",
            "Training Iteration 1996, Loss: 3.4374868869781494\n",
            "Training Iteration 1997, Loss: 4.0126872062683105\n",
            "Training Iteration 1998, Loss: 5.285882472991943\n",
            "Training Iteration 1999, Loss: 3.870593309402466\n",
            "Training Iteration 2000, Loss: 3.0601110458374023\n",
            "Training Iteration 2001, Loss: 2.246609687805176\n",
            "Training Iteration 2002, Loss: 2.8987936973571777\n",
            "Training Iteration 2003, Loss: 3.7935006618499756\n",
            "Training Iteration 2004, Loss: 3.2987518310546875\n",
            "Training Iteration 2005, Loss: 3.9357075691223145\n",
            "Training Iteration 2006, Loss: 3.2869768142700195\n",
            "Training Iteration 2007, Loss: 2.6792173385620117\n",
            "Training Iteration 2008, Loss: 3.7444612979888916\n",
            "Training Iteration 2009, Loss: 2.89481520652771\n",
            "Training Iteration 2010, Loss: 2.696101188659668\n",
            "Training Iteration 2011, Loss: 4.281576156616211\n",
            "Training Iteration 2012, Loss: 3.3679025173187256\n",
            "Training Iteration 2013, Loss: 6.2077717781066895\n",
            "Training Iteration 2014, Loss: 4.7964067459106445\n",
            "Training Iteration 2015, Loss: 3.1761133670806885\n",
            "Training Iteration 2016, Loss: 1.1179718971252441\n",
            "Training Iteration 2017, Loss: 5.558614730834961\n",
            "Training Iteration 2018, Loss: 5.924910545349121\n",
            "Training Iteration 2019, Loss: 3.5699968338012695\n",
            "Training Iteration 2020, Loss: 1.4682979583740234\n",
            "Training Iteration 2021, Loss: 4.578391075134277\n",
            "Training Iteration 2022, Loss: 6.16872501373291\n",
            "Training Iteration 2023, Loss: 4.219552516937256\n",
            "Training Iteration 2024, Loss: 5.559394836425781\n",
            "Training Iteration 2025, Loss: 3.369476556777954\n",
            "Training Iteration 2026, Loss: 9.369336128234863\n",
            "Training Iteration 2027, Loss: 2.920227289199829\n",
            "Training Iteration 2028, Loss: 2.939065456390381\n",
            "Training Iteration 2029, Loss: 3.6889119148254395\n",
            "Training Iteration 2030, Loss: 3.4934017658233643\n",
            "Training Iteration 2031, Loss: 6.676004409790039\n",
            "Training Iteration 2032, Loss: 2.6417715549468994\n",
            "Training Iteration 2033, Loss: 3.760174036026001\n",
            "Training Iteration 2034, Loss: 6.246665954589844\n",
            "Training Iteration 2035, Loss: 4.421081066131592\n",
            "Training Iteration 2036, Loss: 1.7071452140808105\n",
            "Training Iteration 2037, Loss: 5.745376110076904\n",
            "Training Iteration 2038, Loss: 5.0972208976745605\n",
            "Training Iteration 2039, Loss: 5.680783271789551\n",
            "Training Iteration 2040, Loss: 3.6473593711853027\n",
            "Training Iteration 2041, Loss: 5.534114360809326\n",
            "Training Iteration 2042, Loss: 3.951158285140991\n",
            "Training Iteration 2043, Loss: 4.577746868133545\n",
            "Training Iteration 2044, Loss: 5.663053512573242\n",
            "Training Iteration 2045, Loss: 2.7111477851867676\n",
            "Training Iteration 2046, Loss: 4.320646286010742\n",
            "Training Iteration 2047, Loss: 1.984055519104004\n",
            "Training Iteration 2048, Loss: 5.367934226989746\n",
            "Training Iteration 2049, Loss: 2.355952024459839\n",
            "Training Iteration 2050, Loss: 3.929990768432617\n",
            "Training Iteration 2051, Loss: 2.740309715270996\n",
            "Training Iteration 2052, Loss: 4.101656436920166\n",
            "Training Iteration 2053, Loss: 5.9308695793151855\n",
            "Training Iteration 2054, Loss: 4.181337356567383\n",
            "Training Iteration 2055, Loss: 4.027975559234619\n",
            "Training Iteration 2056, Loss: 4.587976455688477\n",
            "Training Iteration 2057, Loss: 2.396733522415161\n",
            "Training Iteration 2058, Loss: 2.0850510597229004\n",
            "Training Iteration 2059, Loss: 6.150847911834717\n",
            "Training Iteration 2060, Loss: 4.710456848144531\n",
            "Training Iteration 2061, Loss: 5.4686479568481445\n",
            "Training Iteration 2062, Loss: 4.001012802124023\n",
            "Training Iteration 2063, Loss: 3.7272777557373047\n",
            "Training Iteration 2064, Loss: 4.94550085067749\n",
            "Training Iteration 2065, Loss: 4.899594783782959\n",
            "Training Iteration 2066, Loss: 2.0275514125823975\n",
            "Training Iteration 2067, Loss: 3.130681276321411\n",
            "Training Iteration 2068, Loss: 2.4227371215820312\n",
            "Training Iteration 2069, Loss: 3.4579176902770996\n",
            "Training Iteration 2070, Loss: 3.0031559467315674\n",
            "Training Iteration 2071, Loss: 3.5083084106445312\n",
            "Training Iteration 2072, Loss: 2.9547340869903564\n",
            "Training Iteration 2073, Loss: 5.321708679199219\n",
            "Training Iteration 2074, Loss: 4.712777614593506\n",
            "Training Iteration 2075, Loss: 5.877231597900391\n",
            "Training Iteration 2076, Loss: 4.813171863555908\n",
            "Training Iteration 2077, Loss: 4.305395603179932\n",
            "Training Iteration 2078, Loss: 5.319990158081055\n",
            "Training Iteration 2079, Loss: 4.303285121917725\n",
            "Training Iteration 2080, Loss: 4.583566188812256\n",
            "Training Iteration 2081, Loss: 5.0890793800354\n",
            "Training Iteration 2082, Loss: 3.890944242477417\n",
            "Training Iteration 2083, Loss: 2.9878933429718018\n",
            "Training Iteration 2084, Loss: 5.593967437744141\n",
            "Training Iteration 2085, Loss: 5.679990768432617\n",
            "Training Iteration 2086, Loss: 1.9732214212417603\n",
            "Training Iteration 2087, Loss: 5.192745685577393\n",
            "Training Iteration 2088, Loss: 4.047882556915283\n",
            "Training Iteration 2089, Loss: 7.926620960235596\n",
            "Training Iteration 2090, Loss: 4.060722351074219\n",
            "Training Iteration 2091, Loss: 4.429610729217529\n",
            "Training Iteration 2092, Loss: 4.939051628112793\n",
            "Training Iteration 2093, Loss: 5.97128963470459\n",
            "Training Iteration 2094, Loss: 1.201275110244751\n",
            "Training Iteration 2095, Loss: 4.4083685874938965\n",
            "Training Iteration 2096, Loss: 6.20920467376709\n",
            "Training Iteration 2097, Loss: 6.034139633178711\n",
            "Training Iteration 2098, Loss: 4.883325099945068\n",
            "Training Iteration 2099, Loss: 3.840815544128418\n",
            "Training Iteration 2100, Loss: 3.3852548599243164\n",
            "Training Iteration 2101, Loss: 4.200414180755615\n",
            "Training Iteration 2102, Loss: 1.6106319427490234\n",
            "Training Iteration 2103, Loss: 8.13444995880127\n",
            "Training Iteration 2104, Loss: 4.876534938812256\n",
            "Training Iteration 2105, Loss: 2.908838987350464\n",
            "Training Iteration 2106, Loss: 6.085748195648193\n",
            "Training Iteration 2107, Loss: 3.3164730072021484\n",
            "Training Iteration 2108, Loss: 2.3891208171844482\n",
            "Training Iteration 2109, Loss: 5.366814136505127\n",
            "Training Iteration 2110, Loss: 2.357018232345581\n",
            "Training Iteration 2111, Loss: 3.6806647777557373\n",
            "Training Iteration 2112, Loss: 3.82680082321167\n",
            "Training Iteration 2113, Loss: 2.2248663902282715\n",
            "Training Iteration 2114, Loss: 2.750032424926758\n",
            "Training Iteration 2115, Loss: 4.40212869644165\n",
            "Training Iteration 2116, Loss: 5.747817516326904\n",
            "Training Iteration 2117, Loss: 3.8070456981658936\n",
            "Training Iteration 2118, Loss: 3.6267499923706055\n",
            "Training Iteration 2119, Loss: 5.1965203285217285\n",
            "Training Iteration 2120, Loss: 8.65708065032959\n",
            "Training Iteration 2121, Loss: 2.1038615703582764\n",
            "Training Iteration 2122, Loss: 4.740159034729004\n",
            "Training Iteration 2123, Loss: 3.74051833152771\n",
            "Training Iteration 2124, Loss: 5.233386516571045\n",
            "Training Iteration 2125, Loss: 2.7712583541870117\n",
            "Training Iteration 2126, Loss: 3.2778306007385254\n",
            "Training Iteration 2127, Loss: 2.202754497528076\n",
            "Training Iteration 2128, Loss: 4.247456073760986\n",
            "Training Iteration 2129, Loss: 4.026421546936035\n",
            "Training Iteration 2130, Loss: 4.7329583168029785\n",
            "Training Iteration 2131, Loss: 6.020459175109863\n",
            "Training Iteration 2132, Loss: 3.110450029373169\n",
            "Training Iteration 2133, Loss: 3.5358810424804688\n",
            "Training Iteration 2134, Loss: 4.209504127502441\n",
            "Training Iteration 2135, Loss: 8.78066349029541\n",
            "Training Iteration 2136, Loss: 6.123155117034912\n",
            "Training Iteration 2137, Loss: 6.859068870544434\n",
            "Training Iteration 2138, Loss: 4.629398345947266\n",
            "Training Iteration 2139, Loss: 5.447066307067871\n",
            "Training Iteration 2140, Loss: 7.165260314941406\n",
            "Training Iteration 2141, Loss: 4.714790344238281\n",
            "Training Iteration 2142, Loss: 3.9286155700683594\n",
            "Training Iteration 2143, Loss: 5.244852066040039\n",
            "Training Iteration 2144, Loss: 3.2557320594787598\n",
            "Training Iteration 2145, Loss: 4.1323018074035645\n",
            "Training Iteration 2146, Loss: 3.7319319248199463\n",
            "Training Iteration 2147, Loss: 5.784193515777588\n",
            "Training Iteration 2148, Loss: 4.44880485534668\n",
            "Training Iteration 2149, Loss: 6.256479263305664\n",
            "Training Iteration 2150, Loss: 7.355899810791016\n",
            "Training Iteration 2151, Loss: 5.309313774108887\n",
            "Training Iteration 2152, Loss: 7.2161784172058105\n",
            "Training Iteration 2153, Loss: 3.5745739936828613\n",
            "Training Iteration 2154, Loss: 6.427161693572998\n",
            "Training Iteration 2155, Loss: 5.798790454864502\n",
            "Training Iteration 2156, Loss: 5.067954063415527\n",
            "Training Iteration 2157, Loss: 7.078893661499023\n",
            "Training Iteration 2158, Loss: 4.562175273895264\n",
            "Training Iteration 2159, Loss: 4.019759178161621\n",
            "Training Iteration 2160, Loss: 1.8876559734344482\n",
            "Training Iteration 2161, Loss: 5.608084678649902\n",
            "Training Iteration 2162, Loss: 5.139030933380127\n",
            "Training Iteration 2163, Loss: 0.9217786192893982\n",
            "Training Iteration 2164, Loss: 4.255646228790283\n",
            "Training Iteration 2165, Loss: 5.096845626831055\n",
            "Training Iteration 2166, Loss: 2.687882661819458\n",
            "Training Iteration 2167, Loss: 4.338136196136475\n",
            "Training Iteration 2168, Loss: 4.078673362731934\n",
            "Training Iteration 2169, Loss: 6.395779132843018\n",
            "Training Iteration 2170, Loss: 2.424142837524414\n",
            "Training Iteration 2171, Loss: 6.21701192855835\n",
            "Training Iteration 2172, Loss: 5.360490798950195\n",
            "Training Iteration 2173, Loss: 6.429877281188965\n",
            "Training Iteration 2174, Loss: 3.5246834754943848\n",
            "Training Iteration 2175, Loss: 5.198351860046387\n",
            "Training Iteration 2176, Loss: 5.575603485107422\n",
            "Training Iteration 2177, Loss: 7.91417121887207\n",
            "Training Iteration 2178, Loss: 2.768354654312134\n",
            "Training Iteration 2179, Loss: 3.3178141117095947\n",
            "Training Iteration 2180, Loss: 3.747933864593506\n",
            "Training Iteration 2181, Loss: 3.1300888061523438\n",
            "Training Iteration 2182, Loss: 5.348108291625977\n",
            "Training Iteration 2183, Loss: 5.77769660949707\n",
            "Training Iteration 2184, Loss: 1.2598576545715332\n",
            "Training Iteration 2185, Loss: 4.105758190155029\n",
            "Training Iteration 2186, Loss: 5.013814449310303\n",
            "Training Iteration 2187, Loss: 2.362946033477783\n",
            "Training Iteration 2188, Loss: 5.723621845245361\n",
            "Training Iteration 2189, Loss: 6.884079933166504\n",
            "Training Iteration 2190, Loss: 3.929582118988037\n",
            "Training Iteration 2191, Loss: 3.734178066253662\n",
            "Training Iteration 2192, Loss: 3.881831169128418\n",
            "Training Iteration 2193, Loss: 2.423038959503174\n",
            "Training Iteration 2194, Loss: 3.74784779548645\n",
            "Training Iteration 2195, Loss: 2.7629523277282715\n",
            "Training Iteration 2196, Loss: 4.751664638519287\n",
            "Training Iteration 2197, Loss: 3.7300846576690674\n",
            "Training Iteration 2198, Loss: 5.026450157165527\n",
            "Training Iteration 2199, Loss: 1.0699079036712646\n",
            "Training Iteration 2200, Loss: 4.6507248878479\n",
            "Training Iteration 2201, Loss: 4.788194179534912\n",
            "Training Iteration 2202, Loss: 2.602468967437744\n",
            "Training Iteration 2203, Loss: 3.5209269523620605\n",
            "Training Iteration 2204, Loss: 2.5831148624420166\n",
            "Training Iteration 2205, Loss: 2.8557372093200684\n",
            "Training Iteration 2206, Loss: 3.0874176025390625\n",
            "Training Iteration 2207, Loss: 3.36061954498291\n",
            "Training Iteration 2208, Loss: 7.383501052856445\n",
            "Training Iteration 2209, Loss: 3.907421588897705\n",
            "Training Iteration 2210, Loss: 5.995809078216553\n",
            "Training Iteration 2211, Loss: 5.065056800842285\n",
            "Training Iteration 2212, Loss: 3.7437312602996826\n",
            "Training Iteration 2213, Loss: 5.462446212768555\n",
            "Training Iteration 2214, Loss: 4.255345344543457\n",
            "Training Iteration 2215, Loss: 4.778398513793945\n",
            "Training Iteration 2216, Loss: 5.067012786865234\n",
            "Training Iteration 2217, Loss: 3.1632819175720215\n",
            "Training Iteration 2218, Loss: 3.6196136474609375\n",
            "Training Iteration 2219, Loss: 2.9911186695098877\n",
            "Training Iteration 2220, Loss: 4.418501853942871\n",
            "Training Iteration 2221, Loss: 2.993661880493164\n",
            "Training Iteration 2222, Loss: 6.436675071716309\n",
            "Training Iteration 2223, Loss: 3.0564956665039062\n",
            "Training Iteration 2224, Loss: 5.082767486572266\n",
            "Training Iteration 2225, Loss: 5.320326328277588\n",
            "Training Iteration 2226, Loss: 4.410935401916504\n",
            "Training Iteration 2227, Loss: 4.854903221130371\n",
            "Training Iteration 2228, Loss: 1.9929810762405396\n",
            "Training Iteration 2229, Loss: 5.0985426902771\n",
            "Training Iteration 2230, Loss: 4.237675666809082\n",
            "Training Iteration 2231, Loss: 4.1303391456604\n",
            "Training Iteration 2232, Loss: 3.2053771018981934\n",
            "Training Iteration 2233, Loss: 3.42919921875\n",
            "Training Iteration 2234, Loss: 1.9603142738342285\n",
            "Training Iteration 2235, Loss: 1.7945833206176758\n",
            "Training Iteration 2236, Loss: 5.2960100173950195\n",
            "Training Iteration 2237, Loss: 3.953740119934082\n",
            "Training Iteration 2238, Loss: 7.329829692840576\n",
            "Training Iteration 2239, Loss: 3.582444667816162\n",
            "Training Iteration 2240, Loss: 3.960705041885376\n",
            "Training Iteration 2241, Loss: 6.189424991607666\n",
            "Training Iteration 2242, Loss: 5.952210903167725\n",
            "Training Iteration 2243, Loss: 4.113088607788086\n",
            "Training Iteration 2244, Loss: 4.580803871154785\n",
            "Training Iteration 2245, Loss: 2.669402837753296\n",
            "Training Iteration 2246, Loss: 3.9933996200561523\n",
            "Training Iteration 2247, Loss: 2.9895284175872803\n",
            "Training Iteration 2248, Loss: 3.273286819458008\n",
            "Training Iteration 2249, Loss: 6.059848785400391\n",
            "Training Iteration 2250, Loss: 2.9881138801574707\n",
            "Training Iteration 2251, Loss: 4.971179008483887\n",
            "Training Iteration 2252, Loss: 8.35162353515625\n",
            "Training Iteration 2253, Loss: 9.924038887023926\n",
            "Training Iteration 2254, Loss: 7.80426025390625\n",
            "Training Iteration 2255, Loss: 7.69196891784668\n",
            "Training Iteration 2256, Loss: 7.189818382263184\n",
            "Training Iteration 2257, Loss: 7.110021114349365\n",
            "Training Iteration 2258, Loss: 7.629176139831543\n",
            "Training Iteration 2259, Loss: 5.022922039031982\n",
            "Training Iteration 2260, Loss: 2.746516704559326\n",
            "Training Iteration 2261, Loss: 8.90169906616211\n",
            "Training Iteration 2262, Loss: 3.2396907806396484\n",
            "Training Iteration 2263, Loss: 8.28873348236084\n",
            "Training Iteration 2264, Loss: 3.502872943878174\n",
            "Training Iteration 2265, Loss: 4.755174160003662\n",
            "Training Iteration 2266, Loss: 6.073370933532715\n",
            "Training Iteration 2267, Loss: 11.434609413146973\n",
            "Training Iteration 2268, Loss: 2.412832021713257\n",
            "Training Iteration 2269, Loss: 4.452341556549072\n",
            "Training Iteration 2270, Loss: 7.390114784240723\n",
            "Training Iteration 2271, Loss: 8.506410598754883\n",
            "Training Iteration 2272, Loss: 7.625046730041504\n",
            "Training Iteration 2273, Loss: 3.629511833190918\n",
            "Training Iteration 2274, Loss: 4.388899803161621\n",
            "Training Iteration 2275, Loss: 3.722170829772949\n",
            "Training Iteration 2276, Loss: 3.152557373046875\n",
            "Training Iteration 2277, Loss: 4.4753336906433105\n",
            "Training Iteration 2278, Loss: 4.359079837799072\n",
            "Training Iteration 2279, Loss: 8.752509117126465\n",
            "Training Iteration 2280, Loss: 1.946106195449829\n",
            "Training Iteration 2281, Loss: 4.572681903839111\n",
            "Training Iteration 2282, Loss: 5.790930271148682\n",
            "Training Iteration 2283, Loss: 7.8969621658325195\n",
            "Training Iteration 2284, Loss: 5.420871257781982\n",
            "Training Iteration 2285, Loss: 2.7017385959625244\n",
            "Training Iteration 2286, Loss: 4.561290740966797\n",
            "Training Iteration 2287, Loss: 4.1145243644714355\n",
            "Training Iteration 2288, Loss: 3.433250665664673\n",
            "Training Iteration 2289, Loss: 3.1042306423187256\n",
            "Training Iteration 2290, Loss: 4.4555158615112305\n",
            "Training Iteration 2291, Loss: 1.4673538208007812\n",
            "Training Iteration 2292, Loss: 7.608841419219971\n",
            "Training Iteration 2293, Loss: 6.141687393188477\n",
            "Training Iteration 2294, Loss: 5.884863376617432\n",
            "Training Iteration 2295, Loss: 5.20556640625\n",
            "Training Iteration 2296, Loss: 4.526993751525879\n",
            "Training Iteration 2297, Loss: 4.31585168838501\n",
            "Training Iteration 2298, Loss: 4.81299352645874\n",
            "Training Iteration 2299, Loss: 3.7545595169067383\n",
            "Training Iteration 2300, Loss: 4.340065002441406\n",
            "Training Iteration 2301, Loss: 4.1389336585998535\n",
            "Training Iteration 2302, Loss: 3.863858461380005\n",
            "Training Iteration 2303, Loss: 2.5351829528808594\n",
            "Training Iteration 2304, Loss: 5.399303913116455\n",
            "Training Iteration 2305, Loss: 3.5126757621765137\n",
            "Training Iteration 2306, Loss: 4.076479434967041\n",
            "Training Iteration 2307, Loss: 1.8871541023254395\n",
            "Training Iteration 2308, Loss: 4.126943588256836\n",
            "Training Iteration 2309, Loss: 3.3758339881896973\n",
            "Training Iteration 2310, Loss: 6.019049167633057\n",
            "Training Iteration 2311, Loss: 3.392570972442627\n",
            "Training Iteration 2312, Loss: 3.475658893585205\n",
            "Training Iteration 2313, Loss: 5.033393859863281\n",
            "Training Iteration 2314, Loss: 2.721801280975342\n",
            "Training Iteration 2315, Loss: 2.8721847534179688\n",
            "Training Iteration 2316, Loss: 4.789316654205322\n",
            "Training Iteration 2317, Loss: 4.8685526847839355\n",
            "Training Iteration 2318, Loss: 2.051356792449951\n",
            "Training Iteration 2319, Loss: 5.620922565460205\n",
            "Training Iteration 2320, Loss: 3.9744584560394287\n",
            "Training Iteration 2321, Loss: 3.95438814163208\n",
            "Training Iteration 2322, Loss: 4.219106674194336\n",
            "Training Iteration 2323, Loss: 4.30983829498291\n",
            "Training Iteration 2324, Loss: 3.609004497528076\n",
            "Training Iteration 2325, Loss: 2.7276878356933594\n",
            "Training Iteration 2326, Loss: 3.408777952194214\n",
            "Training Iteration 2327, Loss: 2.7878828048706055\n",
            "Training Iteration 2328, Loss: 5.236905574798584\n",
            "Training Iteration 2329, Loss: 4.035468578338623\n",
            "Training Iteration 2330, Loss: 4.721995830535889\n",
            "Training Iteration 2331, Loss: 3.974259614944458\n",
            "Training Iteration 2332, Loss: 4.109910488128662\n",
            "Training Iteration 2333, Loss: 2.0989415645599365\n",
            "Training Iteration 2334, Loss: 2.888546943664551\n",
            "Training Iteration 2335, Loss: 8.69277572631836\n",
            "Training Iteration 2336, Loss: 2.3321104049682617\n",
            "Training Iteration 2337, Loss: 6.143681526184082\n",
            "Training Iteration 2338, Loss: 2.737050771713257\n",
            "Training Iteration 2339, Loss: 4.247759819030762\n",
            "Training Iteration 2340, Loss: 2.012392520904541\n",
            "Training Iteration 2341, Loss: 2.1433844566345215\n",
            "Training Iteration 2342, Loss: 3.0228049755096436\n",
            "Training Iteration 2343, Loss: 3.6534640789031982\n",
            "Training Iteration 2344, Loss: 4.411805152893066\n",
            "Training Iteration 2345, Loss: 2.4362900257110596\n",
            "Training Iteration 2346, Loss: 3.83925724029541\n",
            "Training Iteration 2347, Loss: 4.477558612823486\n",
            "Training Iteration 2348, Loss: 3.188352108001709\n",
            "Training Iteration 2349, Loss: 3.0930604934692383\n",
            "Training Iteration 2350, Loss: 4.1029229164123535\n",
            "Training Iteration 2351, Loss: 4.0621185302734375\n",
            "Training Iteration 2352, Loss: 3.945897340774536\n",
            "Training Iteration 2353, Loss: 2.5711705684661865\n",
            "Training Iteration 2354, Loss: 4.980015754699707\n",
            "Training Iteration 2355, Loss: 6.129119396209717\n",
            "Training Iteration 2356, Loss: 4.075296878814697\n",
            "Training Iteration 2357, Loss: 6.135683059692383\n",
            "Training Iteration 2358, Loss: 4.618859767913818\n",
            "Training Iteration 2359, Loss: 2.773019790649414\n",
            "Training Iteration 2360, Loss: 4.174127578735352\n",
            "Training Iteration 2361, Loss: 2.9279699325561523\n",
            "Training Iteration 2362, Loss: 3.706913471221924\n",
            "Training Iteration 2363, Loss: 4.024535179138184\n",
            "Training Iteration 2364, Loss: 7.111824035644531\n",
            "Training Iteration 2365, Loss: 4.4777960777282715\n",
            "Training Iteration 2366, Loss: 6.46531867980957\n",
            "Training Iteration 2367, Loss: 4.631688594818115\n",
            "Training Iteration 2368, Loss: 3.130612373352051\n",
            "Training Iteration 2369, Loss: 2.120095729827881\n",
            "Training Iteration 2370, Loss: 3.1390984058380127\n",
            "Training Iteration 2371, Loss: 4.58381462097168\n",
            "Training Iteration 2372, Loss: 2.849297523498535\n",
            "Training Iteration 2373, Loss: 5.075247287750244\n",
            "Training Iteration 2374, Loss: 6.84317684173584\n",
            "Training Iteration 2375, Loss: 3.4231691360473633\n",
            "Training Iteration 2376, Loss: 4.972681999206543\n",
            "Training Iteration 2377, Loss: 6.0688090324401855\n",
            "Training Iteration 2378, Loss: 8.05950927734375\n",
            "Training Iteration 2379, Loss: 4.352406978607178\n",
            "Training Iteration 2380, Loss: 1.8276411294937134\n",
            "Training Iteration 2381, Loss: 8.447003364562988\n",
            "Training Iteration 2382, Loss: 6.34575080871582\n",
            "Training Iteration 2383, Loss: 5.172146797180176\n",
            "Training Iteration 2384, Loss: 6.355501174926758\n",
            "Training Iteration 2385, Loss: 4.893383026123047\n",
            "Training Iteration 2386, Loss: 4.421194553375244\n",
            "Training Iteration 2387, Loss: 7.094226360321045\n",
            "Training Iteration 2388, Loss: 4.15936803817749\n",
            "Training Iteration 2389, Loss: 5.919948577880859\n",
            "Training Iteration 2390, Loss: 10.28769588470459\n",
            "Training Iteration 2391, Loss: 8.045083999633789\n",
            "Training Iteration 2392, Loss: 3.9890191555023193\n",
            "Training Iteration 2393, Loss: 3.5602242946624756\n",
            "Training Iteration 2394, Loss: 4.369311332702637\n",
            "Training Iteration 2395, Loss: 4.3413214683532715\n",
            "Training Iteration 2396, Loss: 6.038986682891846\n",
            "Training Iteration 2397, Loss: 1.2528260946273804\n",
            "Training Iteration 2398, Loss: 4.640489101409912\n",
            "Training Iteration 2399, Loss: 2.5904860496520996\n",
            "Training Iteration 2400, Loss: 4.5896735191345215\n",
            "Training Iteration 2401, Loss: 2.5979630947113037\n",
            "Training Iteration 2402, Loss: 2.0270535945892334\n",
            "Training Iteration 2403, Loss: 3.962336778640747\n",
            "Training Iteration 2404, Loss: 9.584931373596191\n",
            "Training Iteration 2405, Loss: 3.422419548034668\n",
            "Training Iteration 2406, Loss: 5.751924514770508\n",
            "Training Iteration 2407, Loss: 2.6336772441864014\n",
            "Training Iteration 2408, Loss: 5.980902194976807\n",
            "Training Iteration 2409, Loss: 2.763718605041504\n",
            "Training Iteration 2410, Loss: 3.689586877822876\n",
            "Training Iteration 2411, Loss: 6.18508243560791\n",
            "Training Iteration 2412, Loss: 4.605792999267578\n",
            "Training Iteration 2413, Loss: 8.967497825622559\n",
            "Training Iteration 2414, Loss: 8.464195251464844\n",
            "Training Iteration 2415, Loss: 5.166265964508057\n",
            "Training Iteration 2416, Loss: 2.689056396484375\n",
            "Training Iteration 2417, Loss: 2.7342398166656494\n",
            "Training Iteration 2418, Loss: 3.330475330352783\n",
            "Training Iteration 2419, Loss: 6.822992324829102\n",
            "Training Iteration 2420, Loss: 6.122298240661621\n",
            "Training Iteration 2421, Loss: 4.489242076873779\n",
            "Training Iteration 2422, Loss: 4.987783432006836\n",
            "Training Iteration 2423, Loss: 7.031546592712402\n",
            "Training Iteration 2424, Loss: 3.857720375061035\n",
            "Training Iteration 2425, Loss: 6.102435111999512\n",
            "Training Iteration 2426, Loss: 11.204412460327148\n",
            "Training Iteration 2427, Loss: 11.01823902130127\n",
            "Training Iteration 2428, Loss: 10.366592407226562\n",
            "Training Iteration 2429, Loss: 10.794974327087402\n",
            "Training Iteration 2430, Loss: 6.575201988220215\n",
            "Training Iteration 2431, Loss: 4.050952911376953\n",
            "Training Iteration 2432, Loss: 5.5383758544921875\n",
            "Training Iteration 2433, Loss: 5.91179084777832\n",
            "Training Iteration 2434, Loss: 3.6522789001464844\n",
            "Training Iteration 2435, Loss: 4.605933666229248\n",
            "Training Iteration 2436, Loss: 5.813671112060547\n",
            "Training Iteration 2437, Loss: 5.601778030395508\n",
            "Training Iteration 2438, Loss: 4.110123157501221\n",
            "Training Iteration 2439, Loss: 3.3911147117614746\n",
            "Training Iteration 2440, Loss: 4.587722301483154\n",
            "Training Iteration 2441, Loss: 5.8887505531311035\n",
            "Training Iteration 2442, Loss: 3.0088133811950684\n",
            "Training Iteration 2443, Loss: 2.7011990547180176\n",
            "Training Iteration 2444, Loss: 2.225954055786133\n",
            "Training Iteration 2445, Loss: 3.214284896850586\n",
            "Training Iteration 2446, Loss: 5.956364631652832\n",
            "Training Iteration 2447, Loss: 7.257075309753418\n",
            "Training Iteration 2448, Loss: 2.55085825920105\n",
            "Training Iteration 2449, Loss: 3.330751657485962\n",
            "Training Iteration 2450, Loss: 5.678983211517334\n",
            "Training Iteration 2451, Loss: 6.0280961990356445\n",
            "Training Iteration 2452, Loss: 4.562262058258057\n",
            "Training Iteration 2453, Loss: 3.4816701412200928\n",
            "Training Iteration 2454, Loss: 6.7038984298706055\n",
            "Training Iteration 2455, Loss: 5.496216297149658\n",
            "Training Iteration 2456, Loss: 2.812406539916992\n",
            "Training Iteration 2457, Loss: 6.322347164154053\n",
            "Training Iteration 2458, Loss: 5.941493034362793\n",
            "Training Iteration 2459, Loss: 5.918424129486084\n",
            "Training Iteration 2460, Loss: 1.9133718013763428\n",
            "Training Iteration 2461, Loss: 4.761159420013428\n",
            "Training Iteration 2462, Loss: 3.8882346153259277\n",
            "Training Iteration 2463, Loss: 4.433083534240723\n",
            "Training Iteration 2464, Loss: 5.263312816619873\n",
            "Training Iteration 2465, Loss: 8.312792778015137\n",
            "Training Iteration 2466, Loss: 5.037768840789795\n",
            "Training Iteration 2467, Loss: 4.72835111618042\n",
            "Training Iteration 2468, Loss: 3.035912275314331\n",
            "Training Iteration 2469, Loss: 3.3858349323272705\n",
            "Training Iteration 2470, Loss: 6.587826728820801\n",
            "Training Iteration 2471, Loss: 7.033324718475342\n",
            "Training Iteration 2472, Loss: 4.987895488739014\n",
            "Training Iteration 2473, Loss: 3.9576165676116943\n",
            "Training Iteration 2474, Loss: 6.262633800506592\n",
            "Training Iteration 2475, Loss: 6.688726425170898\n",
            "Training Iteration 2476, Loss: 5.86632776260376\n",
            "Training Iteration 2477, Loss: 6.875011444091797\n",
            "Training Iteration 2478, Loss: 5.538172245025635\n",
            "Training Iteration 2479, Loss: 3.755849599838257\n",
            "Training Iteration 2480, Loss: 5.599648952484131\n",
            "Training Iteration 2481, Loss: 6.960000514984131\n",
            "Training Iteration 2482, Loss: 6.551607608795166\n",
            "Training Iteration 2483, Loss: 7.242547988891602\n",
            "Training Iteration 2484, Loss: 4.605452060699463\n",
            "Training Iteration 2485, Loss: 6.166749954223633\n",
            "Training Iteration 2486, Loss: 3.7878260612487793\n",
            "Training Iteration 2487, Loss: 8.709564208984375\n",
            "Training Iteration 2488, Loss: 3.659604549407959\n",
            "Training Iteration 2489, Loss: 6.877842903137207\n",
            "Training Iteration 2490, Loss: 6.5676703453063965\n",
            "Training Iteration 2491, Loss: 5.895317077636719\n",
            "Training Iteration 2492, Loss: 4.719642639160156\n",
            "Training Iteration 2493, Loss: 8.4537992477417\n",
            "Training Iteration 2494, Loss: 4.252257347106934\n",
            "Training Iteration 2495, Loss: 6.823009490966797\n",
            "Training Iteration 2496, Loss: 6.2225518226623535\n",
            "Training Iteration 2497, Loss: 10.42406177520752\n",
            "Training Iteration 2498, Loss: 8.574728012084961\n",
            "Training Iteration 2499, Loss: 6.149452209472656\n",
            "Training Iteration 2500, Loss: 4.95377254486084\n",
            "Training Iteration 2501, Loss: 2.338764190673828\n",
            "Training Iteration 2502, Loss: 3.692145824432373\n",
            "Training Iteration 2503, Loss: 8.772133827209473\n",
            "Training Iteration 2504, Loss: 5.665090560913086\n",
            "Training Iteration 2505, Loss: 5.32530403137207\n",
            "Training Iteration 2506, Loss: 4.073119640350342\n",
            "Training Iteration 2507, Loss: 6.203855514526367\n",
            "Training Iteration 2508, Loss: 4.173150539398193\n",
            "Training Iteration 2509, Loss: 5.232733249664307\n",
            "Training Iteration 2510, Loss: 6.6798200607299805\n",
            "Training Iteration 2511, Loss: 5.773169040679932\n",
            "Training Iteration 2512, Loss: 5.8979997634887695\n",
            "Training Iteration 2513, Loss: 2.4767532348632812\n",
            "Training Iteration 2514, Loss: 3.0769906044006348\n",
            "Training Iteration 2515, Loss: 5.362377166748047\n",
            "Training Iteration 2516, Loss: 5.421834945678711\n",
            "Training Iteration 2517, Loss: 7.460148811340332\n",
            "Training Iteration 2518, Loss: 2.6361255645751953\n",
            "Training Iteration 2519, Loss: 1.5990684032440186\n",
            "Training Iteration 2520, Loss: 4.918642044067383\n",
            "Training Iteration 2521, Loss: 5.095426082611084\n",
            "Training Iteration 2522, Loss: 8.44755744934082\n",
            "Training Iteration 2523, Loss: 5.877580642700195\n",
            "Training Iteration 2524, Loss: 5.707751274108887\n",
            "Training Iteration 2525, Loss: 5.702424049377441\n",
            "Training Iteration 2526, Loss: 4.81480073928833\n",
            "Training Iteration 2527, Loss: 3.410785675048828\n",
            "Training Iteration 2528, Loss: 8.264723777770996\n",
            "Training Iteration 2529, Loss: 4.7254109382629395\n",
            "Training Iteration 2530, Loss: 8.329447746276855\n",
            "Training Iteration 2531, Loss: 7.492210865020752\n",
            "Training Iteration 2532, Loss: 4.989992618560791\n",
            "Training Iteration 2533, Loss: 7.563533306121826\n",
            "Training Iteration 2534, Loss: 6.1608476638793945\n",
            "Training Iteration 2535, Loss: 7.842810153961182\n",
            "Training Iteration 2536, Loss: 6.131748199462891\n",
            "Training Iteration 2537, Loss: 6.0912981033325195\n",
            "Training Iteration 2538, Loss: 7.031414031982422\n",
            "Training Iteration 2539, Loss: 4.381070137023926\n",
            "Training Iteration 2540, Loss: 6.863631248474121\n",
            "Training Iteration 2541, Loss: 6.946422576904297\n",
            "Training Iteration 2542, Loss: 8.46663761138916\n",
            "Training Iteration 2543, Loss: 5.668025016784668\n",
            "Training Iteration 2544, Loss: 4.65042781829834\n",
            "Training Iteration 2545, Loss: 6.828561782836914\n",
            "Training Iteration 2546, Loss: 4.799787998199463\n",
            "Training Iteration 2547, Loss: 9.94876766204834\n",
            "Training Iteration 2548, Loss: 3.16736102104187\n",
            "Training Iteration 2549, Loss: 2.2485837936401367\n",
            "Training Iteration 2550, Loss: 5.04307746887207\n",
            "Training Iteration 2551, Loss: 5.966614723205566\n",
            "Training Iteration 2552, Loss: 8.277822494506836\n",
            "Training Iteration 2553, Loss: 2.746154546737671\n",
            "Training Iteration 2554, Loss: 4.310710906982422\n",
            "Training Iteration 2555, Loss: 5.706008434295654\n",
            "Training Iteration 2556, Loss: 4.641121864318848\n",
            "Training Iteration 2557, Loss: 4.781989097595215\n",
            "Training Iteration 2558, Loss: 7.1634721755981445\n",
            "Training Iteration 2559, Loss: 3.3454694747924805\n",
            "Training Iteration 2560, Loss: 6.730218887329102\n",
            "Training Iteration 2561, Loss: 6.468092918395996\n",
            "Training Iteration 2562, Loss: 3.915908098220825\n",
            "Training Iteration 2563, Loss: 5.085912227630615\n",
            "Training Iteration 2564, Loss: 3.9103336334228516\n",
            "Training Iteration 2565, Loss: 4.918476581573486\n",
            "Training Iteration 2566, Loss: 6.665258884429932\n",
            "Training Iteration 2567, Loss: 4.40913724899292\n",
            "Training Iteration 2568, Loss: 4.042471885681152\n",
            "Training Iteration 2569, Loss: 5.642060279846191\n",
            "Training Iteration 2570, Loss: 6.253776550292969\n",
            "Training Iteration 2571, Loss: 4.8732099533081055\n",
            "Training Iteration 2572, Loss: 7.253431797027588\n",
            "Training Iteration 2573, Loss: 5.148061752319336\n",
            "Training Iteration 2574, Loss: 3.9567999839782715\n",
            "Training Iteration 2575, Loss: 3.74159836769104\n",
            "Training Iteration 2576, Loss: 4.625317096710205\n",
            "Training Iteration 2577, Loss: 4.182884216308594\n",
            "Training Iteration 2578, Loss: 5.328865051269531\n",
            "Training Iteration 2579, Loss: 4.9958624839782715\n",
            "Training Iteration 2580, Loss: 4.392365455627441\n",
            "Training Iteration 2581, Loss: 4.6432037353515625\n",
            "Training Iteration 2582, Loss: 2.99861216545105\n",
            "Training Iteration 2583, Loss: 3.3238792419433594\n",
            "Training Iteration 2584, Loss: 2.5936591625213623\n",
            "Training Iteration 2585, Loss: 5.159266471862793\n",
            "Training Iteration 2586, Loss: 3.4327049255371094\n",
            "Training Iteration 2587, Loss: 2.657865047454834\n",
            "Training Iteration 2588, Loss: 6.144378185272217\n",
            "Training Iteration 2589, Loss: 6.226953506469727\n",
            "Training Iteration 2590, Loss: 2.238697052001953\n",
            "Training Iteration 2591, Loss: 3.931473731994629\n",
            "Training Iteration 2592, Loss: 3.936946153640747\n",
            "Training Iteration 2593, Loss: 9.151069641113281\n",
            "Training Iteration 2594, Loss: 3.976053237915039\n",
            "Training Iteration 2595, Loss: 3.8361642360687256\n",
            "Training Iteration 2596, Loss: 4.752986907958984\n",
            "Training Iteration 2597, Loss: 4.105457782745361\n",
            "Training Iteration 2598, Loss: 4.299604892730713\n",
            "Training Iteration 2599, Loss: 6.25501012802124\n",
            "Training Iteration 2600, Loss: 5.090224742889404\n",
            "Training Iteration 2601, Loss: 9.515387535095215\n",
            "Training Iteration 2602, Loss: 6.491306781768799\n",
            "Training Iteration 2603, Loss: 2.484729051589966\n",
            "Training Iteration 2604, Loss: 5.682387828826904\n",
            "Training Iteration 2605, Loss: 4.76236629486084\n",
            "Training Iteration 2606, Loss: 5.618427753448486\n",
            "Training Iteration 2607, Loss: 2.8833606243133545\n",
            "Training Iteration 2608, Loss: 7.732773303985596\n",
            "Training Iteration 2609, Loss: 3.5804197788238525\n",
            "Training Iteration 2610, Loss: 5.593708038330078\n",
            "Training Iteration 2611, Loss: 3.522361993789673\n",
            "Training Iteration 2612, Loss: 4.3328704833984375\n",
            "Training Iteration 2613, Loss: 4.091621398925781\n",
            "Training Iteration 2614, Loss: 3.3563835620880127\n",
            "Training Iteration 2615, Loss: 2.0201661586761475\n",
            "Training Iteration 2616, Loss: 4.540193557739258\n",
            "Training Iteration 2617, Loss: 5.0477294921875\n",
            "Training Iteration 2618, Loss: 3.221940279006958\n",
            "Training Iteration 2619, Loss: 2.870479106903076\n",
            "Training Iteration 2620, Loss: 3.614145040512085\n",
            "Training Iteration 2621, Loss: 3.9166922569274902\n",
            "Training Iteration 2622, Loss: 3.2697086334228516\n",
            "Training Iteration 2623, Loss: 3.375143527984619\n",
            "Training Iteration 2624, Loss: 3.7383906841278076\n",
            "Training Iteration 2625, Loss: 5.598796844482422\n",
            "Training Iteration 2626, Loss: 4.842458724975586\n",
            "Training Iteration 2627, Loss: 3.514803171157837\n",
            "Training Iteration 2628, Loss: 2.5472562313079834\n",
            "Training Iteration 2629, Loss: 6.502501487731934\n",
            "Training Iteration 2630, Loss: 2.2797775268554688\n",
            "Training Iteration 2631, Loss: 5.19949197769165\n",
            "Training Iteration 2632, Loss: 3.5874087810516357\n",
            "Training Iteration 2633, Loss: 4.376825332641602\n",
            "Training Iteration 2634, Loss: 4.125059127807617\n",
            "Training Iteration 2635, Loss: 4.238714218139648\n",
            "Training Iteration 2636, Loss: 6.605138301849365\n",
            "Training Iteration 2637, Loss: 3.82612943649292\n",
            "Training Iteration 2638, Loss: 4.5145368576049805\n",
            "Training Iteration 2639, Loss: 3.5420303344726562\n",
            "Training Iteration 2640, Loss: 4.288661956787109\n",
            "Training Iteration 2641, Loss: 4.0736188888549805\n",
            "Training Iteration 2642, Loss: 5.151998996734619\n",
            "Training Iteration 2643, Loss: 3.5319747924804688\n",
            "Training Iteration 2644, Loss: 2.1047306060791016\n",
            "Training Iteration 2645, Loss: 4.441863536834717\n",
            "Training Iteration 2646, Loss: 2.2279603481292725\n",
            "Training Iteration 2647, Loss: 2.5590872764587402\n",
            "Training Iteration 2648, Loss: 6.097443580627441\n",
            "Training Iteration 2649, Loss: 2.6645517349243164\n",
            "Training Iteration 2650, Loss: 1.7317068576812744\n",
            "Training Iteration 2651, Loss: 3.707489013671875\n",
            "Training Iteration 2652, Loss: 4.753210544586182\n",
            "Training Iteration 2653, Loss: 3.6180920600891113\n",
            "Training Iteration 2654, Loss: 4.277904510498047\n",
            "Training Iteration 2655, Loss: 2.4730913639068604\n",
            "Training Iteration 2656, Loss: 3.162770986557007\n",
            "Training Iteration 2657, Loss: 4.506228446960449\n",
            "Training Iteration 2658, Loss: 3.8820953369140625\n",
            "Training Iteration 2659, Loss: 3.4690287113189697\n",
            "Training Iteration 2660, Loss: 5.887210845947266\n",
            "Training Iteration 2661, Loss: 3.588611125946045\n",
            "Training Iteration 2662, Loss: 3.4454338550567627\n",
            "Training Iteration 2663, Loss: 2.7502362728118896\n",
            "Training Iteration 2664, Loss: 4.831017971038818\n",
            "Training Iteration 2665, Loss: 2.7331299781799316\n",
            "Training Iteration 2666, Loss: 2.340146780014038\n",
            "Training Iteration 2667, Loss: 4.92281436920166\n",
            "Training Iteration 2668, Loss: 3.816619396209717\n",
            "Training Iteration 2669, Loss: 3.553584098815918\n",
            "Training Iteration 2670, Loss: 5.9810686111450195\n",
            "Training Iteration 2671, Loss: 3.7442798614501953\n",
            "Training Iteration 2672, Loss: 5.758975982666016\n",
            "Training Iteration 2673, Loss: 5.466257572174072\n",
            "Training Iteration 2674, Loss: 5.350415229797363\n",
            "Training Iteration 2675, Loss: 3.138998508453369\n",
            "Training Iteration 2676, Loss: 2.9984941482543945\n",
            "Training Iteration 2677, Loss: 4.000632286071777\n",
            "Training Iteration 2678, Loss: 6.26318359375\n",
            "Training Iteration 2679, Loss: 6.665241718292236\n",
            "Training Iteration 2680, Loss: 3.7282614707946777\n",
            "Training Iteration 2681, Loss: 3.1355347633361816\n",
            "Training Iteration 2682, Loss: 2.5187625885009766\n",
            "Training Iteration 2683, Loss: 5.5003228187561035\n",
            "Training Iteration 2684, Loss: 5.12254524230957\n",
            "Training Iteration 2685, Loss: 4.963274955749512\n",
            "Training Iteration 2686, Loss: 3.3545236587524414\n",
            "Training Iteration 2687, Loss: 2.774052143096924\n",
            "Training Iteration 2688, Loss: 2.6949610710144043\n",
            "Training Iteration 2689, Loss: 3.5167744159698486\n",
            "Training Iteration 2690, Loss: 6.209461688995361\n",
            "Training Iteration 2691, Loss: 3.713801622390747\n",
            "Training Iteration 2692, Loss: 3.5746991634368896\n",
            "Training Iteration 2693, Loss: 3.4322428703308105\n",
            "Training Iteration 2694, Loss: 3.32322359085083\n",
            "Training Iteration 2695, Loss: 3.4861514568328857\n",
            "Training Iteration 2696, Loss: 3.190244674682617\n",
            "Training Iteration 2697, Loss: 2.166033983230591\n",
            "Training Iteration 2698, Loss: 4.601016998291016\n",
            "Training Iteration 2699, Loss: 2.608344554901123\n",
            "Training Iteration 2700, Loss: 3.06284236907959\n",
            "Training Iteration 2701, Loss: 4.284918308258057\n",
            "Training Iteration 2702, Loss: 4.521655559539795\n",
            "Training Iteration 2703, Loss: 6.712666034698486\n",
            "Training Iteration 2704, Loss: 2.462390184402466\n",
            "Training Iteration 2705, Loss: 3.669168710708618\n",
            "Training Iteration 2706, Loss: 3.895139217376709\n",
            "Training Iteration 2707, Loss: 5.606546401977539\n",
            "Training Iteration 2708, Loss: 3.8550751209259033\n",
            "Training Iteration 2709, Loss: 3.741166353225708\n",
            "Training Iteration 2710, Loss: 3.8401546478271484\n",
            "Training Iteration 2711, Loss: 5.824827671051025\n",
            "Training Iteration 2712, Loss: 2.976569652557373\n",
            "Training Iteration 2713, Loss: 2.370743751525879\n",
            "Training Iteration 2714, Loss: 2.1897389888763428\n",
            "Training Iteration 2715, Loss: 3.4385576248168945\n",
            "Training Iteration 2716, Loss: 2.6255550384521484\n",
            "Training Iteration 2717, Loss: 1.3311113119125366\n",
            "Training Iteration 2718, Loss: 7.270374298095703\n",
            "Training Iteration 2719, Loss: 7.673890113830566\n",
            "Training Iteration 2720, Loss: 3.9181301593780518\n",
            "Training Iteration 2721, Loss: 4.741725921630859\n",
            "Training Iteration 2722, Loss: 3.844968318939209\n",
            "Training Iteration 2723, Loss: 7.7478251457214355\n",
            "Training Iteration 2724, Loss: 2.536681652069092\n",
            "Training Iteration 2725, Loss: 6.310461044311523\n",
            "Training Iteration 2726, Loss: 2.1171412467956543\n",
            "Training Iteration 2727, Loss: 4.4390387535095215\n",
            "Training Iteration 2728, Loss: 2.708728075027466\n",
            "Training Iteration 2729, Loss: 6.0447998046875\n",
            "Training Iteration 2730, Loss: 5.14129638671875\n",
            "Training Iteration 2731, Loss: 5.107812404632568\n",
            "Training Iteration 2732, Loss: 5.185690879821777\n",
            "Training Iteration 2733, Loss: 7.064539909362793\n",
            "Training Iteration 2734, Loss: 4.122827529907227\n",
            "Training Iteration 2735, Loss: 8.314764976501465\n",
            "Training Iteration 2736, Loss: 7.541563510894775\n",
            "Training Iteration 2737, Loss: 4.478879451751709\n",
            "Training Iteration 2738, Loss: 3.3967931270599365\n",
            "Training Iteration 2739, Loss: 5.436085224151611\n",
            "Training Iteration 2740, Loss: 2.6991829872131348\n",
            "Training Iteration 2741, Loss: 4.05077600479126\n",
            "Training Iteration 2742, Loss: 4.173959255218506\n",
            "Training Iteration 2743, Loss: 3.861280918121338\n",
            "Training Iteration 2744, Loss: 3.9127793312072754\n",
            "Training Iteration 2745, Loss: 4.579592704772949\n",
            "Training Iteration 2746, Loss: 3.6791839599609375\n",
            "Training Iteration 2747, Loss: 3.4175169467926025\n",
            "Training Iteration 2748, Loss: 2.2913291454315186\n",
            "Training Iteration 2749, Loss: 3.60186767578125\n",
            "Training Iteration 2750, Loss: 2.5335311889648438\n",
            "Training Iteration 2751, Loss: 2.853670358657837\n",
            "Training Iteration 2752, Loss: 4.452385902404785\n",
            "Training Iteration 2753, Loss: 2.914560556411743\n",
            "Training Iteration 2754, Loss: 2.704342842102051\n",
            "Training Iteration 2755, Loss: 5.0400848388671875\n",
            "Training Iteration 2756, Loss: 4.1796417236328125\n",
            "Training Iteration 2757, Loss: 5.530336856842041\n",
            "Training Iteration 2758, Loss: 10.628181457519531\n",
            "Training Iteration 2759, Loss: 6.751439094543457\n",
            "Training Iteration 2760, Loss: 4.369510173797607\n",
            "Training Iteration 2761, Loss: 5.618340969085693\n",
            "Training Iteration 2762, Loss: 5.283859729766846\n",
            "Training Iteration 2763, Loss: 5.4416890144348145\n",
            "Training Iteration 2764, Loss: 4.238897323608398\n",
            "Training Iteration 2765, Loss: 7.815152645111084\n",
            "Training Iteration 2766, Loss: 4.942409992218018\n",
            "Training Iteration 2767, Loss: 4.6307454109191895\n",
            "Training Iteration 2768, Loss: 1.9312366247177124\n",
            "Training Iteration 2769, Loss: 6.293769836425781\n",
            "Training Iteration 2770, Loss: 5.337488651275635\n",
            "Training Iteration 2771, Loss: 4.811646461486816\n",
            "Training Iteration 2772, Loss: 8.33464241027832\n",
            "Training Iteration 2773, Loss: 3.6943225860595703\n",
            "Training Iteration 2774, Loss: 4.12393856048584\n",
            "Training Iteration 2775, Loss: 6.97780704498291\n",
            "Training Iteration 2776, Loss: 10.0449857711792\n",
            "Training Iteration 2777, Loss: 2.4183638095855713\n",
            "Training Iteration 2778, Loss: 6.269661903381348\n",
            "Training Iteration 2779, Loss: 5.993310928344727\n",
            "Training Iteration 2780, Loss: 8.415436744689941\n",
            "Training Iteration 2781, Loss: 4.915190696716309\n",
            "Training Iteration 2782, Loss: 8.935635566711426\n",
            "Training Iteration 2783, Loss: 6.455342769622803\n",
            "Training Iteration 2784, Loss: 4.907215595245361\n",
            "Training Iteration 2785, Loss: 5.601863861083984\n",
            "Training Iteration 2786, Loss: 3.0928215980529785\n",
            "Training Iteration 2787, Loss: 7.390315055847168\n",
            "Training Iteration 2788, Loss: 8.765623092651367\n",
            "Training Iteration 2789, Loss: 6.233456611633301\n",
            "Training Iteration 2790, Loss: 6.661312103271484\n",
            "Training Iteration 2791, Loss: 6.723637580871582\n",
            "Training Iteration 2792, Loss: 6.175882339477539\n",
            "Training Iteration 2793, Loss: 3.9432642459869385\n",
            "Training Iteration 2794, Loss: 5.152935981750488\n",
            "Training Iteration 2795, Loss: 3.466632843017578\n",
            "Training Iteration 2796, Loss: 9.076695442199707\n",
            "Training Iteration 2797, Loss: 7.511399745941162\n",
            "Training Iteration 2798, Loss: 6.672885417938232\n",
            "Training Iteration 2799, Loss: 7.738459587097168\n",
            "Training Iteration 2800, Loss: 6.218595504760742\n",
            "Training Iteration 2801, Loss: 2.6834969520568848\n",
            "Training Iteration 2802, Loss: 4.051495552062988\n",
            "Training Iteration 2803, Loss: 3.265280246734619\n",
            "Training Iteration 2804, Loss: 3.761979341506958\n",
            "Training Iteration 2805, Loss: 5.005712509155273\n",
            "Training Iteration 2806, Loss: 6.472493648529053\n",
            "Training Iteration 2807, Loss: 3.7729415893554688\n",
            "Training Iteration 2808, Loss: 6.399724960327148\n",
            "Training Iteration 2809, Loss: 4.410110950469971\n",
            "Training Iteration 2810, Loss: 4.162083148956299\n",
            "Training Iteration 2811, Loss: 1.937607765197754\n",
            "Training Iteration 2812, Loss: 5.5778489112854\n",
            "Training Iteration 2813, Loss: 2.2372312545776367\n",
            "Training Iteration 2814, Loss: 4.867954254150391\n",
            "Training Iteration 2815, Loss: 6.188547134399414\n",
            "Training Iteration 2816, Loss: 4.4107584953308105\n",
            "Training Iteration 2817, Loss: 3.7570385932922363\n",
            "Training Iteration 2818, Loss: 5.108952045440674\n",
            "Training Iteration 2819, Loss: 4.565346717834473\n",
            "Training Iteration 2820, Loss: 5.056925296783447\n",
            "Training Iteration 2821, Loss: 6.797484874725342\n",
            "Training Iteration 2822, Loss: 4.601219654083252\n",
            "Training Iteration 2823, Loss: 4.861151695251465\n",
            "Training Iteration 2824, Loss: 3.242377281188965\n",
            "Training Iteration 2825, Loss: 3.743252754211426\n",
            "Training Iteration 2826, Loss: 5.204721927642822\n",
            "Training Iteration 2827, Loss: 5.067951202392578\n",
            "Training Iteration 2828, Loss: 3.8025941848754883\n",
            "Training Iteration 2829, Loss: 4.439825534820557\n",
            "Training Iteration 2830, Loss: 5.997807502746582\n",
            "Training Iteration 2831, Loss: 5.755186080932617\n",
            "Training Iteration 2832, Loss: 4.898566722869873\n",
            "Training Iteration 2833, Loss: 8.694092750549316\n",
            "Training Iteration 2834, Loss: 7.094508647918701\n",
            "Training Iteration 2835, Loss: 9.08283519744873\n",
            "Training Iteration 2836, Loss: 2.952934741973877\n",
            "Training Iteration 2837, Loss: 5.2912917137146\n",
            "Training Iteration 2838, Loss: 4.9719133377075195\n",
            "Training Iteration 2839, Loss: 5.514644622802734\n",
            "Training Iteration 2840, Loss: 6.234013557434082\n",
            "Training Iteration 2841, Loss: 4.515446662902832\n",
            "Training Iteration 2842, Loss: 3.3775386810302734\n",
            "Training Iteration 2843, Loss: 4.22114372253418\n",
            "Training Iteration 2844, Loss: 3.258963108062744\n",
            "Training Iteration 2845, Loss: 6.416594505310059\n",
            "Training Iteration 2846, Loss: 2.0750274658203125\n",
            "Training Iteration 2847, Loss: 5.977308750152588\n",
            "Training Iteration 2848, Loss: 6.461118221282959\n",
            "Training Iteration 2849, Loss: 10.44728946685791\n",
            "Training Iteration 2850, Loss: 5.651122093200684\n",
            "Training Iteration 2851, Loss: 4.024712562561035\n",
            "Training Iteration 2852, Loss: 3.2558298110961914\n",
            "Training Iteration 2853, Loss: 4.813529968261719\n",
            "Training Iteration 2854, Loss: 5.475080490112305\n",
            "Training Iteration 2855, Loss: 3.9965710639953613\n",
            "Training Iteration 2856, Loss: 4.093398094177246\n",
            "Training Iteration 2857, Loss: 5.0044050216674805\n",
            "Training Iteration 2858, Loss: 3.1266651153564453\n",
            "Training Iteration 2859, Loss: 8.38062858581543\n",
            "Training Iteration 2860, Loss: 2.4631824493408203\n",
            "Training Iteration 2861, Loss: 3.5900819301605225\n",
            "Training Iteration 2862, Loss: 3.936025857925415\n",
            "Training Iteration 2863, Loss: 3.3579516410827637\n",
            "Training Iteration 2864, Loss: 5.199177265167236\n",
            "Training Iteration 2865, Loss: 3.6864235401153564\n",
            "Training Iteration 2866, Loss: 6.909768581390381\n",
            "Training Iteration 2867, Loss: 5.585988521575928\n",
            "Training Iteration 2868, Loss: 3.5808653831481934\n",
            "Training Iteration 2869, Loss: 2.782341957092285\n",
            "Training Iteration 2870, Loss: 4.363955020904541\n",
            "Training Iteration 2871, Loss: 3.273070812225342\n",
            "Training Iteration 2872, Loss: 5.98965311050415\n",
            "Training Iteration 2873, Loss: 4.3605523109436035\n",
            "Training Iteration 2874, Loss: 4.280458450317383\n",
            "Training Iteration 2875, Loss: 5.447772026062012\n",
            "Training Iteration 2876, Loss: 3.3458268642425537\n",
            "Training Iteration 2877, Loss: 7.103843688964844\n",
            "Training Iteration 2878, Loss: 3.5677127838134766\n",
            "Training Iteration 2879, Loss: 3.7794158458709717\n",
            "Training Iteration 2880, Loss: 3.8446056842803955\n",
            "Training Iteration 2881, Loss: 6.055415630340576\n",
            "Training Iteration 2882, Loss: 8.099091529846191\n",
            "Training Iteration 2883, Loss: 4.2096028327941895\n",
            "Training Iteration 2884, Loss: 3.904451370239258\n",
            "Training Iteration 2885, Loss: 7.59675931930542\n",
            "Training Iteration 2886, Loss: 5.900542259216309\n",
            "Training Iteration 2887, Loss: 4.064481258392334\n",
            "Training Iteration 2888, Loss: 4.344141006469727\n",
            "Training Iteration 2889, Loss: 4.3020477294921875\n",
            "Training Iteration 2890, Loss: 3.164569139480591\n",
            "Training Iteration 2891, Loss: 1.5501974821090698\n",
            "Training Iteration 2892, Loss: 6.028434753417969\n",
            "Training Iteration 2893, Loss: 4.44058895111084\n",
            "Training Iteration 2894, Loss: 4.998926639556885\n",
            "Training Iteration 2895, Loss: 5.24862813949585\n",
            "Training Iteration 2896, Loss: 5.710524082183838\n",
            "Training Iteration 2897, Loss: 6.658840179443359\n",
            "Training Iteration 2898, Loss: 5.330818176269531\n",
            "Training Iteration 2899, Loss: 1.9964243173599243\n",
            "Training Iteration 2900, Loss: 7.660274505615234\n",
            "Training Iteration 2901, Loss: 5.520786285400391\n",
            "Training Iteration 2902, Loss: 3.0932273864746094\n",
            "Training Iteration 2903, Loss: 6.264915943145752\n",
            "Training Iteration 2904, Loss: 3.2708282470703125\n",
            "Training Iteration 2905, Loss: 3.3857264518737793\n",
            "Training Iteration 2906, Loss: 3.642514705657959\n",
            "Training Iteration 2907, Loss: 4.408490180969238\n",
            "Training Iteration 2908, Loss: 8.380926132202148\n",
            "Training Iteration 2909, Loss: 8.40869426727295\n",
            "Training Iteration 2910, Loss: 6.6845011711120605\n",
            "Training Iteration 2911, Loss: 5.545602798461914\n",
            "Training Iteration 2912, Loss: 4.614361763000488\n",
            "Training Iteration 2913, Loss: 3.622558116912842\n",
            "Training Iteration 2914, Loss: 6.843820571899414\n",
            "Training Iteration 2915, Loss: 3.759726047515869\n",
            "Training Iteration 2916, Loss: 3.473615884780884\n",
            "Training Iteration 2917, Loss: 2.4188528060913086\n",
            "Training Iteration 2918, Loss: 2.91352915763855\n",
            "Training Iteration 2919, Loss: 5.416316509246826\n",
            "Training Iteration 2920, Loss: 4.586244106292725\n",
            "Training Iteration 2921, Loss: 4.397228717803955\n",
            "Training Iteration 2922, Loss: 4.914188385009766\n",
            "Training Iteration 2923, Loss: 4.957907199859619\n",
            "Training Iteration 2924, Loss: 2.2805733680725098\n",
            "Training Iteration 2925, Loss: 6.552909851074219\n",
            "Training Iteration 2926, Loss: 5.665469169616699\n",
            "Training Iteration 2927, Loss: 4.153070449829102\n",
            "Training Iteration 2928, Loss: 4.452470302581787\n",
            "Training Iteration 2929, Loss: 4.738591194152832\n",
            "Training Iteration 2930, Loss: 1.899357557296753\n",
            "Training Iteration 2931, Loss: 3.0770325660705566\n",
            "Training Iteration 2932, Loss: 1.1691925525665283\n",
            "Training Iteration 2933, Loss: 4.041048049926758\n",
            "Training Iteration 2934, Loss: 2.019460678100586\n",
            "Training Iteration 2935, Loss: 4.420174598693848\n",
            "Training Iteration 2936, Loss: 3.472801685333252\n",
            "Training Iteration 2937, Loss: 3.690669536590576\n",
            "Training Iteration 2938, Loss: 3.8677921295166016\n",
            "Training Iteration 2939, Loss: 4.054961204528809\n",
            "Training Iteration 2940, Loss: 4.843377113342285\n",
            "Training Iteration 2941, Loss: 4.147113800048828\n",
            "Training Iteration 2942, Loss: 4.414809703826904\n",
            "Training Iteration 2943, Loss: 4.033916473388672\n",
            "Training Iteration 2944, Loss: 3.5440549850463867\n",
            "Training Iteration 2945, Loss: 2.4918606281280518\n",
            "Training Iteration 2946, Loss: 4.658050060272217\n",
            "Training Iteration 2947, Loss: 6.067339897155762\n",
            "Training Iteration 2948, Loss: 3.592581033706665\n",
            "Training Iteration 2949, Loss: 3.5745534896850586\n",
            "Training Iteration 2950, Loss: 4.699762344360352\n",
            "Training Iteration 2951, Loss: 5.407058238983154\n",
            "Training Iteration 2952, Loss: 3.4768149852752686\n",
            "Training Iteration 2953, Loss: 4.659226894378662\n",
            "Training Iteration 2954, Loss: 5.138431549072266\n",
            "Training Iteration 2955, Loss: 4.7466349601745605\n",
            "Training Iteration 2956, Loss: 4.886027812957764\n",
            "Training Iteration 2957, Loss: 2.560514211654663\n",
            "Training Iteration 2958, Loss: 2.5860559940338135\n",
            "Training Iteration 2959, Loss: 4.15030574798584\n",
            "Training Iteration 2960, Loss: 4.440916061401367\n",
            "Training Iteration 2961, Loss: 6.681867599487305\n",
            "Training Iteration 2962, Loss: 6.615916728973389\n",
            "Training Iteration 2963, Loss: 4.6476359367370605\n",
            "Training Iteration 2964, Loss: 7.971884727478027\n",
            "Training Iteration 2965, Loss: 4.7660813331604\n",
            "Training Iteration 2966, Loss: 5.012134552001953\n",
            "Training Iteration 2967, Loss: 3.449249029159546\n",
            "Training Iteration 2968, Loss: 4.016415596008301\n",
            "Training Iteration 2969, Loss: 7.216423034667969\n",
            "Training Iteration 2970, Loss: 3.2898571491241455\n",
            "Training Iteration 2971, Loss: 3.7522757053375244\n",
            "Training Iteration 2972, Loss: 4.375655651092529\n",
            "Training Iteration 2973, Loss: 4.1411285400390625\n",
            "Training Iteration 2974, Loss: 3.4906535148620605\n",
            "Training Iteration 2975, Loss: 4.681732654571533\n",
            "Training Iteration 2976, Loss: 3.514824867248535\n",
            "Training Iteration 2977, Loss: 5.660829544067383\n",
            "Training Iteration 2978, Loss: 5.268380165100098\n",
            "Training Iteration 2979, Loss: 4.7338690757751465\n",
            "Training Iteration 2980, Loss: 3.805636167526245\n",
            "Training Iteration 2981, Loss: 3.8946120738983154\n",
            "Training Iteration 2982, Loss: 2.956798791885376\n",
            "Training Iteration 2983, Loss: 1.9994980096817017\n",
            "Training Iteration 2984, Loss: 3.772404432296753\n",
            "Training Iteration 2985, Loss: 3.7157716751098633\n",
            "Training Iteration 2986, Loss: 7.955556869506836\n",
            "Training Iteration 2987, Loss: 4.0830078125\n",
            "Training Iteration 2988, Loss: 4.457859516143799\n",
            "Training Iteration 2989, Loss: 6.287117004394531\n",
            "Training Iteration 2990, Loss: 6.501168727874756\n",
            "Training Iteration 2991, Loss: 5.998663902282715\n",
            "Training Iteration 2992, Loss: 6.648831844329834\n",
            "Training Iteration 2993, Loss: 4.2752685546875\n",
            "Training Iteration 2994, Loss: 5.637784957885742\n",
            "Training Iteration 2995, Loss: 6.648513317108154\n",
            "Training Iteration 2996, Loss: 4.474306106567383\n",
            "Training Iteration 2997, Loss: 5.765869140625\n",
            "Training Iteration 2998, Loss: 4.669191360473633\n",
            "Training Iteration 2999, Loss: 3.771322727203369\n",
            "Training Iteration 3000, Loss: 5.052593231201172\n",
            "Training Iteration 3001, Loss: 3.542449951171875\n",
            "Training Iteration 3002, Loss: 4.543359756469727\n",
            "Training Iteration 3003, Loss: 3.469749927520752\n",
            "Training Iteration 3004, Loss: 4.1523027420043945\n",
            "Training Iteration 3005, Loss: 6.531255722045898\n",
            "Training Iteration 3006, Loss: 3.0006768703460693\n",
            "Training Iteration 3007, Loss: 4.971996307373047\n",
            "Training Iteration 3008, Loss: 9.018738746643066\n",
            "Training Iteration 3009, Loss: 6.326622009277344\n",
            "Training Iteration 3010, Loss: 5.292025089263916\n",
            "Training Iteration 3011, Loss: 5.456788063049316\n",
            "Training Iteration 3012, Loss: 6.881444931030273\n",
            "Training Iteration 3013, Loss: 9.887341499328613\n",
            "Training Iteration 3014, Loss: 5.166690349578857\n",
            "Training Iteration 3015, Loss: 2.6065776348114014\n",
            "Training Iteration 3016, Loss: 4.520450592041016\n",
            "Training Iteration 3017, Loss: 7.497702121734619\n",
            "Training Iteration 3018, Loss: 4.899139404296875\n",
            "Training Iteration 3019, Loss: 7.247903347015381\n",
            "Training Iteration 3020, Loss: 4.290144443511963\n",
            "Training Iteration 3021, Loss: 3.018876791000366\n",
            "Training Iteration 3022, Loss: 4.929117202758789\n",
            "Training Iteration 3023, Loss: 5.2809977531433105\n",
            "Training Iteration 3024, Loss: 5.993549346923828\n",
            "Training Iteration 3025, Loss: 7.971792697906494\n",
            "Training Iteration 3026, Loss: 3.395024299621582\n",
            "Training Iteration 3027, Loss: 5.091979026794434\n",
            "Training Iteration 3028, Loss: 2.652857542037964\n",
            "Training Iteration 3029, Loss: 5.118495941162109\n",
            "Training Iteration 3030, Loss: 5.070464611053467\n",
            "Training Iteration 3031, Loss: 4.941534996032715\n",
            "Training Iteration 3032, Loss: 5.643260478973389\n",
            "Training Iteration 3033, Loss: 4.919189453125\n",
            "Training Iteration 3034, Loss: 4.546232223510742\n",
            "Training Iteration 3035, Loss: 3.8268814086914062\n",
            "Training Iteration 3036, Loss: 3.4421353340148926\n",
            "Training Iteration 3037, Loss: 4.61852502822876\n",
            "Training Iteration 3038, Loss: 3.3727190494537354\n",
            "Training Iteration 3039, Loss: 3.8440394401550293\n",
            "Training Iteration 3040, Loss: 4.9950947761535645\n",
            "Training Iteration 3041, Loss: 2.4035136699676514\n",
            "Training Iteration 3042, Loss: 4.834979057312012\n",
            "Training Iteration 3043, Loss: 7.6752238273620605\n",
            "Training Iteration 3044, Loss: 4.081782817840576\n",
            "Training Iteration 3045, Loss: 4.458384037017822\n",
            "Training Iteration 3046, Loss: 3.746293306350708\n",
            "Training Iteration 3047, Loss: 4.194260597229004\n",
            "Training Iteration 3048, Loss: 6.04093074798584\n",
            "Training Iteration 3049, Loss: 2.4727942943573\n",
            "Training Iteration 3050, Loss: 5.707187175750732\n",
            "Training Iteration 3051, Loss: 3.7001354694366455\n",
            "Training Iteration 3052, Loss: 3.7642030715942383\n",
            "Training Iteration 3053, Loss: 2.833540439605713\n",
            "Training Iteration 3054, Loss: 3.7888216972351074\n",
            "Training Iteration 3055, Loss: 5.312576770782471\n",
            "Training Iteration 3056, Loss: 4.842231750488281\n",
            "Training Iteration 3057, Loss: 4.728257656097412\n",
            "Training Iteration 3058, Loss: 4.493984699249268\n",
            "Training Iteration 3059, Loss: 2.4153473377227783\n",
            "Training Iteration 3060, Loss: 3.456817626953125\n",
            "Training Iteration 3061, Loss: 2.3431222438812256\n",
            "Training Iteration 3062, Loss: 4.268458366394043\n",
            "Training Iteration 3063, Loss: 5.890911102294922\n",
            "Training Iteration 3064, Loss: 3.7194600105285645\n",
            "Training Iteration 3065, Loss: 2.8408424854278564\n",
            "Training Iteration 3066, Loss: 2.156841516494751\n",
            "Training Iteration 3067, Loss: 2.034196615219116\n",
            "Training Iteration 3068, Loss: 5.12625789642334\n",
            "Training Iteration 3069, Loss: 2.70596981048584\n",
            "Training Iteration 3070, Loss: 2.280823230743408\n",
            "Training Iteration 3071, Loss: 3.9135971069335938\n",
            "Training Iteration 3072, Loss: 4.317322731018066\n",
            "Training Iteration 3073, Loss: 2.523900032043457\n",
            "Training Iteration 3074, Loss: 6.042061805725098\n",
            "Training Iteration 3075, Loss: 5.486764907836914\n",
            "Training Iteration 3076, Loss: 3.1001296043395996\n",
            "Training Iteration 3077, Loss: 6.175545692443848\n",
            "Training Iteration 3078, Loss: 4.2368483543396\n",
            "Training Iteration 3079, Loss: 5.562757968902588\n",
            "Training Iteration 3080, Loss: 3.827579975128174\n",
            "Training Iteration 3081, Loss: 3.1786108016967773\n",
            "Training Iteration 3082, Loss: 5.443774700164795\n",
            "Training Iteration 3083, Loss: 5.771893501281738\n",
            "Training Iteration 3084, Loss: 2.0544285774230957\n",
            "Training Iteration 3085, Loss: 3.9517409801483154\n",
            "Training Iteration 3086, Loss: 3.6433510780334473\n",
            "Training Iteration 3087, Loss: 6.277552604675293\n",
            "Training Iteration 3088, Loss: 5.216872692108154\n",
            "Training Iteration 3089, Loss: 6.515100002288818\n",
            "Training Iteration 3090, Loss: 6.734443187713623\n",
            "Training Iteration 3091, Loss: 7.733217239379883\n",
            "Training Iteration 3092, Loss: 4.492948532104492\n",
            "Training Iteration 3093, Loss: 7.184490203857422\n",
            "Training Iteration 3094, Loss: 3.56754994392395\n",
            "Training Iteration 3095, Loss: 5.943813800811768\n",
            "Training Iteration 3096, Loss: 2.5916965007781982\n",
            "Training Iteration 3097, Loss: 2.1643238067626953\n",
            "Training Iteration 3098, Loss: 2.5327253341674805\n",
            "Training Iteration 3099, Loss: 6.420976638793945\n",
            "Training Iteration 3100, Loss: 7.509036540985107\n",
            "Training Iteration 3101, Loss: 5.009244918823242\n",
            "Training Iteration 3102, Loss: 5.51239013671875\n",
            "Training Iteration 3103, Loss: 8.944426536560059\n",
            "Training Iteration 3104, Loss: 10.000995635986328\n",
            "Training Iteration 3105, Loss: 5.2011027336120605\n",
            "Training Iteration 3106, Loss: 5.3359761238098145\n",
            "Training Iteration 3107, Loss: 5.86303186416626\n",
            "Training Iteration 3108, Loss: 3.9037277698516846\n",
            "Training Iteration 3109, Loss: 5.688098907470703\n",
            "Training Iteration 3110, Loss: 4.434422492980957\n",
            "Training Iteration 3111, Loss: 7.459500312805176\n",
            "Training Iteration 3112, Loss: 5.630221843719482\n",
            "Training Iteration 3113, Loss: 3.3683245182037354\n",
            "Training Iteration 3114, Loss: 5.442740440368652\n",
            "Training Iteration 3115, Loss: 5.053828239440918\n",
            "Training Iteration 3116, Loss: 4.501953601837158\n",
            "Training Iteration 3117, Loss: 5.5016984939575195\n",
            "Training Iteration 3118, Loss: 9.000055313110352\n",
            "Training Iteration 3119, Loss: 7.306190490722656\n",
            "Training Iteration 3120, Loss: 6.851346015930176\n",
            "Training Iteration 3121, Loss: 1.3695652484893799\n",
            "Training Iteration 3122, Loss: 8.789959907531738\n",
            "Training Iteration 3123, Loss: 9.462648391723633\n",
            "Training Iteration 3124, Loss: 7.340879917144775\n",
            "Training Iteration 3125, Loss: 3.813612222671509\n",
            "Training Iteration 3126, Loss: 6.841624736785889\n",
            "Training Iteration 3127, Loss: 6.431262016296387\n",
            "Training Iteration 3128, Loss: 4.119983673095703\n",
            "Training Iteration 3129, Loss: 4.818456649780273\n",
            "Training Iteration 3130, Loss: 4.2591776847839355\n",
            "Training Iteration 3131, Loss: 5.644557952880859\n",
            "Training Iteration 3132, Loss: 7.0420989990234375\n",
            "Training Iteration 3133, Loss: 3.9632582664489746\n",
            "Training Iteration 3134, Loss: 7.048274517059326\n",
            "Training Iteration 3135, Loss: 2.7421324253082275\n",
            "Training Iteration 3136, Loss: 4.970661163330078\n",
            "Training Iteration 3137, Loss: 5.267627716064453\n",
            "Training Iteration 3138, Loss: 7.448324203491211\n",
            "Training Iteration 3139, Loss: 7.43093204498291\n",
            "Training Iteration 3140, Loss: 5.227227210998535\n",
            "Training Iteration 3141, Loss: 3.2169606685638428\n",
            "Training Iteration 3142, Loss: 2.2108333110809326\n",
            "Training Iteration 3143, Loss: 7.43204402923584\n",
            "Training Iteration 3144, Loss: 3.7039921283721924\n",
            "Training Iteration 3145, Loss: 7.204765319824219\n",
            "Training Iteration 3146, Loss: 7.293071746826172\n",
            "Training Iteration 3147, Loss: 4.506684303283691\n",
            "Training Iteration 3148, Loss: 5.881087303161621\n",
            "Training Iteration 3149, Loss: 3.169173240661621\n",
            "Training Iteration 3150, Loss: 4.853485107421875\n",
            "Training Iteration 3151, Loss: 4.074008941650391\n",
            "Training Iteration 3152, Loss: 4.693493366241455\n",
            "Training Iteration 3153, Loss: 2.7086212635040283\n",
            "Training Iteration 3154, Loss: 5.080256462097168\n",
            "Training Iteration 3155, Loss: 3.2284932136535645\n",
            "Training Iteration 3156, Loss: 4.09298849105835\n",
            "Training Iteration 3157, Loss: 2.3856041431427\n",
            "Training Iteration 3158, Loss: 2.410130739212036\n",
            "Training Iteration 3159, Loss: 4.053648948669434\n",
            "Training Iteration 3160, Loss: 6.0220723152160645\n",
            "Training Iteration 3161, Loss: 3.6263656616210938\n",
            "Training Iteration 3162, Loss: 4.941161632537842\n",
            "Training Iteration 3163, Loss: 7.551217555999756\n",
            "Training Iteration 3164, Loss: 4.941225051879883\n",
            "Training Iteration 3165, Loss: 4.328009128570557\n",
            "Training Iteration 3166, Loss: 2.996901273727417\n",
            "Training Iteration 3167, Loss: 8.660173416137695\n",
            "Training Iteration 3168, Loss: 5.515852451324463\n",
            "Training Iteration 3169, Loss: 6.23211145401001\n",
            "Training Iteration 3170, Loss: 6.075506210327148\n",
            "Training Iteration 3171, Loss: 4.579591751098633\n",
            "Training Iteration 3172, Loss: 6.2349629402160645\n",
            "Training Iteration 3173, Loss: 2.488049030303955\n",
            "Training Iteration 3174, Loss: 3.6873199939727783\n",
            "Training Iteration 3175, Loss: 5.934804916381836\n",
            "Training Iteration 3176, Loss: 3.6913363933563232\n",
            "Training Iteration 3177, Loss: 6.586371898651123\n",
            "Training Iteration 3178, Loss: 1.6005890369415283\n",
            "Training Iteration 3179, Loss: 8.950175285339355\n",
            "Training Iteration 3180, Loss: 4.277514934539795\n",
            "Training Iteration 3181, Loss: 3.271000862121582\n",
            "Training Iteration 3182, Loss: 7.209597587585449\n",
            "Training Iteration 3183, Loss: 4.232359409332275\n",
            "Training Iteration 3184, Loss: 3.1517655849456787\n",
            "Training Iteration 3185, Loss: 4.121830463409424\n",
            "Training Iteration 3186, Loss: 4.14250373840332\n",
            "Training Iteration 3187, Loss: 5.387759685516357\n",
            "Training Iteration 3188, Loss: 4.243496894836426\n",
            "Training Iteration 3189, Loss: 7.885565757751465\n",
            "Training Iteration 3190, Loss: 9.696733474731445\n",
            "Training Iteration 3191, Loss: 7.628498077392578\n",
            "Training Iteration 3192, Loss: 6.2971601486206055\n",
            "Training Iteration 3193, Loss: 7.453155517578125\n",
            "Training Iteration 3194, Loss: 4.35978889465332\n",
            "Training Iteration 3195, Loss: 4.1058030128479\n",
            "Training Iteration 3196, Loss: 6.244486331939697\n",
            "Training Iteration 3197, Loss: 4.192304611206055\n",
            "Training Iteration 3198, Loss: 5.408599376678467\n",
            "Training Iteration 3199, Loss: 2.8743057250976562\n",
            "Training Iteration 3200, Loss: 10.512154579162598\n",
            "Training Iteration 3201, Loss: 6.48398494720459\n",
            "Training Iteration 3202, Loss: 8.96957778930664\n",
            "Training Iteration 3203, Loss: 5.797147274017334\n",
            "Training Iteration 3204, Loss: 4.751152038574219\n",
            "Training Iteration 3205, Loss: 4.9463419914245605\n",
            "Training Iteration 3206, Loss: 2.8798651695251465\n",
            "Training Iteration 3207, Loss: 3.741899013519287\n",
            "Training Iteration 3208, Loss: 5.102281093597412\n",
            "Training Iteration 3209, Loss: 6.807492256164551\n",
            "Training Iteration 3210, Loss: 3.0500688552856445\n",
            "Training Iteration 3211, Loss: 7.72579288482666\n",
            "Training Iteration 3212, Loss: 14.01121711730957\n",
            "Training Iteration 3213, Loss: 3.905710220336914\n",
            "Training Iteration 3214, Loss: 2.9910638332366943\n",
            "Training Iteration 3215, Loss: 5.10392427444458\n",
            "Training Iteration 3216, Loss: 4.641244411468506\n",
            "Training Iteration 3217, Loss: 3.9544763565063477\n",
            "Training Iteration 3218, Loss: 2.159945011138916\n",
            "Training Iteration 3219, Loss: 2.7190122604370117\n",
            "Training Iteration 3220, Loss: 5.669027805328369\n",
            "Training Iteration 3221, Loss: 7.29203462600708\n",
            "Training Iteration 3222, Loss: 7.877586364746094\n",
            "Training Iteration 3223, Loss: 3.817077875137329\n",
            "Training Iteration 3224, Loss: 3.7406816482543945\n",
            "Training Iteration 3225, Loss: 7.656730651855469\n",
            "Training Iteration 3226, Loss: 3.482325553894043\n",
            "Training Iteration 3227, Loss: 4.685615539550781\n",
            "Training Iteration 3228, Loss: 5.270692825317383\n",
            "Training Iteration 3229, Loss: 4.720938682556152\n",
            "Training Iteration 3230, Loss: 3.8898658752441406\n",
            "Training Iteration 3231, Loss: 3.3865299224853516\n",
            "Training Iteration 3232, Loss: 3.7764840126037598\n",
            "Training Iteration 3233, Loss: 3.0166733264923096\n",
            "Training Iteration 3234, Loss: 5.397829532623291\n",
            "Training Iteration 3235, Loss: 3.4913251399993896\n",
            "Training Iteration 3236, Loss: 4.600074768066406\n",
            "Training Iteration 3237, Loss: 3.3636527061462402\n",
            "Training Iteration 3238, Loss: 6.108170509338379\n",
            "Training Iteration 3239, Loss: 4.435693740844727\n",
            "Training Iteration 3240, Loss: 4.060978889465332\n",
            "Training Iteration 3241, Loss: 4.540340423583984\n",
            "Training Iteration 3242, Loss: 6.770267486572266\n",
            "Training Iteration 3243, Loss: 7.68185567855835\n",
            "Training Iteration 3244, Loss: 6.391226768493652\n",
            "Training Iteration 3245, Loss: 6.60317850112915\n",
            "Training Iteration 3246, Loss: 4.7738566398620605\n",
            "Training Iteration 3247, Loss: 3.9498801231384277\n",
            "Training Iteration 3248, Loss: 4.7407965660095215\n",
            "Training Iteration 3249, Loss: 9.802142143249512\n",
            "Training Iteration 3250, Loss: 5.667516708374023\n",
            "Training Iteration 3251, Loss: 5.097752571105957\n",
            "Training Iteration 3252, Loss: 4.985640048980713\n",
            "Training Iteration 3253, Loss: 3.7350471019744873\n",
            "Training Iteration 3254, Loss: 3.9273438453674316\n",
            "Training Iteration 3255, Loss: 2.0887928009033203\n",
            "Training Iteration 3256, Loss: 3.9833099842071533\n",
            "Training Iteration 3257, Loss: 3.3227884769439697\n",
            "Training Iteration 3258, Loss: 6.543212890625\n",
            "Training Iteration 3259, Loss: 4.093381404876709\n",
            "Training Iteration 3260, Loss: 4.927507400512695\n",
            "Training Iteration 3261, Loss: 3.404184341430664\n",
            "Training Iteration 3262, Loss: 3.1401479244232178\n",
            "Training Iteration 3263, Loss: 3.8894808292388916\n",
            "Training Iteration 3264, Loss: 5.239041805267334\n",
            "Training Iteration 3265, Loss: 4.671353816986084\n",
            "Training Iteration 3266, Loss: 4.491320610046387\n",
            "Training Iteration 3267, Loss: 1.997758150100708\n",
            "Training Iteration 3268, Loss: 3.988008737564087\n",
            "Training Iteration 3269, Loss: 8.895102500915527\n",
            "Training Iteration 3270, Loss: 3.7859411239624023\n",
            "Training Iteration 3271, Loss: 2.4227356910705566\n",
            "Training Iteration 3272, Loss: 2.9529356956481934\n",
            "Training Iteration 3273, Loss: 4.794417858123779\n",
            "Training Iteration 3274, Loss: 2.889394760131836\n",
            "Training Iteration 3275, Loss: 4.593928337097168\n",
            "Training Iteration 3276, Loss: 3.941343069076538\n",
            "Training Iteration 3277, Loss: 6.299767017364502\n",
            "Training Iteration 3278, Loss: 0.6870759725570679\n",
            "Training Iteration 3279, Loss: 6.696952819824219\n",
            "Training Iteration 3280, Loss: 5.055641174316406\n",
            "Training Iteration 3281, Loss: 1.4268996715545654\n",
            "Training Iteration 3282, Loss: 3.7849764823913574\n",
            "Training Iteration 3283, Loss: 5.050468921661377\n",
            "Training Iteration 3284, Loss: 5.609674453735352\n",
            "Training Iteration 3285, Loss: 5.273684501647949\n",
            "Training Iteration 3286, Loss: 5.492565631866455\n",
            "Training Iteration 3287, Loss: 6.661925792694092\n",
            "Training Iteration 3288, Loss: 5.3774590492248535\n",
            "Training Iteration 3289, Loss: 7.0988688468933105\n",
            "Training Iteration 3290, Loss: 3.2890267372131348\n",
            "Training Iteration 3291, Loss: 0.843474805355072\n",
            "Training Iteration 3292, Loss: 4.314852237701416\n",
            "Training Iteration 3293, Loss: 3.639970302581787\n",
            "Training Iteration 3294, Loss: 2.9059736728668213\n",
            "Training Iteration 3295, Loss: 6.534344673156738\n",
            "Training Iteration 3296, Loss: 5.404074192047119\n",
            "Training Iteration 3297, Loss: 5.724175930023193\n",
            "Training Iteration 3298, Loss: 4.371389389038086\n",
            "Training Iteration 3299, Loss: 5.7260589599609375\n",
            "Training Iteration 3300, Loss: 5.272541046142578\n",
            "Training Iteration 3301, Loss: 3.398800849914551\n",
            "Training Iteration 3302, Loss: 2.241753339767456\n",
            "Training Iteration 3303, Loss: 2.513911008834839\n",
            "Training Iteration 3304, Loss: 4.1579909324646\n",
            "Training Iteration 3305, Loss: 9.234474182128906\n",
            "Training Iteration 3306, Loss: 2.6685893535614014\n",
            "Training Iteration 3307, Loss: 4.789045810699463\n",
            "Training Iteration 3308, Loss: 4.7363152503967285\n",
            "Training Iteration 3309, Loss: 4.847860336303711\n",
            "Training Iteration 3310, Loss: 4.392766952514648\n",
            "Training Iteration 3311, Loss: 4.5161309242248535\n",
            "Training Iteration 3312, Loss: 4.947336196899414\n",
            "Training Iteration 3313, Loss: 5.575249671936035\n",
            "Training Iteration 3314, Loss: 4.058896541595459\n",
            "Training Iteration 3315, Loss: 5.546630382537842\n",
            "Training Iteration 3316, Loss: 2.0378029346466064\n",
            "Training Iteration 3317, Loss: 3.456097364425659\n",
            "Training Iteration 3318, Loss: 4.3376898765563965\n",
            "Training Iteration 3319, Loss: 2.631126642227173\n",
            "Training Iteration 3320, Loss: 5.124413013458252\n",
            "Training Iteration 3321, Loss: 6.477032661437988\n",
            "Training Iteration 3322, Loss: 3.251127004623413\n",
            "Training Iteration 3323, Loss: 5.479582786560059\n",
            "Training Iteration 3324, Loss: 4.268670558929443\n",
            "Training Iteration 3325, Loss: 5.014544486999512\n",
            "Training Iteration 3326, Loss: 3.4655802249908447\n",
            "Training Iteration 3327, Loss: 3.249682903289795\n",
            "Training Iteration 3328, Loss: 2.43605375289917\n",
            "Training Iteration 3329, Loss: 5.059525966644287\n",
            "Training Iteration 3330, Loss: 5.371545791625977\n",
            "Training Iteration 3331, Loss: 4.0256476402282715\n",
            "Training Iteration 3332, Loss: 2.595127820968628\n",
            "Training Iteration 3333, Loss: 5.3577728271484375\n",
            "Training Iteration 3334, Loss: 2.310845375061035\n",
            "Training Iteration 3335, Loss: 3.835115909576416\n",
            "Training Iteration 3336, Loss: 3.69099497795105\n",
            "Training Iteration 3337, Loss: 4.107399940490723\n",
            "Training Iteration 3338, Loss: 2.5015597343444824\n",
            "Training Iteration 3339, Loss: 4.612305641174316\n",
            "Training Iteration 3340, Loss: 3.9222164154052734\n",
            "Training Iteration 3341, Loss: 4.873183727264404\n",
            "Training Iteration 3342, Loss: 8.447148323059082\n",
            "Training Iteration 3343, Loss: 8.053155899047852\n",
            "Training Iteration 3344, Loss: 4.235335350036621\n",
            "Training Iteration 3345, Loss: 7.446334362030029\n",
            "Training Iteration 3346, Loss: 8.67639446258545\n",
            "Training Iteration 3347, Loss: 2.1376595497131348\n",
            "Training Iteration 3348, Loss: 5.273385047912598\n",
            "Training Iteration 3349, Loss: 3.737657308578491\n",
            "Training Iteration 3350, Loss: 7.051068305969238\n",
            "Training Iteration 3351, Loss: 6.545527935028076\n",
            "Training Iteration 3352, Loss: 3.839318037033081\n",
            "Training Iteration 3353, Loss: 6.731357097625732\n",
            "Training Iteration 3354, Loss: 5.329902648925781\n",
            "Training Iteration 3355, Loss: 6.577847480773926\n",
            "Training Iteration 3356, Loss: 4.747283935546875\n",
            "Training Iteration 3357, Loss: 7.062526702880859\n",
            "Training Iteration 3358, Loss: 2.7992348670959473\n",
            "Training Iteration 3359, Loss: 0.3298172950744629\n",
            "Training Iteration 3360, Loss: 3.9161324501037598\n",
            "Training Iteration 3361, Loss: 6.017017364501953\n",
            "Training Iteration 3362, Loss: 3.9531755447387695\n",
            "Training Iteration 3363, Loss: 7.1110968589782715\n",
            "Training Iteration 3364, Loss: 3.3651649951934814\n",
            "Training Iteration 3365, Loss: 4.421010971069336\n",
            "Training Iteration 3366, Loss: 4.612817764282227\n",
            "Training Iteration 3367, Loss: 2.914212942123413\n",
            "Training Iteration 3368, Loss: 5.094010829925537\n",
            "Training Iteration 3369, Loss: 3.551806926727295\n",
            "Training Iteration 3370, Loss: 2.9237470626831055\n",
            "Training Iteration 3371, Loss: 5.676004886627197\n",
            "Training Iteration 3372, Loss: 5.4217987060546875\n",
            "Training Iteration 3373, Loss: 6.082095146179199\n",
            "Training Iteration 3374, Loss: 5.899232387542725\n",
            "Training Iteration 3375, Loss: 5.379386901855469\n",
            "Training Iteration 3376, Loss: 4.014111042022705\n",
            "Training Iteration 3377, Loss: 2.5962605476379395\n",
            "Training Iteration 3378, Loss: 2.0544698238372803\n",
            "Training Iteration 3379, Loss: 3.3787214756011963\n",
            "Training Iteration 3380, Loss: 3.3004775047302246\n",
            "Training Iteration 3381, Loss: 4.567324161529541\n",
            "Training Iteration 3382, Loss: 5.195036888122559\n",
            "Training Iteration 3383, Loss: 6.800409317016602\n",
            "Training Iteration 3384, Loss: 4.688288688659668\n",
            "Training Iteration 3385, Loss: 4.657415866851807\n",
            "Training Iteration 3386, Loss: 4.260908126831055\n",
            "Training Iteration 3387, Loss: 1.1872293949127197\n",
            "Training Iteration 3388, Loss: 4.088597297668457\n",
            "Training Iteration 3389, Loss: 4.333465576171875\n",
            "Training Iteration 3390, Loss: 4.337490081787109\n",
            "Training Iteration 3391, Loss: 7.938511848449707\n",
            "Training Iteration 3392, Loss: 2.21901798248291\n",
            "Training Iteration 3393, Loss: 2.4445765018463135\n",
            "Training Iteration 3394, Loss: 3.9799647331237793\n",
            "Training Iteration 3395, Loss: 3.727585554122925\n",
            "Training Iteration 3396, Loss: 5.170259952545166\n",
            "Training Iteration 3397, Loss: 5.282594680786133\n",
            "Training Iteration 3398, Loss: 4.945709228515625\n",
            "Training Iteration 3399, Loss: 2.895313262939453\n",
            "Training Iteration 3400, Loss: 4.4518723487854\n",
            "Training Iteration 3401, Loss: 2.8500545024871826\n",
            "Training Iteration 3402, Loss: 5.544403076171875\n",
            "Training Iteration 3403, Loss: 2.342817544937134\n",
            "Training Iteration 3404, Loss: 5.5802998542785645\n",
            "Training Iteration 3405, Loss: 3.1239588260650635\n",
            "Training Iteration 3406, Loss: 6.263629913330078\n",
            "Training Iteration 3407, Loss: 4.080965995788574\n",
            "Training Iteration 3408, Loss: 4.186607837677002\n",
            "Training Iteration 3409, Loss: 3.9483373165130615\n",
            "Training Iteration 3410, Loss: 3.2696728706359863\n",
            "Training Iteration 3411, Loss: 6.5859198570251465\n",
            "Training Iteration 3412, Loss: 3.2147672176361084\n",
            "Training Iteration 3413, Loss: 4.205019950866699\n",
            "Training Iteration 3414, Loss: 5.401983261108398\n",
            "Training Iteration 3415, Loss: 1.9095587730407715\n",
            "Training Iteration 3416, Loss: 3.077531337738037\n",
            "Training Iteration 3417, Loss: 2.579482316970825\n",
            "Training Iteration 3418, Loss: 3.4764492511749268\n",
            "Training Iteration 3419, Loss: 5.794035911560059\n",
            "Training Iteration 3420, Loss: 5.334682464599609\n",
            "Training Iteration 3421, Loss: 5.4127092361450195\n",
            "Training Iteration 3422, Loss: 7.256778717041016\n",
            "Training Iteration 3423, Loss: 3.5161004066467285\n",
            "Training Iteration 3424, Loss: 3.895220994949341\n",
            "Training Iteration 3425, Loss: 1.638578176498413\n",
            "Training Iteration 3426, Loss: 3.7941153049468994\n",
            "Training Iteration 3427, Loss: 5.641878128051758\n",
            "Training Iteration 3428, Loss: 5.3699164390563965\n",
            "Training Iteration 3429, Loss: 2.7499608993530273\n",
            "Training Iteration 3430, Loss: 3.152463674545288\n",
            "Training Iteration 3431, Loss: 4.252885818481445\n",
            "Training Iteration 3432, Loss: 5.040396213531494\n",
            "Training Iteration 3433, Loss: 2.77717924118042\n",
            "Training Iteration 3434, Loss: 3.980189085006714\n",
            "Training Iteration 3435, Loss: 5.18431282043457\n",
            "Training Iteration 3436, Loss: 7.154635906219482\n",
            "Training Iteration 3437, Loss: 3.9216694831848145\n",
            "Training Iteration 3438, Loss: 2.3482162952423096\n",
            "Training Iteration 3439, Loss: 3.520704746246338\n",
            "Training Iteration 3440, Loss: 4.867396354675293\n",
            "Training Iteration 3441, Loss: 3.6587278842926025\n",
            "Training Iteration 3442, Loss: 6.0082197189331055\n",
            "Training Iteration 3443, Loss: 6.225502967834473\n",
            "Training Iteration 3444, Loss: 3.5129237174987793\n",
            "Training Iteration 3445, Loss: 5.336182117462158\n",
            "Training Iteration 3446, Loss: 7.390384674072266\n",
            "Training Iteration 3447, Loss: 5.113367080688477\n",
            "Training Iteration 3448, Loss: 3.638737201690674\n",
            "Training Iteration 3449, Loss: 4.437073707580566\n",
            "Training Iteration 3450, Loss: 5.6219048500061035\n",
            "Training Iteration 3451, Loss: 5.873676300048828\n",
            "Training Iteration 3452, Loss: 3.63364839553833\n",
            "Training Iteration 3453, Loss: 3.800257682800293\n",
            "Training Iteration 3454, Loss: 4.911947250366211\n",
            "Training Iteration 3455, Loss: 2.9038054943084717\n",
            "Training Iteration 3456, Loss: 3.7641091346740723\n",
            "Training Iteration 3457, Loss: 2.249488592147827\n",
            "Training Iteration 3458, Loss: 2.7886242866516113\n",
            "Training Iteration 3459, Loss: 3.725320339202881\n",
            "Training Iteration 3460, Loss: 3.655195713043213\n",
            "Training Iteration 3461, Loss: 3.6650636196136475\n",
            "Training Iteration 3462, Loss: 5.651511192321777\n",
            "Training Iteration 3463, Loss: 4.707269191741943\n",
            "Training Iteration 3464, Loss: 4.952760219573975\n",
            "Training Iteration 3465, Loss: 2.3405702114105225\n",
            "Training Iteration 3466, Loss: 3.3133127689361572\n",
            "Training Iteration 3467, Loss: 1.7849986553192139\n",
            "Training Iteration 3468, Loss: 7.493498802185059\n",
            "Training Iteration 3469, Loss: 3.1026384830474854\n",
            "Training Iteration 3470, Loss: 3.7974438667297363\n",
            "Training Iteration 3471, Loss: 6.416407108306885\n",
            "Training Iteration 3472, Loss: 5.381269454956055\n",
            "Training Iteration 3473, Loss: 3.6963107585906982\n",
            "Training Iteration 3474, Loss: 6.118438720703125\n",
            "Training Iteration 3475, Loss: 6.235018253326416\n",
            "Training Iteration 3476, Loss: 7.28447151184082\n",
            "Training Iteration 3477, Loss: 3.7257702350616455\n",
            "Training Iteration 3478, Loss: 2.4249188899993896\n",
            "Training Iteration 3479, Loss: 6.255372047424316\n",
            "Training Iteration 3480, Loss: 4.197800636291504\n",
            "Training Iteration 3481, Loss: 2.2390003204345703\n",
            "Training Iteration 3482, Loss: 4.891017436981201\n",
            "Training Iteration 3483, Loss: 3.1396284103393555\n",
            "Training Iteration 3484, Loss: 3.153348445892334\n",
            "Training Iteration 3485, Loss: 2.56954026222229\n",
            "Training Iteration 3486, Loss: 4.41187047958374\n",
            "Training Iteration 3487, Loss: 5.791101455688477\n",
            "Training Iteration 3488, Loss: 3.174494981765747\n",
            "Training Iteration 3489, Loss: 5.680959701538086\n",
            "Training Iteration 3490, Loss: 4.550726413726807\n",
            "Training Iteration 3491, Loss: 5.598946571350098\n",
            "Training Iteration 3492, Loss: 1.5715001821517944\n",
            "Training Iteration 3493, Loss: 4.932013034820557\n",
            "Training Iteration 3494, Loss: 4.310189247131348\n",
            "Training Iteration 3495, Loss: 1.8516730070114136\n",
            "Training Iteration 3496, Loss: 3.7922744750976562\n",
            "Training Iteration 3497, Loss: 3.876845121383667\n",
            "Training Iteration 3498, Loss: 6.240018367767334\n",
            "Training Iteration 3499, Loss: 1.9743781089782715\n",
            "Training Iteration 3500, Loss: 3.877272844314575\n",
            "Training Iteration 3501, Loss: 3.6409175395965576\n",
            "Training Iteration 3502, Loss: 3.234536647796631\n",
            "Training Iteration 3503, Loss: 4.627113342285156\n",
            "Training Iteration 3504, Loss: 5.077630519866943\n",
            "Training Iteration 3505, Loss: 10.369941711425781\n",
            "Training Iteration 3506, Loss: 3.070634603500366\n",
            "Training Iteration 3507, Loss: 2.1411309242248535\n",
            "Training Iteration 3508, Loss: 4.02034854888916\n",
            "Training Iteration 3509, Loss: 4.999351978302002\n",
            "Training Iteration 3510, Loss: 7.214698791503906\n",
            "Training Iteration 3511, Loss: 4.720569133758545\n",
            "Training Iteration 3512, Loss: 4.173815727233887\n",
            "Training Iteration 3513, Loss: 5.608973026275635\n",
            "Training Iteration 3514, Loss: 2.0754342079162598\n",
            "Training Iteration 3515, Loss: 7.100276947021484\n",
            "Training Iteration 3516, Loss: 2.8114824295043945\n",
            "Training Iteration 3517, Loss: 5.405492782592773\n",
            "Training Iteration 3518, Loss: 6.725722312927246\n",
            "Training Iteration 3519, Loss: 6.623936176300049\n",
            "Training Iteration 3520, Loss: 3.9155972003936768\n",
            "Training Iteration 3521, Loss: 5.6363606452941895\n",
            "Training Iteration 3522, Loss: 6.58640718460083\n",
            "Training Iteration 3523, Loss: 3.780378818511963\n",
            "Training Iteration 3524, Loss: 3.0918445587158203\n",
            "Training Iteration 3525, Loss: 1.323622226715088\n",
            "Training Iteration 3526, Loss: 4.939493656158447\n",
            "Training Iteration 3527, Loss: 5.171029090881348\n",
            "Training Iteration 3528, Loss: 4.3715386390686035\n",
            "Training Iteration 3529, Loss: 4.404369354248047\n",
            "Training Iteration 3530, Loss: 8.812722206115723\n",
            "Training Iteration 3531, Loss: 8.344029426574707\n",
            "Training Iteration 3532, Loss: 6.470279693603516\n",
            "Training Iteration 3533, Loss: 1.4376198053359985\n",
            "Training Iteration 3534, Loss: 5.062880992889404\n",
            "Training Iteration 3535, Loss: 3.272700786590576\n",
            "Training Iteration 3536, Loss: 6.10896635055542\n",
            "Training Iteration 3537, Loss: 8.152731895446777\n",
            "Training Iteration 3538, Loss: 6.469825744628906\n",
            "Training Iteration 3539, Loss: 2.6938819885253906\n",
            "Training Iteration 3540, Loss: 3.725983142852783\n",
            "Training Iteration 3541, Loss: 4.214583396911621\n",
            "Training Iteration 3542, Loss: 3.961056709289551\n",
            "Training Iteration 3543, Loss: 2.6785082817077637\n",
            "Training Iteration 3544, Loss: 3.2294607162475586\n",
            "Training Iteration 3545, Loss: 2.0710437297821045\n",
            "Training Iteration 3546, Loss: 3.1793551445007324\n",
            "Training Iteration 3547, Loss: 3.089972972869873\n",
            "Training Iteration 3548, Loss: 2.8806614875793457\n",
            "Training Iteration 3549, Loss: 3.0760157108306885\n",
            "Training Iteration 3550, Loss: 5.5637526512146\n",
            "Training Iteration 3551, Loss: 2.9919962882995605\n",
            "Training Iteration 3552, Loss: 3.0024874210357666\n",
            "Training Iteration 3553, Loss: 4.2905402183532715\n",
            "Training Iteration 3554, Loss: 4.238619804382324\n",
            "Training Iteration 3555, Loss: 3.527240753173828\n",
            "Training Iteration 3556, Loss: 4.886904716491699\n",
            "Training Iteration 3557, Loss: 5.391071319580078\n",
            "Training Iteration 3558, Loss: 4.650993347167969\n",
            "Training Iteration 3559, Loss: 2.222174644470215\n",
            "Training Iteration 3560, Loss: 5.618969440460205\n",
            "Training Iteration 3561, Loss: 4.828895568847656\n",
            "Training Iteration 3562, Loss: 5.373991966247559\n",
            "Training Iteration 3563, Loss: 4.904766082763672\n",
            "Training Iteration 3564, Loss: 3.961519479751587\n",
            "Training Iteration 3565, Loss: 9.662521362304688\n",
            "Training Iteration 3566, Loss: 3.0442681312561035\n",
            "Training Iteration 3567, Loss: 2.2290518283843994\n",
            "Training Iteration 3568, Loss: 5.895376205444336\n",
            "Training Iteration 3569, Loss: 3.094874143600464\n",
            "Training Iteration 3570, Loss: 2.0549204349517822\n",
            "Training Iteration 3571, Loss: 4.9833149909973145\n",
            "Training Iteration 3572, Loss: 2.2736473083496094\n",
            "Training Iteration 3573, Loss: 5.276299476623535\n",
            "Training Iteration 3574, Loss: 2.5945584774017334\n",
            "Training Iteration 3575, Loss: 5.991988658905029\n",
            "Training Iteration 3576, Loss: 5.864622592926025\n",
            "Training Iteration 3577, Loss: 5.016584873199463\n",
            "Training Iteration 3578, Loss: 5.0011515617370605\n",
            "Training Iteration 3579, Loss: 3.636817455291748\n",
            "Training Iteration 3580, Loss: 4.119239807128906\n",
            "Training Iteration 3581, Loss: 4.392484664916992\n",
            "Training Iteration 3582, Loss: 4.476192474365234\n",
            "Training Iteration 3583, Loss: 3.8705146312713623\n",
            "Training Iteration 3584, Loss: 3.584320068359375\n",
            "Training Iteration 3585, Loss: 4.5602006912231445\n",
            "Training Iteration 3586, Loss: 3.408264636993408\n",
            "Training Iteration 3587, Loss: 6.507761478424072\n",
            "Training Iteration 3588, Loss: 3.032688856124878\n",
            "Training Iteration 3589, Loss: 2.2690606117248535\n",
            "Training Iteration 3590, Loss: 5.059948444366455\n",
            "Training Iteration 3591, Loss: 5.234139442443848\n",
            "Training Iteration 3592, Loss: 4.33070182800293\n",
            "Training Iteration 3593, Loss: 4.1945013999938965\n",
            "Training Iteration 3594, Loss: 4.855998992919922\n",
            "Training Iteration 3595, Loss: 5.393136978149414\n",
            "Training Iteration 3596, Loss: 3.567967414855957\n",
            "Training Iteration 3597, Loss: 4.196597576141357\n",
            "Training Iteration 3598, Loss: 1.9313793182373047\n",
            "Training Iteration 3599, Loss: 6.771784782409668\n",
            "Training Iteration 3600, Loss: 3.7840795516967773\n",
            "Training Iteration 3601, Loss: 3.8203959465026855\n",
            "Training Iteration 3602, Loss: 5.781122207641602\n",
            "Training Iteration 3603, Loss: 3.7106876373291016\n",
            "Training Iteration 3604, Loss: 2.117192506790161\n",
            "Training Iteration 3605, Loss: 3.665158987045288\n",
            "Training Iteration 3606, Loss: 7.821857929229736\n",
            "Training Iteration 3607, Loss: 5.271797180175781\n",
            "Training Iteration 3608, Loss: 3.117687940597534\n",
            "Training Iteration 3609, Loss: 7.874996185302734\n",
            "Training Iteration 3610, Loss: 5.1666059494018555\n",
            "Training Iteration 3611, Loss: 3.274895429611206\n",
            "Training Iteration 3612, Loss: 1.4447405338287354\n",
            "Training Iteration 3613, Loss: 3.2745020389556885\n",
            "Training Iteration 3614, Loss: 5.897489547729492\n",
            "Training Iteration 3615, Loss: 7.271173477172852\n",
            "Training Iteration 3616, Loss: 6.451421737670898\n",
            "Training Iteration 3617, Loss: 3.7043797969818115\n",
            "Training Iteration 3618, Loss: 2.349696159362793\n",
            "Training Iteration 3619, Loss: 3.5616159439086914\n",
            "Training Iteration 3620, Loss: 4.6007232666015625\n",
            "Training Iteration 3621, Loss: 4.659650802612305\n",
            "Training Iteration 3622, Loss: 3.5769317150115967\n",
            "Training Iteration 3623, Loss: 4.257487773895264\n",
            "Training Iteration 3624, Loss: 2.7760937213897705\n",
            "Training Iteration 3625, Loss: 3.3527305126190186\n",
            "Training Iteration 3626, Loss: 2.337458610534668\n",
            "Training Iteration 3627, Loss: 6.801261901855469\n",
            "Training Iteration 3628, Loss: 4.943719863891602\n",
            "Training Iteration 3629, Loss: 3.3438470363616943\n",
            "Training Iteration 3630, Loss: 7.521487712860107\n",
            "Training Iteration 3631, Loss: 5.156804084777832\n",
            "Training Iteration 3632, Loss: 3.5632822513580322\n",
            "Training Iteration 3633, Loss: 3.9630541801452637\n",
            "Training Iteration 3634, Loss: 5.8071465492248535\n",
            "Training Iteration 3635, Loss: 5.067080497741699\n",
            "Training Iteration 3636, Loss: 3.859157085418701\n",
            "Training Iteration 3637, Loss: 5.203408718109131\n",
            "Training Iteration 3638, Loss: 6.825389385223389\n",
            "Training Iteration 3639, Loss: 3.947145938873291\n",
            "Training Iteration 3640, Loss: 2.8204782009124756\n",
            "Training Iteration 3641, Loss: 3.7370095252990723\n",
            "Training Iteration 3642, Loss: 1.1615492105484009\n",
            "Training Iteration 3643, Loss: 2.7435734272003174\n",
            "Training Iteration 3644, Loss: 4.523151874542236\n",
            "Training Iteration 3645, Loss: 8.175521850585938\n",
            "Training Iteration 3646, Loss: 4.734109878540039\n",
            "Training Iteration 3647, Loss: 2.3527286052703857\n",
            "Training Iteration 3648, Loss: 3.136406660079956\n",
            "Training Iteration 3649, Loss: 5.564870357513428\n",
            "Training Iteration 3650, Loss: 3.359408378601074\n",
            "Training Iteration 3651, Loss: 4.08717679977417\n",
            "Training Iteration 3652, Loss: 5.898665904998779\n",
            "Training Iteration 3653, Loss: 5.110692977905273\n",
            "Training Iteration 3654, Loss: 2.5967376232147217\n",
            "Training Iteration 3655, Loss: 5.767486572265625\n",
            "Training Iteration 3656, Loss: 2.8698525428771973\n",
            "Training Iteration 3657, Loss: 4.736368179321289\n",
            "Training Iteration 3658, Loss: 4.481822490692139\n",
            "Training Iteration 3659, Loss: 2.3221678733825684\n",
            "Training Iteration 3660, Loss: 2.433084011077881\n",
            "Training Iteration 3661, Loss: 2.8608078956604004\n",
            "Training Iteration 3662, Loss: 6.873166561126709\n",
            "Training Iteration 3663, Loss: 5.742544174194336\n",
            "Training Iteration 3664, Loss: 3.2694687843322754\n",
            "Training Iteration 3665, Loss: 4.341944694519043\n",
            "Training Iteration 3666, Loss: 3.5611276626586914\n",
            "Training Iteration 3667, Loss: 7.065434455871582\n",
            "Training Iteration 3668, Loss: 3.2429356575012207\n",
            "Training Iteration 3669, Loss: 4.696965217590332\n",
            "Training Iteration 3670, Loss: 5.437363624572754\n",
            "Training Iteration 3671, Loss: 3.6468372344970703\n",
            "Training Iteration 3672, Loss: 4.3752641677856445\n",
            "Training Iteration 3673, Loss: 2.069234609603882\n",
            "Training Iteration 3674, Loss: 4.294840335845947\n",
            "Training Iteration 3675, Loss: 7.138789176940918\n",
            "Training Iteration 3676, Loss: 5.901264190673828\n",
            "Training Iteration 3677, Loss: 4.77128267288208\n",
            "Training Iteration 3678, Loss: 2.3012797832489014\n",
            "Training Iteration 3679, Loss: 4.541665554046631\n",
            "Training Iteration 3680, Loss: 3.2359085083007812\n",
            "Training Iteration 3681, Loss: 3.7809970378875732\n",
            "Training Iteration 3682, Loss: 3.4809296131134033\n",
            "Training Iteration 3683, Loss: 6.960000514984131\n",
            "Training Iteration 3684, Loss: 4.996603488922119\n",
            "Training Iteration 3685, Loss: 2.1930019855499268\n",
            "Training Iteration 3686, Loss: 4.203520774841309\n",
            "Training Iteration 3687, Loss: 4.047539710998535\n",
            "Training Iteration 3688, Loss: 5.365627288818359\n",
            "Training Iteration 3689, Loss: 5.434424877166748\n",
            "Training Iteration 3690, Loss: 5.552672386169434\n",
            "Training Iteration 3691, Loss: 4.775447845458984\n",
            "Training Iteration 3692, Loss: 2.7086799144744873\n",
            "Training Iteration 3693, Loss: 5.19484806060791\n",
            "Training Iteration 3694, Loss: 4.027335166931152\n",
            "Training Iteration 3695, Loss: 3.364297866821289\n",
            "Training Iteration 3696, Loss: 4.3353776931762695\n",
            "Training Iteration 3697, Loss: 4.087409973144531\n",
            "Training Iteration 3698, Loss: 3.3265466690063477\n",
            "Training Iteration 3699, Loss: 3.025887966156006\n",
            "Training Iteration 3700, Loss: 5.061093330383301\n",
            "Training Iteration 3701, Loss: 3.8393545150756836\n",
            "Training Iteration 3702, Loss: 6.042540073394775\n",
            "Training Iteration 3703, Loss: 2.913468360900879\n",
            "Training Iteration 3704, Loss: 2.81437087059021\n",
            "Training Iteration 3705, Loss: 4.799190044403076\n",
            "Training Iteration 3706, Loss: 3.89615535736084\n",
            "Training Iteration 3707, Loss: 4.137392520904541\n",
            "Training Iteration 3708, Loss: 5.947423934936523\n",
            "Training Iteration 3709, Loss: 5.417994976043701\n",
            "Training Iteration 3710, Loss: 6.53351354598999\n",
            "Training Iteration 3711, Loss: 1.1596897840499878\n",
            "Training Iteration 3712, Loss: 4.618854999542236\n",
            "Training Iteration 3713, Loss: 7.248340606689453\n",
            "Training Iteration 3714, Loss: 4.469510555267334\n",
            "Training Iteration 3715, Loss: 6.830825328826904\n",
            "Training Iteration 3716, Loss: 12.665962219238281\n",
            "Training Iteration 3717, Loss: 5.768667221069336\n",
            "Training Iteration 3718, Loss: 3.591686487197876\n",
            "Training Iteration 3719, Loss: 3.489731788635254\n",
            "Training Iteration 3720, Loss: 6.544072151184082\n",
            "Training Iteration 3721, Loss: 5.2528581619262695\n",
            "Training Iteration 3722, Loss: 4.530727386474609\n",
            "Training Iteration 3723, Loss: 3.6821885108947754\n",
            "Training Iteration 3724, Loss: 6.5479230880737305\n",
            "Training Iteration 3725, Loss: 4.573212623596191\n",
            "Training Iteration 3726, Loss: 4.0012359619140625\n",
            "Training Iteration 3727, Loss: 6.671608924865723\n",
            "Training Iteration 3728, Loss: 2.4382646083831787\n",
            "Training Iteration 3729, Loss: 8.400025367736816\n",
            "Training Iteration 3730, Loss: 3.7843966484069824\n",
            "Training Iteration 3731, Loss: 5.647083759307861\n",
            "Training Iteration 3732, Loss: 5.514798164367676\n",
            "Training Iteration 3733, Loss: 5.514185428619385\n",
            "Training Iteration 3734, Loss: 5.424685478210449\n",
            "Training Iteration 3735, Loss: 3.5819969177246094\n",
            "Training Iteration 3736, Loss: 11.29582691192627\n",
            "Training Iteration 3737, Loss: 5.524739742279053\n",
            "Training Iteration 3738, Loss: 3.5051705837249756\n",
            "Training Iteration 3739, Loss: 4.255879878997803\n",
            "Training Iteration 3740, Loss: 6.727085113525391\n",
            "Training Iteration 3741, Loss: 4.748172283172607\n",
            "Training Iteration 3742, Loss: 5.480504512786865\n",
            "Training Iteration 3743, Loss: 6.699191093444824\n",
            "Training Iteration 3744, Loss: 4.963987350463867\n",
            "Training Iteration 3745, Loss: 5.309005260467529\n",
            "Training Iteration 3746, Loss: 4.005488872528076\n",
            "Training Iteration 3747, Loss: 5.788130760192871\n",
            "Training Iteration 3748, Loss: 3.331491708755493\n",
            "Training Iteration 3749, Loss: 4.375338077545166\n",
            "Training Iteration 3750, Loss: 5.289088249206543\n",
            "Training Iteration 3751, Loss: 6.2823872566223145\n",
            "Training Iteration 3752, Loss: 4.240667343139648\n",
            "Training Iteration 3753, Loss: 3.0301811695098877\n",
            "Training Iteration 3754, Loss: 9.40568733215332\n",
            "Training Iteration 3755, Loss: 4.715993881225586\n",
            "Training Iteration 3756, Loss: 7.430322647094727\n",
            "Training Iteration 3757, Loss: 3.656850814819336\n",
            "Training Iteration 3758, Loss: 2.6090383529663086\n",
            "Training Iteration 3759, Loss: 5.6796793937683105\n",
            "Training Iteration 3760, Loss: 5.272693157196045\n",
            "Training Iteration 3761, Loss: 5.980118751525879\n",
            "Training Iteration 3762, Loss: 4.831721305847168\n",
            "Training Iteration 3763, Loss: 5.138583183288574\n",
            "Training Iteration 3764, Loss: 4.375858783721924\n",
            "Training Iteration 3765, Loss: 2.158938407897949\n",
            "Training Iteration 3766, Loss: 5.342879772186279\n",
            "Training Iteration 3767, Loss: 4.42171573638916\n",
            "Training Iteration 3768, Loss: 4.319466590881348\n",
            "Training Iteration 3769, Loss: 4.3606672286987305\n",
            "Training Iteration 3770, Loss: 3.8825149536132812\n",
            "Training Iteration 3771, Loss: 4.86520528793335\n",
            "Training Iteration 3772, Loss: 4.7954511642456055\n",
            "Training Iteration 3773, Loss: 4.0392255783081055\n",
            "Training Iteration 3774, Loss: 4.132538795471191\n",
            "Training Iteration 3775, Loss: 4.807013034820557\n",
            "Training Iteration 3776, Loss: 3.7761383056640625\n",
            "Training Iteration 3777, Loss: 6.740765571594238\n",
            "Training Iteration 3778, Loss: 5.742984294891357\n",
            "Training Iteration 3779, Loss: 3.3131725788116455\n",
            "Training Iteration 3780, Loss: 3.0055694580078125\n",
            "Training Iteration 3781, Loss: 3.156024694442749\n",
            "Training Iteration 3782, Loss: 2.647240400314331\n",
            "Training Iteration 3783, Loss: 6.121591091156006\n",
            "Training Iteration 3784, Loss: 3.565178394317627\n",
            "Training Iteration 3785, Loss: 2.699153423309326\n",
            "Training Iteration 3786, Loss: 5.890659809112549\n",
            "Training Iteration 3787, Loss: 3.4218881130218506\n",
            "Training Iteration 3788, Loss: 3.5250115394592285\n",
            "Training Iteration 3789, Loss: 2.0382823944091797\n",
            "Training Iteration 3790, Loss: 4.724461078643799\n",
            "Training Iteration 3791, Loss: 5.31746768951416\n",
            "Training Iteration 3792, Loss: 7.170343399047852\n",
            "Training Iteration 3793, Loss: 4.147860527038574\n",
            "Training Iteration 3794, Loss: 6.956235885620117\n",
            "Training Iteration 3795, Loss: 4.5386271476745605\n",
            "Training Iteration 3796, Loss: 4.866660118103027\n",
            "Training Iteration 3797, Loss: 4.463029861450195\n",
            "Training Iteration 3798, Loss: 2.8977222442626953\n",
            "Training Iteration 3799, Loss: 5.601203918457031\n",
            "Training Iteration 3800, Loss: 3.5981781482696533\n",
            "Training Iteration 3801, Loss: 6.3453850746154785\n",
            "Training Iteration 3802, Loss: 2.8771157264709473\n",
            "Training Iteration 3803, Loss: 3.670945167541504\n",
            "Training Iteration 3804, Loss: 4.218930721282959\n",
            "Training Iteration 3805, Loss: 5.414240837097168\n",
            "Training Iteration 3806, Loss: 2.853538990020752\n",
            "Training Iteration 3807, Loss: 5.743838787078857\n",
            "Training Iteration 3808, Loss: 5.080136299133301\n",
            "Training Iteration 3809, Loss: 4.222603797912598\n",
            "Training Iteration 3810, Loss: 5.920297145843506\n",
            "Training Iteration 3811, Loss: 5.0063581466674805\n",
            "Training Iteration 3812, Loss: 4.251020431518555\n",
            "Training Iteration 3813, Loss: 2.259456157684326\n",
            "Training Iteration 3814, Loss: 3.626084327697754\n",
            "Training Iteration 3815, Loss: 3.101252794265747\n",
            "Training Iteration 3816, Loss: 4.4893717765808105\n",
            "Training Iteration 3817, Loss: 3.47652530670166\n",
            "Training Iteration 3818, Loss: 5.427250385284424\n",
            "Training Iteration 3819, Loss: 5.503913879394531\n",
            "Training Iteration 3820, Loss: 3.140899419784546\n",
            "Training Iteration 3821, Loss: 4.494511604309082\n",
            "Training Iteration 3822, Loss: 2.8577046394348145\n",
            "Training Iteration 3823, Loss: 4.431058883666992\n",
            "Training Iteration 3824, Loss: 11.231792449951172\n",
            "Training Iteration 3825, Loss: 1.5464736223220825\n",
            "Training Iteration 3826, Loss: 3.8725507259368896\n",
            "Training Iteration 3827, Loss: 5.636783123016357\n",
            "Training Iteration 3828, Loss: 3.7937240600585938\n",
            "Training Iteration 3829, Loss: 3.095101833343506\n",
            "Training Iteration 3830, Loss: 2.2939188480377197\n",
            "Training Iteration 3831, Loss: 4.009424209594727\n",
            "Training Iteration 3832, Loss: 2.8060519695281982\n",
            "Training Iteration 3833, Loss: 3.228175163269043\n",
            "Training Iteration 3834, Loss: 3.496976613998413\n",
            "Training Iteration 3835, Loss: 2.209650754928589\n",
            "Training Iteration 3836, Loss: 4.111843109130859\n",
            "Training Iteration 3837, Loss: 2.1754024028778076\n",
            "Training Iteration 3838, Loss: 3.428948402404785\n",
            "Training Iteration 3839, Loss: 2.3063509464263916\n",
            "Training Iteration 3840, Loss: 4.511735916137695\n",
            "Training Iteration 3841, Loss: 3.737382411956787\n",
            "Training Iteration 3842, Loss: 4.478119373321533\n",
            "Training Iteration 3843, Loss: 4.142247676849365\n",
            "Training Iteration 3844, Loss: 2.2951745986938477\n",
            "Training Iteration 3845, Loss: 3.3831934928894043\n",
            "Training Iteration 3846, Loss: 4.626443862915039\n",
            "Training Iteration 3847, Loss: 1.7318238019943237\n",
            "Training Iteration 3848, Loss: 2.0673224925994873\n",
            "Training Iteration 3849, Loss: 4.514754295349121\n",
            "Training Iteration 3850, Loss: 4.2420854568481445\n",
            "Training Iteration 3851, Loss: 5.716534614562988\n",
            "Training Iteration 3852, Loss: 5.203681468963623\n",
            "Training Iteration 3853, Loss: 7.147930145263672\n",
            "Training Iteration 3854, Loss: 1.8583508729934692\n",
            "Training Iteration 3855, Loss: 3.6188406944274902\n",
            "Training Iteration 3856, Loss: 6.574307918548584\n",
            "Training Iteration 3857, Loss: 5.21333122253418\n",
            "Training Iteration 3858, Loss: 4.198507785797119\n",
            "Training Iteration 3859, Loss: 2.5085232257843018\n",
            "Training Iteration 3860, Loss: 4.086267471313477\n",
            "Training Iteration 3861, Loss: 5.094367504119873\n",
            "Training Iteration 3862, Loss: 3.0851054191589355\n",
            "Training Iteration 3863, Loss: 4.96684455871582\n",
            "Training Iteration 3864, Loss: 6.712957382202148\n",
            "Training Iteration 3865, Loss: 5.815365314483643\n",
            "Training Iteration 3866, Loss: 7.295688629150391\n",
            "Training Iteration 3867, Loss: 4.406064510345459\n",
            "Training Iteration 3868, Loss: 1.294447898864746\n",
            "Training Iteration 3869, Loss: 6.098084926605225\n",
            "Training Iteration 3870, Loss: 3.8199055194854736\n",
            "Training Iteration 3871, Loss: 2.2167069911956787\n",
            "Training Iteration 3872, Loss: 1.6059919595718384\n",
            "Training Iteration 3873, Loss: 5.33634614944458\n",
            "Training Iteration 3874, Loss: 5.999142646789551\n",
            "Training Iteration 3875, Loss: 4.3789143562316895\n",
            "Training Iteration 3876, Loss: 5.206575870513916\n",
            "Training Iteration 3877, Loss: 4.791571140289307\n",
            "Training Iteration 3878, Loss: 5.250709056854248\n",
            "Training Iteration 3879, Loss: 4.2192864418029785\n",
            "Training Iteration 3880, Loss: 5.063295364379883\n",
            "Training Iteration 3881, Loss: 3.8701376914978027\n",
            "Training Iteration 3882, Loss: 5.480319499969482\n",
            "Training Iteration 3883, Loss: 3.8885765075683594\n",
            "Training Iteration 3884, Loss: 6.418462753295898\n",
            "Training Iteration 3885, Loss: 5.928852558135986\n",
            "Training Iteration 3886, Loss: 3.9376325607299805\n",
            "Training Iteration 3887, Loss: 3.9380674362182617\n",
            "Training Iteration 3888, Loss: 2.8810927867889404\n",
            "Training Iteration 3889, Loss: 7.640810966491699\n",
            "Training Iteration 3890, Loss: 3.088451385498047\n",
            "Training Iteration 3891, Loss: 3.2508113384246826\n",
            "Training Iteration 3892, Loss: 3.054990768432617\n",
            "Training Iteration 3893, Loss: 3.745656967163086\n",
            "Training Iteration 3894, Loss: 1.245421051979065\n",
            "Training Iteration 3895, Loss: 3.7727136611938477\n",
            "Training Iteration 3896, Loss: 4.883001804351807\n",
            "Training Iteration 3897, Loss: 2.2717628479003906\n",
            "Training Iteration 3898, Loss: 4.349084854125977\n",
            "Training Iteration 3899, Loss: 7.317272186279297\n",
            "Training Iteration 3900, Loss: 4.146878242492676\n",
            "Training Iteration 3901, Loss: 5.03317403793335\n",
            "Training Iteration 3902, Loss: 5.360673904418945\n",
            "Training Iteration 3903, Loss: 4.694684028625488\n",
            "Training Iteration 3904, Loss: 4.706801891326904\n",
            "Training Iteration 3905, Loss: 6.631453037261963\n",
            "Training Iteration 3906, Loss: 8.589789390563965\n",
            "Training Iteration 3907, Loss: 3.2687132358551025\n",
            "Training Iteration 3908, Loss: 3.275862693786621\n",
            "Training Iteration 3909, Loss: 2.498450756072998\n",
            "Training Iteration 3910, Loss: 4.532384395599365\n",
            "Training Iteration 3911, Loss: 5.209048748016357\n",
            "Training Iteration 3912, Loss: 3.6271169185638428\n",
            "Training Iteration 3913, Loss: 5.636794090270996\n",
            "Training Iteration 3914, Loss: 4.581473350524902\n",
            "Training Iteration 3915, Loss: 3.3260576725006104\n",
            "Training Iteration 3916, Loss: 3.6814725399017334\n",
            "Training Iteration 3917, Loss: 2.9243876934051514\n",
            "Training Iteration 3918, Loss: 5.9398512840271\n",
            "Training Iteration 3919, Loss: 7.343916416168213\n",
            "Training Iteration 3920, Loss: 5.479740142822266\n",
            "Training Iteration 3921, Loss: 2.892101764678955\n",
            "Training Iteration 3922, Loss: 2.2072622776031494\n",
            "Training Iteration 3923, Loss: 5.37938117980957\n",
            "Training Iteration 3924, Loss: 6.33717155456543\n",
            "Training Iteration 3925, Loss: 2.8579959869384766\n",
            "Training Iteration 3926, Loss: 8.576033592224121\n",
            "Training Iteration 3927, Loss: 5.846397876739502\n",
            "Training Iteration 3928, Loss: 8.225727081298828\n",
            "Training Iteration 3929, Loss: 8.137968063354492\n",
            "Training Iteration 3930, Loss: 6.206719398498535\n",
            "Training Iteration 3931, Loss: 4.681297302246094\n",
            "Training Iteration 3932, Loss: 3.130096435546875\n",
            "Training Iteration 3933, Loss: 5.708751678466797\n",
            "Training Iteration 3934, Loss: 11.863961219787598\n",
            "Training Iteration 3935, Loss: 7.458529949188232\n",
            "Training Iteration 3936, Loss: 4.587250709533691\n",
            "Training Iteration 3937, Loss: 6.122332572937012\n",
            "Training Iteration 3938, Loss: 4.962062358856201\n",
            "Training Iteration 3939, Loss: 4.511894702911377\n",
            "Training Iteration 3940, Loss: 7.281697750091553\n",
            "Training Iteration 3941, Loss: 8.827692031860352\n",
            "Training Iteration 3942, Loss: 5.89504337310791\n",
            "Training Iteration 3943, Loss: 6.1719818115234375\n",
            "Training Iteration 3944, Loss: 6.494712829589844\n",
            "Training Iteration 3945, Loss: 3.228208541870117\n",
            "Training Iteration 3946, Loss: 4.963441371917725\n",
            "Training Iteration 3947, Loss: 4.261386394500732\n",
            "Training Iteration 3948, Loss: 7.282420635223389\n",
            "Training Iteration 3949, Loss: 7.219266891479492\n",
            "Training Iteration 3950, Loss: 7.696706771850586\n",
            "Training Iteration 3951, Loss: 2.368492364883423\n",
            "Training Iteration 3952, Loss: 6.46355676651001\n",
            "Training Iteration 3953, Loss: 5.780123233795166\n",
            "Training Iteration 3954, Loss: 9.229357719421387\n",
            "Training Iteration 3955, Loss: 7.019970417022705\n",
            "Training Iteration 3956, Loss: 7.023899078369141\n",
            "Training Iteration 3957, Loss: 3.1819489002227783\n",
            "Training Iteration 3958, Loss: 5.300917148590088\n",
            "Training Iteration 3959, Loss: 4.971167087554932\n",
            "Training Iteration 3960, Loss: 7.334005355834961\n",
            "Training Iteration 3961, Loss: 6.21975564956665\n",
            "Training Iteration 3962, Loss: 5.337791442871094\n",
            "Training Iteration 3963, Loss: 3.9612607955932617\n",
            "Training Iteration 3964, Loss: 2.9850709438323975\n",
            "Training Iteration 3965, Loss: 4.368171691894531\n",
            "Training Iteration 3966, Loss: 7.041206359863281\n",
            "Training Iteration 3967, Loss: 3.3371589183807373\n",
            "Training Iteration 3968, Loss: 6.0405097007751465\n",
            "Training Iteration 3969, Loss: 6.9190826416015625\n",
            "Training Iteration 3970, Loss: 5.445598125457764\n",
            "Training Iteration 3971, Loss: 7.859673500061035\n",
            "Training Iteration 3972, Loss: 5.378575801849365\n",
            "Training Iteration 3973, Loss: 4.44173526763916\n",
            "Training Iteration 3974, Loss: 3.909852981567383\n",
            "Training Iteration 3975, Loss: 3.54544734954834\n",
            "Training Iteration 3976, Loss: 4.160508632659912\n",
            "Training Iteration 3977, Loss: 3.1377346515655518\n",
            "Training Iteration 3978, Loss: 2.1302340030670166\n",
            "Training Iteration 3979, Loss: 5.595584869384766\n",
            "Training Iteration 3980, Loss: 9.848882675170898\n",
            "Training Iteration 3981, Loss: 3.7651865482330322\n",
            "Training Iteration 3982, Loss: 4.028515815734863\n",
            "Training Iteration 3983, Loss: 2.919839859008789\n",
            "Training Iteration 3984, Loss: 3.8666465282440186\n",
            "Training Iteration 3985, Loss: 5.515630722045898\n",
            "Training Iteration 3986, Loss: 4.971781253814697\n",
            "Training Iteration 3987, Loss: 4.164214611053467\n",
            "Training Iteration 3988, Loss: 2.488299608230591\n",
            "Training Iteration 3989, Loss: 2.5537075996398926\n",
            "Training Iteration 3990, Loss: 2.828878402709961\n",
            "Training Iteration 3991, Loss: 3.682391405105591\n",
            "Training Iteration 3992, Loss: 2.8775670528411865\n",
            "Training Iteration 3993, Loss: 3.665114402770996\n",
            "Training Iteration 3994, Loss: 4.1954731941223145\n",
            "Training Iteration 3995, Loss: 5.8434600830078125\n",
            "Training Iteration 3996, Loss: 6.276128768920898\n",
            "Training Iteration 3997, Loss: 2.949760913848877\n",
            "Training Iteration 3998, Loss: 3.6018378734588623\n",
            "Training Iteration 3999, Loss: 5.229796409606934\n",
            "Training Iteration 4000, Loss: 1.5830086469650269\n",
            "Training Iteration 4001, Loss: 6.475110054016113\n",
            "Training Iteration 4002, Loss: 10.247390747070312\n",
            "Training Iteration 4003, Loss: 6.091207027435303\n",
            "Training Iteration 4004, Loss: 7.008646011352539\n",
            "Training Iteration 4005, Loss: 4.804808616638184\n",
            "Training Iteration 4006, Loss: 4.232234001159668\n",
            "Training Iteration 4007, Loss: 3.8619296550750732\n",
            "Training Iteration 4008, Loss: 3.8631887435913086\n",
            "Training Iteration 4009, Loss: 6.494028568267822\n",
            "Training Iteration 4010, Loss: 7.758379936218262\n",
            "Training Iteration 4011, Loss: 5.848636150360107\n",
            "Training Iteration 4012, Loss: 4.206789016723633\n",
            "Training Iteration 4013, Loss: 1.4263811111450195\n",
            "Training Iteration 4014, Loss: 7.650007724761963\n",
            "Training Iteration 4015, Loss: 6.905269622802734\n",
            "Training Iteration 4016, Loss: 6.2876715660095215\n",
            "Training Iteration 4017, Loss: 7.940754413604736\n",
            "Training Iteration 4018, Loss: 8.771498680114746\n",
            "Training Iteration 4019, Loss: 3.745793581008911\n",
            "Training Iteration 4020, Loss: 4.407115936279297\n",
            "Training Iteration 4021, Loss: 10.341182708740234\n",
            "Training Iteration 4022, Loss: 6.882938385009766\n",
            "Training Iteration 4023, Loss: 9.296754837036133\n",
            "Training Iteration 4024, Loss: 7.402998447418213\n",
            "Training Iteration 4025, Loss: 4.501925945281982\n",
            "Training Iteration 4026, Loss: 5.491308212280273\n",
            "Training Iteration 4027, Loss: 2.9673056602478027\n",
            "Training Iteration 4028, Loss: 4.0913777351379395\n",
            "Training Iteration 4029, Loss: 6.905596733093262\n",
            "Training Iteration 4030, Loss: 5.415861129760742\n",
            "Training Iteration 4031, Loss: 6.192905902862549\n",
            "Training Iteration 4032, Loss: 4.29078483581543\n",
            "Training Iteration 4033, Loss: 5.149562835693359\n",
            "Training Iteration 4034, Loss: 5.638454437255859\n",
            "Training Iteration 4035, Loss: 4.277342796325684\n",
            "Training Iteration 4036, Loss: 4.158781051635742\n",
            "Training Iteration 4037, Loss: 7.851165771484375\n",
            "Training Iteration 4038, Loss: 7.99783182144165\n",
            "Training Iteration 4039, Loss: 6.053077220916748\n",
            "Training Iteration 4040, Loss: 4.332399845123291\n",
            "Training Iteration 4041, Loss: 6.269627094268799\n",
            "Training Iteration 4042, Loss: 2.5587985515594482\n",
            "Training Iteration 4043, Loss: 1.3380542993545532\n",
            "Training Iteration 4044, Loss: 5.266918182373047\n",
            "Training Iteration 4045, Loss: 4.456822395324707\n",
            "Training Iteration 4046, Loss: 4.1665449142456055\n",
            "Training Iteration 4047, Loss: 2.9974474906921387\n",
            "Training Iteration 4048, Loss: 1.2692667245864868\n",
            "Training Iteration 4049, Loss: 4.338305473327637\n",
            "Training Iteration 4050, Loss: 6.218968391418457\n",
            "Training Iteration 4051, Loss: 9.03963851928711\n",
            "Training Iteration 4052, Loss: 5.175100803375244\n",
            "Training Iteration 4053, Loss: 5.929335594177246\n",
            "Training Iteration 4054, Loss: 3.7746219635009766\n",
            "Training Iteration 4055, Loss: 8.105315208435059\n",
            "Training Iteration 4056, Loss: 4.453182697296143\n",
            "Training Iteration 4057, Loss: 2.276385545730591\n",
            "Training Iteration 4058, Loss: 1.734890341758728\n",
            "Training Iteration 4059, Loss: 3.4403345584869385\n",
            "Training Iteration 4060, Loss: 5.454838275909424\n",
            "Training Iteration 4061, Loss: 4.735105991363525\n",
            "Training Iteration 4062, Loss: 3.6056747436523438\n",
            "Training Iteration 4063, Loss: 7.0628838539123535\n",
            "Training Iteration 4064, Loss: 7.162271499633789\n",
            "Training Iteration 4065, Loss: 4.157910346984863\n",
            "Training Iteration 4066, Loss: 4.0085625648498535\n",
            "Training Iteration 4067, Loss: 7.282011985778809\n",
            "Training Iteration 4068, Loss: 3.446859121322632\n",
            "Training Iteration 4069, Loss: 5.040278434753418\n",
            "Training Iteration 4070, Loss: 3.8824877738952637\n",
            "Training Iteration 4071, Loss: 3.6655232906341553\n",
            "Training Iteration 4072, Loss: 3.6127095222473145\n",
            "Training Iteration 4073, Loss: 3.4545722007751465\n",
            "Training Iteration 4074, Loss: 6.691623210906982\n",
            "Training Iteration 4075, Loss: 3.1211769580841064\n",
            "Training Iteration 4076, Loss: 2.609062671661377\n",
            "Training Iteration 4077, Loss: 4.580451965332031\n",
            "Training Iteration 4078, Loss: 5.576607704162598\n",
            "Training Iteration 4079, Loss: 5.131265640258789\n",
            "Training Iteration 4080, Loss: 5.165329456329346\n",
            "Training Iteration 4081, Loss: 9.485184669494629\n",
            "Training Iteration 4082, Loss: 5.607719898223877\n",
            "Training Iteration 4083, Loss: 1.7745705842971802\n",
            "Training Iteration 4084, Loss: 2.8083856105804443\n",
            "Training Iteration 4085, Loss: 6.722774505615234\n",
            "Training Iteration 4086, Loss: 5.86652946472168\n",
            "Training Iteration 4087, Loss: 6.1411519050598145\n",
            "Training Iteration 4088, Loss: 9.331018447875977\n",
            "Training Iteration 4089, Loss: 5.186330795288086\n",
            "Training Iteration 4090, Loss: 5.059966087341309\n",
            "Training Iteration 4091, Loss: 4.439301490783691\n",
            "Training Iteration 4092, Loss: 3.6823434829711914\n",
            "Training Iteration 4093, Loss: 5.687753677368164\n",
            "Training Iteration 4094, Loss: 6.4231390953063965\n",
            "Training Iteration 4095, Loss: 9.498405456542969\n",
            "Training Iteration 4096, Loss: 5.983968257904053\n",
            "Training Iteration 4097, Loss: 2.7867865562438965\n",
            "Training Iteration 4098, Loss: 3.951988697052002\n",
            "Training Iteration 4099, Loss: 3.525750160217285\n",
            "Training Iteration 4100, Loss: 5.563779354095459\n",
            "Training Iteration 4101, Loss: 6.96705961227417\n",
            "Training Iteration 4102, Loss: 5.391073226928711\n",
            "Training Iteration 4103, Loss: 3.6780622005462646\n",
            "Training Iteration 4104, Loss: 4.253379821777344\n",
            "Training Iteration 4105, Loss: 6.283925533294678\n",
            "Training Iteration 4106, Loss: 8.77519702911377\n",
            "Training Iteration 4107, Loss: 1.82754647731781\n",
            "Training Iteration 4108, Loss: 4.425861835479736\n",
            "Training Iteration 4109, Loss: 7.744459629058838\n",
            "Training Iteration 4110, Loss: 2.698714256286621\n",
            "Training Iteration 4111, Loss: 8.078412055969238\n",
            "Training Iteration 4112, Loss: 3.5264334678649902\n",
            "Training Iteration 4113, Loss: 2.612771511077881\n",
            "Training Iteration 4114, Loss: 3.4496796131134033\n",
            "Training Iteration 4115, Loss: 7.4141950607299805\n",
            "Training Iteration 4116, Loss: 5.388796329498291\n",
            "Training Iteration 4117, Loss: 6.419881820678711\n",
            "Training Iteration 4118, Loss: 2.3149514198303223\n",
            "Training Iteration 4119, Loss: 4.815699100494385\n",
            "Training Iteration 4120, Loss: 3.675060749053955\n",
            "Training Iteration 4121, Loss: 4.161425590515137\n",
            "Training Iteration 4122, Loss: 3.302781581878662\n",
            "Training Iteration 4123, Loss: 6.16594934463501\n",
            "Training Iteration 4124, Loss: 3.1071925163269043\n",
            "Training Iteration 4125, Loss: 5.275158882141113\n",
            "Training Iteration 4126, Loss: 3.8396995067596436\n",
            "Training Iteration 4127, Loss: 2.444380044937134\n",
            "Training Iteration 4128, Loss: 4.8726887702941895\n",
            "Training Iteration 4129, Loss: 4.406095504760742\n",
            "Training Iteration 4130, Loss: 9.072251319885254\n",
            "Training Iteration 4131, Loss: 5.415417194366455\n",
            "Training Iteration 4132, Loss: 7.446193218231201\n",
            "Training Iteration 4133, Loss: 4.1283440589904785\n",
            "Training Iteration 4134, Loss: 5.515029430389404\n",
            "Training Iteration 4135, Loss: 3.7737042903900146\n",
            "Training Iteration 4136, Loss: 5.857032299041748\n",
            "Training Iteration 4137, Loss: 5.893804550170898\n",
            "Training Iteration 4138, Loss: 5.345057964324951\n",
            "Training Iteration 4139, Loss: 4.57520866394043\n",
            "Training Iteration 4140, Loss: 3.023041248321533\n",
            "Training Iteration 4141, Loss: 7.5780510902404785\n",
            "Training Iteration 4142, Loss: 0.9276162385940552\n",
            "Training Iteration 4143, Loss: 10.871332168579102\n",
            "Training Iteration 4144, Loss: 4.766231536865234\n",
            "Training Iteration 4145, Loss: 3.6458120346069336\n",
            "Training Iteration 4146, Loss: 2.919343948364258\n",
            "Training Iteration 4147, Loss: 4.007784366607666\n",
            "Training Iteration 4148, Loss: 8.524633407592773\n",
            "Training Iteration 4149, Loss: 4.65716552734375\n",
            "Training Iteration 4150, Loss: 6.121704578399658\n",
            "Training Iteration 4151, Loss: 8.401905059814453\n",
            "Training Iteration 4152, Loss: 4.223398685455322\n",
            "Training Iteration 4153, Loss: 4.503818511962891\n",
            "Training Iteration 4154, Loss: 6.701789379119873\n",
            "Training Iteration 4155, Loss: 2.6035268306732178\n",
            "Training Iteration 4156, Loss: 5.110951900482178\n",
            "Training Iteration 4157, Loss: 5.2738261222839355\n",
            "Training Iteration 4158, Loss: 5.596160888671875\n",
            "Training Iteration 4159, Loss: 3.9493842124938965\n",
            "Training Iteration 4160, Loss: 3.301210880279541\n",
            "Training Iteration 4161, Loss: 3.2036030292510986\n",
            "Training Iteration 4162, Loss: 6.973127365112305\n",
            "Training Iteration 4163, Loss: 3.6150078773498535\n",
            "Training Iteration 4164, Loss: 1.6825592517852783\n",
            "Training Iteration 4165, Loss: 6.9135589599609375\n",
            "Training Iteration 4166, Loss: 5.562017917633057\n",
            "Training Iteration 4167, Loss: 4.016101360321045\n",
            "Training Iteration 4168, Loss: 3.840308427810669\n",
            "Training Iteration 4169, Loss: 2.2068493366241455\n",
            "Training Iteration 4170, Loss: 3.5470681190490723\n",
            "Training Iteration 4171, Loss: 2.2592387199401855\n",
            "Training Iteration 4172, Loss: 5.797901630401611\n",
            "Training Iteration 4173, Loss: 6.7733659744262695\n",
            "Training Iteration 4174, Loss: 3.2531986236572266\n",
            "Training Iteration 4175, Loss: 4.155192852020264\n",
            "Training Iteration 4176, Loss: 3.3247203826904297\n",
            "Training Iteration 4177, Loss: 2.745384454727173\n",
            "Training Iteration 4178, Loss: 2.391752004623413\n",
            "Training Iteration 4179, Loss: 5.226753234863281\n",
            "Training Iteration 4180, Loss: 5.34220027923584\n",
            "Training Iteration 4181, Loss: 2.736966609954834\n",
            "Training Iteration 4182, Loss: 3.301305055618286\n",
            "Training Iteration 4183, Loss: 4.653628349304199\n",
            "Training Iteration 4184, Loss: 3.0469889640808105\n",
            "Training Iteration 4185, Loss: 4.107411861419678\n",
            "Training Iteration 4186, Loss: 3.7349584102630615\n",
            "Training Iteration 4187, Loss: 3.352426528930664\n",
            "Training Iteration 4188, Loss: 6.295612335205078\n",
            "Training Iteration 4189, Loss: 3.0098681449890137\n",
            "Training Iteration 4190, Loss: 3.177097797393799\n",
            "Training Iteration 4191, Loss: 4.555210113525391\n",
            "Training Iteration 4192, Loss: 4.729273796081543\n",
            "Training Iteration 4193, Loss: 2.3515079021453857\n",
            "Training Iteration 4194, Loss: 5.447015762329102\n",
            "Training Iteration 4195, Loss: 4.457712173461914\n",
            "Training Iteration 4196, Loss: 4.184415817260742\n",
            "Training Iteration 4197, Loss: 2.103205680847168\n",
            "Training Iteration 4198, Loss: 3.489422559738159\n",
            "Training Iteration 4199, Loss: 2.866149663925171\n",
            "Training Iteration 4200, Loss: 3.263868808746338\n",
            "Training Iteration 4201, Loss: 5.6705708503723145\n",
            "Training Iteration 4202, Loss: 4.413843154907227\n",
            "Training Iteration 4203, Loss: 3.096890449523926\n",
            "Training Iteration 4204, Loss: 4.386131286621094\n",
            "Training Iteration 4205, Loss: 4.950626373291016\n",
            "Training Iteration 4206, Loss: 3.1897225379943848\n",
            "Training Iteration 4207, Loss: 4.164096832275391\n",
            "Training Iteration 4208, Loss: 3.143829584121704\n",
            "Training Iteration 4209, Loss: 6.919273853302002\n",
            "Training Iteration 4210, Loss: 5.793405532836914\n",
            "Training Iteration 4211, Loss: 4.264890670776367\n",
            "Training Iteration 4212, Loss: 5.823065280914307\n",
            "Training Iteration 4213, Loss: 4.328586578369141\n",
            "Training Iteration 4214, Loss: 5.065395355224609\n",
            "Training Iteration 4215, Loss: 5.644148826599121\n",
            "Training Iteration 4216, Loss: 3.0514254570007324\n",
            "Training Iteration 4217, Loss: 4.786256313323975\n",
            "Training Iteration 4218, Loss: 2.2452001571655273\n",
            "Training Iteration 4219, Loss: 2.565135955810547\n",
            "Training Iteration 4220, Loss: 7.9184699058532715\n",
            "Training Iteration 4221, Loss: 4.615112781524658\n",
            "Training Iteration 4222, Loss: 3.3209171295166016\n",
            "Training Iteration 4223, Loss: 2.3827359676361084\n",
            "Training Iteration 4224, Loss: 2.5041894912719727\n",
            "Training Iteration 4225, Loss: 3.54026460647583\n",
            "Training Iteration 4226, Loss: 4.625359535217285\n",
            "Training Iteration 4227, Loss: 5.387271881103516\n",
            "Training Iteration 4228, Loss: 3.023812770843506\n",
            "Training Iteration 4229, Loss: 2.85441517829895\n",
            "Training Iteration 4230, Loss: 1.9175007343292236\n",
            "Training Iteration 4231, Loss: 0.8759248852729797\n",
            "Training Iteration 4232, Loss: 4.910248279571533\n",
            "Training Iteration 4233, Loss: 5.516336441040039\n",
            "Training Iteration 4234, Loss: 7.416175842285156\n",
            "Training Iteration 4235, Loss: 2.1050381660461426\n",
            "Training Iteration 4236, Loss: 2.160679340362549\n",
            "Training Iteration 4237, Loss: 1.490982174873352\n",
            "Training Iteration 4238, Loss: 5.3949785232543945\n",
            "Training Iteration 4239, Loss: 6.084278106689453\n",
            "Training Iteration 4240, Loss: 7.671449184417725\n",
            "Training Iteration 4241, Loss: 4.448298454284668\n",
            "Training Iteration 4242, Loss: 4.1926116943359375\n",
            "Training Iteration 4243, Loss: 3.841036558151245\n",
            "Training Iteration 4244, Loss: 5.177765846252441\n",
            "Training Iteration 4245, Loss: 4.850866317749023\n",
            "Training Iteration 4246, Loss: 2.7539291381835938\n",
            "Training Iteration 4247, Loss: 3.7245583534240723\n",
            "Training Iteration 4248, Loss: 3.507012367248535\n",
            "Training Iteration 4249, Loss: 1.6839202642440796\n",
            "Training Iteration 4250, Loss: 4.407040119171143\n",
            "Training Iteration 4251, Loss: 4.930905818939209\n",
            "Training Iteration 4252, Loss: 3.829864740371704\n",
            "Training Iteration 4253, Loss: 5.630372047424316\n",
            "Training Iteration 4254, Loss: 5.176506042480469\n",
            "Training Iteration 4255, Loss: 5.754261016845703\n",
            "Training Iteration 4256, Loss: 2.944333553314209\n",
            "Training Iteration 4257, Loss: 3.1755857467651367\n",
            "Training Iteration 4258, Loss: 7.563754558563232\n",
            "Training Iteration 4259, Loss: 5.934231281280518\n",
            "Training Iteration 4260, Loss: 2.831530809402466\n",
            "Training Iteration 4261, Loss: 5.0012288093566895\n",
            "Training Iteration 4262, Loss: 5.257632255554199\n",
            "Training Iteration 4263, Loss: 3.6388661861419678\n",
            "Training Iteration 4264, Loss: 4.283856391906738\n",
            "Training Iteration 4265, Loss: 8.55337905883789\n",
            "Training Iteration 4266, Loss: 7.90071439743042\n",
            "Training Iteration 4267, Loss: 7.577429294586182\n",
            "Training Iteration 4268, Loss: 2.420625925064087\n",
            "Training Iteration 4269, Loss: 4.117276668548584\n",
            "Training Iteration 4270, Loss: 2.418280601501465\n",
            "Training Iteration 4271, Loss: 5.912810802459717\n",
            "Training Iteration 4272, Loss: 6.892007827758789\n",
            "Training Iteration 4273, Loss: 5.979816436767578\n",
            "Training Iteration 4274, Loss: 4.8974456787109375\n",
            "Training Iteration 4275, Loss: 5.7732110023498535\n",
            "Training Iteration 4276, Loss: 6.298555850982666\n",
            "Training Iteration 4277, Loss: 3.5253498554229736\n",
            "Training Iteration 4278, Loss: 4.815165996551514\n",
            "Training Iteration 4279, Loss: 2.2922515869140625\n",
            "Training Iteration 4280, Loss: 7.355673313140869\n",
            "Training Iteration 4281, Loss: 3.29780912399292\n",
            "Training Iteration 4282, Loss: 9.06771469116211\n",
            "Training Iteration 4283, Loss: 7.040874481201172\n",
            "Training Iteration 4284, Loss: 7.126643657684326\n",
            "Training Iteration 4285, Loss: 3.2964320182800293\n",
            "Training Iteration 4286, Loss: 4.345893859863281\n",
            "Training Iteration 4287, Loss: 5.466310977935791\n",
            "Training Iteration 4288, Loss: 4.7085065841674805\n",
            "Training Iteration 4289, Loss: 6.024079322814941\n",
            "Training Iteration 4290, Loss: 9.335689544677734\n",
            "Training Iteration 4291, Loss: 6.502495765686035\n",
            "Training Iteration 4292, Loss: 3.9821908473968506\n",
            "Training Iteration 4293, Loss: 9.235077857971191\n",
            "Training Iteration 4294, Loss: 3.3773810863494873\n",
            "Training Iteration 4295, Loss: 3.328916072845459\n",
            "Training Iteration 4296, Loss: 5.379322528839111\n",
            "Training Iteration 4297, Loss: 3.986104965209961\n",
            "Training Iteration 4298, Loss: 4.064253807067871\n",
            "Training Iteration 4299, Loss: 4.1103386878967285\n",
            "Training Iteration 4300, Loss: 7.20089054107666\n",
            "Training Iteration 4301, Loss: 6.471047878265381\n",
            "Training Iteration 4302, Loss: 6.375275611877441\n",
            "Training Iteration 4303, Loss: 3.3426454067230225\n",
            "Training Iteration 4304, Loss: 5.147769927978516\n",
            "Training Iteration 4305, Loss: 4.159908294677734\n",
            "Training Iteration 4306, Loss: 6.644771575927734\n",
            "Training Iteration 4307, Loss: 5.463469505310059\n",
            "Training Iteration 4308, Loss: 3.897965669631958\n",
            "Training Iteration 4309, Loss: 4.647505760192871\n",
            "Training Iteration 4310, Loss: 2.802863836288452\n",
            "Training Iteration 4311, Loss: 4.787690162658691\n",
            "Training Iteration 4312, Loss: 4.627444744110107\n",
            "Training Iteration 4313, Loss: 5.466983795166016\n",
            "Training Iteration 4314, Loss: 3.425909996032715\n",
            "Training Iteration 4315, Loss: 3.428473711013794\n",
            "Training Iteration 4316, Loss: 2.4056828022003174\n",
            "Training Iteration 4317, Loss: 6.040184020996094\n",
            "Training Iteration 4318, Loss: 3.9511899948120117\n",
            "Training Iteration 4319, Loss: 5.597414970397949\n",
            "Training Iteration 4320, Loss: 2.145998001098633\n",
            "Training Iteration 4321, Loss: 2.912623405456543\n",
            "Training Iteration 4322, Loss: 3.6824851036071777\n",
            "Training Iteration 4323, Loss: 8.93714427947998\n",
            "Training Iteration 4324, Loss: 7.175187587738037\n",
            "Training Iteration 4325, Loss: 5.336241245269775\n",
            "Training Iteration 4326, Loss: 5.256910800933838\n",
            "Training Iteration 4327, Loss: 5.700929641723633\n",
            "Training Iteration 4328, Loss: 5.2753071784973145\n",
            "Training Iteration 4329, Loss: 4.652419090270996\n",
            "Training Iteration 4330, Loss: 5.179915904998779\n",
            "Training Iteration 4331, Loss: 6.251669406890869\n",
            "Training Iteration 4332, Loss: 5.62506628036499\n",
            "Training Iteration 4333, Loss: 7.727496147155762\n",
            "Training Iteration 4334, Loss: 8.814404487609863\n",
            "Training Iteration 4335, Loss: 6.276106834411621\n",
            "Training Iteration 4336, Loss: 4.344348430633545\n",
            "Training Iteration 4337, Loss: 5.5926642417907715\n",
            "Training Iteration 4338, Loss: 3.0535261631011963\n",
            "Training Iteration 4339, Loss: 6.581840991973877\n",
            "Training Iteration 4340, Loss: 9.103178977966309\n",
            "Training Iteration 4341, Loss: 2.870807409286499\n",
            "Training Iteration 4342, Loss: 2.84666109085083\n",
            "Training Iteration 4343, Loss: 5.781364440917969\n",
            "Training Iteration 4344, Loss: 5.088918685913086\n",
            "Training Iteration 4345, Loss: 6.028594017028809\n",
            "Training Iteration 4346, Loss: 5.743739128112793\n",
            "Training Iteration 4347, Loss: 4.80708122253418\n",
            "Training Iteration 4348, Loss: 3.4407150745391846\n",
            "Training Iteration 4349, Loss: 2.746068000793457\n",
            "Training Iteration 4350, Loss: 3.7821972370147705\n",
            "Training Iteration 4351, Loss: 4.143111228942871\n",
            "Training Iteration 4352, Loss: 5.995501518249512\n",
            "Training Iteration 4353, Loss: 6.251534461975098\n",
            "Training Iteration 4354, Loss: 1.8077490329742432\n",
            "Training Iteration 4355, Loss: 4.94233512878418\n",
            "Training Iteration 4356, Loss: 6.13862419128418\n",
            "Training Iteration 4357, Loss: 4.839865207672119\n",
            "Training Iteration 4358, Loss: 5.851847171783447\n",
            "Training Iteration 4359, Loss: 3.6692395210266113\n",
            "Training Iteration 4360, Loss: 3.524569511413574\n",
            "Training Iteration 4361, Loss: 4.925271034240723\n",
            "Training Iteration 4362, Loss: 5.560235977172852\n",
            "Training Iteration 4363, Loss: 4.527055740356445\n",
            "Training Iteration 4364, Loss: 2.709268569946289\n",
            "Training Iteration 4365, Loss: 4.990832328796387\n",
            "Training Iteration 4366, Loss: 5.1575493812561035\n",
            "Training Iteration 4367, Loss: 4.273484230041504\n",
            "Training Iteration 4368, Loss: 3.268770933151245\n",
            "Training Iteration 4369, Loss: 2.0868582725524902\n",
            "Training Iteration 4370, Loss: 7.040047645568848\n",
            "Training Iteration 4371, Loss: 2.1249918937683105\n",
            "Training Iteration 4372, Loss: 3.4918713569641113\n",
            "Training Iteration 4373, Loss: 4.389053821563721\n",
            "Training Iteration 4374, Loss: 3.3814802169799805\n",
            "Training Iteration 4375, Loss: 5.125949859619141\n",
            "Training Iteration 4376, Loss: 4.427846908569336\n",
            "Training Iteration 4377, Loss: 3.3495123386383057\n",
            "Training Iteration 4378, Loss: 5.11262845993042\n",
            "Training Iteration 4379, Loss: 3.3619329929351807\n",
            "Training Iteration 4380, Loss: 3.8598520755767822\n",
            "Training Iteration 4381, Loss: 4.428847789764404\n",
            "Training Iteration 4382, Loss: 1.6184155941009521\n",
            "Training Iteration 4383, Loss: 2.767779588699341\n",
            "Training Iteration 4384, Loss: 1.7992074489593506\n",
            "Training Iteration 4385, Loss: 2.5179057121276855\n",
            "Training Iteration 4386, Loss: 6.755765914916992\n",
            "Training Iteration 4387, Loss: 3.5458860397338867\n",
            "Training Iteration 4388, Loss: 3.8426098823547363\n",
            "Training Iteration 4389, Loss: 3.0576395988464355\n",
            "Training Iteration 4390, Loss: 3.490058422088623\n",
            "Training Iteration 4391, Loss: 5.883790969848633\n",
            "Training Iteration 4392, Loss: 2.8177194595336914\n",
            "Training Iteration 4393, Loss: 4.31671142578125\n",
            "Training Iteration 4394, Loss: 3.5530011653900146\n",
            "Training Iteration 4395, Loss: 2.972104549407959\n",
            "Training Iteration 4396, Loss: 4.99377965927124\n",
            "Training Iteration 4397, Loss: 2.093101978302002\n",
            "Training Iteration 4398, Loss: 3.6414005756378174\n",
            "Training Iteration 4399, Loss: 6.258785247802734\n",
            "Training Iteration 4400, Loss: 4.018267631530762\n",
            "Training Iteration 4401, Loss: 4.143115997314453\n",
            "Training Iteration 4402, Loss: 3.105964183807373\n",
            "Training Iteration 4403, Loss: 5.973780632019043\n",
            "Training Iteration 4404, Loss: 3.403010606765747\n",
            "Training Iteration 4405, Loss: 5.535788536071777\n",
            "Training Iteration 4406, Loss: 2.90901780128479\n",
            "Training Iteration 4407, Loss: 5.515472412109375\n",
            "Training Iteration 4408, Loss: 4.872382164001465\n",
            "Training Iteration 4409, Loss: 2.7815535068511963\n",
            "Training Iteration 4410, Loss: 4.4437255859375\n",
            "Training Iteration 4411, Loss: 3.2472221851348877\n",
            "Training Iteration 4412, Loss: 3.27858567237854\n",
            "Training Iteration 4413, Loss: 4.4514689445495605\n",
            "Training Iteration 4414, Loss: 2.433389186859131\n",
            "Training Iteration 4415, Loss: 2.593313455581665\n",
            "Training Iteration 4416, Loss: 1.6285037994384766\n",
            "Training Iteration 4417, Loss: 4.03672981262207\n",
            "Training Iteration 4418, Loss: 4.631936073303223\n",
            "Training Iteration 4419, Loss: 6.150551795959473\n",
            "Training Iteration 4420, Loss: 2.2489736080169678\n",
            "Training Iteration 4421, Loss: 4.110547065734863\n",
            "Training Iteration 4422, Loss: 5.130587577819824\n",
            "Training Iteration 4423, Loss: 2.3193836212158203\n",
            "Training Iteration 4424, Loss: 2.8617188930511475\n",
            "Training Iteration 4425, Loss: 3.7256124019622803\n",
            "Training Iteration 4426, Loss: 5.964976787567139\n",
            "Training Iteration 4427, Loss: 3.5107686519622803\n",
            "Training Iteration 4428, Loss: 3.5218822956085205\n",
            "Training Iteration 4429, Loss: 3.3964266777038574\n",
            "Training Iteration 4430, Loss: 4.275514125823975\n",
            "Training Iteration 4431, Loss: 4.953130722045898\n",
            "Training Iteration 4432, Loss: 3.3159048557281494\n",
            "Training Iteration 4433, Loss: 3.3764114379882812\n",
            "Training Iteration 4434, Loss: 4.604767799377441\n",
            "Training Iteration 4435, Loss: 5.703458786010742\n",
            "Training Iteration 4436, Loss: 4.865307331085205\n",
            "Training Iteration 4437, Loss: 4.95423698425293\n",
            "Training Iteration 4438, Loss: 4.789999961853027\n",
            "Training Iteration 4439, Loss: 5.597504138946533\n",
            "Training Iteration 4440, Loss: 5.1295599937438965\n",
            "Training Iteration 4441, Loss: 4.898667335510254\n",
            "Training Iteration 4442, Loss: 8.864809036254883\n",
            "Training Iteration 4443, Loss: 5.100676536560059\n",
            "Training Iteration 4444, Loss: 7.965283393859863\n",
            "Training Iteration 4445, Loss: 3.400202751159668\n",
            "Training Iteration 4446, Loss: 3.768650770187378\n",
            "Training Iteration 4447, Loss: 3.1363959312438965\n",
            "Training Iteration 4448, Loss: 3.4023795127868652\n",
            "Training Iteration 4449, Loss: 5.298487663269043\n",
            "Training Iteration 4450, Loss: 6.980340480804443\n",
            "Training Iteration 4451, Loss: 4.271230697631836\n",
            "Training Iteration 4452, Loss: 5.531146049499512\n",
            "Training Iteration 4453, Loss: 6.48240327835083\n",
            "Training Iteration 4454, Loss: 2.427337646484375\n",
            "Training Iteration 4455, Loss: 3.1949448585510254\n",
            "Training Iteration 4456, Loss: 4.48590087890625\n",
            "Training Iteration 4457, Loss: 4.602395534515381\n",
            "Training Iteration 4458, Loss: 4.801539897918701\n",
            "Training Iteration 4459, Loss: 4.499375820159912\n",
            "Training Iteration 4460, Loss: 5.2269182205200195\n",
            "Training Iteration 4461, Loss: 3.495814323425293\n",
            "Training Iteration 4462, Loss: 6.169884204864502\n",
            "Training Iteration 4463, Loss: 4.786823272705078\n",
            "Training Iteration 4464, Loss: 2.503082513809204\n",
            "Training Iteration 4465, Loss: 2.650695323944092\n",
            "Training Iteration 4466, Loss: 3.0210790634155273\n",
            "Training Iteration 4467, Loss: 4.511617660522461\n",
            "Training Iteration 4468, Loss: 5.284031391143799\n",
            "Training Iteration 4469, Loss: 4.9580769538879395\n",
            "Training Iteration 4470, Loss: 6.055886745452881\n",
            "Training Iteration 4471, Loss: 4.684665203094482\n",
            "Training Iteration 4472, Loss: 3.883392572402954\n",
            "Training Iteration 4473, Loss: 7.791991233825684\n",
            "Training Iteration 4474, Loss: 6.128389358520508\n",
            "Training Iteration 4475, Loss: 4.906554222106934\n",
            "Training Iteration 4476, Loss: 6.718639373779297\n",
            "Training Iteration 4477, Loss: 3.6926615238189697\n",
            "Training Iteration 4478, Loss: 4.0156402587890625\n",
            "Training Iteration 4479, Loss: 5.931331157684326\n",
            "Training Iteration 4480, Loss: 4.6782026290893555\n",
            "Training Iteration 4481, Loss: 6.363150596618652\n",
            "Training Iteration 4482, Loss: 6.422501087188721\n",
            "Training Iteration 4483, Loss: 6.43551778793335\n",
            "Training Iteration 4484, Loss: 5.45623779296875\n",
            "Training Iteration 4485, Loss: 5.322662353515625\n",
            "Training Iteration 4486, Loss: 4.409796714782715\n",
            "Training Iteration 4487, Loss: 4.283730506896973\n",
            "Training Iteration 4488, Loss: 6.492330551147461\n",
            "Training Iteration 4489, Loss: 2.896392345428467\n",
            "Training Iteration 4490, Loss: 8.352506637573242\n",
            "Training Iteration 4491, Loss: 3.878100872039795\n",
            "Training Iteration 4492, Loss: 5.404512405395508\n",
            "Training Iteration 4493, Loss: 6.468067169189453\n",
            "Training Iteration 4494, Loss: 7.2946553230285645\n",
            "Training Iteration 4495, Loss: 4.984485626220703\n",
            "Training Iteration 4496, Loss: 5.492833137512207\n",
            "Training Iteration 4497, Loss: 5.147760391235352\n",
            "Training Iteration 4498, Loss: 4.5429205894470215\n",
            "Training Iteration 4499, Loss: 5.539949893951416\n",
            "Training Iteration 4500, Loss: 4.8722333908081055\n",
            "Training Iteration 4501, Loss: 3.5088016986846924\n",
            "Training Iteration 4502, Loss: 6.206213474273682\n",
            "Training Iteration 4503, Loss: 5.633032321929932\n",
            "Training Iteration 4504, Loss: 2.7932307720184326\n",
            "Training Iteration 4505, Loss: 5.531091690063477\n",
            "Training Iteration 4506, Loss: 6.975098133087158\n",
            "Training Iteration 4507, Loss: 1.7097066640853882\n",
            "Training Iteration 4508, Loss: 4.884827136993408\n",
            "Training Iteration 4509, Loss: 3.420574426651001\n",
            "Training Iteration 4510, Loss: 3.966047763824463\n",
            "Training Iteration 4511, Loss: 3.755310297012329\n",
            "Training Iteration 4512, Loss: 6.012889862060547\n",
            "Training Iteration 4513, Loss: 2.3330676555633545\n",
            "Training Iteration 4514, Loss: 4.183196067810059\n",
            "Training Iteration 4515, Loss: 4.778670310974121\n",
            "Training Iteration 4516, Loss: 3.848435163497925\n",
            "Training Iteration 4517, Loss: 2.9551267623901367\n",
            "Training Iteration 4518, Loss: 2.8784189224243164\n",
            "Training Iteration 4519, Loss: 3.74277663230896\n",
            "Training Iteration 4520, Loss: 2.7287838459014893\n",
            "Training Iteration 4521, Loss: 5.583117961883545\n",
            "Training Iteration 4522, Loss: 4.892092704772949\n",
            "Training Iteration 4523, Loss: 2.901719570159912\n",
            "Training Iteration 4524, Loss: 4.2655792236328125\n",
            "Training Iteration 4525, Loss: 2.0440280437469482\n",
            "Training Iteration 4526, Loss: 4.238076210021973\n",
            "Training Iteration 4527, Loss: 7.0261006355285645\n",
            "Training Iteration 4528, Loss: 7.793342590332031\n",
            "Training Iteration 4529, Loss: 5.0685577392578125\n",
            "Training Iteration 4530, Loss: 3.809734582901001\n",
            "Training Iteration 4531, Loss: 2.8309080600738525\n",
            "Training Iteration 4532, Loss: 3.1937336921691895\n",
            "Training Iteration 4533, Loss: 6.690595626831055\n",
            "Training Iteration 4534, Loss: 7.2513837814331055\n",
            "Training Iteration 4535, Loss: 3.6251580715179443\n",
            "Training Iteration 4536, Loss: 3.7075490951538086\n",
            "Training Iteration 4537, Loss: 3.061000108718872\n",
            "Training Iteration 4538, Loss: 3.1715497970581055\n",
            "Training Iteration 4539, Loss: 4.034568786621094\n",
            "Training Iteration 4540, Loss: 3.147083044052124\n",
            "Training Iteration 4541, Loss: 4.542724132537842\n",
            "Training Iteration 4542, Loss: 3.5201945304870605\n",
            "Training Iteration 4543, Loss: 2.7513251304626465\n",
            "Training Iteration 4544, Loss: 4.096823692321777\n",
            "Training Iteration 4545, Loss: 5.368839263916016\n",
            "Training Iteration 4546, Loss: 4.507019996643066\n",
            "Training Iteration 4547, Loss: 3.540740489959717\n",
            "Training Iteration 4548, Loss: 5.15271520614624\n",
            "Training Iteration 4549, Loss: 2.133790969848633\n",
            "Training Iteration 4550, Loss: 5.7406487464904785\n",
            "Training Iteration 4551, Loss: 3.5584473609924316\n",
            "Training Iteration 4552, Loss: 3.7890682220458984\n",
            "Training Iteration 4553, Loss: 4.338581562042236\n",
            "Training Iteration 4554, Loss: 6.058230400085449\n",
            "Training Iteration 4555, Loss: 4.8260087966918945\n",
            "Training Iteration 4556, Loss: 3.6000494956970215\n",
            "Training Iteration 4557, Loss: 5.356168270111084\n",
            "Training Iteration 4558, Loss: 4.871660232543945\n",
            "Training Iteration 4559, Loss: 6.869484901428223\n",
            "Training Iteration 4560, Loss: 4.898395538330078\n",
            "Training Iteration 4561, Loss: 2.0676426887512207\n",
            "Training Iteration 4562, Loss: 3.9578332901000977\n",
            "Training Iteration 4563, Loss: 3.0792953968048096\n",
            "Training Iteration 4564, Loss: 3.686871290206909\n",
            "Training Iteration 4565, Loss: 5.607082366943359\n",
            "Training Iteration 4566, Loss: 3.9232378005981445\n",
            "Training Iteration 4567, Loss: 2.5212624073028564\n",
            "Training Iteration 4568, Loss: 3.68864369392395\n",
            "Training Iteration 4569, Loss: 3.518665075302124\n",
            "Training Iteration 4570, Loss: 9.14928913116455\n",
            "Training Iteration 4571, Loss: 3.8691608905792236\n",
            "Training Iteration 4572, Loss: 1.5686954259872437\n",
            "Training Iteration 4573, Loss: 4.122580051422119\n",
            "Training Iteration 4574, Loss: 3.8691234588623047\n",
            "Training Iteration 4575, Loss: 3.943424940109253\n",
            "Training Iteration 4576, Loss: 3.666806697845459\n",
            "Training Iteration 4577, Loss: 6.281400680541992\n",
            "Training Iteration 4578, Loss: 6.29062557220459\n",
            "Training Iteration 4579, Loss: 3.8956146240234375\n",
            "Training Iteration 4580, Loss: 4.254057884216309\n",
            "Training Iteration 4581, Loss: 4.486156463623047\n",
            "Training Iteration 4582, Loss: 5.089414596557617\n",
            "Training Iteration 4583, Loss: 9.091897964477539\n",
            "Training Iteration 4584, Loss: 2.1111700534820557\n",
            "Training Iteration 4585, Loss: 4.073357582092285\n",
            "Training Iteration 4586, Loss: 4.542482376098633\n",
            "Training Iteration 4587, Loss: 3.529756546020508\n",
            "Training Iteration 4588, Loss: 5.378817081451416\n",
            "Training Iteration 4589, Loss: 4.312960147857666\n",
            "Training Iteration 4590, Loss: 4.086038589477539\n",
            "Training Iteration 4591, Loss: 4.21501350402832\n",
            "Training Iteration 4592, Loss: 4.245440483093262\n",
            "Training Iteration 4593, Loss: 6.000974178314209\n",
            "Training Iteration 4594, Loss: 5.021943092346191\n",
            "Training Iteration 4595, Loss: 6.858306407928467\n",
            "Training Iteration 4596, Loss: 4.372970104217529\n",
            "Training Iteration 4597, Loss: 4.100945472717285\n",
            "Training Iteration 4598, Loss: 2.0973103046417236\n",
            "Training Iteration 4599, Loss: 3.7299001216888428\n",
            "Training Iteration 4600, Loss: 6.928380489349365\n",
            "Training Iteration 4601, Loss: 3.1563234329223633\n",
            "Training Iteration 4602, Loss: 3.2120702266693115\n",
            "Training Iteration 4603, Loss: 2.546785593032837\n",
            "Training Iteration 4604, Loss: 1.8355281352996826\n",
            "Training Iteration 4605, Loss: 5.551828861236572\n",
            "Training Iteration 4606, Loss: 4.478137016296387\n",
            "Training Iteration 4607, Loss: 5.223850727081299\n",
            "Training Iteration 4608, Loss: 2.1979005336761475\n",
            "Training Iteration 4609, Loss: 5.97465705871582\n",
            "Training Iteration 4610, Loss: 7.933996200561523\n",
            "Training Iteration 4611, Loss: 5.45848274230957\n",
            "Training Iteration 4612, Loss: 1.8625954389572144\n",
            "Training Iteration 4613, Loss: 3.3341619968414307\n",
            "Training Iteration 4614, Loss: 4.315609931945801\n",
            "Training Iteration 4615, Loss: 3.8040621280670166\n",
            "Training Iteration 4616, Loss: 4.536761283874512\n",
            "Training Iteration 4617, Loss: 1.47771155834198\n",
            "Training Iteration 4618, Loss: 7.2505364418029785\n",
            "Training Iteration 4619, Loss: 6.390738010406494\n",
            "Training Iteration 4620, Loss: 2.0535287857055664\n",
            "Training Iteration 4621, Loss: 3.5282251834869385\n",
            "Training Iteration 4622, Loss: 1.6951342821121216\n",
            "Training Iteration 4623, Loss: 1.8502275943756104\n",
            "Training Iteration 4624, Loss: 6.4396867752075195\n",
            "Training Iteration 4625, Loss: 5.08180046081543\n",
            "Training Iteration 4626, Loss: 2.6555614471435547\n",
            "Training Iteration 4627, Loss: 4.229994773864746\n",
            "Training Iteration 4628, Loss: 4.157798767089844\n",
            "Training Iteration 4629, Loss: 5.289036750793457\n",
            "Training Iteration 4630, Loss: 2.4600768089294434\n",
            "Training Iteration 4631, Loss: 2.932809352874756\n",
            "Training Iteration 4632, Loss: 4.841835021972656\n",
            "Training Iteration 4633, Loss: 4.574950218200684\n",
            "Training Iteration 4634, Loss: 4.919044494628906\n",
            "Training Iteration 4635, Loss: 5.957721710205078\n",
            "Training Iteration 4636, Loss: 6.289377689361572\n",
            "Training Iteration 4637, Loss: 2.0960540771484375\n",
            "Training Iteration 4638, Loss: 6.3231658935546875\n",
            "Training Iteration 4639, Loss: 7.098901748657227\n",
            "Training Iteration 4640, Loss: 7.328096866607666\n",
            "Training Iteration 4641, Loss: 3.2579805850982666\n",
            "Training Iteration 4642, Loss: 5.199354648590088\n",
            "Training Iteration 4643, Loss: 3.4282965660095215\n",
            "Training Iteration 4644, Loss: 7.201844692230225\n",
            "Training Iteration 4645, Loss: 2.550657272338867\n",
            "Training Iteration 4646, Loss: 4.1639628410339355\n",
            "Training Iteration 4647, Loss: 3.3094279766082764\n",
            "Training Iteration 4648, Loss: 5.0429606437683105\n",
            "Training Iteration 4649, Loss: 3.2893993854522705\n",
            "Training Iteration 4650, Loss: 2.6826679706573486\n",
            "Training Iteration 4651, Loss: 4.925234794616699\n",
            "Training Iteration 4652, Loss: 5.665430068969727\n",
            "Training Iteration 4653, Loss: 5.930891036987305\n",
            "Training Iteration 4654, Loss: 3.864718198776245\n",
            "Training Iteration 4655, Loss: 3.8037517070770264\n",
            "Training Iteration 4656, Loss: 7.820444107055664\n",
            "Training Iteration 4657, Loss: 6.8791890144348145\n",
            "Training Iteration 4658, Loss: 4.325884819030762\n",
            "Training Iteration 4659, Loss: 1.6062827110290527\n",
            "Training Iteration 4660, Loss: 7.6164164543151855\n",
            "Training Iteration 4661, Loss: 3.692108631134033\n",
            "Training Iteration 4662, Loss: 7.176303863525391\n",
            "Training Iteration 4663, Loss: 4.4242095947265625\n",
            "Training Iteration 4664, Loss: 3.6438088417053223\n",
            "Training Iteration 4665, Loss: 7.911115646362305\n",
            "Training Iteration 4666, Loss: 5.037102222442627\n",
            "Training Iteration 4667, Loss: 7.252223968505859\n",
            "Training Iteration 4668, Loss: 5.922440052032471\n",
            "Training Iteration 4669, Loss: 9.307905197143555\n",
            "Training Iteration 4670, Loss: 2.6366937160491943\n",
            "Training Iteration 4671, Loss: 5.96011209487915\n",
            "Training Iteration 4672, Loss: 6.9113569259643555\n",
            "Training Iteration 4673, Loss: 4.645524024963379\n",
            "Training Iteration 4674, Loss: 2.31520676612854\n",
            "Training Iteration 4675, Loss: 7.1629791259765625\n",
            "Training Iteration 4676, Loss: 7.54720401763916\n",
            "Training Iteration 4677, Loss: 3.531057596206665\n",
            "Training Iteration 4678, Loss: 7.743422985076904\n",
            "Training Iteration 4679, Loss: 6.33031702041626\n",
            "Training Iteration 4680, Loss: 6.1966729164123535\n",
            "Training Iteration 4681, Loss: 3.1005749702453613\n",
            "Training Iteration 4682, Loss: 5.892965316772461\n",
            "Training Iteration 4683, Loss: 5.366937160491943\n",
            "Training Iteration 4684, Loss: 6.67559289932251\n",
            "Training Iteration 4685, Loss: 5.533022880554199\n",
            "Training Iteration 4686, Loss: 2.002070665359497\n",
            "Training Iteration 4687, Loss: 2.574756383895874\n",
            "Training Iteration 4688, Loss: 5.9028167724609375\n",
            "Training Iteration 4689, Loss: 10.484833717346191\n",
            "Training Iteration 4690, Loss: 2.6996634006500244\n",
            "Training Iteration 4691, Loss: 4.373353958129883\n",
            "Training Iteration 4692, Loss: 5.552753448486328\n",
            "Training Iteration 4693, Loss: 4.466246128082275\n",
            "Training Iteration 4694, Loss: 4.1690897941589355\n",
            "Training Iteration 4695, Loss: 5.288857460021973\n",
            "Training Iteration 4696, Loss: 9.224980354309082\n",
            "Training Iteration 4697, Loss: 13.304070472717285\n",
            "Training Iteration 4698, Loss: 2.472322463989258\n",
            "Training Iteration 4699, Loss: 7.338594436645508\n",
            "Training Iteration 4700, Loss: 8.038202285766602\n",
            "Training Iteration 4701, Loss: 3.3211171627044678\n",
            "Training Iteration 4702, Loss: 2.310218334197998\n",
            "Training Iteration 4703, Loss: 5.688906669616699\n",
            "Training Iteration 4704, Loss: 2.3907361030578613\n",
            "Training Iteration 4705, Loss: 4.156681060791016\n",
            "Training Iteration 4706, Loss: 4.837018013000488\n",
            "Training Iteration 4707, Loss: 3.566159725189209\n",
            "Training Iteration 4708, Loss: 4.933839797973633\n",
            "Training Iteration 4709, Loss: 2.7577574253082275\n",
            "Training Iteration 4710, Loss: 1.6785492897033691\n",
            "Training Iteration 4711, Loss: 3.72965669631958\n",
            "Training Iteration 4712, Loss: 3.435422658920288\n",
            "Training Iteration 4713, Loss: 2.029676914215088\n",
            "Training Iteration 4714, Loss: 3.4146947860717773\n",
            "Training Iteration 4715, Loss: 4.0406060218811035\n",
            "Training Iteration 4716, Loss: 3.5878376960754395\n",
            "Training Iteration 4717, Loss: 6.099979400634766\n",
            "Training Iteration 4718, Loss: 3.6958513259887695\n",
            "Training Iteration 4719, Loss: 5.379904270172119\n",
            "Training Iteration 4720, Loss: 4.64173698425293\n",
            "Training Iteration 4721, Loss: 4.025883674621582\n",
            "Training Iteration 4722, Loss: 3.2391834259033203\n",
            "Training Iteration 4723, Loss: 4.802658557891846\n",
            "Training Iteration 4724, Loss: 3.140352487564087\n",
            "Training Iteration 4725, Loss: 7.535807132720947\n",
            "Training Iteration 4726, Loss: 4.4429030418396\n",
            "Training Iteration 4727, Loss: 5.55661153793335\n",
            "Training Iteration 4728, Loss: 9.04172134399414\n",
            "Training Iteration 4729, Loss: 3.7335920333862305\n",
            "Training Iteration 4730, Loss: 2.72574520111084\n",
            "Training Iteration 4731, Loss: 3.4506802558898926\n",
            "Training Iteration 4732, Loss: 3.818697929382324\n",
            "Training Iteration 4733, Loss: 4.253717422485352\n",
            "Training Iteration 4734, Loss: 4.707834243774414\n",
            "Training Iteration 4735, Loss: 5.1871657371521\n",
            "Training Iteration 4736, Loss: 2.3841347694396973\n",
            "Training Iteration 4737, Loss: 5.147618770599365\n",
            "Training Iteration 4738, Loss: 3.808774948120117\n",
            "Training Iteration 4739, Loss: 3.043304920196533\n",
            "Training Iteration 4740, Loss: 2.7385880947113037\n",
            "Training Iteration 4741, Loss: 2.0479679107666016\n",
            "Training Iteration 4742, Loss: 5.001504898071289\n",
            "Training Iteration 4743, Loss: 4.9406208992004395\n",
            "Training Iteration 4744, Loss: 2.9942727088928223\n",
            "Training Iteration 4745, Loss: 5.520833492279053\n",
            "Training Iteration 4746, Loss: 2.846804618835449\n",
            "Training Iteration 4747, Loss: 4.283078670501709\n",
            "Training Iteration 4748, Loss: 3.040654182434082\n",
            "Training Iteration 4749, Loss: 2.4794557094573975\n",
            "Training Iteration 4750, Loss: 3.7018625736236572\n",
            "Training Iteration 4751, Loss: 6.037491798400879\n",
            "Training Iteration 4752, Loss: 2.573810577392578\n",
            "Training Iteration 4753, Loss: 5.252178192138672\n",
            "Training Iteration 4754, Loss: 1.28175687789917\n",
            "Training Iteration 4755, Loss: 5.48063850402832\n",
            "Training Iteration 4756, Loss: 3.499309778213501\n",
            "Training Iteration 4757, Loss: 4.937826633453369\n",
            "Training Iteration 4758, Loss: 5.910715103149414\n",
            "Training Iteration 4759, Loss: 2.283961296081543\n",
            "Training Iteration 4760, Loss: 3.7992641925811768\n",
            "Training Iteration 4761, Loss: 2.174082040786743\n",
            "Training Iteration 4762, Loss: 3.3344438076019287\n",
            "Training Iteration 4763, Loss: 5.809633255004883\n",
            "Training Iteration 4764, Loss: 4.736247539520264\n",
            "Training Iteration 4765, Loss: 4.333174228668213\n",
            "Training Iteration 4766, Loss: 1.4509339332580566\n",
            "Training Iteration 4767, Loss: 4.058721542358398\n",
            "Training Iteration 4768, Loss: 3.868889570236206\n",
            "Training Iteration 4769, Loss: 6.624673843383789\n",
            "Training Iteration 4770, Loss: 3.4354159832000732\n",
            "Training Iteration 4771, Loss: 3.4463860988616943\n",
            "Training Iteration 4772, Loss: 7.074785232543945\n",
            "Training Iteration 4773, Loss: 5.451834678649902\n",
            "Training Iteration 4774, Loss: 5.18511438369751\n",
            "Training Iteration 4775, Loss: 7.445932865142822\n",
            "Training Iteration 4776, Loss: 6.399904251098633\n",
            "Training Iteration 4777, Loss: 4.916316032409668\n",
            "Training Iteration 4778, Loss: 7.92177677154541\n",
            "Training Iteration 4779, Loss: 5.881811618804932\n",
            "Training Iteration 4780, Loss: 3.5982868671417236\n",
            "Training Iteration 4781, Loss: 2.6939852237701416\n",
            "Training Iteration 4782, Loss: 5.568831443786621\n",
            "Training Iteration 4783, Loss: 7.440555572509766\n",
            "Training Iteration 4784, Loss: 5.89785099029541\n",
            "Training Iteration 4785, Loss: 6.7821550369262695\n",
            "Training Iteration 4786, Loss: 6.405086040496826\n",
            "Training Iteration 4787, Loss: 7.169679641723633\n",
            "Training Iteration 4788, Loss: 3.794166088104248\n",
            "Training Iteration 4789, Loss: 4.369871616363525\n",
            "Training Iteration 4790, Loss: 7.167375087738037\n",
            "Training Iteration 4791, Loss: 5.431366920471191\n",
            "Training Iteration 4792, Loss: 5.485800743103027\n",
            "Training Iteration 4793, Loss: 5.542850017547607\n",
            "Training Iteration 4794, Loss: 3.5868163108825684\n",
            "Training Iteration 4795, Loss: 2.5953421592712402\n",
            "Training Iteration 4796, Loss: 3.6270687580108643\n",
            "Training Iteration 4797, Loss: 4.511123180389404\n",
            "Training Iteration 4798, Loss: 3.160355806350708\n",
            "Training Iteration 4799, Loss: 7.023736000061035\n",
            "Training Iteration 4800, Loss: 4.410106658935547\n",
            "Training Iteration 4801, Loss: 3.887033700942993\n",
            "Training Iteration 4802, Loss: 6.604879379272461\n",
            "Training Iteration 4803, Loss: 5.129006385803223\n",
            "Training Iteration 4804, Loss: 6.563845634460449\n",
            "Training Iteration 4805, Loss: 2.1139180660247803\n",
            "Training Iteration 4806, Loss: 4.727753162384033\n",
            "Training Iteration 4807, Loss: 3.3395018577575684\n",
            "Training Iteration 4808, Loss: 2.283865213394165\n",
            "Training Iteration 4809, Loss: 4.087278842926025\n",
            "Training Iteration 4810, Loss: 5.1025590896606445\n",
            "Training Iteration 4811, Loss: 4.107345104217529\n",
            "Training Iteration 4812, Loss: 4.501626968383789\n",
            "Training Iteration 4813, Loss: 3.984589099884033\n",
            "Training Iteration 4814, Loss: 1.6590869426727295\n",
            "Training Iteration 4815, Loss: 7.246555328369141\n",
            "Training Iteration 4816, Loss: 6.462474822998047\n",
            "Training Iteration 4817, Loss: 1.0930746793746948\n",
            "Training Iteration 4818, Loss: 4.410473823547363\n",
            "Training Iteration 4819, Loss: 4.88043737411499\n",
            "Training Iteration 4820, Loss: 7.284258842468262\n",
            "Training Iteration 4821, Loss: 4.779934406280518\n",
            "Training Iteration 4822, Loss: 4.537909030914307\n",
            "Training Iteration 4823, Loss: 4.823982238769531\n",
            "Training Iteration 4824, Loss: 3.7635445594787598\n",
            "Training Iteration 4825, Loss: 4.195279598236084\n",
            "Training Iteration 4826, Loss: 3.250128746032715\n",
            "Training Iteration 4827, Loss: 4.95882511138916\n",
            "Training Iteration 4828, Loss: 5.7017741203308105\n",
            "Training Iteration 4829, Loss: 6.876199722290039\n",
            "Training Iteration 4830, Loss: 7.244073390960693\n",
            "Training Iteration 4831, Loss: 6.597676753997803\n",
            "Training Iteration 4832, Loss: 6.009446620941162\n",
            "Training Iteration 4833, Loss: 2.5662670135498047\n",
            "Training Iteration 4834, Loss: 2.245096206665039\n",
            "Training Iteration 4835, Loss: 4.14703893661499\n",
            "Training Iteration 4836, Loss: 2.932309865951538\n",
            "Training Iteration 4837, Loss: 2.7384986877441406\n",
            "Training Iteration 4838, Loss: 2.9502909183502197\n",
            "Training Iteration 4839, Loss: 6.664485931396484\n",
            "Training Iteration 4840, Loss: 3.244371175765991\n",
            "Training Iteration 4841, Loss: 4.44478702545166\n",
            "Training Iteration 4842, Loss: 4.760315418243408\n",
            "Training Iteration 4843, Loss: 4.622669219970703\n",
            "Training Iteration 4844, Loss: 3.730570077896118\n",
            "Training Iteration 4845, Loss: 3.617055654525757\n",
            "Training Iteration 4846, Loss: 4.701842784881592\n",
            "Training Iteration 4847, Loss: 4.673316955566406\n",
            "Training Iteration 4848, Loss: 4.953608512878418\n",
            "Training Iteration 4849, Loss: 5.1898112297058105\n",
            "Training Iteration 4850, Loss: 10.772989273071289\n",
            "Training Iteration 4851, Loss: 5.696401119232178\n",
            "Training Iteration 4852, Loss: 4.096967697143555\n",
            "Training Iteration 4853, Loss: 3.62941575050354\n",
            "Training Iteration 4854, Loss: 2.693957567214966\n",
            "Training Iteration 4855, Loss: 3.508146047592163\n",
            "Training Iteration 4856, Loss: 6.919416427612305\n",
            "Training Iteration 4857, Loss: 6.150879859924316\n",
            "Training Iteration 4858, Loss: 5.28558349609375\n",
            "Training Iteration 4859, Loss: 3.966357946395874\n",
            "Training Iteration 4860, Loss: 5.951291561126709\n",
            "Training Iteration 4861, Loss: 4.124764919281006\n",
            "Training Iteration 4862, Loss: 2.393089771270752\n",
            "Training Iteration 4863, Loss: 3.493854284286499\n",
            "Training Iteration 4864, Loss: 4.485723495483398\n",
            "Training Iteration 4865, Loss: 4.772565841674805\n",
            "Training Iteration 4866, Loss: 7.364963054656982\n",
            "Training Iteration 4867, Loss: 1.9653356075286865\n",
            "Training Iteration 4868, Loss: 3.061237335205078\n",
            "Training Iteration 4869, Loss: 3.148352861404419\n",
            "Training Iteration 4870, Loss: 4.019220352172852\n",
            "Training Iteration 4871, Loss: 4.629344463348389\n",
            "Training Iteration 4872, Loss: 3.951232433319092\n",
            "Training Iteration 4873, Loss: 7.444474220275879\n",
            "Training Iteration 4874, Loss: 7.075705528259277\n",
            "Training Iteration 4875, Loss: 3.952390432357788\n",
            "Training Iteration 4876, Loss: 1.9745774269104004\n",
            "Training Iteration 4877, Loss: 4.191390037536621\n",
            "Training Iteration 4878, Loss: 2.1806163787841797\n",
            "Training Iteration 4879, Loss: 6.5332746505737305\n",
            "Training Iteration 4880, Loss: 4.632224082946777\n",
            "Training Iteration 4881, Loss: 3.7638051509857178\n",
            "Training Iteration 4882, Loss: 3.7611083984375\n",
            "Training Iteration 4883, Loss: 6.412991523742676\n",
            "Training Iteration 4884, Loss: 2.3718154430389404\n",
            "Training Iteration 4885, Loss: 5.451910018920898\n",
            "Training Iteration 4886, Loss: 2.975905179977417\n",
            "Training Iteration 4887, Loss: 4.3905816078186035\n",
            "Training Iteration 4888, Loss: 2.99066162109375\n",
            "Training Iteration 4889, Loss: 3.1327030658721924\n",
            "Training Iteration 4890, Loss: 5.955258846282959\n",
            "Training Iteration 4891, Loss: 6.82431697845459\n",
            "Training Iteration 4892, Loss: 3.580989360809326\n",
            "Training Iteration 4893, Loss: 4.060404300689697\n",
            "Training Iteration 4894, Loss: 3.2349305152893066\n",
            "Training Iteration 4895, Loss: 1.4100406169891357\n",
            "Training Iteration 4896, Loss: 2.908809185028076\n",
            "Training Iteration 4897, Loss: 1.8840315341949463\n",
            "Training Iteration 4898, Loss: 2.1388394832611084\n",
            "Training Iteration 4899, Loss: 5.530333042144775\n",
            "Training Iteration 4900, Loss: 5.048100471496582\n",
            "Training Iteration 4901, Loss: 3.7252414226531982\n",
            "Training Iteration 4902, Loss: 6.323073863983154\n",
            "Training Iteration 4903, Loss: 4.45404577255249\n",
            "Training Iteration 4904, Loss: 2.99946665763855\n",
            "Training Iteration 4905, Loss: 4.874518394470215\n",
            "Training Iteration 4906, Loss: 3.886364221572876\n",
            "Training Iteration 4907, Loss: 4.542994976043701\n",
            "Training Iteration 4908, Loss: 4.057468414306641\n",
            "Training Iteration 4909, Loss: 3.37746262550354\n",
            "Training Iteration 4910, Loss: 3.3484134674072266\n",
            "Training Iteration 4911, Loss: 1.8348289728164673\n",
            "Training Iteration 4912, Loss: 8.850945472717285\n",
            "Training Iteration 4913, Loss: 5.4373779296875\n",
            "Training Iteration 4914, Loss: 3.931607961654663\n",
            "Training Iteration 4915, Loss: 5.014621257781982\n",
            "Training Iteration 4916, Loss: 1.9028680324554443\n",
            "Training Iteration 4917, Loss: 4.538553237915039\n",
            "Training Iteration 4918, Loss: 3.256770610809326\n",
            "Training Iteration 4919, Loss: 4.905514717102051\n",
            "Training Iteration 4920, Loss: 5.671942234039307\n",
            "Training Iteration 4921, Loss: 4.711302757263184\n",
            "Training Iteration 4922, Loss: 3.559673547744751\n",
            "Training Iteration 4923, Loss: 4.791500091552734\n",
            "Training Iteration 4924, Loss: 3.2945427894592285\n",
            "Training Iteration 4925, Loss: 8.542163848876953\n",
            "Training Iteration 4926, Loss: 3.094841480255127\n",
            "Training Iteration 4927, Loss: 2.36720609664917\n",
            "Training Iteration 4928, Loss: 3.548086166381836\n",
            "Training Iteration 4929, Loss: 4.6533074378967285\n",
            "Training Iteration 4930, Loss: 1.762838363647461\n",
            "Training Iteration 4931, Loss: 3.0283565521240234\n",
            "Training Iteration 4932, Loss: 2.4432108402252197\n",
            "Training Iteration 4933, Loss: 4.793236255645752\n",
            "Training Iteration 4934, Loss: 4.741854667663574\n",
            "Training Iteration 4935, Loss: 4.01772928237915\n",
            "Training Iteration 4936, Loss: 2.7837555408477783\n",
            "Training Iteration 4937, Loss: 4.28657341003418\n",
            "Training Iteration 4938, Loss: 4.012719631195068\n",
            "Training Iteration 4939, Loss: 2.842010498046875\n",
            "Training Iteration 4940, Loss: 2.7003259658813477\n",
            "Training Iteration 4941, Loss: 7.325478553771973\n",
            "Training Iteration 4942, Loss: 4.074509143829346\n",
            "Training Iteration 4943, Loss: 4.2400803565979\n",
            "Training Iteration 4944, Loss: 3.38097882270813\n",
            "Training Iteration 4945, Loss: 5.876587867736816\n",
            "Training Iteration 4946, Loss: 5.688541412353516\n",
            "Training Iteration 4947, Loss: 2.6703290939331055\n",
            "Training Iteration 4948, Loss: 4.3082194328308105\n",
            "Training Iteration 4949, Loss: 5.566532611846924\n",
            "Training Iteration 4950, Loss: 2.337486743927002\n",
            "Training Iteration 4951, Loss: 3.612334728240967\n",
            "Training Iteration 4952, Loss: 5.811227321624756\n",
            "Training Iteration 4953, Loss: 3.209190607070923\n",
            "Training Iteration 4954, Loss: 4.2126359939575195\n",
            "Training Iteration 4955, Loss: 2.184401035308838\n",
            "Training Iteration 4956, Loss: 8.195749282836914\n",
            "Training Iteration 4957, Loss: 4.4613447189331055\n",
            "Training Iteration 4958, Loss: 4.636966705322266\n",
            "Training Iteration 4959, Loss: 4.041049957275391\n",
            "Training Iteration 4960, Loss: 4.420401573181152\n",
            "Training Iteration 4961, Loss: 2.416058301925659\n",
            "Training Iteration 4962, Loss: 4.136659145355225\n",
            "Training Iteration 4963, Loss: 6.966309070587158\n",
            "Training Iteration 4964, Loss: 4.3099684715271\n",
            "Training Iteration 4965, Loss: 5.448781490325928\n",
            "Training Iteration 4966, Loss: 4.353997230529785\n",
            "Training Iteration 4967, Loss: 6.246342182159424\n",
            "Training Iteration 4968, Loss: 5.639518737792969\n",
            "Training Iteration 4969, Loss: 6.648293495178223\n",
            "Training Iteration 4970, Loss: 3.8538615703582764\n",
            "Training Iteration 4971, Loss: 4.788106441497803\n",
            "Training Iteration 4972, Loss: 3.696232795715332\n",
            "Training Iteration 4973, Loss: 5.147585868835449\n",
            "Training Iteration 4974, Loss: 2.145817995071411\n",
            "Training Iteration 4975, Loss: 4.867486476898193\n",
            "Training Iteration 4976, Loss: 2.861482620239258\n",
            "Training Iteration 4977, Loss: 2.8033547401428223\n",
            "Training Iteration 4978, Loss: 4.501111030578613\n",
            "Training Iteration 4979, Loss: 4.833493232727051\n",
            "Training Iteration 4980, Loss: 4.767103672027588\n",
            "Training Iteration 4981, Loss: 5.154575824737549\n",
            "Training Iteration 4982, Loss: 5.536559104919434\n",
            "Training Iteration 4983, Loss: 4.591764450073242\n",
            "Training Iteration 4984, Loss: 5.635102272033691\n",
            "Training Iteration 4985, Loss: 5.783992767333984\n",
            "Training Iteration 4986, Loss: 4.036200046539307\n",
            "Training Iteration 4987, Loss: 2.2133629322052\n",
            "Training Iteration 4988, Loss: 5.94546365737915\n",
            "Training Iteration 4989, Loss: 5.829510688781738\n",
            "Training Iteration 4990, Loss: 4.517233848571777\n",
            "Training Iteration 4991, Loss: 5.592097282409668\n",
            "Training Iteration 4992, Loss: 5.688679218292236\n",
            "Training Iteration 4993, Loss: 3.973970413208008\n",
            "Training Iteration 4994, Loss: 7.320733070373535\n",
            "Training Iteration 4995, Loss: 5.226784706115723\n",
            "Training Iteration 4996, Loss: 5.8564276695251465\n",
            "Training Iteration 4997, Loss: 4.807282447814941\n",
            "Training Iteration 4998, Loss: 3.091080904006958\n",
            "Training Iteration 4999, Loss: 4.062659740447998\n",
            "Training Iteration 5000, Loss: 2.113420248031616\n",
            "Training Iteration 5001, Loss: 3.223844528198242\n",
            "Training Iteration 5002, Loss: 7.0796709060668945\n",
            "Training Iteration 5003, Loss: 2.416640043258667\n",
            "Training Iteration 5004, Loss: 4.038898944854736\n",
            "Training Iteration 5005, Loss: 4.228811264038086\n",
            "Training Iteration 5006, Loss: 5.02425479888916\n",
            "Training Iteration 5007, Loss: 4.187745094299316\n",
            "Training Iteration 5008, Loss: 6.783400535583496\n",
            "Training Iteration 5009, Loss: 4.759331226348877\n",
            "Training Iteration 5010, Loss: 4.543451309204102\n",
            "Training Iteration 5011, Loss: 2.681480646133423\n",
            "Training Iteration 5012, Loss: 4.36536169052124\n",
            "Training Iteration 5013, Loss: 6.4298624992370605\n",
            "Training Iteration 5014, Loss: 5.955864906311035\n",
            "Training Iteration 5015, Loss: 7.998114109039307\n",
            "Training Iteration 5016, Loss: 8.2092866897583\n",
            "Training Iteration 5017, Loss: 6.6097187995910645\n",
            "Training Iteration 5018, Loss: 3.3384459018707275\n",
            "Training Iteration 5019, Loss: 3.611233949661255\n",
            "Training Iteration 5020, Loss: 3.1558949947357178\n",
            "Training Iteration 5021, Loss: 5.641238212585449\n",
            "Training Iteration 5022, Loss: 7.271585464477539\n",
            "Training Iteration 5023, Loss: 4.096378803253174\n",
            "Training Iteration 5024, Loss: 2.974647045135498\n",
            "Training Iteration 5025, Loss: 5.414857864379883\n",
            "Training Iteration 5026, Loss: 5.355576515197754\n",
            "Training Iteration 5027, Loss: 3.4490668773651123\n",
            "Training Iteration 5028, Loss: 4.2996134757995605\n",
            "Training Iteration 5029, Loss: 3.5099518299102783\n",
            "Training Iteration 5030, Loss: 4.553642749786377\n",
            "Training Iteration 5031, Loss: 3.9519269466400146\n",
            "Training Iteration 5032, Loss: 4.2791948318481445\n",
            "Training Iteration 5033, Loss: 3.030207395553589\n",
            "Training Iteration 5034, Loss: 2.8042924404144287\n",
            "Training Iteration 5035, Loss: 5.213314533233643\n",
            "Training Iteration 5036, Loss: 6.767660140991211\n",
            "Training Iteration 5037, Loss: 4.865906715393066\n",
            "Training Iteration 5038, Loss: 5.783935546875\n",
            "Training Iteration 5039, Loss: 2.677795171737671\n",
            "Training Iteration 5040, Loss: 2.4346024990081787\n",
            "Training Iteration 5041, Loss: 6.3034844398498535\n",
            "Training Iteration 5042, Loss: 3.2634708881378174\n",
            "Training Iteration 5043, Loss: 4.124009609222412\n",
            "Training Iteration 5044, Loss: 3.7298429012298584\n",
            "Training Iteration 5045, Loss: 6.509042263031006\n",
            "Training Iteration 5046, Loss: 5.295443058013916\n",
            "Training Iteration 5047, Loss: 3.08516788482666\n",
            "Training Iteration 5048, Loss: 3.9142203330993652\n",
            "Training Iteration 5049, Loss: 4.184020519256592\n",
            "Training Iteration 5050, Loss: 5.6900153160095215\n",
            "Training Iteration 5051, Loss: 3.310351848602295\n",
            "Training Iteration 5052, Loss: 3.628492832183838\n",
            "Training Iteration 5053, Loss: 4.0021586418151855\n",
            "Training Iteration 5054, Loss: 3.8902595043182373\n",
            "Training Iteration 5055, Loss: 5.953008651733398\n",
            "Training Iteration 5056, Loss: 3.3019845485687256\n",
            "Training Iteration 5057, Loss: 6.420100688934326\n",
            "Training Iteration 5058, Loss: 6.370183944702148\n",
            "Training Iteration 5059, Loss: 3.9074020385742188\n",
            "Training Iteration 5060, Loss: 3.477964162826538\n",
            "Training Iteration 5061, Loss: 4.098817825317383\n",
            "Training Iteration 5062, Loss: 1.5275404453277588\n",
            "Training Iteration 5063, Loss: 2.7995786666870117\n",
            "Training Iteration 5064, Loss: 3.487423896789551\n",
            "Training Iteration 5065, Loss: 2.348379135131836\n",
            "Training Iteration 5066, Loss: 4.039686679840088\n",
            "Training Iteration 5067, Loss: 3.137967348098755\n",
            "Training Iteration 5068, Loss: 2.7212533950805664\n",
            "Training Iteration 5069, Loss: 4.356101036071777\n",
            "Training Iteration 5070, Loss: 6.694799423217773\n",
            "Training Iteration 5071, Loss: 2.005910634994507\n",
            "Training Iteration 5072, Loss: 2.628661632537842\n",
            "Training Iteration 5073, Loss: 5.997793197631836\n",
            "Training Iteration 5074, Loss: 4.224466323852539\n",
            "Training Iteration 5075, Loss: 2.734058141708374\n",
            "Training Iteration 5076, Loss: 5.787478446960449\n",
            "Training Iteration 5077, Loss: 4.495719909667969\n",
            "Training Iteration 5078, Loss: 4.282893657684326\n",
            "Training Iteration 5079, Loss: 3.2481043338775635\n",
            "Training Iteration 5080, Loss: 4.1196441650390625\n",
            "Training Iteration 5081, Loss: 3.8117971420288086\n",
            "Training Iteration 5082, Loss: 5.986806869506836\n",
            "Training Iteration 5083, Loss: 3.476283311843872\n",
            "Training Iteration 5084, Loss: 4.469344139099121\n",
            "Training Iteration 5085, Loss: 5.877958297729492\n",
            "Training Iteration 5086, Loss: 4.712226390838623\n",
            "Training Iteration 5087, Loss: 3.2175681591033936\n",
            "Training Iteration 5088, Loss: 4.82020902633667\n",
            "Training Iteration 5089, Loss: 5.277578830718994\n",
            "Training Iteration 5090, Loss: 2.9900832176208496\n",
            "Training Iteration 5091, Loss: 5.931386947631836\n",
            "Training Iteration 5092, Loss: 3.3682174682617188\n",
            "Training Iteration 5093, Loss: 3.288844585418701\n",
            "Training Iteration 5094, Loss: 2.2169082164764404\n",
            "Training Iteration 5095, Loss: 7.422719955444336\n",
            "Training Iteration 5096, Loss: 4.329229354858398\n",
            "Training Iteration 5097, Loss: 3.9871373176574707\n",
            "Training Iteration 5098, Loss: 6.30446195602417\n",
            "Training Iteration 5099, Loss: 3.8617827892303467\n",
            "Training Iteration 5100, Loss: 5.902737140655518\n",
            "Training Iteration 5101, Loss: 4.038210868835449\n",
            "Training Iteration 5102, Loss: 3.7387266159057617\n",
            "Training Iteration 5103, Loss: 6.274556636810303\n",
            "Training Iteration 5104, Loss: 2.168954372406006\n",
            "Training Iteration 5105, Loss: 1.9549462795257568\n",
            "Training Iteration 5106, Loss: 4.012173652648926\n",
            "Training Iteration 5107, Loss: 4.336160182952881\n",
            "Training Iteration 5108, Loss: 5.1084723472595215\n",
            "Training Iteration 5109, Loss: 5.512053966522217\n",
            "Training Iteration 5110, Loss: 2.8437516689300537\n",
            "Training Iteration 5111, Loss: 4.690053462982178\n",
            "Training Iteration 5112, Loss: 6.421786785125732\n",
            "Training Iteration 5113, Loss: 5.244219779968262\n",
            "Training Iteration 5114, Loss: 4.641693592071533\n",
            "Training Iteration 5115, Loss: 3.885016679763794\n",
            "Training Iteration 5116, Loss: 3.620069980621338\n",
            "Training Iteration 5117, Loss: 5.676471710205078\n",
            "Training Iteration 5118, Loss: 3.432652235031128\n",
            "Training Iteration 5119, Loss: 3.645103931427002\n",
            "Training Iteration 5120, Loss: 5.312254428863525\n",
            "Training Iteration 5121, Loss: 3.8202474117279053\n",
            "Training Iteration 5122, Loss: 3.437587261199951\n",
            "Training Iteration 5123, Loss: 5.287192344665527\n",
            "Training Iteration 5124, Loss: 3.760537624359131\n",
            "Training Iteration 5125, Loss: 3.601390838623047\n",
            "Training Iteration 5126, Loss: 6.415443420410156\n",
            "Training Iteration 5127, Loss: 4.926995754241943\n",
            "Training Iteration 5128, Loss: 4.37653923034668\n",
            "Training Iteration 5129, Loss: 3.487558364868164\n",
            "Training Iteration 5130, Loss: 7.30006217956543\n",
            "Training Iteration 5131, Loss: 4.027200222015381\n",
            "Training Iteration 5132, Loss: 3.6140518188476562\n",
            "Training Iteration 5133, Loss: 1.3795181512832642\n",
            "Training Iteration 5134, Loss: 4.781764984130859\n",
            "Training Iteration 5135, Loss: 4.118298053741455\n",
            "Training Iteration 5136, Loss: 3.143517017364502\n",
            "Training Iteration 5137, Loss: 2.321176528930664\n",
            "Training Iteration 5138, Loss: 5.354818344116211\n",
            "Training Iteration 5139, Loss: 6.630924224853516\n",
            "Training Iteration 5140, Loss: 4.918027877807617\n",
            "Training Iteration 5141, Loss: 4.944993495941162\n",
            "Training Iteration 5142, Loss: 3.4542605876922607\n",
            "Training Iteration 5143, Loss: 5.021584510803223\n",
            "Training Iteration 5144, Loss: 5.121638298034668\n",
            "Training Iteration 5145, Loss: 5.606016159057617\n",
            "Training Iteration 5146, Loss: 4.525481224060059\n",
            "Training Iteration 5147, Loss: 4.255336761474609\n",
            "Training Iteration 5148, Loss: 4.407810688018799\n",
            "Training Iteration 5149, Loss: 3.9862771034240723\n",
            "Training Iteration 5150, Loss: 2.818173408508301\n",
            "Training Iteration 5151, Loss: 5.23846435546875\n",
            "Training Iteration 5152, Loss: 4.971013069152832\n",
            "Training Iteration 5153, Loss: 4.282447814941406\n",
            "Training Iteration 5154, Loss: 3.421898126602173\n",
            "Training Iteration 5155, Loss: 4.2802414894104\n",
            "Training Iteration 5156, Loss: 4.007985591888428\n",
            "Training Iteration 5157, Loss: 3.8971750736236572\n",
            "Training Iteration 5158, Loss: 2.873055934906006\n",
            "Training Iteration 5159, Loss: 4.337906837463379\n",
            "Training Iteration 5160, Loss: 5.261478424072266\n",
            "Training Iteration 5161, Loss: 3.4138169288635254\n",
            "Training Iteration 5162, Loss: 4.9723615646362305\n",
            "Training Iteration 5163, Loss: 4.447878837585449\n",
            "Training Iteration 5164, Loss: 4.468844890594482\n",
            "Training Iteration 5165, Loss: 5.713818073272705\n",
            "Training Iteration 5166, Loss: 1.747344732284546\n",
            "Training Iteration 5167, Loss: 7.333617687225342\n",
            "Training Iteration 5168, Loss: 5.519439697265625\n",
            "Training Iteration 5169, Loss: 6.200568675994873\n",
            "Training Iteration 5170, Loss: 7.044888973236084\n",
            "Training Iteration 5171, Loss: 2.2414042949676514\n",
            "Training Iteration 5172, Loss: 3.5926096439361572\n",
            "Training Iteration 5173, Loss: 2.2062387466430664\n",
            "Training Iteration 5174, Loss: 4.057217121124268\n",
            "Training Iteration 5175, Loss: 4.0113372802734375\n",
            "Training Iteration 5176, Loss: 7.552590847015381\n",
            "Training Iteration 5177, Loss: 5.561087131500244\n",
            "Training Iteration 5178, Loss: 7.269604682922363\n",
            "Training Iteration 5179, Loss: 2.0829038619995117\n",
            "Training Iteration 5180, Loss: 3.2439181804656982\n",
            "Training Iteration 5181, Loss: 3.814293146133423\n",
            "Training Iteration 5182, Loss: 4.986515522003174\n",
            "Training Iteration 5183, Loss: 4.94976806640625\n",
            "Training Iteration 5184, Loss: 6.094256401062012\n",
            "Training Iteration 5185, Loss: 3.371314764022827\n",
            "Training Iteration 5186, Loss: 2.7831287384033203\n",
            "Training Iteration 5187, Loss: 3.9933581352233887\n",
            "Training Iteration 5188, Loss: 3.1644389629364014\n",
            "Training Iteration 5189, Loss: 5.055636405944824\n",
            "Training Iteration 5190, Loss: 4.400434494018555\n",
            "Training Iteration 5191, Loss: 5.1276021003723145\n",
            "Training Iteration 5192, Loss: 2.8659729957580566\n",
            "Training Iteration 5193, Loss: 3.8163371086120605\n",
            "Training Iteration 5194, Loss: 4.750555992126465\n",
            "Training Iteration 5195, Loss: 2.1462206840515137\n",
            "Training Iteration 5196, Loss: 3.5245039463043213\n",
            "Training Iteration 5197, Loss: 6.0497236251831055\n",
            "Training Iteration 5198, Loss: 6.130792617797852\n",
            "Training Iteration 5199, Loss: 2.2661919593811035\n",
            "Training Iteration 5200, Loss: 7.150449752807617\n",
            "Training Iteration 5201, Loss: 3.6304593086242676\n",
            "Training Iteration 5202, Loss: 5.025272369384766\n",
            "Training Iteration 5203, Loss: 5.127676010131836\n",
            "Training Iteration 5204, Loss: 4.186095237731934\n",
            "Training Iteration 5205, Loss: 3.884016752243042\n",
            "Training Iteration 5206, Loss: 5.679878234863281\n",
            "Training Iteration 5207, Loss: 4.039159297943115\n",
            "Training Iteration 5208, Loss: 2.6628823280334473\n",
            "Training Iteration 5209, Loss: 7.095573902130127\n",
            "Training Iteration 5210, Loss: 7.5552263259887695\n",
            "Training Iteration 5211, Loss: 4.357079982757568\n",
            "Training Iteration 5212, Loss: 9.561250686645508\n",
            "Training Iteration 5213, Loss: 7.250769138336182\n",
            "Training Iteration 5214, Loss: 6.713757038116455\n",
            "Training Iteration 5215, Loss: 5.323056221008301\n",
            "Training Iteration 5216, Loss: 3.2384238243103027\n",
            "Training Iteration 5217, Loss: 6.545312404632568\n",
            "Training Iteration 5218, Loss: 7.682548999786377\n",
            "Training Iteration 5219, Loss: 8.615653038024902\n",
            "Training Iteration 5220, Loss: 4.983598709106445\n",
            "Training Iteration 5221, Loss: 2.9550013542175293\n",
            "Training Iteration 5222, Loss: 5.780508518218994\n",
            "Training Iteration 5223, Loss: 4.131148338317871\n",
            "Training Iteration 5224, Loss: 3.331308126449585\n",
            "Training Iteration 5225, Loss: 2.763418436050415\n",
            "Training Iteration 5226, Loss: 5.74530553817749\n",
            "Training Iteration 5227, Loss: 2.7863214015960693\n",
            "Training Iteration 5228, Loss: 2.7438604831695557\n",
            "Training Iteration 5229, Loss: 2.175441026687622\n",
            "Training Iteration 5230, Loss: 4.054285049438477\n",
            "Training Iteration 5231, Loss: 8.416990280151367\n",
            "Training Iteration 5232, Loss: 1.676313877105713\n",
            "Training Iteration 5233, Loss: 2.518340587615967\n",
            "Training Iteration 5234, Loss: 9.755716323852539\n",
            "Training Iteration 5235, Loss: 4.896429061889648\n",
            "Training Iteration 5236, Loss: 7.412745475769043\n",
            "Training Iteration 5237, Loss: 6.890111923217773\n",
            "Training Iteration 5238, Loss: 8.329888343811035\n",
            "Training Iteration 5239, Loss: 1.6779332160949707\n",
            "Training Iteration 5240, Loss: 5.525412082672119\n",
            "Training Iteration 5241, Loss: 4.209592819213867\n",
            "Training Iteration 5242, Loss: 4.925821304321289\n",
            "Training Iteration 5243, Loss: 3.1346938610076904\n",
            "Training Iteration 5244, Loss: 2.9430298805236816\n",
            "Training Iteration 5245, Loss: 3.957465171813965\n",
            "Training Iteration 5246, Loss: 6.265936374664307\n",
            "Training Iteration 5247, Loss: 4.763756275177002\n",
            "Training Iteration 5248, Loss: 3.3742692470550537\n",
            "Training Iteration 5249, Loss: 3.1699647903442383\n",
            "Training Iteration 5250, Loss: 5.7279229164123535\n",
            "Training Iteration 5251, Loss: 3.099785089492798\n",
            "Training Iteration 5252, Loss: 1.8853925466537476\n",
            "Training Iteration 5253, Loss: 3.9350452423095703\n",
            "Training Iteration 5254, Loss: 5.159464359283447\n",
            "Training Iteration 5255, Loss: 4.6608991622924805\n",
            "Training Iteration 5256, Loss: 4.511305809020996\n",
            "Training Iteration 5257, Loss: 5.019552230834961\n",
            "Training Iteration 5258, Loss: 1.848679780960083\n",
            "Training Iteration 5259, Loss: 2.507786512374878\n",
            "Training Iteration 5260, Loss: 3.373368501663208\n",
            "Training Iteration 5261, Loss: 3.301839828491211\n",
            "Training Iteration 5262, Loss: 4.550946235656738\n",
            "Training Iteration 5263, Loss: 2.3353233337402344\n",
            "Training Iteration 5264, Loss: 4.6828484535217285\n",
            "Training Iteration 5265, Loss: 4.853984832763672\n",
            "Training Iteration 5266, Loss: 4.199249744415283\n",
            "Training Iteration 5267, Loss: 5.5751729011535645\n",
            "Training Iteration 5268, Loss: 4.105198383331299\n",
            "Training Iteration 5269, Loss: 3.7090659141540527\n",
            "Training Iteration 5270, Loss: 6.075695514678955\n",
            "Training Iteration 5271, Loss: 3.479926586151123\n",
            "Training Iteration 5272, Loss: 4.572957992553711\n",
            "Training Iteration 5273, Loss: 4.976746082305908\n",
            "Training Iteration 5274, Loss: 2.7572622299194336\n",
            "Training Iteration 5275, Loss: 5.542496681213379\n",
            "Training Iteration 5276, Loss: 4.496591091156006\n",
            "Training Iteration 5277, Loss: 2.1525468826293945\n",
            "Training Iteration 5278, Loss: 5.04488468170166\n",
            "Training Iteration 5279, Loss: 5.1903510093688965\n",
            "Training Iteration 5280, Loss: 4.50390100479126\n",
            "Training Iteration 5281, Loss: 3.327580690383911\n",
            "Training Iteration 5282, Loss: 2.725490093231201\n",
            "Training Iteration 5283, Loss: 4.44558572769165\n",
            "Training Iteration 5284, Loss: 4.401946544647217\n",
            "Training Iteration 5285, Loss: 4.951544761657715\n",
            "Training Iteration 5286, Loss: 7.875921726226807\n",
            "Training Iteration 5287, Loss: 1.9479506015777588\n",
            "Training Iteration 5288, Loss: 5.322819709777832\n",
            "Training Iteration 5289, Loss: 3.2262818813323975\n",
            "Training Iteration 5290, Loss: 5.138081073760986\n",
            "Training Iteration 5291, Loss: 4.217923164367676\n",
            "Training Iteration 5292, Loss: 6.430407524108887\n",
            "Training Iteration 5293, Loss: 3.334156036376953\n",
            "Training Iteration 5294, Loss: 4.693455696105957\n",
            "Training Iteration 5295, Loss: 2.6224544048309326\n",
            "Training Iteration 5296, Loss: 6.36724853515625\n",
            "Training Iteration 5297, Loss: 3.40175724029541\n",
            "Training Iteration 5298, Loss: 4.9201459884643555\n",
            "Training Iteration 5299, Loss: 5.449672698974609\n",
            "Training Iteration 5300, Loss: 2.9492781162261963\n",
            "Training Iteration 5301, Loss: 5.821605205535889\n",
            "Training Iteration 5302, Loss: 4.269001007080078\n",
            "Training Iteration 5303, Loss: 4.113440036773682\n",
            "Training Iteration 5304, Loss: 4.764168739318848\n",
            "Training Iteration 5305, Loss: 6.74763822555542\n",
            "Training Iteration 5306, Loss: 5.195706844329834\n",
            "Training Iteration 5307, Loss: 4.590417385101318\n",
            "Training Iteration 5308, Loss: 3.8265161514282227\n",
            "Training Iteration 5309, Loss: 3.363642454147339\n",
            "Training Iteration 5310, Loss: 5.3941969871521\n",
            "Training Iteration 5311, Loss: 4.943359851837158\n",
            "Training Iteration 5312, Loss: 5.939958572387695\n",
            "Training Iteration 5313, Loss: 5.020702838897705\n",
            "Training Iteration 5314, Loss: 4.847054481506348\n",
            "Training Iteration 5315, Loss: 7.309271335601807\n",
            "Training Iteration 5316, Loss: 5.071830749511719\n",
            "Training Iteration 5317, Loss: 4.029359340667725\n",
            "Training Iteration 5318, Loss: 3.703005790710449\n",
            "Training Iteration 5319, Loss: 3.352055311203003\n",
            "Training Iteration 5320, Loss: 2.695333242416382\n",
            "Training Iteration 5321, Loss: 5.4375224113464355\n",
            "Training Iteration 5322, Loss: 2.3496711254119873\n",
            "Training Iteration 5323, Loss: 3.9886770248413086\n",
            "Training Iteration 5324, Loss: 4.956665992736816\n",
            "Training Iteration 5325, Loss: 1.810401439666748\n",
            "Training Iteration 5326, Loss: 3.117666482925415\n",
            "Training Iteration 5327, Loss: 2.027939796447754\n",
            "Training Iteration 5328, Loss: 2.9392144680023193\n",
            "Training Iteration 5329, Loss: 2.168144464492798\n",
            "Training Iteration 5330, Loss: 5.014404773712158\n",
            "Training Iteration 5331, Loss: 3.3598804473876953\n",
            "Training Iteration 5332, Loss: 5.298061370849609\n",
            "Training Iteration 5333, Loss: 3.7126286029815674\n",
            "Training Iteration 5334, Loss: 4.369381427764893\n",
            "Training Iteration 5335, Loss: 4.260528087615967\n",
            "Training Iteration 5336, Loss: 2.825894355773926\n",
            "Training Iteration 5337, Loss: 3.2235138416290283\n",
            "Training Iteration 5338, Loss: 3.7375807762145996\n",
            "Training Iteration 5339, Loss: 4.201829433441162\n",
            "Training Iteration 5340, Loss: 1.482844352722168\n",
            "Training Iteration 5341, Loss: 2.7785234451293945\n",
            "Training Iteration 5342, Loss: 3.6299855709075928\n",
            "Training Iteration 5343, Loss: 6.7861104011535645\n",
            "Training Iteration 5344, Loss: 3.692636013031006\n",
            "Training Iteration 5345, Loss: 4.85107421875\n",
            "Training Iteration 5346, Loss: 7.700814723968506\n",
            "Training Iteration 5347, Loss: 3.9951202869415283\n",
            "Training Iteration 5348, Loss: 5.696514129638672\n",
            "Training Iteration 5349, Loss: 4.783366680145264\n",
            "Training Iteration 5350, Loss: 4.3255414962768555\n",
            "Training Iteration 5351, Loss: 3.920010805130005\n",
            "Training Iteration 5352, Loss: 3.391869306564331\n",
            "Training Iteration 5353, Loss: 4.854867935180664\n",
            "Training Iteration 5354, Loss: 5.267618656158447\n",
            "Training Iteration 5355, Loss: 2.5993995666503906\n",
            "Training Iteration 5356, Loss: 3.0404796600341797\n",
            "Training Iteration 5357, Loss: 2.466527223587036\n",
            "Training Iteration 5358, Loss: 10.453271865844727\n",
            "Training Iteration 5359, Loss: 1.2261464595794678\n",
            "Training Iteration 5360, Loss: 5.28298807144165\n",
            "Training Iteration 5361, Loss: 7.173750877380371\n",
            "Training Iteration 5362, Loss: 4.12801456451416\n",
            "Training Iteration 5363, Loss: 4.6507439613342285\n",
            "Training Iteration 5364, Loss: 1.565889596939087\n",
            "Training Iteration 5365, Loss: 2.0860438346862793\n",
            "Training Iteration 5366, Loss: 4.963984966278076\n",
            "Training Iteration 5367, Loss: 2.9554603099823\n",
            "Training Iteration 5368, Loss: 2.7286195755004883\n",
            "Training Iteration 5369, Loss: 2.9827797412872314\n",
            "Training Iteration 5370, Loss: 4.616626262664795\n",
            "Training Iteration 5371, Loss: 5.514486312866211\n",
            "Training Iteration 5372, Loss: 4.436061859130859\n",
            "Training Iteration 5373, Loss: 3.09659743309021\n",
            "Training Iteration 5374, Loss: 4.071279525756836\n",
            "Training Iteration 5375, Loss: 4.497124195098877\n",
            "Training Iteration 5376, Loss: 4.0164055824279785\n",
            "Training Iteration 5377, Loss: 2.7695958614349365\n",
            "Training Iteration 5378, Loss: 6.237703323364258\n",
            "Training Iteration 5379, Loss: 2.2647783756256104\n",
            "Training Iteration 5380, Loss: 5.437856674194336\n",
            "Training Iteration 5381, Loss: 3.1422362327575684\n",
            "Training Iteration 5382, Loss: 2.2614927291870117\n",
            "Training Iteration 5383, Loss: 3.5398950576782227\n",
            "Training Iteration 5384, Loss: 5.021473407745361\n",
            "Training Iteration 5385, Loss: 6.814406394958496\n",
            "Training Iteration 5386, Loss: 6.219715118408203\n",
            "Training Iteration 5387, Loss: 7.493551254272461\n",
            "Training Iteration 5388, Loss: 4.856927871704102\n",
            "Training Iteration 5389, Loss: 2.209965467453003\n",
            "Training Iteration 5390, Loss: 3.0786046981811523\n",
            "Training Iteration 5391, Loss: 7.532702922821045\n",
            "Training Iteration 5392, Loss: 3.9305713176727295\n",
            "Training Iteration 5393, Loss: 5.921633720397949\n",
            "Training Iteration 5394, Loss: 4.530959129333496\n",
            "Training Iteration 5395, Loss: 3.9648001194000244\n",
            "Training Iteration 5396, Loss: 4.380166530609131\n",
            "Training Iteration 5397, Loss: 4.116163730621338\n",
            "Training Iteration 5398, Loss: 8.07252311706543\n",
            "Training Iteration 5399, Loss: 6.023702144622803\n",
            "Training Iteration 5400, Loss: 7.48110294342041\n",
            "Training Iteration 5401, Loss: 2.524869918823242\n",
            "Training Iteration 5402, Loss: 7.494274616241455\n",
            "Training Iteration 5403, Loss: 4.618898391723633\n",
            "Training Iteration 5404, Loss: 5.966795444488525\n",
            "Training Iteration 5405, Loss: 5.081582546234131\n",
            "Training Iteration 5406, Loss: 3.646125316619873\n",
            "Training Iteration 5407, Loss: 4.90574836730957\n",
            "Training Iteration 5408, Loss: 2.3034608364105225\n",
            "Training Iteration 5409, Loss: 4.78463888168335\n",
            "Training Iteration 5410, Loss: 5.977353096008301\n",
            "Training Iteration 5411, Loss: 6.048965930938721\n",
            "Training Iteration 5412, Loss: 4.442381858825684\n",
            "Training Iteration 5413, Loss: 2.5457935333251953\n",
            "Training Iteration 5414, Loss: 4.572990417480469\n",
            "Training Iteration 5415, Loss: 5.325451374053955\n",
            "Training Iteration 5416, Loss: 6.1975321769714355\n",
            "Training Iteration 5417, Loss: 6.417835712432861\n",
            "Training Iteration 5418, Loss: 2.4431188106536865\n",
            "Training Iteration 5419, Loss: 1.9594393968582153\n",
            "Training Iteration 5420, Loss: 2.067744493484497\n",
            "Training Iteration 5421, Loss: 5.885688781738281\n",
            "Training Iteration 5422, Loss: 5.6387128829956055\n",
            "Training Iteration 5423, Loss: 3.662494421005249\n",
            "Training Iteration 5424, Loss: 3.2450711727142334\n",
            "Training Iteration 5425, Loss: 2.9958837032318115\n",
            "Training Iteration 5426, Loss: 3.77428936958313\n",
            "Training Iteration 5427, Loss: 2.925995349884033\n",
            "Training Iteration 5428, Loss: 2.7027151584625244\n",
            "Training Iteration 5429, Loss: 4.786179065704346\n",
            "Training Iteration 5430, Loss: 4.454565048217773\n",
            "Training Iteration 5431, Loss: 7.18397855758667\n",
            "Training Iteration 5432, Loss: 7.060065269470215\n",
            "Training Iteration 5433, Loss: 6.850991725921631\n",
            "Training Iteration 5434, Loss: 4.2704362869262695\n",
            "Training Iteration 5435, Loss: 4.114191055297852\n",
            "Training Iteration 5436, Loss: 5.883631706237793\n",
            "Training Iteration 5437, Loss: 1.4627567529678345\n",
            "Training Iteration 5438, Loss: 5.470020294189453\n",
            "Training Iteration 5439, Loss: 3.2641441822052\n",
            "Training Iteration 5440, Loss: 6.4860124588012695\n",
            "Training Iteration 5441, Loss: 3.0236079692840576\n",
            "Training Iteration 5442, Loss: 4.375238418579102\n",
            "Training Iteration 5443, Loss: 5.360652446746826\n",
            "Training Iteration 5444, Loss: 4.261207103729248\n",
            "Training Iteration 5445, Loss: 9.105441093444824\n",
            "Training Iteration 5446, Loss: 2.6344051361083984\n",
            "Training Iteration 5447, Loss: 2.730217456817627\n",
            "Training Iteration 5448, Loss: 4.441568851470947\n",
            "Training Iteration 5449, Loss: 4.985474586486816\n",
            "Training Iteration 5450, Loss: 3.9920079708099365\n",
            "Training Iteration 5451, Loss: 3.831061840057373\n",
            "Training Iteration 5452, Loss: 3.3859310150146484\n",
            "Training Iteration 5453, Loss: 4.598344802856445\n",
            "Training Iteration 5454, Loss: 6.220147132873535\n",
            "Training Iteration 5455, Loss: 3.6459438800811768\n",
            "Training Iteration 5456, Loss: 6.963319778442383\n",
            "Training Iteration 5457, Loss: 7.1161417961120605\n",
            "Training Iteration 5458, Loss: 2.3015706539154053\n",
            "Training Iteration 5459, Loss: 6.430276870727539\n",
            "Training Iteration 5460, Loss: 3.4580163955688477\n",
            "Training Iteration 5461, Loss: 4.367710113525391\n",
            "Training Iteration 5462, Loss: 4.908514022827148\n",
            "Training Iteration 5463, Loss: 6.61551570892334\n",
            "Training Iteration 5464, Loss: 3.6254196166992188\n",
            "Training Iteration 5465, Loss: 2.9680967330932617\n",
            "Training Iteration 5466, Loss: 4.859707355499268\n",
            "Training Iteration 5467, Loss: 2.053039312362671\n",
            "Training Iteration 5468, Loss: 4.220576286315918\n",
            "Training Iteration 5469, Loss: 4.6858649253845215\n",
            "Training Iteration 5470, Loss: 4.1984968185424805\n",
            "Training Iteration 5471, Loss: 5.32940149307251\n",
            "Training Iteration 5472, Loss: 2.912982940673828\n",
            "Training Iteration 5473, Loss: 4.364701747894287\n",
            "Training Iteration 5474, Loss: 3.475327968597412\n",
            "Training Iteration 5475, Loss: 4.769845962524414\n",
            "Training Iteration 5476, Loss: 7.202635765075684\n",
            "Training Iteration 5477, Loss: 4.249822616577148\n",
            "Training Iteration 5478, Loss: 4.270589351654053\n",
            "Training Iteration 5479, Loss: 3.812284469604492\n",
            "Training Iteration 5480, Loss: 3.8926942348480225\n",
            "Training Iteration 5481, Loss: 2.678769826889038\n",
            "Training Iteration 5482, Loss: 3.486255407333374\n",
            "Training Iteration 5483, Loss: 2.3881211280822754\n",
            "Training Iteration 5484, Loss: 4.261150360107422\n",
            "Training Iteration 5485, Loss: 2.614924669265747\n",
            "Training Iteration 5486, Loss: 4.952857494354248\n",
            "Training Iteration 5487, Loss: 3.08284068107605\n",
            "Training Iteration 5488, Loss: 8.91513729095459\n",
            "Training Iteration 5489, Loss: 6.332888603210449\n",
            "Training Iteration 5490, Loss: 2.9757771492004395\n",
            "Training Iteration 5491, Loss: 3.061871290206909\n",
            "Training Iteration 5492, Loss: 6.604104518890381\n",
            "Training Iteration 5493, Loss: 3.1045961380004883\n",
            "Training Iteration 5494, Loss: 3.5036063194274902\n",
            "Training Iteration 5495, Loss: 6.495903968811035\n",
            "Training Iteration 5496, Loss: 5.175166606903076\n",
            "Training Iteration 5497, Loss: 6.242785453796387\n",
            "Training Iteration 5498, Loss: 5.5269060134887695\n",
            "Training Iteration 5499, Loss: 5.531153202056885\n",
            "Training Iteration 5500, Loss: 5.260867118835449\n",
            "Training Iteration 5501, Loss: 5.516302585601807\n",
            "Training Iteration 5502, Loss: 9.212295532226562\n",
            "Training Iteration 5503, Loss: 5.551690578460693\n",
            "Training Iteration 5504, Loss: 6.843160629272461\n",
            "Training Iteration 5505, Loss: 3.3689777851104736\n",
            "Training Iteration 5506, Loss: 5.158087253570557\n",
            "Training Iteration 5507, Loss: 3.106259346008301\n",
            "Training Iteration 5508, Loss: 1.3827955722808838\n",
            "Training Iteration 5509, Loss: 7.1825852394104\n",
            "Training Iteration 5510, Loss: 3.7867848873138428\n",
            "Training Iteration 5511, Loss: 3.705310821533203\n",
            "Training Iteration 5512, Loss: 5.024191379547119\n",
            "Training Iteration 5513, Loss: 4.874225616455078\n",
            "Training Iteration 5514, Loss: 5.982946395874023\n",
            "Training Iteration 5515, Loss: 7.732059478759766\n",
            "Training Iteration 5516, Loss: 3.588568925857544\n",
            "Training Iteration 5517, Loss: 5.414747714996338\n",
            "Training Iteration 5518, Loss: 9.080236434936523\n",
            "Training Iteration 5519, Loss: 8.206491470336914\n",
            "Training Iteration 5520, Loss: 4.075787544250488\n",
            "Training Iteration 5521, Loss: 2.862234115600586\n",
            "Training Iteration 5522, Loss: 4.29189920425415\n",
            "Training Iteration 5523, Loss: 4.822915077209473\n",
            "Training Iteration 5524, Loss: 6.336950778961182\n",
            "Training Iteration 5525, Loss: 3.896038770675659\n",
            "Training Iteration 5526, Loss: 6.173923492431641\n",
            "Training Iteration 5527, Loss: 3.710625410079956\n",
            "Training Iteration 5528, Loss: 1.6275745630264282\n",
            "Training Iteration 5529, Loss: 4.150319576263428\n",
            "Training Iteration 5530, Loss: 3.442139148712158\n",
            "Training Iteration 5531, Loss: 3.5673134326934814\n",
            "Training Iteration 5532, Loss: 2.417034149169922\n",
            "Training Iteration 5533, Loss: 2.996237277984619\n",
            "Training Iteration 5534, Loss: 5.606473922729492\n",
            "Training Iteration 5535, Loss: 3.9734768867492676\n",
            "Training Iteration 5536, Loss: 3.0241281986236572\n",
            "Training Iteration 5537, Loss: 6.5938520431518555\n",
            "Training Iteration 5538, Loss: 3.7445831298828125\n",
            "Training Iteration 5539, Loss: 2.3559632301330566\n",
            "Training Iteration 5540, Loss: 5.675607204437256\n",
            "Training Iteration 5541, Loss: 3.8137059211730957\n",
            "Training Iteration 5542, Loss: 2.637258291244507\n",
            "Training Iteration 5543, Loss: 6.474783420562744\n",
            "Training Iteration 5544, Loss: 2.5407142639160156\n",
            "Training Iteration 5545, Loss: 4.4382123947143555\n",
            "Training Iteration 5546, Loss: 4.531193733215332\n",
            "Training Iteration 5547, Loss: 4.585829734802246\n",
            "Training Iteration 5548, Loss: 3.436169147491455\n",
            "Training Iteration 5549, Loss: 2.708050489425659\n",
            "Training Iteration 5550, Loss: 4.712628364562988\n",
            "Training Iteration 5551, Loss: 2.6932077407836914\n",
            "Training Iteration 5552, Loss: 7.214189052581787\n",
            "Training Iteration 5553, Loss: 7.018483638763428\n",
            "Training Iteration 5554, Loss: 2.434164047241211\n",
            "Training Iteration 5555, Loss: 6.858306407928467\n",
            "Training Iteration 5556, Loss: 4.646867275238037\n",
            "Training Iteration 5557, Loss: 5.797420024871826\n",
            "Training Iteration 5558, Loss: 4.645003795623779\n",
            "Training Iteration 5559, Loss: 4.328624725341797\n",
            "Training Iteration 5560, Loss: 5.104764938354492\n",
            "Training Iteration 5561, Loss: 4.016170024871826\n",
            "Training Iteration 5562, Loss: 4.356657028198242\n",
            "Training Iteration 5563, Loss: 4.331765651702881\n",
            "Training Iteration 5564, Loss: 3.931415319442749\n",
            "Training Iteration 5565, Loss: 5.156170845031738\n",
            "Training Iteration 5566, Loss: 7.2483367919921875\n",
            "Training Iteration 5567, Loss: 2.6019833087921143\n",
            "Training Iteration 5568, Loss: 5.419944763183594\n",
            "Training Iteration 5569, Loss: 4.38070011138916\n",
            "Training Iteration 5570, Loss: 4.253878593444824\n",
            "Training Iteration 5571, Loss: 3.463860273361206\n",
            "Training Iteration 5572, Loss: 5.242404460906982\n",
            "Training Iteration 5573, Loss: 6.264068603515625\n",
            "Training Iteration 5574, Loss: 4.553429126739502\n",
            "Training Iteration 5575, Loss: 2.3170621395111084\n",
            "Training Iteration 5576, Loss: 3.895476818084717\n",
            "Training Iteration 5577, Loss: 3.053579807281494\n",
            "Training Iteration 5578, Loss: 3.093964099884033\n",
            "Training Iteration 5579, Loss: 5.148486137390137\n",
            "Training Iteration 5580, Loss: 5.5312066078186035\n",
            "Training Iteration 5581, Loss: 4.3103485107421875\n",
            "Training Iteration 5582, Loss: 8.558932304382324\n",
            "Training Iteration 5583, Loss: 6.701066493988037\n",
            "Training Iteration 5584, Loss: 4.725523471832275\n",
            "Training Iteration 5585, Loss: 3.899188995361328\n",
            "Training Iteration 5586, Loss: 3.7090084552764893\n",
            "Training Iteration 5587, Loss: 7.280130386352539\n",
            "Training Iteration 5588, Loss: 4.429604530334473\n",
            "Training Iteration 5589, Loss: 8.11539363861084\n",
            "Training Iteration 5590, Loss: 7.111254692077637\n",
            "Training Iteration 5591, Loss: 3.886507034301758\n",
            "Training Iteration 5592, Loss: 3.804759979248047\n",
            "Training Iteration 5593, Loss: 2.0144481658935547\n",
            "Training Iteration 5594, Loss: 1.726028561592102\n",
            "Training Iteration 5595, Loss: 1.7922378778457642\n",
            "Training Iteration 5596, Loss: 3.8809306621551514\n",
            "Training Iteration 5597, Loss: 5.284901142120361\n",
            "Training Iteration 5598, Loss: 5.622025012969971\n",
            "Training Iteration 5599, Loss: 4.140602111816406\n",
            "Training Iteration 5600, Loss: 7.782558917999268\n",
            "Training Iteration 5601, Loss: 2.3083505630493164\n",
            "Training Iteration 5602, Loss: 2.244943141937256\n",
            "Training Iteration 5603, Loss: 2.869992256164551\n",
            "Training Iteration 5604, Loss: 7.570857048034668\n",
            "Training Iteration 5605, Loss: 3.283698558807373\n",
            "Training Iteration 5606, Loss: 5.671422481536865\n",
            "Training Iteration 5607, Loss: 3.512225866317749\n",
            "Training Iteration 5608, Loss: 4.40848445892334\n",
            "Training Iteration 5609, Loss: 4.149349212646484\n",
            "Training Iteration 5610, Loss: 6.376971244812012\n",
            "Training Iteration 5611, Loss: 8.541576385498047\n",
            "Training Iteration 5612, Loss: 4.593732833862305\n",
            "Training Iteration 5613, Loss: 5.018492698669434\n",
            "Training Iteration 5614, Loss: 8.484644889831543\n",
            "Training Iteration 5615, Loss: 4.087516784667969\n",
            "Training Iteration 5616, Loss: 2.2520549297332764\n",
            "Training Iteration 5617, Loss: 5.972827911376953\n",
            "Training Iteration 5618, Loss: 3.3548526763916016\n",
            "Training Iteration 5619, Loss: 4.566411018371582\n",
            "Training Iteration 5620, Loss: 3.5814850330352783\n",
            "Training Iteration 5621, Loss: 4.92328405380249\n",
            "Training Iteration 5622, Loss: 2.643401622772217\n",
            "Training Iteration 5623, Loss: 6.093003273010254\n",
            "Training Iteration 5624, Loss: 3.290095090866089\n",
            "Training Iteration 5625, Loss: 5.021477699279785\n",
            "Training Iteration 5626, Loss: 13.131195068359375\n",
            "Training Iteration 5627, Loss: 3.0228075981140137\n",
            "Training Iteration 5628, Loss: 6.289696216583252\n",
            "Training Iteration 5629, Loss: 2.8716750144958496\n",
            "Training Iteration 5630, Loss: 4.821259498596191\n",
            "Training Iteration 5631, Loss: 4.033791542053223\n",
            "Training Iteration 5632, Loss: 7.948771953582764\n",
            "Training Iteration 5633, Loss: 6.635807037353516\n",
            "Training Iteration 5634, Loss: 6.430852890014648\n",
            "Training Iteration 5635, Loss: 5.180227756500244\n",
            "Training Iteration 5636, Loss: 5.207242965698242\n",
            "Training Iteration 5637, Loss: 4.347165584564209\n",
            "Training Iteration 5638, Loss: 6.075167179107666\n",
            "Training Iteration 5639, Loss: 6.188857078552246\n",
            "Training Iteration 5640, Loss: 3.7692410945892334\n",
            "Training Iteration 5641, Loss: 4.088555335998535\n",
            "Training Iteration 5642, Loss: 12.135601043701172\n",
            "Training Iteration 5643, Loss: 9.135248184204102\n",
            "Training Iteration 5644, Loss: 3.5758018493652344\n",
            "Training Iteration 5645, Loss: 3.156506299972534\n",
            "Training Iteration 5646, Loss: 3.7231321334838867\n",
            "Training Iteration 5647, Loss: 5.762227535247803\n",
            "Training Iteration 5648, Loss: 3.0308127403259277\n",
            "Training Iteration 5649, Loss: 6.904150009155273\n",
            "Training Iteration 5650, Loss: 7.590729713439941\n",
            "Training Iteration 5651, Loss: 3.714323043823242\n",
            "Training Iteration 5652, Loss: 4.672082901000977\n",
            "Training Iteration 5653, Loss: 5.474485397338867\n",
            "Training Iteration 5654, Loss: 4.883549213409424\n",
            "Training Iteration 5655, Loss: 7.223252296447754\n",
            "Training Iteration 5656, Loss: 5.687173366546631\n",
            "Training Iteration 5657, Loss: 7.158025741577148\n",
            "Training Iteration 5658, Loss: 10.003371238708496\n",
            "Training Iteration 5659, Loss: 6.141683101654053\n",
            "Training Iteration 5660, Loss: 6.5147857666015625\n",
            "Training Iteration 5661, Loss: 4.574611663818359\n",
            "Training Iteration 5662, Loss: 3.900479316711426\n",
            "Training Iteration 5663, Loss: 5.829289436340332\n",
            "Training Iteration 5664, Loss: 2.9732773303985596\n",
            "Training Iteration 5665, Loss: 6.921972274780273\n",
            "Training Iteration 5666, Loss: 8.75162124633789\n",
            "Training Iteration 5667, Loss: 2.1622142791748047\n",
            "Training Iteration 5668, Loss: 5.950714588165283\n",
            "Training Iteration 5669, Loss: 5.268304824829102\n",
            "Training Iteration 5670, Loss: 7.192631721496582\n",
            "Training Iteration 5671, Loss: 9.70443058013916\n",
            "Training Iteration 5672, Loss: 5.577022552490234\n",
            "Training Iteration 5673, Loss: 5.669651031494141\n",
            "Training Iteration 5674, Loss: 2.6539456844329834\n",
            "Training Iteration 5675, Loss: 3.8617777824401855\n",
            "Training Iteration 5676, Loss: 4.658230781555176\n",
            "Training Iteration 5677, Loss: 5.186776161193848\n",
            "Training Iteration 5678, Loss: 7.470815658569336\n",
            "Training Iteration 5679, Loss: 5.635916709899902\n",
            "Training Iteration 5680, Loss: 3.815058708190918\n",
            "Training Iteration 5681, Loss: 4.629767417907715\n",
            "Training Iteration 5682, Loss: 1.3105578422546387\n",
            "Training Iteration 5683, Loss: 5.198404788970947\n",
            "Training Iteration 5684, Loss: 5.931576251983643\n",
            "Training Iteration 5685, Loss: 1.7827775478363037\n",
            "Training Iteration 5686, Loss: 3.098914384841919\n",
            "Training Iteration 5687, Loss: 3.4851372241973877\n",
            "Training Iteration 5688, Loss: 5.699854373931885\n",
            "Training Iteration 5689, Loss: 3.430119752883911\n",
            "Training Iteration 5690, Loss: 9.50092887878418\n",
            "Training Iteration 5691, Loss: 6.502513885498047\n",
            "Training Iteration 5692, Loss: 7.083710193634033\n",
            "Training Iteration 5693, Loss: 6.136280536651611\n",
            "Training Iteration 5694, Loss: 3.9919114112854004\n",
            "Training Iteration 5695, Loss: 3.08293080329895\n",
            "Training Iteration 5696, Loss: 4.3341169357299805\n",
            "Training Iteration 5697, Loss: 6.067106246948242\n",
            "Training Iteration 5698, Loss: 2.109376907348633\n",
            "Training Iteration 5699, Loss: 5.199570178985596\n",
            "Training Iteration 5700, Loss: 5.580996990203857\n",
            "Training Iteration 5701, Loss: 4.3866400718688965\n",
            "Training Iteration 5702, Loss: 5.108494758605957\n",
            "Training Iteration 5703, Loss: 6.728514194488525\n",
            "Training Iteration 5704, Loss: 4.1919755935668945\n",
            "Training Iteration 5705, Loss: 4.662333965301514\n",
            "Training Iteration 5706, Loss: 2.9128386974334717\n",
            "Training Iteration 5707, Loss: 5.555531024932861\n",
            "Training Iteration 5708, Loss: 5.551856517791748\n",
            "Training Iteration 5709, Loss: 3.9325859546661377\n",
            "Training Iteration 5710, Loss: 6.201397895812988\n",
            "Training Iteration 5711, Loss: 5.661922931671143\n",
            "Training Iteration 5712, Loss: 5.2551469802856445\n",
            "Training Iteration 5713, Loss: 2.0157368183135986\n",
            "Training Iteration 5714, Loss: 4.082297325134277\n",
            "Training Iteration 5715, Loss: 2.056933879852295\n",
            "Training Iteration 5716, Loss: 4.421919345855713\n",
            "Training Iteration 5717, Loss: 4.291948318481445\n",
            "Training Iteration 5718, Loss: 3.461794376373291\n",
            "Training Iteration 5719, Loss: 4.008474349975586\n",
            "Training Iteration 5720, Loss: 3.3490610122680664\n",
            "Training Iteration 5721, Loss: 4.127396583557129\n",
            "Training Iteration 5722, Loss: 3.5884344577789307\n",
            "Training Iteration 5723, Loss: 6.416952610015869\n",
            "Training Iteration 5724, Loss: 8.99074649810791\n",
            "Training Iteration 5725, Loss: 4.498591423034668\n",
            "Training Iteration 5726, Loss: 3.773223876953125\n",
            "Training Iteration 5727, Loss: 4.001839637756348\n",
            "Training Iteration 5728, Loss: 4.6915483474731445\n",
            "Training Iteration 5729, Loss: 3.484473943710327\n",
            "Training Iteration 5730, Loss: 5.518256187438965\n",
            "Training Iteration 5731, Loss: 5.072667598724365\n",
            "Training Iteration 5732, Loss: 5.209297180175781\n",
            "Training Iteration 5733, Loss: 3.7408101558685303\n",
            "Training Iteration 5734, Loss: 3.8787598609924316\n",
            "Training Iteration 5735, Loss: 3.8305680751800537\n",
            "Training Iteration 5736, Loss: 4.130226135253906\n",
            "Training Iteration 5737, Loss: 4.938439846038818\n",
            "Training Iteration 5738, Loss: 4.1489667892456055\n",
            "Training Iteration 5739, Loss: 5.278596878051758\n",
            "Training Iteration 5740, Loss: 3.802475690841675\n",
            "Training Iteration 5741, Loss: 4.394739627838135\n",
            "Training Iteration 5742, Loss: 3.928617238998413\n",
            "Training Iteration 5743, Loss: 4.236603260040283\n",
            "Training Iteration 5744, Loss: 4.651802062988281\n",
            "Training Iteration 5745, Loss: 3.446476936340332\n",
            "Training Iteration 5746, Loss: 4.069903373718262\n",
            "Training Iteration 5747, Loss: 5.245669841766357\n",
            "Training Iteration 5748, Loss: 2.4492576122283936\n",
            "Training Iteration 5749, Loss: 3.77603816986084\n",
            "Training Iteration 5750, Loss: 2.8558273315429688\n",
            "Training Iteration 5751, Loss: 3.4186973571777344\n",
            "Training Iteration 5752, Loss: 3.7279207706451416\n",
            "Training Iteration 5753, Loss: 7.777226448059082\n",
            "Training Iteration 5754, Loss: 7.023785591125488\n",
            "Training Iteration 5755, Loss: 1.960868000984192\n",
            "Training Iteration 5756, Loss: 3.5115256309509277\n",
            "Training Iteration 5757, Loss: 5.4803466796875\n",
            "Training Iteration 5758, Loss: 5.2490949630737305\n",
            "Training Iteration 5759, Loss: 2.488616466522217\n",
            "Training Iteration 5760, Loss: 3.666714668273926\n",
            "Training Iteration 5761, Loss: 4.835911750793457\n",
            "Training Iteration 5762, Loss: 3.715818405151367\n",
            "Training Iteration 5763, Loss: 3.876368761062622\n",
            "Training Iteration 5764, Loss: 7.685934543609619\n",
            "Training Iteration 5765, Loss: 5.340590476989746\n",
            "Training Iteration 5766, Loss: 3.4724676609039307\n",
            "Training Iteration 5767, Loss: 6.841846942901611\n",
            "Training Iteration 5768, Loss: 4.2457709312438965\n",
            "Training Iteration 5769, Loss: 4.434792518615723\n",
            "Training Iteration 5770, Loss: 4.634709358215332\n",
            "Training Iteration 5771, Loss: 6.395843505859375\n",
            "Training Iteration 5772, Loss: 5.763824939727783\n",
            "Training Iteration 5773, Loss: 4.83043098449707\n",
            "Training Iteration 5774, Loss: 6.164783477783203\n",
            "Training Iteration 5775, Loss: 3.364515781402588\n",
            "Training Iteration 5776, Loss: 2.1235995292663574\n",
            "Training Iteration 5777, Loss: 2.164422035217285\n",
            "Training Iteration 5778, Loss: 5.89130163192749\n",
            "Training Iteration 5779, Loss: 6.232161998748779\n",
            "Training Iteration 5780, Loss: 4.9999589920043945\n",
            "Training Iteration 5781, Loss: 3.599977493286133\n",
            "Training Iteration 5782, Loss: 5.479039192199707\n",
            "Training Iteration 5783, Loss: 5.800533294677734\n",
            "Training Iteration 5784, Loss: 4.719167232513428\n",
            "Training Iteration 5785, Loss: 3.5734028816223145\n",
            "Training Iteration 5786, Loss: 3.929006576538086\n",
            "Training Iteration 5787, Loss: 3.8737521171569824\n",
            "Training Iteration 5788, Loss: 2.463717460632324\n",
            "Training Iteration 5789, Loss: 4.109435558319092\n",
            "Training Iteration 5790, Loss: 2.5098953247070312\n",
            "Training Iteration 5791, Loss: 3.9444291591644287\n",
            "Training Iteration 5792, Loss: 4.097309112548828\n",
            "Training Iteration 5793, Loss: 3.573845386505127\n",
            "Training Iteration 5794, Loss: 4.023049831390381\n",
            "Training Iteration 5795, Loss: 5.267491340637207\n",
            "Training Iteration 5796, Loss: 3.175065279006958\n",
            "Training Iteration 5797, Loss: 6.849028587341309\n",
            "Training Iteration 5798, Loss: 4.474735736846924\n",
            "Training Iteration 5799, Loss: 2.328834295272827\n",
            "Training Iteration 5800, Loss: 4.635725021362305\n",
            "Training Iteration 5801, Loss: 2.1737284660339355\n",
            "Training Iteration 5802, Loss: 7.0346598625183105\n",
            "Training Iteration 5803, Loss: 3.448514699935913\n",
            "Training Iteration 5804, Loss: 4.371316909790039\n",
            "Training Iteration 5805, Loss: 5.0609612464904785\n",
            "Training Iteration 5806, Loss: 5.62824010848999\n",
            "Training Iteration 5807, Loss: 2.7178990840911865\n",
            "Training Iteration 5808, Loss: 4.631930828094482\n",
            "Training Iteration 5809, Loss: 6.3196024894714355\n",
            "Training Iteration 5810, Loss: 5.367206573486328\n",
            "Training Iteration 5811, Loss: 6.13504695892334\n",
            "Training Iteration 5812, Loss: 2.9486801624298096\n",
            "Training Iteration 5813, Loss: 5.52024507522583\n",
            "Training Iteration 5814, Loss: 3.5841264724731445\n",
            "Training Iteration 5815, Loss: 6.949277877807617\n",
            "Training Iteration 5816, Loss: 5.078175067901611\n",
            "Training Iteration 5817, Loss: 5.202191352844238\n",
            "Training Iteration 5818, Loss: 5.69793701171875\n",
            "Training Iteration 5819, Loss: 4.909008979797363\n",
            "Training Iteration 5820, Loss: 3.96336030960083\n",
            "Training Iteration 5821, Loss: 5.056297302246094\n",
            "Training Iteration 5822, Loss: 7.297250747680664\n",
            "Training Iteration 5823, Loss: 3.4380319118499756\n",
            "Training Iteration 5824, Loss: 4.672914028167725\n",
            "Training Iteration 5825, Loss: 2.969459056854248\n",
            "Training Iteration 5826, Loss: 1.9334352016448975\n",
            "Training Iteration 5827, Loss: 5.565049648284912\n",
            "Training Iteration 5828, Loss: 7.616690635681152\n",
            "Training Iteration 5829, Loss: 3.3644230365753174\n",
            "Training Iteration 5830, Loss: 3.252185106277466\n",
            "Training Iteration 5831, Loss: 2.705770254135132\n",
            "Training Iteration 5832, Loss: 2.551586151123047\n",
            "Training Iteration 5833, Loss: 6.615226745605469\n",
            "Training Iteration 5834, Loss: 2.5366997718811035\n",
            "Training Iteration 5835, Loss: 7.655117988586426\n",
            "Training Iteration 5836, Loss: 3.0969743728637695\n",
            "Training Iteration 5837, Loss: 3.723385810852051\n",
            "Training Iteration 5838, Loss: 6.85513162612915\n",
            "Training Iteration 5839, Loss: 9.944540023803711\n",
            "Training Iteration 5840, Loss: 3.5418736934661865\n",
            "Training Iteration 5841, Loss: 3.7099733352661133\n",
            "Training Iteration 5842, Loss: 4.108641624450684\n",
            "Training Iteration 5843, Loss: 3.3560855388641357\n",
            "Training Iteration 5844, Loss: 5.73073673248291\n",
            "Training Iteration 5845, Loss: 4.758668422698975\n",
            "Training Iteration 5846, Loss: 3.8975954055786133\n",
            "Training Iteration 5847, Loss: 5.514187335968018\n",
            "Training Iteration 5848, Loss: 3.111952781677246\n",
            "Training Iteration 5849, Loss: 2.7159695625305176\n",
            "Training Iteration 5850, Loss: 2.330103635787964\n",
            "Training Iteration 5851, Loss: 5.588601589202881\n",
            "Training Iteration 5852, Loss: 4.105506420135498\n",
            "Training Iteration 5853, Loss: 5.49533748626709\n",
            "Training Iteration 5854, Loss: 3.4412872791290283\n",
            "Training Iteration 5855, Loss: 3.3524720668792725\n",
            "Training Iteration 5856, Loss: 6.787428855895996\n",
            "Training Iteration 5857, Loss: 4.318289279937744\n",
            "Training Iteration 5858, Loss: 3.798949956893921\n",
            "Training Iteration 5859, Loss: 4.776012420654297\n",
            "Training Iteration 5860, Loss: 5.17409086227417\n",
            "Training Iteration 5861, Loss: 2.81015682220459\n",
            "Training Iteration 5862, Loss: 6.423722267150879\n",
            "Training Iteration 5863, Loss: 5.87335729598999\n",
            "Training Iteration 5864, Loss: 5.750852584838867\n",
            "Training Iteration 5865, Loss: 7.1050262451171875\n",
            "Training Iteration 5866, Loss: 5.372305870056152\n",
            "Training Iteration 5867, Loss: 5.911968231201172\n",
            "Training Iteration 5868, Loss: 6.392583847045898\n",
            "Training Iteration 5869, Loss: 3.503486156463623\n",
            "Training Iteration 5870, Loss: 3.9880895614624023\n",
            "Training Iteration 5871, Loss: 3.8700146675109863\n",
            "Training Iteration 5872, Loss: 2.6654670238494873\n",
            "Training Iteration 5873, Loss: 5.202070236206055\n",
            "Training Iteration 5874, Loss: 6.812784671783447\n",
            "Training Iteration 5875, Loss: 3.258784055709839\n",
            "Training Iteration 5876, Loss: 3.4690160751342773\n",
            "Training Iteration 5877, Loss: 3.470316171646118\n",
            "Training Iteration 5878, Loss: 5.640106678009033\n",
            "Training Iteration 5879, Loss: 4.215562343597412\n",
            "Training Iteration 5880, Loss: 2.7438411712646484\n",
            "Training Iteration 5881, Loss: 3.2886202335357666\n",
            "Training Iteration 5882, Loss: 2.709282159805298\n",
            "Training Iteration 5883, Loss: 4.779935359954834\n",
            "Training Iteration 5884, Loss: 4.1667985916137695\n",
            "Training Iteration 5885, Loss: 6.901189804077148\n",
            "Training Iteration 5886, Loss: 5.927800178527832\n",
            "Training Iteration 5887, Loss: 7.974256992340088\n",
            "Training Iteration 5888, Loss: 4.509431838989258\n",
            "Training Iteration 5889, Loss: 6.424334526062012\n",
            "Training Iteration 5890, Loss: 5.286154270172119\n",
            "Training Iteration 5891, Loss: 6.183457851409912\n",
            "Training Iteration 5892, Loss: 4.564703464508057\n",
            "Training Iteration 5893, Loss: 3.006986141204834\n",
            "Training Iteration 5894, Loss: 4.220300197601318\n",
            "Training Iteration 5895, Loss: 5.277091979980469\n",
            "Training Iteration 5896, Loss: 4.668069362640381\n",
            "Training Iteration 5897, Loss: 3.003614664077759\n",
            "Training Iteration 5898, Loss: 3.506648063659668\n",
            "Training Iteration 5899, Loss: 3.5525927543640137\n",
            "Training Iteration 5900, Loss: 5.6142354011535645\n",
            "Training Iteration 5901, Loss: 1.555163860321045\n",
            "Training Iteration 5902, Loss: 2.772080659866333\n",
            "Training Iteration 5903, Loss: 1.2456316947937012\n",
            "Training Iteration 5904, Loss: 8.851351737976074\n",
            "Training Iteration 5905, Loss: 9.50533390045166\n",
            "Training Iteration 5906, Loss: 4.598788261413574\n",
            "Training Iteration 5907, Loss: 4.889594554901123\n",
            "Training Iteration 5908, Loss: 7.828329086303711\n",
            "Training Iteration 5909, Loss: 9.808172225952148\n",
            "Training Iteration 5910, Loss: 4.8296332359313965\n",
            "Training Iteration 5911, Loss: 5.700221538543701\n",
            "Training Iteration 5912, Loss: 3.153830051422119\n",
            "Training Iteration 5913, Loss: 2.697373390197754\n",
            "Training Iteration 5914, Loss: 2.8837246894836426\n",
            "Training Iteration 5915, Loss: 4.693347930908203\n",
            "Training Iteration 5916, Loss: 6.547820091247559\n",
            "Training Iteration 5917, Loss: 6.435610771179199\n",
            "Training Iteration 5918, Loss: 3.7174692153930664\n",
            "Training Iteration 5919, Loss: 3.901930332183838\n",
            "Training Iteration 5920, Loss: 3.188544273376465\n",
            "Training Iteration 5921, Loss: 4.3807783126831055\n",
            "Training Iteration 5922, Loss: 4.856518745422363\n",
            "Training Iteration 5923, Loss: 0.9591864943504333\n",
            "Training Iteration 5924, Loss: 7.01591157913208\n",
            "Training Iteration 5925, Loss: 2.9793336391448975\n",
            "Training Iteration 5926, Loss: 2.7048444747924805\n",
            "Training Iteration 5927, Loss: 4.207523822784424\n",
            "Training Iteration 5928, Loss: 9.661009788513184\n",
            "Training Iteration 5929, Loss: 6.3022027015686035\n",
            "Training Iteration 5930, Loss: 5.922090530395508\n",
            "Training Iteration 5931, Loss: 6.428286552429199\n",
            "Training Iteration 5932, Loss: 4.775136947631836\n",
            "Training Iteration 5933, Loss: 4.486256122589111\n",
            "Training Iteration 5934, Loss: 5.320011615753174\n",
            "Training Iteration 5935, Loss: 3.671262741088867\n",
            "Training Iteration 5936, Loss: 5.592142105102539\n",
            "Training Iteration 5937, Loss: 6.599689483642578\n",
            "Training Iteration 5938, Loss: 4.065181732177734\n",
            "Training Iteration 5939, Loss: 3.1686348915100098\n",
            "Training Iteration 5940, Loss: 3.418694019317627\n",
            "Training Iteration 5941, Loss: 3.7807650566101074\n",
            "Training Iteration 5942, Loss: 5.684020042419434\n",
            "Training Iteration 5943, Loss: 2.2647829055786133\n",
            "Training Iteration 5944, Loss: 6.149272441864014\n",
            "Training Iteration 5945, Loss: 2.5984058380126953\n",
            "Training Iteration 5946, Loss: 4.983530044555664\n",
            "Training Iteration 5947, Loss: 7.305747032165527\n",
            "Training Iteration 5948, Loss: 6.106632709503174\n",
            "Training Iteration 5949, Loss: 3.428264856338501\n",
            "Training Iteration 5950, Loss: 3.3927462100982666\n",
            "Training Iteration 5951, Loss: 5.792654037475586\n",
            "Training Iteration 5952, Loss: 5.706760406494141\n",
            "Training Iteration 5953, Loss: 7.593113422393799\n",
            "Training Iteration 5954, Loss: 2.6065452098846436\n",
            "Training Iteration 5955, Loss: 4.45577335357666\n",
            "Training Iteration 5956, Loss: 5.139758586883545\n",
            "Training Iteration 5957, Loss: 5.264697074890137\n",
            "Training Iteration 5958, Loss: 5.821934223175049\n",
            "Training Iteration 5959, Loss: 6.007775783538818\n",
            "Training Iteration 5960, Loss: 6.285392761230469\n",
            "Training Iteration 5961, Loss: 6.7208967208862305\n",
            "Training Iteration 5962, Loss: 4.449835777282715\n",
            "Training Iteration 5963, Loss: 3.471971035003662\n",
            "Training Iteration 5964, Loss: 3.6578869819641113\n",
            "Training Iteration 5965, Loss: 3.1871631145477295\n",
            "Training Iteration 5966, Loss: 4.950202941894531\n",
            "Training Iteration 5967, Loss: 5.800218105316162\n",
            "Training Iteration 5968, Loss: 7.858870029449463\n",
            "Training Iteration 5969, Loss: 2.413099527359009\n",
            "Training Iteration 5970, Loss: 3.3330438137054443\n",
            "Training Iteration 5971, Loss: 5.222668170928955\n",
            "Training Iteration 5972, Loss: 6.417392730712891\n",
            "Training Iteration 5973, Loss: 2.784846544265747\n",
            "Training Iteration 5974, Loss: 6.749065399169922\n",
            "Training Iteration 5975, Loss: 3.946397304534912\n",
            "Training Iteration 5976, Loss: 1.5044647455215454\n",
            "Training Iteration 5977, Loss: 7.961341381072998\n",
            "Training Iteration 5978, Loss: 4.092110633850098\n",
            "Training Iteration 5979, Loss: 5.691128730773926\n",
            "Training Iteration 5980, Loss: 3.170405864715576\n",
            "Training Iteration 5981, Loss: 6.054635047912598\n",
            "Training Iteration 5982, Loss: 8.83509635925293\n",
            "Training Iteration 5983, Loss: 7.077856063842773\n",
            "Training Iteration 5984, Loss: 4.180145263671875\n",
            "Training Iteration 5985, Loss: 5.795699596405029\n",
            "Training Iteration 5986, Loss: 6.481231212615967\n",
            "Training Iteration 5987, Loss: 2.7304534912109375\n",
            "Training Iteration 5988, Loss: 7.144016742706299\n",
            "Training Iteration 5989, Loss: 7.9939446449279785\n",
            "Training Iteration 5990, Loss: 5.144445896148682\n",
            "Training Iteration 5991, Loss: 7.354800701141357\n",
            "Training Iteration 5992, Loss: 3.155198097229004\n",
            "Training Iteration 5993, Loss: 7.774417400360107\n",
            "Training Iteration 5994, Loss: 5.6854963302612305\n",
            "Training Iteration 5995, Loss: 5.256486892700195\n",
            "Training Iteration 5996, Loss: 6.198001384735107\n",
            "Training Iteration 5997, Loss: 8.795425415039062\n",
            "Training Iteration 5998, Loss: 5.324562072753906\n",
            "Training Iteration 5999, Loss: 3.8697221279144287\n",
            "Training Iteration 6000, Loss: 6.961498260498047\n",
            "Training Iteration 6001, Loss: 3.476362466812134\n",
            "Training Iteration 6002, Loss: 2.546902656555176\n",
            "Training Iteration 6003, Loss: 4.197213649749756\n",
            "Training Iteration 6004, Loss: 3.913583278656006\n",
            "Training Iteration 6005, Loss: 6.000164985656738\n",
            "Training Iteration 6006, Loss: 4.998489856719971\n",
            "Training Iteration 6007, Loss: 7.4350056648254395\n",
            "Training Iteration 6008, Loss: 2.632425308227539\n",
            "Training Iteration 6009, Loss: 4.173410415649414\n",
            "Training Iteration 6010, Loss: 2.6594245433807373\n",
            "Training Iteration 6011, Loss: 1.3675216436386108\n",
            "Training Iteration 6012, Loss: 6.176133632659912\n",
            "Training Iteration 6013, Loss: 4.289425373077393\n",
            "Training Iteration 6014, Loss: 3.397754669189453\n",
            "Training Iteration 6015, Loss: 6.305235862731934\n",
            "Training Iteration 6016, Loss: 5.225751876831055\n",
            "Training Iteration 6017, Loss: 5.625455379486084\n",
            "Training Iteration 6018, Loss: 4.028892517089844\n",
            "Training Iteration 6019, Loss: 6.2547607421875\n",
            "Training Iteration 6020, Loss: 2.6100776195526123\n",
            "Training Iteration 6021, Loss: 4.117653846740723\n",
            "Training Iteration 6022, Loss: 3.807373523712158\n",
            "Training Iteration 6023, Loss: 4.290526390075684\n",
            "Training Iteration 6024, Loss: 6.0181756019592285\n",
            "Training Iteration 6025, Loss: 3.671421766281128\n",
            "Training Iteration 6026, Loss: 3.8120245933532715\n",
            "Training Iteration 6027, Loss: 3.691202163696289\n",
            "Training Iteration 6028, Loss: 2.930572032928467\n",
            "Training Iteration 6029, Loss: 5.029016971588135\n",
            "Training Iteration 6030, Loss: 4.967461585998535\n",
            "Training Iteration 6031, Loss: 8.69892692565918\n",
            "Training Iteration 6032, Loss: 5.785760879516602\n",
            "Training Iteration 6033, Loss: 4.676397323608398\n",
            "Training Iteration 6034, Loss: 5.711465358734131\n",
            "Training Iteration 6035, Loss: 7.835193157196045\n",
            "Training Iteration 6036, Loss: 2.735530138015747\n",
            "Training Iteration 6037, Loss: 5.192030429840088\n",
            "Training Iteration 6038, Loss: 4.719637393951416\n",
            "Training Iteration 6039, Loss: 5.234478950500488\n",
            "Training Iteration 6040, Loss: 5.793379783630371\n",
            "Training Iteration 6041, Loss: 5.604308605194092\n",
            "Training Iteration 6042, Loss: 6.36067533493042\n",
            "Training Iteration 6043, Loss: 2.788180351257324\n",
            "Training Iteration 6044, Loss: 2.6420934200286865\n",
            "Training Iteration 6045, Loss: 9.653181076049805\n",
            "Training Iteration 6046, Loss: 8.692307472229004\n",
            "Training Iteration 6047, Loss: 7.198566436767578\n",
            "Training Iteration 6048, Loss: 2.87998628616333\n",
            "Training Iteration 6049, Loss: 5.788073539733887\n",
            "Training Iteration 6050, Loss: 3.4647936820983887\n",
            "Training Iteration 6051, Loss: 6.605145454406738\n",
            "Training Iteration 6052, Loss: 3.365809679031372\n",
            "Training Iteration 6053, Loss: 4.666301250457764\n",
            "Training Iteration 6054, Loss: 6.259560585021973\n",
            "Training Iteration 6055, Loss: 4.793356895446777\n",
            "Training Iteration 6056, Loss: 5.865635395050049\n",
            "Training Iteration 6057, Loss: 2.8851776123046875\n",
            "Training Iteration 6058, Loss: 4.0143561363220215\n",
            "Training Iteration 6059, Loss: 4.626399517059326\n",
            "Training Iteration 6060, Loss: 5.3157758712768555\n",
            "Training Iteration 6061, Loss: 3.123896360397339\n",
            "Training Iteration 6062, Loss: 4.018405914306641\n",
            "Training Iteration 6063, Loss: 2.7560782432556152\n",
            "Training Iteration 6064, Loss: 4.722067832946777\n",
            "Training Iteration 6065, Loss: 3.1134274005889893\n",
            "Training Iteration 6066, Loss: 1.5904978513717651\n",
            "Training Iteration 6067, Loss: 2.53371524810791\n",
            "Training Iteration 6068, Loss: 3.4983325004577637\n",
            "Training Iteration 6069, Loss: 4.320516109466553\n",
            "Training Iteration 6070, Loss: 7.127386569976807\n",
            "Training Iteration 6071, Loss: 4.389510154724121\n",
            "Training Iteration 6072, Loss: 2.6481361389160156\n",
            "Training Iteration 6073, Loss: 3.570436477661133\n",
            "Training Iteration 6074, Loss: 3.2887308597564697\n",
            "Training Iteration 6075, Loss: 4.399241924285889\n",
            "Training Iteration 6076, Loss: 4.97979211807251\n",
            "Training Iteration 6077, Loss: 4.184507369995117\n",
            "Training Iteration 6078, Loss: 3.0541486740112305\n",
            "Training Iteration 6079, Loss: 4.174047946929932\n",
            "Training Iteration 6080, Loss: 8.630714416503906\n",
            "Training Iteration 6081, Loss: 4.696215629577637\n",
            "Training Iteration 6082, Loss: 1.7542672157287598\n",
            "Training Iteration 6083, Loss: 0.9960516691207886\n",
            "Training Iteration 6084, Loss: 5.79084587097168\n",
            "Training Iteration 6085, Loss: 6.365669250488281\n",
            "Training Iteration 6086, Loss: 3.0360453128814697\n",
            "Training Iteration 6087, Loss: 7.584378719329834\n",
            "Training Iteration 6088, Loss: 3.718189001083374\n",
            "Training Iteration 6089, Loss: 6.855814456939697\n",
            "Training Iteration 6090, Loss: 4.180957794189453\n",
            "Training Iteration 6091, Loss: 3.6415913105010986\n",
            "Training Iteration 6092, Loss: 4.970515727996826\n",
            "Training Iteration 6093, Loss: 4.6976823806762695\n",
            "Training Iteration 6094, Loss: 2.780078887939453\n",
            "Training Iteration 6095, Loss: 4.415952682495117\n",
            "Training Iteration 6096, Loss: 6.338386535644531\n",
            "Training Iteration 6097, Loss: 4.876349925994873\n",
            "Training Iteration 6098, Loss: 2.1693830490112305\n",
            "Training Iteration 6099, Loss: 4.349215984344482\n",
            "Training Iteration 6100, Loss: 6.781300067901611\n",
            "Training Iteration 6101, Loss: 4.034814834594727\n",
            "Training Iteration 6102, Loss: 3.8667659759521484\n",
            "Training Iteration 6103, Loss: 5.90386962890625\n",
            "Training Iteration 6104, Loss: 4.237316131591797\n",
            "Training Iteration 6105, Loss: 1.2136183977127075\n",
            "Training Iteration 6106, Loss: 5.2446160316467285\n",
            "Training Iteration 6107, Loss: 5.484865665435791\n",
            "Training Iteration 6108, Loss: 8.018827438354492\n",
            "Training Iteration 6109, Loss: 3.327373504638672\n",
            "Training Iteration 6110, Loss: 6.630013465881348\n",
            "Training Iteration 6111, Loss: 1.2858296632766724\n",
            "Training Iteration 6112, Loss: 3.7969377040863037\n",
            "Training Iteration 6113, Loss: 6.849110126495361\n",
            "Training Iteration 6114, Loss: 7.096914291381836\n",
            "Training Iteration 6115, Loss: 3.7409439086914062\n",
            "Training Iteration 6116, Loss: 4.1396565437316895\n",
            "Training Iteration 6117, Loss: 3.155386209487915\n",
            "Training Iteration 6118, Loss: 3.303317070007324\n",
            "Training Iteration 6119, Loss: 10.628056526184082\n",
            "Training Iteration 6120, Loss: 10.906517028808594\n",
            "Training Iteration 6121, Loss: 5.450727462768555\n",
            "Training Iteration 6122, Loss: 5.242278099060059\n",
            "Training Iteration 6123, Loss: 7.961989879608154\n",
            "Training Iteration 6124, Loss: 5.372844696044922\n",
            "Training Iteration 6125, Loss: 8.616196632385254\n",
            "Training Iteration 6126, Loss: 9.506564140319824\n",
            "Training Iteration 6127, Loss: 5.544303894042969\n",
            "Training Iteration 6128, Loss: 4.458663463592529\n",
            "Training Iteration 6129, Loss: 4.765337944030762\n",
            "Training Iteration 6130, Loss: 5.308790683746338\n",
            "Training Iteration 6131, Loss: 6.355015754699707\n",
            "Training Iteration 6132, Loss: 2.8505566120147705\n",
            "Training Iteration 6133, Loss: 5.6920881271362305\n",
            "Training Iteration 6134, Loss: 6.341214179992676\n",
            "Training Iteration 6135, Loss: 4.509809494018555\n",
            "Training Iteration 6136, Loss: 4.867519855499268\n",
            "Training Iteration 6137, Loss: 6.01339054107666\n",
            "Training Iteration 6138, Loss: 3.794036865234375\n",
            "Training Iteration 6139, Loss: 5.8948163986206055\n",
            "Training Iteration 6140, Loss: 6.4812116622924805\n",
            "Training Iteration 6141, Loss: 3.578460693359375\n",
            "Training Iteration 6142, Loss: 1.9614617824554443\n",
            "Training Iteration 6143, Loss: 4.626979827880859\n",
            "Training Iteration 6144, Loss: 4.73128604888916\n",
            "Training Iteration 6145, Loss: 2.926528215408325\n",
            "Training Iteration 6146, Loss: 4.691379070281982\n",
            "Training Iteration 6147, Loss: 5.397677421569824\n",
            "Training Iteration 6148, Loss: 4.385036468505859\n",
            "Training Iteration 6149, Loss: 6.428073883056641\n",
            "Training Iteration 6150, Loss: 4.239719867706299\n",
            "Training Iteration 6151, Loss: 7.228747844696045\n",
            "Training Iteration 6152, Loss: 2.50053071975708\n",
            "Training Iteration 6153, Loss: 3.0174951553344727\n",
            "Training Iteration 6154, Loss: 2.7839581966400146\n",
            "Training Iteration 6155, Loss: 2.619357109069824\n",
            "Training Iteration 6156, Loss: 0.9057060480117798\n",
            "Training Iteration 6157, Loss: 6.256166934967041\n",
            "Training Iteration 6158, Loss: 6.011088848114014\n",
            "Training Iteration 6159, Loss: 4.19965124130249\n",
            "Training Iteration 6160, Loss: 4.444321632385254\n",
            "Training Iteration 6161, Loss: 7.6405463218688965\n",
            "Training Iteration 6162, Loss: 4.392435073852539\n",
            "Training Iteration 6163, Loss: 4.218446254730225\n",
            "Training Iteration 6164, Loss: 3.376533031463623\n",
            "Training Iteration 6165, Loss: 3.3393235206604004\n",
            "Training Iteration 6166, Loss: 3.738112449645996\n",
            "Training Iteration 6167, Loss: 5.015714645385742\n",
            "Training Iteration 6168, Loss: 5.258838653564453\n",
            "Training Iteration 6169, Loss: 6.819800853729248\n",
            "Training Iteration 6170, Loss: 7.385610103607178\n",
            "Training Iteration 6171, Loss: 3.8554255962371826\n",
            "Training Iteration 6172, Loss: 2.940429925918579\n",
            "Training Iteration 6173, Loss: 9.547257423400879\n",
            "Training Iteration 6174, Loss: 6.617392539978027\n",
            "Training Iteration 6175, Loss: 6.082635402679443\n",
            "Training Iteration 6176, Loss: 2.3567991256713867\n",
            "Training Iteration 6177, Loss: 5.818567276000977\n",
            "Training Iteration 6178, Loss: 3.596818208694458\n",
            "Training Iteration 6179, Loss: 4.007081031799316\n",
            "Training Iteration 6180, Loss: 4.4078779220581055\n",
            "Training Iteration 6181, Loss: 3.039557456970215\n",
            "Training Iteration 6182, Loss: 2.0846002101898193\n",
            "Training Iteration 6183, Loss: 5.447997093200684\n",
            "Training Iteration 6184, Loss: 2.703437566757202\n",
            "Training Iteration 6185, Loss: 3.9733991622924805\n",
            "Training Iteration 6186, Loss: 4.8881683349609375\n",
            "Training Iteration 6187, Loss: 4.728592872619629\n",
            "Training Iteration 6188, Loss: 4.476985931396484\n",
            "Training Iteration 6189, Loss: 2.8570096492767334\n",
            "Training Iteration 6190, Loss: 3.1874752044677734\n",
            "Training Iteration 6191, Loss: 5.698773384094238\n",
            "Training Iteration 6192, Loss: 7.430826187133789\n",
            "Training Iteration 6193, Loss: 4.971712589263916\n",
            "Training Iteration 6194, Loss: 4.7482733726501465\n",
            "Training Iteration 6195, Loss: 0.9475476741790771\n",
            "Training Iteration 6196, Loss: 3.4657082557678223\n",
            "Training Iteration 6197, Loss: 3.3726868629455566\n",
            "Training Iteration 6198, Loss: 3.643156051635742\n",
            "Training Iteration 6199, Loss: 3.520695924758911\n",
            "Training Iteration 6200, Loss: 3.377551555633545\n",
            "Training Iteration 6201, Loss: 3.4199936389923096\n",
            "Training Iteration 6202, Loss: 5.984979152679443\n",
            "Training Iteration 6203, Loss: 5.111842155456543\n",
            "Training Iteration 6204, Loss: 4.551631450653076\n",
            "Training Iteration 6205, Loss: 4.400979042053223\n",
            "Training Iteration 6206, Loss: 4.06889533996582\n",
            "Training Iteration 6207, Loss: 4.0761332511901855\n",
            "Training Iteration 6208, Loss: 5.404265403747559\n",
            "Training Iteration 6209, Loss: 5.986599445343018\n",
            "Training Iteration 6210, Loss: 1.8878331184387207\n",
            "Training Iteration 6211, Loss: 5.391345024108887\n",
            "Training Iteration 6212, Loss: 4.417686939239502\n",
            "Training Iteration 6213, Loss: 4.38425874710083\n",
            "Training Iteration 6214, Loss: 4.084450721740723\n",
            "Training Iteration 6215, Loss: 2.7226815223693848\n",
            "Training Iteration 6216, Loss: 6.2580952644348145\n",
            "Training Iteration 6217, Loss: 7.052189350128174\n",
            "Training Iteration 6218, Loss: 1.392175316810608\n",
            "Training Iteration 6219, Loss: 3.261425018310547\n",
            "Training Iteration 6220, Loss: 5.796105861663818\n",
            "Training Iteration 6221, Loss: 2.331618309020996\n",
            "Training Iteration 6222, Loss: 3.207204818725586\n",
            "Training Iteration 6223, Loss: 2.9600086212158203\n",
            "Training Iteration 6224, Loss: 3.715545892715454\n",
            "Training Iteration 6225, Loss: 3.927928924560547\n",
            "Training Iteration 6226, Loss: 5.01847505569458\n",
            "Training Iteration 6227, Loss: 3.940279722213745\n",
            "Training Iteration 6228, Loss: 8.273405075073242\n",
            "Training Iteration 6229, Loss: 4.663211822509766\n",
            "Training Iteration 6230, Loss: 5.967217922210693\n",
            "Training Iteration 6231, Loss: 4.228217601776123\n",
            "Training Iteration 6232, Loss: 3.8561577796936035\n",
            "Training Iteration 6233, Loss: 4.18474006652832\n",
            "Training Iteration 6234, Loss: 5.750182628631592\n",
            "Training Iteration 6235, Loss: 3.853405237197876\n",
            "Training Iteration 6236, Loss: 3.3283581733703613\n",
            "Training Iteration 6237, Loss: 4.067157745361328\n",
            "Training Iteration 6238, Loss: 5.128657341003418\n",
            "Training Iteration 6239, Loss: 3.686924457550049\n",
            "Training Iteration 6240, Loss: 3.276735782623291\n",
            "Training Iteration 6241, Loss: 3.139906644821167\n",
            "Training Iteration 6242, Loss: 3.9424004554748535\n",
            "Training Iteration 6243, Loss: 7.9267191886901855\n",
            "Training Iteration 6244, Loss: 4.1352033615112305\n",
            "Training Iteration 6245, Loss: 3.723407506942749\n",
            "Training Iteration 6246, Loss: 3.120161533355713\n",
            "Training Iteration 6247, Loss: 4.303812503814697\n",
            "Training Iteration 6248, Loss: 4.785945415496826\n",
            "Training Iteration 6249, Loss: 4.031330585479736\n",
            "Training Iteration 6250, Loss: 5.106642246246338\n",
            "Training Iteration 6251, Loss: 2.460695266723633\n",
            "Training Iteration 6252, Loss: 2.6606297492980957\n",
            "Training Iteration 6253, Loss: 6.088046073913574\n",
            "Training Iteration 6254, Loss: 3.0578627586364746\n",
            "Training Iteration 6255, Loss: 5.848158836364746\n",
            "Training Iteration 6256, Loss: 5.650145530700684\n",
            "Training Iteration 6257, Loss: 3.566250801086426\n",
            "Training Iteration 6258, Loss: 4.287916660308838\n",
            "Training Iteration 6259, Loss: 4.657485008239746\n",
            "Training Iteration 6260, Loss: 3.6401803493499756\n",
            "Training Iteration 6261, Loss: 4.802139759063721\n",
            "Training Iteration 6262, Loss: 3.710515260696411\n",
            "Training Iteration 6263, Loss: 5.963101387023926\n",
            "Training Iteration 6264, Loss: 3.1607677936553955\n",
            "Training Iteration 6265, Loss: 3.6148433685302734\n",
            "Training Iteration 6266, Loss: 4.821874141693115\n",
            "Training Iteration 6267, Loss: 3.624913454055786\n",
            "Training Iteration 6268, Loss: 3.150235176086426\n",
            "Training Iteration 6269, Loss: 6.177718639373779\n",
            "Training Iteration 6270, Loss: 7.097485542297363\n",
            "Training Iteration 6271, Loss: 4.5146050453186035\n",
            "Training Iteration 6272, Loss: 3.621492862701416\n",
            "Training Iteration 6273, Loss: 2.6392972469329834\n",
            "Training Iteration 6274, Loss: 2.975094795227051\n",
            "Training Iteration 6275, Loss: 4.13472843170166\n",
            "Training Iteration 6276, Loss: 3.980226993560791\n",
            "Training Iteration 6277, Loss: 1.8977688550949097\n",
            "Training Iteration 6278, Loss: 3.7110908031463623\n",
            "Training Iteration 6279, Loss: 3.6699788570404053\n",
            "Training Iteration 6280, Loss: 3.9648077487945557\n",
            "Training Iteration 6281, Loss: 3.5795700550079346\n",
            "Training Iteration 6282, Loss: 2.71451735496521\n",
            "Training Iteration 6283, Loss: 3.6910998821258545\n",
            "Training Iteration 6284, Loss: 5.34781551361084\n",
            "Training Iteration 6285, Loss: 4.08260440826416\n",
            "Training Iteration 6286, Loss: 4.815051078796387\n",
            "Training Iteration 6287, Loss: 6.381496429443359\n",
            "Training Iteration 6288, Loss: 5.540111064910889\n",
            "Training Iteration 6289, Loss: 2.2030715942382812\n",
            "Training Iteration 6290, Loss: 2.264620780944824\n",
            "Training Iteration 6291, Loss: 3.1008145809173584\n",
            "Training Iteration 6292, Loss: 5.292119979858398\n",
            "Training Iteration 6293, Loss: 2.667118787765503\n",
            "Training Iteration 6294, Loss: 5.04667329788208\n",
            "Training Iteration 6295, Loss: 4.558872222900391\n",
            "Training Iteration 6296, Loss: 3.9257235527038574\n",
            "Training Iteration 6297, Loss: 3.0140891075134277\n",
            "Training Iteration 6298, Loss: 4.303591728210449\n",
            "Training Iteration 6299, Loss: 6.315304756164551\n",
            "Training Iteration 6300, Loss: 4.432006359100342\n",
            "Training Iteration 6301, Loss: 5.611597537994385\n",
            "Training Iteration 6302, Loss: 5.566510200500488\n",
            "Training Iteration 6303, Loss: 4.693482398986816\n",
            "Training Iteration 6304, Loss: 5.436981201171875\n",
            "Training Iteration 6305, Loss: 5.9712300300598145\n",
            "Training Iteration 6306, Loss: 4.511314392089844\n",
            "Training Iteration 6307, Loss: 3.9934794902801514\n",
            "Training Iteration 6308, Loss: 4.712183475494385\n",
            "Training Iteration 6309, Loss: 3.1939029693603516\n",
            "Training Iteration 6310, Loss: 4.679510116577148\n",
            "Training Iteration 6311, Loss: 3.7537264823913574\n",
            "Training Iteration 6312, Loss: 2.8405086994171143\n",
            "Training Iteration 6313, Loss: 6.997472286224365\n",
            "Training Iteration 6314, Loss: 5.909518241882324\n",
            "Training Iteration 6315, Loss: 5.006604194641113\n",
            "Training Iteration 6316, Loss: 5.227497577667236\n",
            "Training Iteration 6317, Loss: 4.106060981750488\n",
            "Training Iteration 6318, Loss: 2.8935861587524414\n",
            "Training Iteration 6319, Loss: 3.5773630142211914\n",
            "Training Iteration 6320, Loss: 4.297388553619385\n",
            "Training Iteration 6321, Loss: 4.783829212188721\n",
            "Training Iteration 6322, Loss: 3.2905077934265137\n",
            "Training Iteration 6323, Loss: 4.848844051361084\n",
            "Training Iteration 6324, Loss: 3.4802639484405518\n",
            "Training Iteration 6325, Loss: 5.582929611206055\n",
            "Training Iteration 6326, Loss: 6.054014682769775\n",
            "Training Iteration 6327, Loss: 4.051337242126465\n",
            "Training Iteration 6328, Loss: 3.7694947719573975\n",
            "Training Iteration 6329, Loss: 4.262840270996094\n",
            "Training Iteration 6330, Loss: 2.323742151260376\n",
            "Training Iteration 6331, Loss: 3.136028289794922\n",
            "Training Iteration 6332, Loss: 4.877580165863037\n",
            "Training Iteration 6333, Loss: 5.217418193817139\n",
            "Training Iteration 6334, Loss: 4.748020648956299\n",
            "Training Iteration 6335, Loss: 3.38002610206604\n",
            "Training Iteration 6336, Loss: 4.30103874206543\n",
            "Training Iteration 6337, Loss: 3.6636240482330322\n",
            "Training Iteration 6338, Loss: 7.100592136383057\n",
            "Training Iteration 6339, Loss: 3.4820897579193115\n",
            "Training Iteration 6340, Loss: 5.159366130828857\n",
            "Training Iteration 6341, Loss: 2.8098559379577637\n",
            "Training Iteration 6342, Loss: 4.048068523406982\n",
            "Training Iteration 6343, Loss: 3.547003746032715\n",
            "Training Iteration 6344, Loss: 2.409257411956787\n",
            "Training Iteration 6345, Loss: 5.065018653869629\n",
            "Training Iteration 6346, Loss: 3.5024404525756836\n",
            "Training Iteration 6347, Loss: 2.529651165008545\n",
            "Training Iteration 6348, Loss: 3.2576818466186523\n",
            "Training Iteration 6349, Loss: 4.986464500427246\n",
            "Training Iteration 6350, Loss: 2.044727325439453\n",
            "Training Iteration 6351, Loss: 5.934081077575684\n",
            "Training Iteration 6352, Loss: 4.571997165679932\n",
            "Training Iteration 6353, Loss: 5.6858649253845215\n",
            "Training Iteration 6354, Loss: 2.9205939769744873\n",
            "Training Iteration 6355, Loss: 2.0978164672851562\n",
            "Training Iteration 6356, Loss: 6.997702598571777\n",
            "Training Iteration 6357, Loss: 3.981140375137329\n",
            "Training Iteration 6358, Loss: 6.7672343254089355\n",
            "Training Iteration 6359, Loss: 1.772505521774292\n",
            "Training Iteration 6360, Loss: 4.365160942077637\n",
            "Training Iteration 6361, Loss: 7.474765777587891\n",
            "Training Iteration 6362, Loss: 3.5618064403533936\n",
            "Training Iteration 6363, Loss: 5.099544525146484\n",
            "Training Iteration 6364, Loss: 4.285170078277588\n",
            "Training Iteration 6365, Loss: 1.7544658184051514\n",
            "Training Iteration 6366, Loss: 3.603349447250366\n",
            "Training Iteration 6367, Loss: 3.2355031967163086\n",
            "Training Iteration 6368, Loss: 6.613882541656494\n",
            "Training Iteration 6369, Loss: 3.6777029037475586\n",
            "Training Iteration 6370, Loss: 3.814154624938965\n",
            "Training Iteration 6371, Loss: 2.8452696800231934\n",
            "Training Iteration 6372, Loss: 7.405642509460449\n",
            "Training Iteration 6373, Loss: 2.8757882118225098\n",
            "Training Iteration 6374, Loss: 4.748682498931885\n",
            "Training Iteration 6375, Loss: 4.203482151031494\n",
            "Training Iteration 6376, Loss: 4.1818318367004395\n",
            "Training Iteration 6377, Loss: 6.163110733032227\n",
            "Training Iteration 6378, Loss: 2.090437889099121\n",
            "Training Iteration 6379, Loss: 2.9172141551971436\n",
            "Training Iteration 6380, Loss: 2.8376426696777344\n",
            "Training Iteration 6381, Loss: 5.335547924041748\n",
            "Training Iteration 6382, Loss: 2.6385481357574463\n",
            "Training Iteration 6383, Loss: 5.879436492919922\n",
            "Training Iteration 6384, Loss: 3.711743116378784\n",
            "Training Iteration 6385, Loss: 4.050504207611084\n",
            "Training Iteration 6386, Loss: 7.809683799743652\n",
            "Training Iteration 6387, Loss: 4.870108127593994\n",
            "Training Iteration 6388, Loss: 4.024711608886719\n",
            "Training Iteration 6389, Loss: 5.291863441467285\n",
            "Training Iteration 6390, Loss: 7.8835272789001465\n",
            "Training Iteration 6391, Loss: 5.369872093200684\n",
            "Training Iteration 6392, Loss: 4.247622013092041\n",
            "Training Iteration 6393, Loss: 2.6338143348693848\n",
            "Training Iteration 6394, Loss: 4.976978302001953\n",
            "Training Iteration 6395, Loss: 4.169353485107422\n",
            "Training Iteration 6396, Loss: 4.427468776702881\n",
            "Training Iteration 6397, Loss: 6.674019813537598\n",
            "Training Iteration 6398, Loss: 1.4896469116210938\n",
            "Training Iteration 6399, Loss: 5.328485012054443\n",
            "Training Iteration 6400, Loss: 3.427738666534424\n",
            "Training Iteration 6401, Loss: 4.947291851043701\n",
            "Training Iteration 6402, Loss: 2.3546040058135986\n",
            "Training Iteration 6403, Loss: 4.990326881408691\n",
            "Training Iteration 6404, Loss: 6.23834228515625\n",
            "Training Iteration 6405, Loss: 3.4603543281555176\n",
            "Training Iteration 6406, Loss: 4.17173957824707\n",
            "Training Iteration 6407, Loss: 4.613050937652588\n",
            "Training Iteration 6408, Loss: 2.3680641651153564\n",
            "Training Iteration 6409, Loss: 2.8374791145324707\n",
            "Training Iteration 6410, Loss: 1.509174108505249\n",
            "Training Iteration 6411, Loss: 4.366148948669434\n",
            "Training Iteration 6412, Loss: 2.901637315750122\n",
            "Training Iteration 6413, Loss: 1.6833536624908447\n",
            "Training Iteration 6414, Loss: 4.324382305145264\n",
            "Training Iteration 6415, Loss: 6.118430137634277\n",
            "Training Iteration 6416, Loss: 1.3932169675827026\n",
            "Training Iteration 6417, Loss: 4.582022666931152\n",
            "Training Iteration 6418, Loss: 3.410722017288208\n",
            "Training Iteration 6419, Loss: 6.051071643829346\n",
            "Training Iteration 6420, Loss: 2.418666124343872\n",
            "Training Iteration 6421, Loss: 4.898723602294922\n",
            "Training Iteration 6422, Loss: 5.154923439025879\n",
            "Training Iteration 6423, Loss: 6.747724533081055\n",
            "Training Iteration 6424, Loss: 6.3010735511779785\n",
            "Training Iteration 6425, Loss: 2.893223762512207\n",
            "Training Iteration 6426, Loss: 3.5609402656555176\n",
            "Training Iteration 6427, Loss: 6.281876564025879\n",
            "Training Iteration 6428, Loss: 7.657637119293213\n",
            "Training Iteration 6429, Loss: 2.2762162685394287\n",
            "Training Iteration 6430, Loss: 4.520456314086914\n",
            "Training Iteration 6431, Loss: 6.4029860496521\n",
            "Training Iteration 6432, Loss: 8.731878280639648\n",
            "Training Iteration 6433, Loss: 5.540587425231934\n",
            "Training Iteration 6434, Loss: 5.061578750610352\n",
            "Training Iteration 6435, Loss: 7.177109718322754\n",
            "Training Iteration 6436, Loss: 6.180243492126465\n",
            "Training Iteration 6437, Loss: 3.7858173847198486\n",
            "Training Iteration 6438, Loss: 3.8846004009246826\n",
            "Training Iteration 6439, Loss: 6.008959770202637\n",
            "Training Iteration 6440, Loss: 3.921476125717163\n",
            "Training Iteration 6441, Loss: 2.9045703411102295\n",
            "Training Iteration 6442, Loss: 5.417671203613281\n",
            "Training Iteration 6443, Loss: 4.087893486022949\n",
            "Training Iteration 6444, Loss: 6.144335746765137\n",
            "Training Iteration 6445, Loss: 11.372337341308594\n",
            "Training Iteration 6446, Loss: 12.17790412902832\n",
            "Training Iteration 6447, Loss: 3.822086811065674\n",
            "Training Iteration 6448, Loss: 4.367815017700195\n",
            "Training Iteration 6449, Loss: 4.405113697052002\n",
            "Training Iteration 6450, Loss: 5.2096123695373535\n",
            "Training Iteration 6451, Loss: 4.547778129577637\n",
            "Training Iteration 6452, Loss: 5.464036464691162\n",
            "Training Iteration 6453, Loss: 4.37447452545166\n",
            "Training Iteration 6454, Loss: 5.084561347961426\n",
            "Training Iteration 6455, Loss: 4.082063674926758\n",
            "Training Iteration 6456, Loss: 5.360762596130371\n",
            "Training Iteration 6457, Loss: 4.112586975097656\n",
            "Training Iteration 6458, Loss: 3.650925397872925\n",
            "Training Iteration 6459, Loss: 3.1515707969665527\n",
            "Training Iteration 6460, Loss: 5.720032215118408\n",
            "Training Iteration 6461, Loss: 5.0875749588012695\n",
            "Training Iteration 6462, Loss: 4.134339809417725\n",
            "Training Iteration 6463, Loss: 5.130977630615234\n",
            "Training Iteration 6464, Loss: 7.999311447143555\n",
            "Training Iteration 6465, Loss: 6.305236339569092\n",
            "Training Iteration 6466, Loss: 4.435795783996582\n",
            "Training Iteration 6467, Loss: 2.322382688522339\n",
            "Training Iteration 6468, Loss: 6.9384331703186035\n",
            "Training Iteration 6469, Loss: 5.72913932800293\n",
            "Training Iteration 6470, Loss: 4.286815166473389\n",
            "Training Iteration 6471, Loss: 4.269357681274414\n",
            "Training Iteration 6472, Loss: 3.5977566242218018\n",
            "Training Iteration 6473, Loss: 4.275336742401123\n",
            "Training Iteration 6474, Loss: 4.884179592132568\n",
            "Training Iteration 6475, Loss: 4.725955963134766\n",
            "Training Iteration 6476, Loss: 4.791123867034912\n",
            "Training Iteration 6477, Loss: 11.074851989746094\n",
            "Training Iteration 6478, Loss: 7.333271026611328\n",
            "Training Iteration 6479, Loss: 6.8531365394592285\n",
            "Training Iteration 6480, Loss: 5.172539710998535\n",
            "Training Iteration 6481, Loss: 4.5514326095581055\n",
            "Training Iteration 6482, Loss: 3.992936849594116\n",
            "Training Iteration 6483, Loss: 1.99277663230896\n",
            "Training Iteration 6484, Loss: 3.12005877494812\n",
            "Training Iteration 6485, Loss: 5.000879764556885\n",
            "Training Iteration 6486, Loss: 6.257810592651367\n",
            "Training Iteration 6487, Loss: 4.723427772521973\n",
            "Training Iteration 6488, Loss: 2.3109936714172363\n",
            "Training Iteration 6489, Loss: 4.170982837677002\n",
            "Training Iteration 6490, Loss: 3.9907662868499756\n",
            "Training Iteration 6491, Loss: 7.359530448913574\n",
            "Training Iteration 6492, Loss: 4.882333278656006\n",
            "Training Iteration 6493, Loss: 2.816802740097046\n",
            "Training Iteration 6494, Loss: 5.714418411254883\n",
            "Training Iteration 6495, Loss: 5.061543941497803\n",
            "Training Iteration 6496, Loss: 2.062434196472168\n",
            "Training Iteration 6497, Loss: 7.271462440490723\n",
            "Training Iteration 6498, Loss: 7.564316272735596\n",
            "Training Iteration 6499, Loss: 4.392569541931152\n",
            "Training Iteration 6500, Loss: 3.9787075519561768\n",
            "Training Iteration 6501, Loss: 5.3616557121276855\n",
            "Training Iteration 6502, Loss: 4.087834358215332\n",
            "Training Iteration 6503, Loss: 4.842752456665039\n",
            "Training Iteration 6504, Loss: 5.868438243865967\n",
            "Training Iteration 6505, Loss: 3.9250364303588867\n",
            "Training Iteration 6506, Loss: 3.157817840576172\n",
            "Training Iteration 6507, Loss: 1.2466353178024292\n",
            "Training Iteration 6508, Loss: 3.7370080947875977\n",
            "Training Iteration 6509, Loss: 4.023860931396484\n",
            "Training Iteration 6510, Loss: 4.548993110656738\n",
            "Training Iteration 6511, Loss: 6.306654930114746\n",
            "Training Iteration 6512, Loss: 1.841526746749878\n",
            "Training Iteration 6513, Loss: 3.879687786102295\n",
            "Training Iteration 6514, Loss: 6.643287658691406\n",
            "Training Iteration 6515, Loss: 6.273109436035156\n",
            "Training Iteration 6516, Loss: 6.076808929443359\n",
            "Training Iteration 6517, Loss: 4.703198432922363\n",
            "Training Iteration 6518, Loss: 4.18826150894165\n",
            "Training Iteration 6519, Loss: 4.925037384033203\n",
            "Training Iteration 6520, Loss: 5.653772354125977\n",
            "Training Iteration 6521, Loss: 7.564294815063477\n",
            "Training Iteration 6522, Loss: 4.602091312408447\n",
            "Training Iteration 6523, Loss: 6.815755844116211\n",
            "Training Iteration 6524, Loss: 4.221049785614014\n",
            "Training Iteration 6525, Loss: 4.307757377624512\n",
            "Training Iteration 6526, Loss: 4.361549377441406\n",
            "Training Iteration 6527, Loss: 4.628683090209961\n",
            "Training Iteration 6528, Loss: 6.067568778991699\n",
            "Training Iteration 6529, Loss: 3.712205410003662\n",
            "Training Iteration 6530, Loss: 5.169854640960693\n",
            "Training Iteration 6531, Loss: 6.864092826843262\n",
            "Training Iteration 6532, Loss: 1.9760786294937134\n",
            "Training Iteration 6533, Loss: 4.411891937255859\n",
            "Training Iteration 6534, Loss: 4.882909297943115\n",
            "Training Iteration 6535, Loss: 4.103549003601074\n",
            "Training Iteration 6536, Loss: 5.050261497497559\n",
            "Training Iteration 6537, Loss: 2.048628807067871\n",
            "Training Iteration 6538, Loss: 4.89152193069458\n",
            "Training Iteration 6539, Loss: 4.836429595947266\n",
            "Training Iteration 6540, Loss: 4.223889350891113\n",
            "Training Iteration 6541, Loss: 6.644924163818359\n",
            "Training Iteration 6542, Loss: 7.826195240020752\n",
            "Training Iteration 6543, Loss: 2.7654550075531006\n",
            "Training Iteration 6544, Loss: 4.720096111297607\n",
            "Training Iteration 6545, Loss: 4.759075164794922\n",
            "Training Iteration 6546, Loss: 2.4475581645965576\n",
            "Training Iteration 6547, Loss: 4.476408004760742\n",
            "Training Iteration 6548, Loss: 5.128602981567383\n",
            "Training Iteration 6549, Loss: 4.7152791023254395\n",
            "Training Iteration 6550, Loss: 1.9694669246673584\n",
            "Training Iteration 6551, Loss: 3.133849620819092\n",
            "Training Iteration 6552, Loss: 3.854228973388672\n",
            "Training Iteration 6553, Loss: 4.016077995300293\n",
            "Training Iteration 6554, Loss: 4.108123779296875\n",
            "tensor([[1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        ...,\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04]])\n",
            "Training loss for epcoh 4: 4.110649795013119\n",
            "Training accuracy for epoch 4: 0.2823789722656697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        ...,\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04],\n",
            "        [1.4190e-02, 9.7346e-01, 4.2595e-04, 3.2995e-03, 8.4423e-03, 1.8424e-04]])\n",
            "Validation loss for epcoh 4: 4.127111217755963\n",
            "Test accuracy for epoch 4: 0.2780956740672923\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch No: 5:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05322f4214f2468e8dc65933269ad0ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Iteration 1, Loss: 4.475252151489258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 4.206798553466797\n",
            "Training Iteration 1565, Loss: 1.4697126150131226\n",
            "Training Iteration 1566, Loss: 6.507330894470215\n",
            "Training Iteration 1567, Loss: 4.1826982498168945\n",
            "Training Iteration 1568, Loss: 5.538692474365234\n",
            "Training Iteration 1569, Loss: 2.920654296875\n",
            "Training Iteration 1570, Loss: 2.9083752632141113\n",
            "Training Iteration 1571, Loss: 4.548991680145264\n",
            "Training Iteration 1572, Loss: 2.486471176147461\n",
            "Training Iteration 1573, Loss: 3.2735376358032227\n",
            "Training Iteration 1574, Loss: 4.0081610679626465\n",
            "Training Iteration 1575, Loss: 2.6354143619537354\n",
            "Training Iteration 1576, Loss: 4.078603744506836\n",
            "Training Iteration 1577, Loss: 2.397029399871826\n",
            "Training Iteration 1578, Loss: 4.25202751159668\n",
            "Training Iteration 1579, Loss: 6.203421592712402\n",
            "Training Iteration 1580, Loss: 3.5837185382843018\n",
            "Training Iteration 1581, Loss: 4.956767559051514\n",
            "Training Iteration 1582, Loss: 4.3175458908081055\n",
            "Training Iteration 1583, Loss: 9.445066452026367\n",
            "Training Iteration 1584, Loss: 6.046712875366211\n",
            "Training Iteration 1585, Loss: 4.55065393447876\n",
            "Training Iteration 1586, Loss: 3.7011213302612305\n",
            "Training Iteration 1587, Loss: 5.452413558959961\n",
            "Training Iteration 1588, Loss: 5.928050518035889\n",
            "Training Iteration 1589, Loss: 2.884523868560791\n",
            "Training Iteration 1590, Loss: 2.070216655731201\n",
            "Training Iteration 1591, Loss: 4.869280815124512\n",
            "Training Iteration 1592, Loss: 5.3130106925964355\n",
            "Training Iteration 1593, Loss: 5.413134574890137\n",
            "Training Iteration 1594, Loss: 8.318779945373535\n",
            "Training Iteration 1595, Loss: 8.951776504516602\n",
            "Training Iteration 1596, Loss: 5.7842912673950195\n",
            "Training Iteration 1597, Loss: 4.562291622161865\n",
            "Training Iteration 1598, Loss: 6.630707740783691\n",
            "Training Iteration 1599, Loss: 1.572879433631897\n",
            "Training Iteration 1600, Loss: 6.7813920974731445\n",
            "Training Iteration 1601, Loss: 6.456923484802246\n",
            "Training Iteration 1602, Loss: 9.36906909942627\n",
            "Training Iteration 1603, Loss: 3.687716007232666\n",
            "Training Iteration 1604, Loss: 6.468115329742432\n",
            "Training Iteration 1605, Loss: 3.5153377056121826\n",
            "Training Iteration 1606, Loss: 4.156789779663086\n",
            "Training Iteration 1607, Loss: 3.3069310188293457\n",
            "Training Iteration 1608, Loss: 5.262557029724121\n",
            "Training Iteration 1609, Loss: 3.937971591949463\n",
            "Training Iteration 1610, Loss: 8.206573486328125\n",
            "Training Iteration 1611, Loss: 3.4265990257263184\n",
            "Training Iteration 1612, Loss: 6.886297225952148\n",
            "Training Iteration 1613, Loss: 4.6709394454956055\n",
            "Training Iteration 1614, Loss: 3.0186989307403564\n",
            "Training Iteration 1615, Loss: 6.936448097229004\n",
            "Training Iteration 1616, Loss: 5.988656044006348\n",
            "Training Iteration 1617, Loss: 4.743789196014404\n",
            "Training Iteration 1618, Loss: 5.378783702850342\n",
            "Training Iteration 1619, Loss: 3.039821147918701\n",
            "Training Iteration 1620, Loss: 2.692361831665039\n",
            "Training Iteration 1621, Loss: 4.624344348907471\n",
            "Training Iteration 1622, Loss: 4.32972526550293\n",
            "Training Iteration 1623, Loss: 5.003667831420898\n",
            "Training Iteration 1624, Loss: 5.168355941772461\n",
            "Training Iteration 1625, Loss: 4.3881144523620605\n",
            "Training Iteration 1626, Loss: 3.209113836288452\n",
            "Training Iteration 1627, Loss: 3.8647844791412354\n",
            "Training Iteration 1628, Loss: 4.9987473487854\n",
            "Training Iteration 1629, Loss: 4.444568157196045\n",
            "Training Iteration 1630, Loss: 3.9947385787963867\n",
            "Training Iteration 1631, Loss: 3.7729971408843994\n",
            "Training Iteration 1632, Loss: 3.5302023887634277\n",
            "Training Iteration 1633, Loss: 3.2847673892974854\n",
            "Training Iteration 1634, Loss: 6.492733955383301\n",
            "Training Iteration 1635, Loss: 2.345611810684204\n",
            "Training Iteration 1636, Loss: 3.293142557144165\n",
            "Training Iteration 1637, Loss: 3.3183400630950928\n",
            "Training Iteration 1638, Loss: 4.491292953491211\n",
            "Training Iteration 1639, Loss: 4.499868392944336\n",
            "Training Iteration 1640, Loss: 4.598942279815674\n",
            "Training Iteration 1641, Loss: 2.7671754360198975\n",
            "Training Iteration 1642, Loss: 7.315892696380615\n",
            "Training Iteration 1643, Loss: 5.5948100090026855\n",
            "Training Iteration 1644, Loss: 4.870452880859375\n",
            "Training Iteration 1645, Loss: 4.798542022705078\n",
            "Training Iteration 1646, Loss: 3.751211643218994\n",
            "Training Iteration 1647, Loss: 3.317004680633545\n",
            "Training Iteration 1648, Loss: 2.853261947631836\n",
            "Training Iteration 1649, Loss: 3.9759702682495117\n",
            "Training Iteration 1650, Loss: 6.198094367980957\n",
            "Training Iteration 1651, Loss: 3.5580530166625977\n",
            "Training Iteration 1652, Loss: 3.670271635055542\n",
            "Training Iteration 1653, Loss: 5.158631324768066\n",
            "Training Iteration 1654, Loss: 2.87368106842041\n",
            "Training Iteration 1655, Loss: 2.0142931938171387\n",
            "Training Iteration 1656, Loss: 2.6545639038085938\n",
            "Training Iteration 1657, Loss: 3.416515350341797\n",
            "Training Iteration 1658, Loss: 4.596878528594971\n",
            "Training Iteration 1659, Loss: 4.404814720153809\n",
            "Training Iteration 1660, Loss: 3.711613893508911\n",
            "Training Iteration 1661, Loss: 5.677419662475586\n",
            "Training Iteration 1662, Loss: 5.270158290863037\n",
            "Training Iteration 1663, Loss: 4.948792457580566\n",
            "Training Iteration 1664, Loss: 2.7529258728027344\n",
            "Training Iteration 1665, Loss: 2.5092506408691406\n",
            "Training Iteration 1666, Loss: 4.891964912414551\n",
            "Training Iteration 1667, Loss: 4.199014663696289\n",
            "Training Iteration 1668, Loss: 4.684990406036377\n",
            "Training Iteration 1669, Loss: 3.3083181381225586\n",
            "Training Iteration 1670, Loss: 3.9843108654022217\n",
            "Training Iteration 1671, Loss: 5.9684953689575195\n",
            "Training Iteration 1672, Loss: 5.817280292510986\n",
            "Training Iteration 1673, Loss: 2.4924302101135254\n",
            "Training Iteration 1674, Loss: 2.117568254470825\n",
            "Training Iteration 1675, Loss: 2.8080544471740723\n",
            "Training Iteration 1676, Loss: 4.50586462020874\n",
            "Training Iteration 1677, Loss: 5.321952819824219\n",
            "Training Iteration 1678, Loss: 3.3169562816619873\n",
            "Training Iteration 1679, Loss: 3.24983549118042\n",
            "Training Iteration 1680, Loss: 4.0854573249816895\n",
            "Training Iteration 1681, Loss: 4.854413032531738\n",
            "Training Iteration 1682, Loss: 3.9918463230133057\n",
            "Training Iteration 1683, Loss: 1.3100552558898926\n",
            "Training Iteration 1684, Loss: 4.0250749588012695\n",
            "Training Iteration 1685, Loss: 5.616054534912109\n",
            "Training Iteration 1686, Loss: 3.619359254837036\n",
            "Training Iteration 1687, Loss: 5.206521511077881\n",
            "Training Iteration 1688, Loss: 5.685578346252441\n",
            "Training Iteration 1689, Loss: 3.0240561962127686\n",
            "Training Iteration 1690, Loss: 3.337369918823242\n",
            "Training Iteration 1691, Loss: 3.777829885482788\n",
            "Training Iteration 1692, Loss: 1.9521870613098145\n",
            "Training Iteration 1693, Loss: 2.529237985610962\n",
            "Training Iteration 1694, Loss: 4.915754318237305\n",
            "Training Iteration 1695, Loss: 4.056641578674316\n",
            "Training Iteration 1696, Loss: 2.3260724544525146\n",
            "Training Iteration 1697, Loss: 3.4692740440368652\n",
            "Training Iteration 1698, Loss: 3.911228656768799\n",
            "Training Iteration 1699, Loss: 2.3288815021514893\n",
            "Training Iteration 1700, Loss: 4.5699992179870605\n",
            "Training Iteration 1701, Loss: 3.158339500427246\n",
            "Training Iteration 1702, Loss: 4.893275737762451\n",
            "Training Iteration 1703, Loss: 2.9665253162384033\n",
            "Training Iteration 1704, Loss: 5.104672908782959\n",
            "Training Iteration 1705, Loss: 3.4071781635284424\n",
            "Training Iteration 1706, Loss: 3.2698378562927246\n",
            "Training Iteration 1707, Loss: 2.862314224243164\n",
            "Training Iteration 1708, Loss: 7.808265209197998\n",
            "Training Iteration 1709, Loss: 6.0590033531188965\n",
            "Training Iteration 1710, Loss: 4.227023601531982\n",
            "Training Iteration 1711, Loss: 4.914076805114746\n",
            "Training Iteration 1712, Loss: 2.6097412109375\n",
            "Training Iteration 1713, Loss: 5.438479900360107\n",
            "Training Iteration 1714, Loss: 3.4683690071105957\n",
            "Training Iteration 1715, Loss: 3.75264835357666\n",
            "Training Iteration 1716, Loss: 4.235693454742432\n",
            "Training Iteration 1717, Loss: 4.202142715454102\n",
            "Training Iteration 1718, Loss: 4.495228290557861\n",
            "Training Iteration 1719, Loss: 5.568758010864258\n",
            "Training Iteration 1720, Loss: 2.6584084033966064\n",
            "Training Iteration 1721, Loss: 6.533281326293945\n",
            "Training Iteration 1722, Loss: 5.068671226501465\n",
            "Training Iteration 1723, Loss: 7.7235026359558105\n",
            "Training Iteration 1724, Loss: 4.265956878662109\n",
            "Training Iteration 1725, Loss: 4.9563307762146\n",
            "Training Iteration 1726, Loss: 1.6561131477355957\n",
            "Training Iteration 1727, Loss: 5.3305134773254395\n",
            "Training Iteration 1728, Loss: 3.545346260070801\n",
            "Training Iteration 1729, Loss: 3.452021598815918\n",
            "Training Iteration 1730, Loss: 4.453912734985352\n",
            "Training Iteration 1731, Loss: 5.388629913330078\n",
            "Training Iteration 1732, Loss: 2.6844630241394043\n",
            "Training Iteration 1733, Loss: 3.7846226692199707\n",
            "Training Iteration 1734, Loss: 3.5791642665863037\n",
            "Training Iteration 1735, Loss: 5.425462245941162\n",
            "Training Iteration 1736, Loss: 4.552129745483398\n",
            "Training Iteration 1737, Loss: 5.345978736877441\n",
            "Training Iteration 1738, Loss: 3.9918212890625\n",
            "Training Iteration 1739, Loss: 6.170978546142578\n",
            "Training Iteration 1740, Loss: 2.858145236968994\n",
            "Training Iteration 1741, Loss: 3.777726650238037\n",
            "Training Iteration 1742, Loss: 1.7160813808441162\n",
            "Training Iteration 1743, Loss: 6.860400676727295\n",
            "Training Iteration 1744, Loss: 7.880197525024414\n",
            "Training Iteration 1745, Loss: 3.469719409942627\n",
            "Training Iteration 1746, Loss: 4.698171615600586\n",
            "Training Iteration 1747, Loss: 4.170289516448975\n",
            "Training Iteration 1748, Loss: 1.850179672241211\n",
            "Training Iteration 1749, Loss: 1.4250904321670532\n",
            "Training Iteration 1750, Loss: 4.238097190856934\n",
            "Training Iteration 1751, Loss: 7.445661544799805\n",
            "Training Iteration 1752, Loss: 3.826693534851074\n",
            "Training Iteration 1753, Loss: 2.413562297821045\n",
            "Training Iteration 1754, Loss: 5.940593242645264\n",
            "Training Iteration 1755, Loss: 1.8757169246673584\n",
            "Training Iteration 1756, Loss: 9.287002563476562\n",
            "Training Iteration 1757, Loss: 5.690800189971924\n",
            "Training Iteration 1758, Loss: 3.5709125995635986\n",
            "Training Iteration 1759, Loss: 5.866778373718262\n",
            "Training Iteration 1760, Loss: 4.793165683746338\n",
            "Training Iteration 1761, Loss: 5.386934280395508\n",
            "Training Iteration 1762, Loss: 8.68315315246582\n",
            "Training Iteration 1763, Loss: 2.8102285861968994\n",
            "Training Iteration 1764, Loss: 4.956804275512695\n",
            "Training Iteration 1765, Loss: 3.1681079864501953\n",
            "Training Iteration 1766, Loss: 4.008665084838867\n",
            "Training Iteration 1767, Loss: 7.581369400024414\n",
            "Training Iteration 1768, Loss: 6.718648433685303\n",
            "Training Iteration 1769, Loss: 4.71563196182251\n",
            "Training Iteration 1770, Loss: 10.52928352355957\n",
            "Training Iteration 1771, Loss: 2.7206573486328125\n",
            "Training Iteration 1772, Loss: 4.727051734924316\n",
            "Training Iteration 1773, Loss: 3.833646774291992\n",
            "Training Iteration 1774, Loss: 9.313197135925293\n",
            "Training Iteration 1775, Loss: 3.6372997760772705\n",
            "Training Iteration 1776, Loss: 3.9234230518341064\n",
            "Training Iteration 1777, Loss: 4.110719680786133\n",
            "Training Iteration 1778, Loss: 5.3883891105651855\n",
            "Training Iteration 1779, Loss: 3.871169090270996\n",
            "Training Iteration 1780, Loss: 5.925018310546875\n",
            "Training Iteration 1781, Loss: 5.26817512512207\n",
            "Training Iteration 1782, Loss: 4.340537071228027\n",
            "Training Iteration 1783, Loss: 4.6766767501831055\n",
            "Training Iteration 1784, Loss: 8.683106422424316\n",
            "Training Iteration 1785, Loss: 2.9750590324401855\n",
            "Training Iteration 1786, Loss: 2.373203754425049\n",
            "Training Iteration 1787, Loss: 4.85649299621582\n",
            "Training Iteration 1788, Loss: 2.6615664958953857\n",
            "Training Iteration 1789, Loss: 3.1667187213897705\n",
            "Training Iteration 1790, Loss: 5.148556709289551\n",
            "Training Iteration 1791, Loss: 3.0741920471191406\n",
            "Training Iteration 1792, Loss: 2.4804916381835938\n",
            "Training Iteration 1793, Loss: 8.536192893981934\n",
            "Training Iteration 1794, Loss: 6.729813098907471\n",
            "Training Iteration 1795, Loss: 8.36320686340332\n",
            "Training Iteration 1796, Loss: 3.5340616703033447\n",
            "Training Iteration 1797, Loss: 7.056057929992676\n",
            "Training Iteration 1798, Loss: 4.304059028625488\n",
            "Training Iteration 1799, Loss: 4.956908226013184\n",
            "Training Iteration 1800, Loss: 5.980556964874268\n",
            "Training Iteration 1801, Loss: 3.7637643814086914\n",
            "Training Iteration 1802, Loss: 3.2597787380218506\n",
            "Training Iteration 1803, Loss: 2.7345540523529053\n",
            "Training Iteration 1804, Loss: 6.265753746032715\n",
            "Training Iteration 1805, Loss: 5.763391017913818\n",
            "Training Iteration 1806, Loss: 0.9499704241752625\n",
            "Training Iteration 1807, Loss: 4.978557109832764\n",
            "Training Iteration 1808, Loss: 4.2061848640441895\n",
            "Training Iteration 1809, Loss: 4.9182844161987305\n",
            "Training Iteration 1810, Loss: 3.6572113037109375\n",
            "Training Iteration 1811, Loss: 4.690741062164307\n",
            "Training Iteration 1812, Loss: 6.085636138916016\n",
            "Training Iteration 1813, Loss: 3.7485883235931396\n",
            "Training Iteration 1814, Loss: 6.4706902503967285\n",
            "Training Iteration 1815, Loss: 8.666229248046875\n",
            "Training Iteration 1816, Loss: 5.6561079025268555\n",
            "Training Iteration 1817, Loss: 3.161731481552124\n",
            "Training Iteration 1818, Loss: 3.082071304321289\n",
            "Training Iteration 1819, Loss: 4.3703083992004395\n",
            "Training Iteration 1820, Loss: 5.573628902435303\n",
            "Training Iteration 1821, Loss: 5.209835052490234\n",
            "Training Iteration 1822, Loss: 5.238697052001953\n",
            "Training Iteration 1823, Loss: 4.746342658996582\n",
            "Training Iteration 1824, Loss: 3.739900827407837\n",
            "Training Iteration 1825, Loss: 3.6323561668395996\n",
            "Training Iteration 1826, Loss: 3.5257177352905273\n",
            "Training Iteration 1827, Loss: 7.392510890960693\n",
            "Training Iteration 1828, Loss: 4.6698150634765625\n",
            "Training Iteration 1829, Loss: 5.488479137420654\n",
            "Training Iteration 1830, Loss: 6.038239479064941\n",
            "Training Iteration 1831, Loss: 3.2035183906555176\n",
            "Training Iteration 1832, Loss: 8.447395324707031\n",
            "Training Iteration 1833, Loss: 5.218818664550781\n",
            "Training Iteration 1834, Loss: 5.302090644836426\n",
            "Training Iteration 1835, Loss: 3.0795350074768066\n",
            "Training Iteration 1836, Loss: 4.323333263397217\n",
            "Training Iteration 1837, Loss: 2.6322853565216064\n",
            "Training Iteration 1838, Loss: 4.61960506439209\n",
            "Training Iteration 1839, Loss: 3.33318829536438\n",
            "Training Iteration 1840, Loss: 5.023988246917725\n",
            "Training Iteration 1841, Loss: 4.37672233581543\n",
            "Training Iteration 1842, Loss: 2.4659841060638428\n",
            "Training Iteration 1843, Loss: 6.449168682098389\n",
            "Training Iteration 1844, Loss: 3.451648235321045\n",
            "Training Iteration 1845, Loss: 5.018628120422363\n",
            "Training Iteration 1846, Loss: 4.130218505859375\n",
            "Training Iteration 1847, Loss: 4.62528133392334\n",
            "Training Iteration 1848, Loss: 5.777557373046875\n",
            "Training Iteration 1849, Loss: 2.155020236968994\n",
            "Training Iteration 1850, Loss: 5.046112537384033\n",
            "Training Iteration 1851, Loss: 2.806407928466797\n",
            "Training Iteration 1852, Loss: 8.059926986694336\n",
            "Training Iteration 1853, Loss: 3.738431692123413\n",
            "Training Iteration 1854, Loss: 2.8579845428466797\n",
            "Training Iteration 1855, Loss: 3.3774120807647705\n",
            "Training Iteration 1856, Loss: 3.144012451171875\n",
            "Training Iteration 1857, Loss: 3.8150105476379395\n",
            "Training Iteration 1858, Loss: 3.4926669597625732\n",
            "Training Iteration 1859, Loss: 3.666844367980957\n",
            "Training Iteration 1860, Loss: 4.848670959472656\n",
            "Training Iteration 1861, Loss: 4.107154846191406\n",
            "Training Iteration 1862, Loss: 4.848662853240967\n",
            "Training Iteration 1863, Loss: 2.819594383239746\n",
            "Training Iteration 1864, Loss: 3.5826821327209473\n",
            "Training Iteration 1865, Loss: 4.059244155883789\n",
            "Training Iteration 1866, Loss: 4.494647979736328\n",
            "Training Iteration 1867, Loss: 4.997612953186035\n",
            "Training Iteration 1868, Loss: 4.729058742523193\n",
            "Training Iteration 1869, Loss: 5.697769641876221\n",
            "Training Iteration 1870, Loss: 3.5321083068847656\n",
            "Training Iteration 1871, Loss: 4.8383893966674805\n",
            "Training Iteration 1872, Loss: 3.890969753265381\n",
            "Training Iteration 1873, Loss: 4.158591270446777\n",
            "Training Iteration 1874, Loss: 5.754604816436768\n",
            "Training Iteration 1875, Loss: 5.179628372192383\n",
            "Training Iteration 1876, Loss: 4.326135158538818\n",
            "Training Iteration 1877, Loss: 4.363055229187012\n",
            "Training Iteration 1878, Loss: 4.464105606079102\n",
            "Training Iteration 1879, Loss: 4.514265060424805\n",
            "Training Iteration 1880, Loss: 5.029046058654785\n",
            "Training Iteration 1881, Loss: 2.9292728900909424\n",
            "Training Iteration 1882, Loss: 3.9350428581237793\n",
            "Training Iteration 1883, Loss: 3.43147349357605\n",
            "Training Iteration 1884, Loss: 4.932069778442383\n",
            "Training Iteration 1885, Loss: 4.066806316375732\n",
            "Training Iteration 1886, Loss: 2.20046329498291\n",
            "Training Iteration 1887, Loss: 3.0897388458251953\n",
            "Training Iteration 1888, Loss: 5.329174041748047\n",
            "Training Iteration 1889, Loss: 7.982390403747559\n",
            "Training Iteration 1890, Loss: 4.452869892120361\n",
            "Training Iteration 1891, Loss: 4.218821048736572\n",
            "Training Iteration 1892, Loss: 5.161552429199219\n",
            "Training Iteration 1893, Loss: 2.7723941802978516\n",
            "Training Iteration 1894, Loss: 2.9875476360321045\n",
            "Training Iteration 1895, Loss: 3.5864508152008057\n",
            "Training Iteration 1896, Loss: 2.531479597091675\n",
            "Training Iteration 1897, Loss: 2.4930286407470703\n",
            "Training Iteration 1898, Loss: 5.230991363525391\n",
            "Training Iteration 1899, Loss: 3.1778221130371094\n",
            "Training Iteration 1900, Loss: 3.0651543140411377\n",
            "Training Iteration 1901, Loss: 3.749115467071533\n",
            "Training Iteration 1902, Loss: 4.534045696258545\n",
            "Training Iteration 1903, Loss: 4.739962577819824\n",
            "Training Iteration 1904, Loss: 3.9474432468414307\n",
            "Training Iteration 1905, Loss: 4.691352844238281\n",
            "Training Iteration 1906, Loss: 2.7247962951660156\n",
            "Training Iteration 1907, Loss: 6.802988529205322\n",
            "Training Iteration 1908, Loss: 4.474644660949707\n",
            "Training Iteration 1909, Loss: 3.287022352218628\n",
            "Training Iteration 1910, Loss: 3.5121617317199707\n",
            "Training Iteration 1911, Loss: 5.346065044403076\n",
            "Training Iteration 1912, Loss: 3.361217498779297\n",
            "Training Iteration 1913, Loss: 5.725379467010498\n",
            "Training Iteration 1914, Loss: 4.5733323097229\n",
            "Training Iteration 1915, Loss: 3.7492458820343018\n",
            "Training Iteration 1916, Loss: 3.348001003265381\n",
            "Training Iteration 1917, Loss: 6.103318214416504\n",
            "Training Iteration 1918, Loss: 4.349175453186035\n",
            "Training Iteration 1919, Loss: 5.729770660400391\n",
            "Training Iteration 1920, Loss: 5.38827657699585\n",
            "Training Iteration 1921, Loss: 4.128332614898682\n",
            "Training Iteration 1922, Loss: 3.4496583938598633\n",
            "Training Iteration 1923, Loss: 5.411899566650391\n",
            "Training Iteration 1924, Loss: 3.4182181358337402\n",
            "Training Iteration 1925, Loss: 5.195069313049316\n",
            "Training Iteration 1926, Loss: 5.0240159034729\n",
            "Training Iteration 1927, Loss: 7.624319076538086\n",
            "Training Iteration 1928, Loss: 3.1454789638519287\n",
            "Training Iteration 1929, Loss: 3.5302088260650635\n",
            "Training Iteration 1930, Loss: 9.930335998535156\n",
            "Training Iteration 1931, Loss: 2.4258627891540527\n",
            "Training Iteration 1932, Loss: 4.835477828979492\n",
            "Training Iteration 1933, Loss: 3.508439064025879\n",
            "Training Iteration 1934, Loss: 3.803834915161133\n",
            "Training Iteration 1935, Loss: 3.4889237880706787\n",
            "Training Iteration 1936, Loss: 5.213383197784424\n",
            "Training Iteration 1937, Loss: 5.658061981201172\n",
            "Training Iteration 1938, Loss: 4.329341411590576\n",
            "Training Iteration 1939, Loss: 4.761958122253418\n",
            "Training Iteration 1940, Loss: 5.089159965515137\n",
            "Training Iteration 1941, Loss: 2.5051145553588867\n",
            "Training Iteration 1942, Loss: 4.084630489349365\n",
            "Training Iteration 1943, Loss: 3.1394765377044678\n",
            "Training Iteration 1944, Loss: 3.527634620666504\n",
            "Training Iteration 1945, Loss: 5.353641033172607\n",
            "Training Iteration 1946, Loss: 5.178893566131592\n",
            "Training Iteration 1947, Loss: 9.393117904663086\n",
            "Training Iteration 1948, Loss: 4.219672203063965\n",
            "Training Iteration 1949, Loss: 5.027167320251465\n",
            "Training Iteration 1950, Loss: 4.153911113739014\n",
            "Training Iteration 1951, Loss: 4.424828052520752\n",
            "Training Iteration 1952, Loss: 4.420844078063965\n",
            "Training Iteration 1953, Loss: 6.282636642456055\n",
            "Training Iteration 1954, Loss: 2.953732490539551\n",
            "Training Iteration 1955, Loss: 2.129592180252075\n",
            "Training Iteration 1956, Loss: 3.739682197570801\n",
            "Training Iteration 1957, Loss: 7.894167900085449\n",
            "Training Iteration 1958, Loss: 3.431508779525757\n",
            "Training Iteration 1959, Loss: 5.81196403503418\n",
            "Training Iteration 1960, Loss: 5.313910961151123\n",
            "Training Iteration 1961, Loss: 3.7327423095703125\n",
            "Training Iteration 1962, Loss: 3.4092764854431152\n",
            "Training Iteration 1963, Loss: 3.8695027828216553\n",
            "Training Iteration 1964, Loss: 4.423619747161865\n",
            "Training Iteration 1965, Loss: 5.927302837371826\n",
            "Training Iteration 1966, Loss: 4.449372291564941\n",
            "Training Iteration 1967, Loss: 4.745757102966309\n",
            "Training Iteration 1968, Loss: 5.300323009490967\n",
            "Training Iteration 1969, Loss: 3.9218645095825195\n",
            "Training Iteration 1970, Loss: 3.3500564098358154\n",
            "Training Iteration 1971, Loss: 5.534437656402588\n",
            "Training Iteration 1972, Loss: 4.779778957366943\n",
            "Training Iteration 1973, Loss: 4.092890739440918\n",
            "Training Iteration 1974, Loss: 5.501630783081055\n",
            "Training Iteration 1975, Loss: 5.755522727966309\n",
            "Training Iteration 1976, Loss: 3.6143293380737305\n",
            "Training Iteration 1977, Loss: 5.515181541442871\n",
            "Training Iteration 1978, Loss: 4.780940055847168\n",
            "Training Iteration 1979, Loss: 3.033599376678467\n",
            "Training Iteration 1980, Loss: 3.0740652084350586\n",
            "Training Iteration 1981, Loss: 3.383284330368042\n",
            "Training Iteration 1982, Loss: 5.6004557609558105\n",
            "Training Iteration 1983, Loss: 3.749934673309326\n",
            "Training Iteration 1984, Loss: 2.7334587574005127\n",
            "Training Iteration 1985, Loss: 4.010777473449707\n",
            "Training Iteration 1986, Loss: 6.060285568237305\n",
            "Training Iteration 1987, Loss: 4.26214599609375\n",
            "Training Iteration 1988, Loss: 5.2559814453125\n",
            "Training Iteration 1989, Loss: 4.058982849121094\n",
            "Training Iteration 1990, Loss: 3.2364213466644287\n",
            "Training Iteration 1991, Loss: 5.873886585235596\n",
            "Training Iteration 1992, Loss: 3.6773898601531982\n",
            "Training Iteration 1993, Loss: 4.597069263458252\n",
            "Training Iteration 1994, Loss: 3.447577953338623\n",
            "Training Iteration 1995, Loss: 5.3763275146484375\n",
            "Training Iteration 1996, Loss: 3.0033185482025146\n",
            "Training Iteration 1997, Loss: 8.28203296661377\n",
            "Training Iteration 1998, Loss: 6.877577304840088\n",
            "Training Iteration 1999, Loss: 2.1805484294891357\n",
            "Training Iteration 2000, Loss: 4.31458044052124\n",
            "Training Iteration 2001, Loss: 7.041598796844482\n",
            "Training Iteration 2002, Loss: 2.823617458343506\n",
            "Training Iteration 2003, Loss: 6.400114059448242\n",
            "Training Iteration 2004, Loss: 2.232272148132324\n",
            "Training Iteration 2005, Loss: 5.77880334854126\n",
            "Training Iteration 2006, Loss: 4.138737201690674\n",
            "Training Iteration 2007, Loss: 3.756950855255127\n",
            "Training Iteration 2008, Loss: 3.6378586292266846\n",
            "Training Iteration 2009, Loss: 4.901480674743652\n",
            "Training Iteration 2010, Loss: 2.137223720550537\n",
            "Training Iteration 2011, Loss: 5.962340831756592\n",
            "Training Iteration 2012, Loss: 3.8578760623931885\n",
            "Training Iteration 2013, Loss: 4.742131233215332\n",
            "Training Iteration 2014, Loss: 3.969372272491455\n",
            "Training Iteration 2015, Loss: 6.080821990966797\n",
            "Training Iteration 2016, Loss: 2.8034074306488037\n",
            "Training Iteration 2017, Loss: 2.3075270652770996\n",
            "Training Iteration 2018, Loss: 4.585674285888672\n",
            "Training Iteration 2019, Loss: 2.879265546798706\n",
            "Training Iteration 2020, Loss: 4.314870357513428\n",
            "Training Iteration 2021, Loss: 3.1510391235351562\n",
            "Training Iteration 2022, Loss: 6.522004127502441\n",
            "Training Iteration 2023, Loss: 3.913130521774292\n",
            "Training Iteration 2024, Loss: 6.993319988250732\n",
            "Training Iteration 2025, Loss: 5.4641923904418945\n",
            "Training Iteration 2026, Loss: 3.4680206775665283\n",
            "Training Iteration 2027, Loss: 3.7494170665740967\n",
            "Training Iteration 2028, Loss: 2.3017752170562744\n",
            "Training Iteration 2029, Loss: 3.677922010421753\n",
            "Training Iteration 2030, Loss: 4.677460670471191\n",
            "Training Iteration 2031, Loss: 5.128286361694336\n",
            "Training Iteration 2032, Loss: 3.526916027069092\n",
            "Training Iteration 2033, Loss: 5.035423755645752\n",
            "Training Iteration 2034, Loss: 5.536270618438721\n",
            "Training Iteration 2035, Loss: 2.7179017066955566\n",
            "Training Iteration 2036, Loss: 3.6451265811920166\n",
            "Training Iteration 2037, Loss: 3.146944522857666\n",
            "Training Iteration 2038, Loss: 1.9805450439453125\n",
            "Training Iteration 2039, Loss: 3.9542083740234375\n",
            "Training Iteration 2040, Loss: 5.020262718200684\n",
            "Training Iteration 2041, Loss: 4.558134078979492\n",
            "Training Iteration 2042, Loss: 6.002474784851074\n",
            "Training Iteration 2043, Loss: 5.558741092681885\n",
            "Training Iteration 2044, Loss: 6.0621562004089355\n",
            "Training Iteration 2045, Loss: 2.3754055500030518\n",
            "Training Iteration 2046, Loss: 2.8598732948303223\n",
            "Training Iteration 2047, Loss: 4.255685806274414\n",
            "Training Iteration 2048, Loss: 1.9429856538772583\n",
            "Training Iteration 2049, Loss: 8.458510398864746\n",
            "Training Iteration 2050, Loss: 4.387434959411621\n",
            "Training Iteration 2051, Loss: 3.4621663093566895\n",
            "Training Iteration 2052, Loss: 5.138060569763184\n",
            "Training Iteration 2053, Loss: 4.820570468902588\n",
            "Training Iteration 2054, Loss: 5.216745376586914\n",
            "Training Iteration 2055, Loss: 4.06984281539917\n",
            "Training Iteration 2056, Loss: 2.8743515014648438\n",
            "Training Iteration 2057, Loss: 5.559835433959961\n",
            "Training Iteration 2058, Loss: 5.054878234863281\n",
            "Training Iteration 2059, Loss: 7.584875106811523\n",
            "Training Iteration 2060, Loss: 3.8605458736419678\n",
            "Training Iteration 2061, Loss: 2.550832509994507\n",
            "Training Iteration 2062, Loss: 2.5027101039886475\n",
            "Training Iteration 2063, Loss: 4.607797622680664\n",
            "Training Iteration 2064, Loss: 3.110356569290161\n",
            "Training Iteration 2065, Loss: 3.466303586959839\n",
            "Training Iteration 2066, Loss: 3.978123903274536\n",
            "Training Iteration 2067, Loss: 6.2765793800354\n",
            "Training Iteration 2068, Loss: 6.145407199859619\n",
            "Training Iteration 2069, Loss: 3.714860439300537\n",
            "Training Iteration 2070, Loss: 5.0621418952941895\n",
            "Training Iteration 2071, Loss: 5.33375883102417\n",
            "Training Iteration 2072, Loss: 3.764707088470459\n",
            "Training Iteration 2073, Loss: 4.743324279785156\n",
            "Training Iteration 2074, Loss: 3.4552834033966064\n",
            "Training Iteration 2075, Loss: 3.6790878772735596\n",
            "Training Iteration 2076, Loss: 2.8170886039733887\n",
            "Training Iteration 2077, Loss: 4.765510559082031\n",
            "Training Iteration 2078, Loss: 3.8659796714782715\n",
            "Training Iteration 2079, Loss: 2.5472543239593506\n",
            "Training Iteration 2080, Loss: 4.402607440948486\n",
            "Training Iteration 2081, Loss: 3.6265780925750732\n",
            "Training Iteration 2082, Loss: 4.874459743499756\n",
            "Training Iteration 2083, Loss: 3.0531766414642334\n",
            "Training Iteration 2084, Loss: 5.299620628356934\n",
            "Training Iteration 2085, Loss: 3.0013904571533203\n",
            "Training Iteration 2086, Loss: 6.642202854156494\n",
            "Training Iteration 2087, Loss: 7.253982067108154\n",
            "Training Iteration 2088, Loss: 2.8529372215270996\n",
            "Training Iteration 2089, Loss: 5.157593250274658\n",
            "Training Iteration 2090, Loss: 3.088773727416992\n",
            "Training Iteration 2091, Loss: 6.691756725311279\n",
            "Training Iteration 2092, Loss: 5.569785118103027\n",
            "Training Iteration 2093, Loss: 4.641693115234375\n",
            "Training Iteration 2094, Loss: 3.6962268352508545\n",
            "Training Iteration 2095, Loss: 2.324432849884033\n",
            "Training Iteration 2096, Loss: 2.3266093730926514\n",
            "Training Iteration 2097, Loss: 3.0087544918060303\n",
            "Training Iteration 2098, Loss: 3.4900121688842773\n",
            "Training Iteration 2099, Loss: 4.758874893188477\n",
            "Training Iteration 2100, Loss: 5.0160064697265625\n",
            "Training Iteration 2101, Loss: 4.185211181640625\n",
            "Training Iteration 2102, Loss: 3.172051191329956\n",
            "Training Iteration 2103, Loss: 5.778596878051758\n",
            "Training Iteration 2104, Loss: 5.323974132537842\n",
            "Training Iteration 2105, Loss: 4.102060317993164\n",
            "Training Iteration 2106, Loss: 4.362213611602783\n",
            "Training Iteration 2107, Loss: 5.7137861251831055\n",
            "Training Iteration 2108, Loss: 4.446365833282471\n",
            "Training Iteration 2109, Loss: 2.9500961303710938\n",
            "Training Iteration 2110, Loss: 5.362691402435303\n",
            "Training Iteration 2111, Loss: 5.021940231323242\n",
            "Training Iteration 2112, Loss: 4.915533065795898\n",
            "Training Iteration 2113, Loss: 5.314723014831543\n",
            "Training Iteration 2114, Loss: 2.072779417037964\n",
            "Training Iteration 2115, Loss: 8.837099075317383\n",
            "Training Iteration 2116, Loss: 4.498073101043701\n",
            "Training Iteration 2117, Loss: 6.2831878662109375\n",
            "Training Iteration 2118, Loss: 6.260012626647949\n",
            "Training Iteration 2119, Loss: 4.7428717613220215\n",
            "Training Iteration 2120, Loss: 4.9532790184021\n",
            "Training Iteration 2121, Loss: 3.586484432220459\n",
            "Training Iteration 2122, Loss: 4.354391574859619\n",
            "Training Iteration 2123, Loss: 5.4225172996521\n",
            "Training Iteration 2124, Loss: 2.916377067565918\n",
            "Training Iteration 2125, Loss: 4.033108234405518\n",
            "Training Iteration 2126, Loss: 3.1052865982055664\n",
            "Training Iteration 2127, Loss: 4.485878944396973\n",
            "Training Iteration 2128, Loss: 5.706347465515137\n",
            "Training Iteration 2129, Loss: 7.887386798858643\n",
            "Training Iteration 2130, Loss: 7.003839015960693\n",
            "Training Iteration 2131, Loss: 3.9300355911254883\n",
            "Training Iteration 2132, Loss: 3.409717321395874\n",
            "Training Iteration 2133, Loss: 4.437046051025391\n",
            "Training Iteration 2134, Loss: 8.315811157226562\n",
            "Training Iteration 2135, Loss: 2.746631622314453\n",
            "Training Iteration 2136, Loss: 2.661151170730591\n",
            "Training Iteration 2137, Loss: 4.571778774261475\n",
            "Training Iteration 2138, Loss: 3.022213935852051\n",
            "Training Iteration 2139, Loss: 2.196054458618164\n",
            "Training Iteration 2140, Loss: 6.151411056518555\n",
            "Training Iteration 2141, Loss: 5.828041076660156\n",
            "Training Iteration 2142, Loss: 1.970205545425415\n",
            "Training Iteration 2143, Loss: 1.5411196947097778\n",
            "Training Iteration 2144, Loss: 10.024538040161133\n",
            "Training Iteration 2145, Loss: 3.413516044616699\n",
            "Training Iteration 2146, Loss: 3.1486682891845703\n",
            "Training Iteration 2147, Loss: 5.834256172180176\n",
            "Training Iteration 2148, Loss: 13.151531219482422\n",
            "Training Iteration 2149, Loss: 6.319594383239746\n",
            "Training Iteration 2150, Loss: 4.6360955238342285\n",
            "Training Iteration 2151, Loss: 3.430213451385498\n",
            "Training Iteration 2152, Loss: 6.170073509216309\n",
            "Training Iteration 2153, Loss: 7.50787353515625\n",
            "Training Iteration 2154, Loss: 4.556437015533447\n",
            "Training Iteration 2155, Loss: 5.541619777679443\n",
            "Training Iteration 2156, Loss: 5.562328338623047\n",
            "Training Iteration 2157, Loss: 6.772729396820068\n",
            "Training Iteration 2158, Loss: 3.2068545818328857\n",
            "Training Iteration 2159, Loss: 4.994772434234619\n",
            "Training Iteration 2160, Loss: 7.221124649047852\n",
            "Training Iteration 2161, Loss: 4.52113151550293\n",
            "Training Iteration 2162, Loss: 6.407714366912842\n",
            "Training Iteration 2163, Loss: 5.117619514465332\n",
            "Training Iteration 2164, Loss: 3.769536256790161\n",
            "Training Iteration 2165, Loss: 5.560423851013184\n",
            "Training Iteration 2166, Loss: 4.754420280456543\n",
            "Training Iteration 2167, Loss: 6.96191930770874\n",
            "Training Iteration 2168, Loss: 5.663935661315918\n",
            "Training Iteration 2169, Loss: 3.8385095596313477\n",
            "Training Iteration 2170, Loss: 3.597799777984619\n",
            "Training Iteration 2171, Loss: 5.207795143127441\n",
            "Training Iteration 2172, Loss: 3.7567851543426514\n",
            "Training Iteration 2173, Loss: 6.892984390258789\n",
            "Training Iteration 2174, Loss: 10.681962013244629\n",
            "Training Iteration 2175, Loss: 8.302949905395508\n",
            "Training Iteration 2176, Loss: 4.959351062774658\n",
            "Training Iteration 2177, Loss: 4.885356903076172\n",
            "Training Iteration 2178, Loss: 4.165234088897705\n",
            "Training Iteration 2179, Loss: 4.794806480407715\n",
            "Training Iteration 2180, Loss: 3.738424777984619\n",
            "Training Iteration 2181, Loss: 4.65523624420166\n",
            "Training Iteration 2182, Loss: 5.203505039215088\n",
            "Training Iteration 2183, Loss: 3.573401927947998\n",
            "Training Iteration 2184, Loss: 2.9464337825775146\n",
            "Training Iteration 2185, Loss: 1.2652802467346191\n",
            "Training Iteration 2186, Loss: 2.5253443717956543\n",
            "Training Iteration 2187, Loss: 4.446564674377441\n",
            "Training Iteration 2188, Loss: 3.1125540733337402\n",
            "Training Iteration 2189, Loss: 8.086297988891602\n",
            "Training Iteration 2190, Loss: 5.439653396606445\n",
            "Training Iteration 2191, Loss: 4.333091735839844\n",
            "Training Iteration 2192, Loss: 7.193892002105713\n",
            "Training Iteration 2193, Loss: 2.555532932281494\n",
            "Training Iteration 2194, Loss: 7.583359241485596\n",
            "Training Iteration 2195, Loss: 5.598755359649658\n",
            "Training Iteration 2196, Loss: 6.633745193481445\n",
            "Training Iteration 2197, Loss: 8.911739349365234\n",
            "Training Iteration 2198, Loss: 3.8506994247436523\n",
            "Training Iteration 2199, Loss: 4.055336952209473\n",
            "Training Iteration 2200, Loss: 6.604185581207275\n",
            "Training Iteration 2201, Loss: 3.8190884590148926\n",
            "Training Iteration 2202, Loss: 3.324930191040039\n",
            "Training Iteration 2203, Loss: 6.27710485458374\n",
            "Training Iteration 2204, Loss: 9.599969863891602\n",
            "Training Iteration 2205, Loss: 8.472099304199219\n",
            "Training Iteration 2206, Loss: 4.955595970153809\n",
            "Training Iteration 2207, Loss: 4.033507347106934\n",
            "Training Iteration 2208, Loss: 3.450035810470581\n",
            "Training Iteration 2209, Loss: 5.546420097351074\n",
            "Training Iteration 2210, Loss: 5.520054817199707\n",
            "Training Iteration 2211, Loss: 2.970489025115967\n",
            "Training Iteration 2212, Loss: 5.714917182922363\n",
            "Training Iteration 2213, Loss: 3.5439200401306152\n",
            "Training Iteration 2214, Loss: 5.243625164031982\n",
            "Training Iteration 2215, Loss: 4.311300754547119\n",
            "Training Iteration 2216, Loss: 3.004770278930664\n",
            "Training Iteration 2217, Loss: 2.890413761138916\n",
            "Training Iteration 2218, Loss: 4.0332159996032715\n",
            "Training Iteration 2219, Loss: 4.644872665405273\n",
            "Training Iteration 2220, Loss: 5.21735143661499\n",
            "Training Iteration 2221, Loss: 4.532387733459473\n",
            "Training Iteration 2222, Loss: 4.822702884674072\n",
            "Training Iteration 2223, Loss: 6.1593780517578125\n",
            "Training Iteration 2224, Loss: 4.827126979827881\n",
            "Training Iteration 2225, Loss: 4.952907085418701\n",
            "Training Iteration 2226, Loss: 6.348603248596191\n",
            "Training Iteration 2227, Loss: 2.603907346725464\n",
            "Training Iteration 2228, Loss: 7.109932899475098\n",
            "Training Iteration 2229, Loss: 3.1608076095581055\n",
            "Training Iteration 2230, Loss: 2.892015218734741\n",
            "Training Iteration 2231, Loss: 5.385488510131836\n",
            "Training Iteration 2232, Loss: 4.183506011962891\n",
            "Training Iteration 2233, Loss: 7.261324405670166\n",
            "Training Iteration 2234, Loss: 5.137199401855469\n",
            "Training Iteration 2235, Loss: 2.59297251701355\n",
            "Training Iteration 2236, Loss: 8.485901832580566\n",
            "Training Iteration 2237, Loss: 4.167570114135742\n",
            "Training Iteration 2238, Loss: 9.484272956848145\n",
            "Training Iteration 2239, Loss: 8.46990966796875\n",
            "Training Iteration 2240, Loss: 5.020065784454346\n",
            "Training Iteration 2241, Loss: 5.979673862457275\n",
            "Training Iteration 2242, Loss: 2.412337064743042\n",
            "Training Iteration 2243, Loss: 4.572653293609619\n",
            "Training Iteration 2244, Loss: 4.157414436340332\n",
            "Training Iteration 2245, Loss: 5.206814289093018\n",
            "Training Iteration 2246, Loss: 8.719637870788574\n",
            "Training Iteration 2247, Loss: 5.792723655700684\n",
            "Training Iteration 2248, Loss: 5.802464962005615\n",
            "Training Iteration 2249, Loss: 5.117599010467529\n",
            "Training Iteration 2250, Loss: 4.023430347442627\n",
            "Training Iteration 2251, Loss: 5.030162811279297\n",
            "Training Iteration 2252, Loss: 2.1993401050567627\n",
            "Training Iteration 2253, Loss: 5.263233661651611\n",
            "Training Iteration 2254, Loss: 5.556946277618408\n",
            "Training Iteration 2255, Loss: 5.974567890167236\n",
            "Training Iteration 2256, Loss: 6.268864631652832\n",
            "Training Iteration 2257, Loss: 3.4685864448547363\n",
            "Training Iteration 2258, Loss: 5.476901054382324\n",
            "Training Iteration 2259, Loss: 5.459936618804932\n",
            "Training Iteration 2260, Loss: 1.9722464084625244\n",
            "Training Iteration 2261, Loss: 5.758848190307617\n",
            "Training Iteration 2262, Loss: 2.605854034423828\n",
            "Training Iteration 2263, Loss: 6.469171524047852\n",
            "Training Iteration 2264, Loss: 2.760587215423584\n",
            "Training Iteration 2265, Loss: 2.8363888263702393\n",
            "Training Iteration 2266, Loss: 4.3402533531188965\n",
            "Training Iteration 2267, Loss: 2.493873119354248\n",
            "Training Iteration 2268, Loss: 4.586225986480713\n",
            "Training Iteration 2269, Loss: 5.255361557006836\n",
            "Training Iteration 2270, Loss: 5.268542289733887\n",
            "Training Iteration 2271, Loss: 6.27961540222168\n",
            "Training Iteration 2272, Loss: 6.890259265899658\n",
            "Training Iteration 2273, Loss: 2.6475400924682617\n",
            "Training Iteration 2274, Loss: 4.5663628578186035\n",
            "Training Iteration 2275, Loss: 4.285961627960205\n",
            "Training Iteration 2276, Loss: 5.843495845794678\n",
            "Training Iteration 2277, Loss: 2.2988712787628174\n",
            "Training Iteration 2278, Loss: 6.41385555267334\n",
            "Training Iteration 2279, Loss: 6.097343444824219\n",
            "Training Iteration 2280, Loss: 4.93050479888916\n",
            "Training Iteration 2281, Loss: 5.152945041656494\n",
            "Training Iteration 2282, Loss: 3.9277212619781494\n",
            "Training Iteration 2283, Loss: 4.137153148651123\n",
            "Training Iteration 2284, Loss: 2.565889835357666\n",
            "Training Iteration 2285, Loss: 3.725313425064087\n",
            "Training Iteration 2286, Loss: 6.501003265380859\n",
            "Training Iteration 2287, Loss: 4.7072434425354\n",
            "Training Iteration 2288, Loss: 2.7235987186431885\n",
            "Training Iteration 2289, Loss: 3.8110475540161133\n",
            "Training Iteration 2290, Loss: 4.157711982727051\n",
            "Training Iteration 2291, Loss: 2.6271140575408936\n",
            "Training Iteration 2292, Loss: 1.3731794357299805\n",
            "Training Iteration 2293, Loss: 3.8202762603759766\n",
            "Training Iteration 2294, Loss: 4.5878190994262695\n",
            "Training Iteration 2295, Loss: 5.1482415199279785\n",
            "Training Iteration 2296, Loss: 5.02755069732666\n",
            "Training Iteration 2297, Loss: 4.2472639083862305\n",
            "Training Iteration 2298, Loss: 3.9578793048858643\n",
            "Training Iteration 2299, Loss: 3.8030295372009277\n",
            "Training Iteration 2300, Loss: 5.4657487869262695\n",
            "Training Iteration 2301, Loss: 4.906660079956055\n",
            "Training Iteration 2302, Loss: 6.422834396362305\n",
            "Training Iteration 2303, Loss: 3.946043014526367\n",
            "Training Iteration 2304, Loss: 5.126140594482422\n",
            "Training Iteration 2305, Loss: 4.635772705078125\n",
            "Training Iteration 2306, Loss: 4.53600549697876\n",
            "Training Iteration 2307, Loss: 9.527511596679688\n",
            "Training Iteration 2308, Loss: 4.576715469360352\n",
            "Training Iteration 2309, Loss: 8.458340644836426\n",
            "Training Iteration 2310, Loss: 3.6226117610931396\n",
            "Training Iteration 2311, Loss: 6.270229339599609\n",
            "Training Iteration 2312, Loss: 5.354334831237793\n",
            "Training Iteration 2313, Loss: 3.4553959369659424\n",
            "Training Iteration 2314, Loss: 4.499873638153076\n",
            "Training Iteration 2315, Loss: 5.706909656524658\n",
            "Training Iteration 2316, Loss: 3.815666675567627\n",
            "Training Iteration 2317, Loss: 4.586763858795166\n",
            "Training Iteration 2318, Loss: 4.594888210296631\n",
            "Training Iteration 2319, Loss: 3.96919584274292\n",
            "Training Iteration 2320, Loss: 6.791756629943848\n",
            "Training Iteration 2321, Loss: 5.9988789558410645\n",
            "Training Iteration 2322, Loss: 6.105771541595459\n",
            "Training Iteration 2323, Loss: 4.7213029861450195\n",
            "Training Iteration 2324, Loss: 3.072258949279785\n",
            "Training Iteration 2325, Loss: 2.972160577774048\n",
            "Training Iteration 2326, Loss: 3.7253780364990234\n",
            "Training Iteration 2327, Loss: 4.919083118438721\n",
            "Training Iteration 2328, Loss: 4.327650547027588\n",
            "Training Iteration 2329, Loss: 2.9272851943969727\n",
            "Training Iteration 2330, Loss: 2.7619402408599854\n",
            "Training Iteration 2331, Loss: 2.595736265182495\n",
            "Training Iteration 2332, Loss: 8.101856231689453\n",
            "Training Iteration 2333, Loss: 6.048452854156494\n",
            "Training Iteration 2334, Loss: 2.9202380180358887\n",
            "Training Iteration 2335, Loss: 2.556170701980591\n",
            "Training Iteration 2336, Loss: 2.3429040908813477\n",
            "Training Iteration 2337, Loss: 3.787872076034546\n",
            "Training Iteration 2338, Loss: 2.6802144050598145\n",
            "Training Iteration 2339, Loss: 4.105646133422852\n",
            "Training Iteration 2340, Loss: 2.07306170463562\n",
            "Training Iteration 2341, Loss: 3.6421282291412354\n",
            "Training Iteration 2342, Loss: 4.152558326721191\n",
            "Training Iteration 2343, Loss: 3.649322032928467\n",
            "Training Iteration 2344, Loss: 2.4189696311950684\n",
            "Training Iteration 2345, Loss: 4.8115668296813965\n",
            "Training Iteration 2346, Loss: 3.465864896774292\n",
            "Training Iteration 2347, Loss: 11.604429244995117\n",
            "Training Iteration 2348, Loss: 3.2398171424865723\n",
            "Training Iteration 2349, Loss: 5.596724987030029\n",
            "Training Iteration 2350, Loss: 2.4405078887939453\n",
            "Training Iteration 2351, Loss: 3.2147042751312256\n",
            "Training Iteration 2352, Loss: 4.020430088043213\n",
            "Training Iteration 2353, Loss: 2.7387325763702393\n",
            "Training Iteration 2354, Loss: 5.856293201446533\n",
            "Training Iteration 2355, Loss: 3.5844109058380127\n",
            "Training Iteration 2356, Loss: 5.610823631286621\n",
            "Training Iteration 2357, Loss: 4.50878381729126\n",
            "Training Iteration 2358, Loss: 2.6452763080596924\n",
            "Training Iteration 2359, Loss: 5.988948822021484\n",
            "Training Iteration 2360, Loss: 4.739558219909668\n",
            "Training Iteration 2361, Loss: 2.4366979598999023\n",
            "Training Iteration 2362, Loss: 5.147036552429199\n",
            "Training Iteration 2363, Loss: 2.791240930557251\n",
            "Training Iteration 2364, Loss: 3.1670262813568115\n",
            "Training Iteration 2365, Loss: 7.6135663986206055\n",
            "Training Iteration 2366, Loss: 2.9878733158111572\n",
            "Training Iteration 2367, Loss: 2.2355785369873047\n",
            "Training Iteration 2368, Loss: 7.27843713760376\n",
            "Training Iteration 2369, Loss: 5.846807956695557\n",
            "Training Iteration 2370, Loss: 4.120542526245117\n",
            "Training Iteration 2371, Loss: 4.022739410400391\n",
            "Training Iteration 2372, Loss: 2.516953468322754\n",
            "Training Iteration 2373, Loss: 4.323455333709717\n",
            "Training Iteration 2374, Loss: 3.4322433471679688\n",
            "Training Iteration 2375, Loss: 4.242377758026123\n",
            "Training Iteration 2376, Loss: 2.929750680923462\n",
            "Training Iteration 2377, Loss: 7.232931137084961\n",
            "Training Iteration 2378, Loss: 3.601392984390259\n",
            "Training Iteration 2379, Loss: 3.731198787689209\n",
            "Training Iteration 2380, Loss: 5.968210220336914\n",
            "Training Iteration 2381, Loss: 5.8725433349609375\n",
            "Training Iteration 2382, Loss: 3.1382970809936523\n",
            "Training Iteration 2383, Loss: 2.2084453105926514\n",
            "Training Iteration 2384, Loss: 8.713723182678223\n",
            "Training Iteration 2385, Loss: 4.0871100425720215\n",
            "Training Iteration 2386, Loss: 8.741260528564453\n",
            "Training Iteration 2387, Loss: 5.827798366546631\n",
            "Training Iteration 2388, Loss: 2.782248020172119\n",
            "Training Iteration 2389, Loss: 4.504505157470703\n",
            "Training Iteration 2390, Loss: 6.348789691925049\n",
            "Training Iteration 2391, Loss: 5.055130958557129\n",
            "Training Iteration 2392, Loss: 3.4591619968414307\n",
            "Training Iteration 2393, Loss: 4.381302833557129\n",
            "Training Iteration 2394, Loss: 5.345127105712891\n",
            "Training Iteration 2395, Loss: 6.217195987701416\n",
            "Training Iteration 2396, Loss: 3.0379512310028076\n",
            "Training Iteration 2397, Loss: 5.070415019989014\n",
            "Training Iteration 2398, Loss: 6.572370529174805\n",
            "Training Iteration 2399, Loss: 8.49666976928711\n",
            "Training Iteration 2400, Loss: 3.1297008991241455\n",
            "Training Iteration 2401, Loss: 4.856618881225586\n",
            "Training Iteration 2402, Loss: 4.416600704193115\n",
            "Training Iteration 2403, Loss: 4.465695858001709\n",
            "Training Iteration 2404, Loss: 6.470329284667969\n",
            "Training Iteration 2405, Loss: 5.375627040863037\n",
            "Training Iteration 2406, Loss: 7.092864513397217\n",
            "Training Iteration 2407, Loss: 6.014981746673584\n",
            "Training Iteration 2408, Loss: 3.3084044456481934\n",
            "Training Iteration 2409, Loss: 4.356186389923096\n",
            "Training Iteration 2410, Loss: 5.507120132446289\n",
            "Training Iteration 2411, Loss: 3.8239548206329346\n",
            "Training Iteration 2412, Loss: 6.315547943115234\n",
            "Training Iteration 2413, Loss: 4.477578163146973\n",
            "Training Iteration 2414, Loss: 6.062896728515625\n",
            "Training Iteration 2415, Loss: 4.526122570037842\n",
            "Training Iteration 2416, Loss: 4.147792339324951\n",
            "Training Iteration 2417, Loss: 6.215099811553955\n",
            "Training Iteration 2418, Loss: 3.218524217605591\n",
            "Training Iteration 2419, Loss: 3.427518844604492\n",
            "Training Iteration 2420, Loss: 3.201165199279785\n",
            "Training Iteration 2421, Loss: 3.7122066020965576\n",
            "Training Iteration 2422, Loss: 2.72471284866333\n",
            "Training Iteration 2423, Loss: 6.1154961585998535\n",
            "Training Iteration 2424, Loss: 5.720352649688721\n",
            "Training Iteration 2425, Loss: 4.504554271697998\n",
            "Training Iteration 2426, Loss: 3.171360492706299\n",
            "Training Iteration 2427, Loss: 6.8576979637146\n",
            "Training Iteration 2428, Loss: 3.1151466369628906\n",
            "Training Iteration 2429, Loss: 1.487139105796814\n",
            "Training Iteration 2430, Loss: 5.784287452697754\n",
            "Training Iteration 2431, Loss: 5.595888614654541\n",
            "Training Iteration 2432, Loss: 5.478847503662109\n",
            "Training Iteration 2433, Loss: 5.749053001403809\n",
            "Training Iteration 2434, Loss: 5.5514960289001465\n",
            "Training Iteration 2435, Loss: 6.497776031494141\n",
            "Training Iteration 2436, Loss: 2.751728057861328\n",
            "Training Iteration 2437, Loss: 4.452859878540039\n",
            "Training Iteration 2438, Loss: 6.760819435119629\n",
            "Training Iteration 2439, Loss: 5.473243236541748\n",
            "Training Iteration 2440, Loss: 5.631192684173584\n",
            "Training Iteration 2441, Loss: 5.311831474304199\n",
            "Training Iteration 2442, Loss: 5.5771636962890625\n",
            "Training Iteration 2443, Loss: 3.3530657291412354\n",
            "Training Iteration 2444, Loss: 2.399374008178711\n",
            "Training Iteration 2445, Loss: 4.6706695556640625\n",
            "Training Iteration 2446, Loss: 7.21279764175415\n",
            "Training Iteration 2447, Loss: 4.518407344818115\n",
            "Training Iteration 2448, Loss: 3.4249353408813477\n",
            "Training Iteration 2449, Loss: 4.164519309997559\n",
            "Training Iteration 2450, Loss: 5.480984687805176\n",
            "Training Iteration 2451, Loss: 3.8199667930603027\n",
            "Training Iteration 2452, Loss: 2.282999277114868\n",
            "Training Iteration 2453, Loss: 4.5688581466674805\n",
            "Training Iteration 2454, Loss: 8.671529769897461\n",
            "Training Iteration 2455, Loss: 5.275290012359619\n",
            "Training Iteration 2456, Loss: 5.928212642669678\n",
            "Training Iteration 2457, Loss: 4.507429122924805\n",
            "Training Iteration 2458, Loss: 6.072628498077393\n",
            "Training Iteration 2459, Loss: 6.493902206420898\n",
            "Training Iteration 2460, Loss: 4.300650119781494\n",
            "Training Iteration 2461, Loss: 5.80153751373291\n",
            "Training Iteration 2462, Loss: 4.707273483276367\n",
            "Training Iteration 2463, Loss: 2.0347647666931152\n",
            "Training Iteration 2464, Loss: 1.3851507902145386\n",
            "Training Iteration 2465, Loss: 4.601476192474365\n",
            "Training Iteration 2466, Loss: 4.5722174644470215\n",
            "Training Iteration 2467, Loss: 4.126671314239502\n",
            "Training Iteration 2468, Loss: 3.3681397438049316\n",
            "Training Iteration 2469, Loss: 3.145162582397461\n",
            "Training Iteration 2470, Loss: 5.088811874389648\n",
            "Training Iteration 2471, Loss: 3.715688705444336\n",
            "Training Iteration 2472, Loss: 4.341681003570557\n",
            "Training Iteration 2473, Loss: 5.1564178466796875\n",
            "Training Iteration 2474, Loss: 2.4907796382904053\n",
            "Training Iteration 2475, Loss: 2.2421140670776367\n",
            "Training Iteration 2476, Loss: 4.239446640014648\n",
            "Training Iteration 2477, Loss: 3.8695669174194336\n",
            "Training Iteration 2478, Loss: 4.156026840209961\n",
            "Training Iteration 2479, Loss: 4.371023654937744\n",
            "Training Iteration 2480, Loss: 3.6076929569244385\n",
            "Training Iteration 2481, Loss: 4.756277561187744\n",
            "Training Iteration 2482, Loss: 5.920975685119629\n",
            "Training Iteration 2483, Loss: 5.2428460121154785\n",
            "Training Iteration 2484, Loss: 4.875487804412842\n",
            "Training Iteration 2485, Loss: 3.06952166557312\n",
            "Training Iteration 2486, Loss: 5.8491902351379395\n",
            "Training Iteration 2487, Loss: 1.8106862306594849\n",
            "Training Iteration 2488, Loss: 6.137528419494629\n",
            "Training Iteration 2489, Loss: 2.4569523334503174\n",
            "Training Iteration 2490, Loss: 3.6270430088043213\n",
            "Training Iteration 2491, Loss: 3.9685537815093994\n",
            "Training Iteration 2492, Loss: 2.2415733337402344\n",
            "Training Iteration 2493, Loss: 3.1982052326202393\n",
            "Training Iteration 2494, Loss: 3.7566795349121094\n",
            "Training Iteration 2495, Loss: 5.38309907913208\n",
            "Training Iteration 2496, Loss: 7.302287578582764\n",
            "Training Iteration 2497, Loss: 5.299935340881348\n",
            "Training Iteration 2498, Loss: 6.529268741607666\n",
            "Training Iteration 2499, Loss: 4.798837661743164\n",
            "Training Iteration 2500, Loss: 3.661473274230957\n",
            "Training Iteration 2501, Loss: 5.535836219787598\n",
            "Training Iteration 2502, Loss: 5.89601993560791\n",
            "Training Iteration 2503, Loss: 3.7761270999908447\n",
            "Training Iteration 2504, Loss: 5.704743385314941\n",
            "Training Iteration 2505, Loss: 5.732145309448242\n",
            "Training Iteration 2506, Loss: 5.065833568572998\n",
            "Training Iteration 2507, Loss: 1.5542800426483154\n",
            "Training Iteration 2508, Loss: 2.730287551879883\n",
            "Training Iteration 2509, Loss: 4.928333759307861\n",
            "Training Iteration 2510, Loss: 2.0396780967712402\n",
            "Training Iteration 2511, Loss: 7.552979469299316\n",
            "Training Iteration 2512, Loss: 7.612616539001465\n",
            "Training Iteration 2513, Loss: 6.4227824211120605\n",
            "Training Iteration 2514, Loss: 4.837494850158691\n",
            "Training Iteration 2515, Loss: 6.031796932220459\n",
            "Training Iteration 2516, Loss: 5.16356086730957\n",
            "Training Iteration 2517, Loss: 2.865738868713379\n",
            "Training Iteration 2518, Loss: 3.9936866760253906\n",
            "Training Iteration 2519, Loss: 3.632401943206787\n",
            "Training Iteration 2520, Loss: 2.5087201595306396\n",
            "Training Iteration 2521, Loss: 5.426671028137207\n",
            "Training Iteration 2522, Loss: 4.134050369262695\n",
            "Training Iteration 2523, Loss: 6.691287994384766\n",
            "Training Iteration 2524, Loss: 7.008904933929443\n",
            "Training Iteration 2525, Loss: 6.828642845153809\n",
            "Training Iteration 2526, Loss: 3.197625160217285\n",
            "Training Iteration 2527, Loss: 2.4322643280029297\n",
            "Training Iteration 2528, Loss: 8.831169128417969\n",
            "Training Iteration 2529, Loss: 8.08648681640625\n",
            "Training Iteration 2530, Loss: 7.922514915466309\n",
            "Training Iteration 2531, Loss: 8.799680709838867\n",
            "Training Iteration 2532, Loss: 6.401548385620117\n",
            "Training Iteration 2533, Loss: 7.916862487792969\n",
            "Training Iteration 2534, Loss: 5.211987495422363\n",
            "Training Iteration 2535, Loss: 2.5843071937561035\n",
            "Training Iteration 2536, Loss: 4.462538242340088\n",
            "Training Iteration 2537, Loss: 9.369954109191895\n",
            "Training Iteration 2538, Loss: 8.333492279052734\n",
            "Training Iteration 2539, Loss: 14.661659240722656\n",
            "Training Iteration 2540, Loss: 4.731623649597168\n",
            "Training Iteration 2541, Loss: 4.575461387634277\n",
            "Training Iteration 2542, Loss: 5.213274002075195\n",
            "Training Iteration 2543, Loss: 4.725706577301025\n",
            "Training Iteration 2544, Loss: 2.8005359172821045\n",
            "Training Iteration 2545, Loss: 4.039039134979248\n",
            "Training Iteration 2546, Loss: 7.284084320068359\n",
            "Training Iteration 2547, Loss: 6.223556041717529\n",
            "Training Iteration 2548, Loss: 6.105962753295898\n",
            "Training Iteration 2549, Loss: 5.518516540527344\n",
            "Training Iteration 2550, Loss: 3.638014554977417\n",
            "Training Iteration 2551, Loss: 10.79692554473877\n",
            "Training Iteration 2552, Loss: 7.365358352661133\n",
            "Training Iteration 2553, Loss: 9.153030395507812\n",
            "Training Iteration 2554, Loss: 6.956331729888916\n",
            "Training Iteration 2555, Loss: 4.8363189697265625\n",
            "Training Iteration 2556, Loss: 4.8528947830200195\n",
            "Training Iteration 2557, Loss: 3.2247416973114014\n",
            "Training Iteration 2558, Loss: 3.4373228549957275\n",
            "Training Iteration 2559, Loss: 4.011748313903809\n",
            "Training Iteration 2560, Loss: 2.537468194961548\n",
            "Training Iteration 2561, Loss: 5.344813823699951\n",
            "Training Iteration 2562, Loss: 7.888280868530273\n",
            "Training Iteration 2563, Loss: 5.4035491943359375\n",
            "Training Iteration 2564, Loss: 6.553913593292236\n",
            "Training Iteration 2565, Loss: 2.923792600631714\n",
            "Training Iteration 2566, Loss: 1.6390975713729858\n",
            "Training Iteration 2567, Loss: 0.8291618227958679\n",
            "Training Iteration 2568, Loss: 5.586323261260986\n",
            "Training Iteration 2569, Loss: 5.552743911743164\n",
            "Training Iteration 2570, Loss: 3.3594419956207275\n",
            "Training Iteration 2571, Loss: 4.084263324737549\n",
            "Training Iteration 2572, Loss: 4.930954456329346\n",
            "Training Iteration 2573, Loss: 3.573315382003784\n",
            "Training Iteration 2574, Loss: 3.633419990539551\n",
            "Training Iteration 2575, Loss: 2.1765360832214355\n",
            "Training Iteration 2576, Loss: 6.949032306671143\n",
            "Training Iteration 2577, Loss: 2.3314309120178223\n",
            "Training Iteration 2578, Loss: 2.1265957355499268\n",
            "Training Iteration 2579, Loss: 5.004344463348389\n",
            "Training Iteration 2580, Loss: 5.47623348236084\n",
            "Training Iteration 2581, Loss: 4.340002536773682\n",
            "Training Iteration 2582, Loss: 4.036564826965332\n",
            "Training Iteration 2583, Loss: 3.785510540008545\n",
            "Training Iteration 2584, Loss: 5.400050163269043\n",
            "Training Iteration 2585, Loss: 3.581437587738037\n",
            "Training Iteration 2586, Loss: 4.203251361846924\n",
            "Training Iteration 2587, Loss: 6.2340989112854\n",
            "Training Iteration 2588, Loss: 4.727414131164551\n",
            "Training Iteration 2589, Loss: 2.932851552963257\n",
            "Training Iteration 2590, Loss: 4.447193145751953\n",
            "Training Iteration 2591, Loss: 3.4277384281158447\n",
            "Training Iteration 2592, Loss: 4.090254783630371\n",
            "Training Iteration 2593, Loss: 4.426539421081543\n",
            "Training Iteration 2594, Loss: 3.6176557540893555\n",
            "Training Iteration 2595, Loss: 5.073909282684326\n",
            "Training Iteration 2596, Loss: 7.446212291717529\n",
            "Training Iteration 2597, Loss: 4.428605556488037\n",
            "Training Iteration 2598, Loss: 3.629755973815918\n",
            "Training Iteration 2599, Loss: 4.998056411743164\n",
            "Training Iteration 2600, Loss: 4.558737277984619\n",
            "Training Iteration 2601, Loss: 3.170895576477051\n",
            "Training Iteration 2602, Loss: 7.863570213317871\n",
            "Training Iteration 2603, Loss: 3.423337459564209\n",
            "Training Iteration 2604, Loss: 3.8211092948913574\n",
            "Training Iteration 2605, Loss: 5.096063613891602\n",
            "Training Iteration 2606, Loss: 6.251022815704346\n",
            "Training Iteration 2607, Loss: 4.295548439025879\n",
            "Training Iteration 2608, Loss: 10.591166496276855\n",
            "Training Iteration 2609, Loss: 3.4308245182037354\n",
            "Training Iteration 2610, Loss: 4.080594062805176\n",
            "Training Iteration 2611, Loss: 2.6721506118774414\n",
            "Training Iteration 2612, Loss: 4.608778476715088\n",
            "Training Iteration 2613, Loss: 6.045770645141602\n",
            "Training Iteration 2614, Loss: 2.9975385665893555\n",
            "Training Iteration 2615, Loss: 4.608600616455078\n",
            "Training Iteration 2616, Loss: 5.017801284790039\n",
            "Training Iteration 2617, Loss: 3.9185352325439453\n",
            "Training Iteration 2618, Loss: 2.4109246730804443\n",
            "Training Iteration 2619, Loss: 6.823489665985107\n",
            "Training Iteration 2620, Loss: 4.304991245269775\n",
            "Training Iteration 2621, Loss: 7.263930320739746\n",
            "Training Iteration 2622, Loss: 2.9853100776672363\n",
            "Training Iteration 2623, Loss: 3.2588539123535156\n",
            "Training Iteration 2624, Loss: 3.9234442710876465\n",
            "Training Iteration 2625, Loss: 3.9398393630981445\n",
            "Training Iteration 2626, Loss: 4.340374946594238\n",
            "Training Iteration 2627, Loss: 2.0299954414367676\n",
            "Training Iteration 2628, Loss: 5.392362117767334\n",
            "Training Iteration 2629, Loss: 2.7523193359375\n",
            "Training Iteration 2630, Loss: 4.391138076782227\n",
            "Training Iteration 2631, Loss: 5.112791061401367\n",
            "Training Iteration 2632, Loss: 4.914408206939697\n",
            "Training Iteration 2633, Loss: 4.90478515625\n",
            "Training Iteration 2634, Loss: 4.572427749633789\n",
            "Training Iteration 2635, Loss: 4.479686260223389\n",
            "Training Iteration 2636, Loss: 2.340770959854126\n",
            "Training Iteration 2637, Loss: 6.008194923400879\n",
            "Training Iteration 2638, Loss: 3.018207311630249\n",
            "Training Iteration 2639, Loss: 3.644786834716797\n",
            "Training Iteration 2640, Loss: 2.9960713386535645\n",
            "Training Iteration 2641, Loss: 4.125296592712402\n",
            "Training Iteration 2642, Loss: 3.6761274337768555\n",
            "Training Iteration 2643, Loss: 3.929220199584961\n",
            "Training Iteration 2644, Loss: 5.801456451416016\n",
            "Training Iteration 2645, Loss: 6.87056303024292\n",
            "Training Iteration 2646, Loss: 3.1472697257995605\n",
            "Training Iteration 2647, Loss: 11.928345680236816\n",
            "Training Iteration 2648, Loss: 7.126533031463623\n",
            "Training Iteration 2649, Loss: 3.862450361251831\n",
            "Training Iteration 2650, Loss: 2.9980945587158203\n",
            "Training Iteration 2651, Loss: 3.534742832183838\n",
            "Training Iteration 2652, Loss: 5.883073329925537\n",
            "Training Iteration 2653, Loss: 3.595484733581543\n",
            "Training Iteration 2654, Loss: 7.687436103820801\n",
            "Training Iteration 2655, Loss: 3.5455453395843506\n",
            "Training Iteration 2656, Loss: 2.906066656112671\n",
            "Training Iteration 2657, Loss: 6.551633358001709\n",
            "Training Iteration 2658, Loss: 2.990410804748535\n",
            "Training Iteration 2659, Loss: 1.8336542844772339\n",
            "Training Iteration 2660, Loss: 2.8137638568878174\n",
            "Training Iteration 2661, Loss: 4.09351110458374\n",
            "Training Iteration 2662, Loss: 5.293464660644531\n",
            "Training Iteration 2663, Loss: 3.0826072692871094\n",
            "Training Iteration 2664, Loss: 8.169490814208984\n",
            "Training Iteration 2665, Loss: 5.547956943511963\n",
            "Training Iteration 2666, Loss: 3.064288854598999\n",
            "Training Iteration 2667, Loss: 2.978817939758301\n",
            "Training Iteration 2668, Loss: 6.698973178863525\n",
            "Training Iteration 2669, Loss: 8.199612617492676\n",
            "Training Iteration 2670, Loss: 3.7701480388641357\n",
            "Training Iteration 2671, Loss: 3.2444958686828613\n",
            "Training Iteration 2672, Loss: 4.069700717926025\n",
            "Training Iteration 2673, Loss: 3.237466335296631\n",
            "Training Iteration 2674, Loss: 4.7596659660339355\n",
            "Training Iteration 2675, Loss: 4.814360618591309\n",
            "Training Iteration 2676, Loss: 3.0224127769470215\n",
            "Training Iteration 2677, Loss: 4.61604118347168\n",
            "Training Iteration 2678, Loss: 8.264073371887207\n",
            "Training Iteration 2679, Loss: 4.449512481689453\n",
            "Training Iteration 2680, Loss: 2.560335159301758\n",
            "Training Iteration 2681, Loss: 4.534127235412598\n",
            "Training Iteration 2682, Loss: 6.808024883270264\n",
            "Training Iteration 2683, Loss: 4.036096572875977\n",
            "Training Iteration 2684, Loss: 4.461114883422852\n",
            "Training Iteration 2685, Loss: 4.988643646240234\n",
            "Training Iteration 2686, Loss: 3.9016435146331787\n",
            "Training Iteration 2687, Loss: 4.099942207336426\n",
            "Training Iteration 2688, Loss: 6.189632415771484\n",
            "Training Iteration 2689, Loss: 3.617908000946045\n",
            "Training Iteration 2690, Loss: 4.569614410400391\n",
            "Training Iteration 2691, Loss: 3.5735507011413574\n",
            "Training Iteration 2692, Loss: 2.063166856765747\n",
            "Training Iteration 2693, Loss: 9.279664993286133\n",
            "Training Iteration 2694, Loss: 8.852855682373047\n",
            "Training Iteration 2695, Loss: 4.038370132446289\n",
            "Training Iteration 2696, Loss: 6.059330463409424\n",
            "Training Iteration 2697, Loss: 4.186463356018066\n",
            "Training Iteration 2698, Loss: 4.352628707885742\n",
            "Training Iteration 2699, Loss: 7.0044941902160645\n",
            "Training Iteration 2700, Loss: 6.683534622192383\n",
            "Training Iteration 2701, Loss: 3.6548004150390625\n",
            "Training Iteration 2702, Loss: 6.674404144287109\n",
            "Training Iteration 2703, Loss: 3.77614688873291\n",
            "Training Iteration 2704, Loss: 6.012652397155762\n",
            "Training Iteration 2705, Loss: 5.32114839553833\n",
            "Training Iteration 2706, Loss: 4.277604103088379\n",
            "Training Iteration 2707, Loss: 5.985561370849609\n",
            "Training Iteration 2708, Loss: 3.8076136112213135\n",
            "Training Iteration 2709, Loss: 6.596258163452148\n",
            "Training Iteration 2710, Loss: 3.8407859802246094\n",
            "Training Iteration 2711, Loss: 4.842919826507568\n",
            "Training Iteration 2712, Loss: 4.168394565582275\n",
            "Training Iteration 2713, Loss: 5.02639627456665\n",
            "Training Iteration 2714, Loss: 3.2824933528900146\n",
            "Training Iteration 2715, Loss: 7.986695289611816\n",
            "Training Iteration 2716, Loss: 4.251153469085693\n",
            "Training Iteration 2717, Loss: 7.763058662414551\n",
            "Training Iteration 2718, Loss: 7.275386810302734\n",
            "Training Iteration 2719, Loss: 4.004510402679443\n",
            "Training Iteration 2720, Loss: 2.007993459701538\n",
            "Training Iteration 2721, Loss: 3.400627613067627\n",
            "Training Iteration 2722, Loss: 3.535942792892456\n",
            "Training Iteration 2723, Loss: 3.8335630893707275\n",
            "Training Iteration 2724, Loss: 1.4588418006896973\n",
            "Training Iteration 2725, Loss: 3.6158714294433594\n",
            "Training Iteration 2726, Loss: 1.9623974561691284\n",
            "Training Iteration 2727, Loss: 2.095568895339966\n",
            "Training Iteration 2728, Loss: 3.893056631088257\n",
            "Training Iteration 2729, Loss: 3.626692056655884\n",
            "Training Iteration 2730, Loss: 6.332054138183594\n",
            "Training Iteration 2731, Loss: 2.6523220539093018\n",
            "Training Iteration 2732, Loss: 3.1155319213867188\n",
            "Training Iteration 2733, Loss: 2.7590138912200928\n",
            "Training Iteration 2734, Loss: 2.675337791442871\n",
            "Training Iteration 2735, Loss: 5.192671775817871\n",
            "Training Iteration 2736, Loss: 2.3064091205596924\n",
            "Training Iteration 2737, Loss: 3.9579315185546875\n",
            "Training Iteration 2738, Loss: 4.772701263427734\n",
            "Training Iteration 2739, Loss: 2.9420721530914307\n",
            "Training Iteration 2740, Loss: 3.4937918186187744\n",
            "Training Iteration 2741, Loss: 7.357852935791016\n",
            "Training Iteration 2742, Loss: 6.033623695373535\n",
            "Training Iteration 2743, Loss: 5.871947765350342\n",
            "Training Iteration 2744, Loss: 5.146635055541992\n",
            "Training Iteration 2745, Loss: 1.390048623085022\n",
            "Training Iteration 2746, Loss: 4.595371246337891\n",
            "Training Iteration 2747, Loss: 5.535219192504883\n",
            "Training Iteration 2748, Loss: 4.9429826736450195\n",
            "Training Iteration 2749, Loss: 4.549800395965576\n",
            "Training Iteration 2750, Loss: 2.4803988933563232\n",
            "Training Iteration 2751, Loss: 3.744152069091797\n",
            "Training Iteration 2752, Loss: 6.457472801208496\n",
            "Training Iteration 2753, Loss: 1.348581314086914\n",
            "Training Iteration 2754, Loss: 3.5722286701202393\n",
            "Training Iteration 2755, Loss: 2.162938117980957\n",
            "Training Iteration 2756, Loss: 5.3654398918151855\n",
            "Training Iteration 2757, Loss: 3.6738626956939697\n",
            "Training Iteration 2758, Loss: 4.585120677947998\n",
            "Training Iteration 2759, Loss: 5.556768894195557\n",
            "Training Iteration 2760, Loss: 2.9393532276153564\n",
            "Training Iteration 2761, Loss: 5.341968059539795\n",
            "Training Iteration 2762, Loss: 3.5199341773986816\n",
            "Training Iteration 2763, Loss: 2.9002652168273926\n",
            "Training Iteration 2764, Loss: 2.6436092853546143\n",
            "Training Iteration 2765, Loss: 4.2945237159729\n",
            "Training Iteration 2766, Loss: 4.798832416534424\n",
            "Training Iteration 2767, Loss: 5.771093368530273\n",
            "Training Iteration 2768, Loss: 4.9257330894470215\n",
            "Training Iteration 2769, Loss: 5.812332630157471\n",
            "Training Iteration 2770, Loss: 4.66560173034668\n",
            "Training Iteration 2771, Loss: 4.34172248840332\n",
            "Training Iteration 2772, Loss: 5.458090782165527\n",
            "Training Iteration 2773, Loss: 9.125588417053223\n",
            "Training Iteration 2774, Loss: 3.9199891090393066\n",
            "Training Iteration 2775, Loss: 3.987816095352173\n",
            "Training Iteration 2776, Loss: 6.861631393432617\n",
            "Training Iteration 2777, Loss: 6.920331954956055\n",
            "Training Iteration 2778, Loss: 5.910279273986816\n",
            "Training Iteration 2779, Loss: 4.8793625831604\n",
            "Training Iteration 2780, Loss: 2.952742338180542\n",
            "Training Iteration 2781, Loss: 2.1672656536102295\n",
            "Training Iteration 2782, Loss: 4.524381637573242\n",
            "Training Iteration 2783, Loss: 2.776214599609375\n",
            "Training Iteration 2784, Loss: 2.580052137374878\n",
            "Training Iteration 2785, Loss: 3.0520546436309814\n",
            "Training Iteration 2786, Loss: 4.069530963897705\n",
            "Training Iteration 2787, Loss: 3.5283753871917725\n",
            "Training Iteration 2788, Loss: 2.4750914573669434\n",
            "Training Iteration 2789, Loss: 4.525366306304932\n",
            "Training Iteration 2790, Loss: 7.6346821784973145\n",
            "Training Iteration 2791, Loss: 6.568624019622803\n",
            "Training Iteration 2792, Loss: 6.099357604980469\n",
            "Training Iteration 2793, Loss: 2.6747894287109375\n",
            "Training Iteration 2794, Loss: 4.193612575531006\n",
            "Training Iteration 2795, Loss: 2.4824204444885254\n",
            "Training Iteration 2796, Loss: 3.5338146686553955\n",
            "Training Iteration 2797, Loss: 4.032681941986084\n",
            "Training Iteration 2798, Loss: 3.687058448791504\n",
            "Training Iteration 2799, Loss: 5.129988670349121\n",
            "Training Iteration 2800, Loss: 7.027491569519043\n",
            "Training Iteration 2801, Loss: 2.8613967895507812\n",
            "Training Iteration 2802, Loss: 4.342749118804932\n",
            "Training Iteration 2803, Loss: 0.8958640098571777\n",
            "Training Iteration 2804, Loss: 4.10442590713501\n",
            "Training Iteration 2805, Loss: 2.527291774749756\n",
            "Training Iteration 2806, Loss: 1.5590585470199585\n",
            "Training Iteration 2807, Loss: 2.7732737064361572\n",
            "Training Iteration 2808, Loss: 6.5924482345581055\n",
            "Training Iteration 2809, Loss: 2.668396472930908\n",
            "Training Iteration 2810, Loss: 5.640735149383545\n",
            "Training Iteration 2811, Loss: 6.423280715942383\n",
            "Training Iteration 2812, Loss: 4.107671737670898\n",
            "Training Iteration 2813, Loss: 6.701657295227051\n",
            "Training Iteration 2814, Loss: 7.671992301940918\n",
            "Training Iteration 2815, Loss: 5.867092132568359\n",
            "Training Iteration 2816, Loss: 2.6552748680114746\n",
            "Training Iteration 2817, Loss: 3.9608545303344727\n",
            "Training Iteration 2818, Loss: 2.772322177886963\n",
            "Training Iteration 2819, Loss: 4.223959922790527\n",
            "Training Iteration 2820, Loss: 2.9663610458374023\n",
            "Training Iteration 2821, Loss: 5.119408130645752\n",
            "Training Iteration 2822, Loss: 2.424265146255493\n",
            "Training Iteration 2823, Loss: 3.1694254875183105\n",
            "Training Iteration 2824, Loss: 4.308964729309082\n",
            "Training Iteration 2825, Loss: 3.7008612155914307\n",
            "Training Iteration 2826, Loss: 2.651951551437378\n",
            "Training Iteration 2827, Loss: 7.649177074432373\n",
            "Training Iteration 2828, Loss: 6.1139607429504395\n",
            "Training Iteration 2829, Loss: 8.731439590454102\n",
            "Training Iteration 2830, Loss: 7.35722541809082\n",
            "Training Iteration 2831, Loss: 2.2846860885620117\n",
            "Training Iteration 2832, Loss: 4.294742584228516\n",
            "Training Iteration 2833, Loss: 6.455808162689209\n",
            "Training Iteration 2834, Loss: 5.8350934982299805\n",
            "Training Iteration 2835, Loss: 2.515789747238159\n",
            "Training Iteration 2836, Loss: 3.0022404193878174\n",
            "Training Iteration 2837, Loss: 1.7252306938171387\n",
            "Training Iteration 2838, Loss: 5.398014068603516\n",
            "Training Iteration 2839, Loss: 5.003725051879883\n",
            "Training Iteration 2840, Loss: 4.061107635498047\n",
            "Training Iteration 2841, Loss: 3.5762758255004883\n",
            "Training Iteration 2842, Loss: 3.4272661209106445\n",
            "Training Iteration 2843, Loss: 3.597496271133423\n",
            "Training Iteration 2844, Loss: 7.2367262840271\n",
            "Training Iteration 2845, Loss: 2.37548828125\n",
            "Training Iteration 2846, Loss: 14.481147766113281\n",
            "Training Iteration 2847, Loss: 4.400684356689453\n",
            "Training Iteration 2848, Loss: 6.657161712646484\n",
            "Training Iteration 2849, Loss: 3.4532084465026855\n",
            "Training Iteration 2850, Loss: 3.180385112762451\n",
            "Training Iteration 2851, Loss: 6.879074573516846\n",
            "Training Iteration 2852, Loss: 4.851679801940918\n",
            "Training Iteration 2853, Loss: 6.043115615844727\n",
            "Training Iteration 2854, Loss: 4.235518455505371\n",
            "Training Iteration 2855, Loss: 3.6378111839294434\n",
            "Training Iteration 2856, Loss: 4.810242176055908\n",
            "Training Iteration 2857, Loss: 5.054190158843994\n",
            "Training Iteration 2858, Loss: 5.554668426513672\n",
            "Training Iteration 2859, Loss: 3.097414970397949\n",
            "Training Iteration 2860, Loss: 2.1030983924865723\n",
            "Training Iteration 2861, Loss: 6.41681432723999\n",
            "Training Iteration 2862, Loss: 4.322490692138672\n",
            "Training Iteration 2863, Loss: 3.168989658355713\n",
            "Training Iteration 2864, Loss: 3.8680148124694824\n",
            "Training Iteration 2865, Loss: 7.142495155334473\n",
            "Training Iteration 2866, Loss: 5.700567245483398\n",
            "Training Iteration 2867, Loss: 3.595334529876709\n",
            "Training Iteration 2868, Loss: 3.224794387817383\n",
            "Training Iteration 2869, Loss: 3.945634365081787\n",
            "Training Iteration 2870, Loss: 1.6542000770568848\n",
            "Training Iteration 2871, Loss: 3.9824771881103516\n",
            "Training Iteration 2872, Loss: 3.2178595066070557\n",
            "Training Iteration 2873, Loss: 3.4588096141815186\n",
            "Training Iteration 2874, Loss: 7.873330593109131\n",
            "Training Iteration 2875, Loss: 5.032129287719727\n",
            "Training Iteration 2876, Loss: 4.372153282165527\n",
            "Training Iteration 2877, Loss: 4.536084175109863\n",
            "Training Iteration 2878, Loss: 3.9443366527557373\n",
            "Training Iteration 2879, Loss: 3.952627658843994\n",
            "Training Iteration 2880, Loss: 4.1547956466674805\n",
            "Training Iteration 2881, Loss: 3.9266412258148193\n",
            "Training Iteration 2882, Loss: 4.366211891174316\n",
            "Training Iteration 2883, Loss: 3.4062907695770264\n",
            "Training Iteration 2884, Loss: 6.341483116149902\n",
            "Training Iteration 2885, Loss: 3.931486129760742\n",
            "Training Iteration 2886, Loss: 4.739562511444092\n",
            "Training Iteration 2887, Loss: 6.1995134353637695\n",
            "Training Iteration 2888, Loss: 5.142086029052734\n",
            "Training Iteration 2889, Loss: 4.924842834472656\n",
            "Training Iteration 2890, Loss: 2.6198673248291016\n",
            "Training Iteration 2891, Loss: 3.6074211597442627\n",
            "Training Iteration 2892, Loss: 3.0931992530822754\n",
            "Training Iteration 2893, Loss: 2.3231935501098633\n",
            "Training Iteration 2894, Loss: 4.995429039001465\n",
            "Training Iteration 2895, Loss: 4.691169738769531\n",
            "Training Iteration 2896, Loss: 3.5019783973693848\n",
            "Training Iteration 2897, Loss: 5.699990749359131\n",
            "Training Iteration 2898, Loss: 4.520724773406982\n",
            "Training Iteration 2899, Loss: 5.633784294128418\n",
            "Training Iteration 2900, Loss: 3.2720398902893066\n",
            "Training Iteration 2901, Loss: 3.542337656021118\n",
            "Training Iteration 2902, Loss: 4.133585453033447\n",
            "Training Iteration 2903, Loss: 1.2400175333023071\n",
            "Training Iteration 2904, Loss: 3.6844470500946045\n",
            "Training Iteration 2905, Loss: 3.7474706172943115\n",
            "Training Iteration 2906, Loss: 1.7687432765960693\n",
            "Training Iteration 2907, Loss: 4.3177170753479\n",
            "Training Iteration 2908, Loss: 5.015563011169434\n",
            "Training Iteration 2909, Loss: 8.206558227539062\n",
            "Training Iteration 2910, Loss: 3.676711082458496\n",
            "Training Iteration 2911, Loss: 3.4913086891174316\n",
            "Training Iteration 2912, Loss: 3.9601736068725586\n",
            "Training Iteration 2913, Loss: 7.454237937927246\n",
            "Training Iteration 2914, Loss: 5.371373176574707\n",
            "Training Iteration 2915, Loss: 5.873830795288086\n",
            "Training Iteration 2916, Loss: 4.0408549308776855\n",
            "Training Iteration 2917, Loss: 5.4162917137146\n",
            "Training Iteration 2918, Loss: 3.854313373565674\n",
            "Training Iteration 2919, Loss: 4.436293601989746\n",
            "Training Iteration 2920, Loss: 4.5179877281188965\n",
            "Training Iteration 2921, Loss: 3.0516953468322754\n",
            "Training Iteration 2922, Loss: 3.9009933471679688\n",
            "Training Iteration 2923, Loss: 3.9572484493255615\n",
            "Training Iteration 2924, Loss: 5.388293266296387\n",
            "Training Iteration 2925, Loss: 4.823365688323975\n",
            "Training Iteration 2926, Loss: 2.9171485900878906\n",
            "Training Iteration 2927, Loss: 9.717378616333008\n",
            "Training Iteration 2928, Loss: 3.985909938812256\n",
            "Training Iteration 2929, Loss: 3.282696485519409\n",
            "Training Iteration 2930, Loss: 2.4136714935302734\n",
            "Training Iteration 2931, Loss: 2.7355315685272217\n",
            "Training Iteration 2932, Loss: 1.9702553749084473\n",
            "Training Iteration 2933, Loss: 2.479116916656494\n",
            "Training Iteration 2934, Loss: 3.362778663635254\n",
            "Training Iteration 2935, Loss: 4.646228313446045\n",
            "Training Iteration 2936, Loss: 5.278704643249512\n",
            "Training Iteration 2937, Loss: 2.8116040229797363\n",
            "Training Iteration 2938, Loss: 3.6940622329711914\n",
            "Training Iteration 2939, Loss: 4.318131446838379\n",
            "Training Iteration 2940, Loss: 7.306070327758789\n",
            "Training Iteration 2941, Loss: 4.303247928619385\n",
            "Training Iteration 2942, Loss: 5.259651184082031\n",
            "Training Iteration 2943, Loss: 4.865030288696289\n",
            "Training Iteration 2944, Loss: 6.23092794418335\n",
            "Training Iteration 2945, Loss: 3.056285858154297\n",
            "Training Iteration 2946, Loss: 4.98181676864624\n",
            "Training Iteration 2947, Loss: 4.047987461090088\n",
            "Training Iteration 2948, Loss: 5.830672740936279\n",
            "Training Iteration 2949, Loss: 3.686966896057129\n",
            "Training Iteration 2950, Loss: 5.465555191040039\n",
            "Training Iteration 2951, Loss: 5.2115068435668945\n",
            "Training Iteration 2952, Loss: 4.271810054779053\n",
            "Training Iteration 2953, Loss: 5.182506084442139\n",
            "Training Iteration 2954, Loss: 6.1786065101623535\n",
            "Training Iteration 2955, Loss: 4.475424766540527\n",
            "Training Iteration 2956, Loss: 5.373308181762695\n",
            "Training Iteration 2957, Loss: 7.866119861602783\n",
            "Training Iteration 2958, Loss: 8.281192779541016\n",
            "Training Iteration 2959, Loss: 3.453286647796631\n",
            "Training Iteration 2960, Loss: 4.126928329467773\n",
            "Training Iteration 2961, Loss: 8.652175903320312\n",
            "Training Iteration 2962, Loss: 2.551717758178711\n",
            "Training Iteration 2963, Loss: 4.080461502075195\n",
            "Training Iteration 2964, Loss: 8.187477111816406\n",
            "Training Iteration 2965, Loss: 6.193848609924316\n",
            "Training Iteration 2966, Loss: 6.327823638916016\n",
            "Training Iteration 2967, Loss: 2.576552629470825\n",
            "Training Iteration 2968, Loss: 10.7423734664917\n",
            "Training Iteration 2969, Loss: 5.772271156311035\n",
            "Training Iteration 2970, Loss: 4.748110771179199\n",
            "Training Iteration 2971, Loss: 3.1027793884277344\n",
            "Training Iteration 2972, Loss: 2.6111483573913574\n",
            "Training Iteration 2973, Loss: 1.8050769567489624\n",
            "Training Iteration 2974, Loss: 3.946415662765503\n",
            "Training Iteration 2975, Loss: 4.830501556396484\n",
            "Training Iteration 2976, Loss: 5.4251604080200195\n",
            "Training Iteration 2977, Loss: 4.107196807861328\n",
            "Training Iteration 2978, Loss: 3.8780925273895264\n",
            "Training Iteration 2979, Loss: 3.4802234172821045\n",
            "Training Iteration 2980, Loss: 3.153831958770752\n",
            "Training Iteration 2981, Loss: 4.7291436195373535\n",
            "Training Iteration 2982, Loss: 5.272683143615723\n",
            "Training Iteration 2983, Loss: 3.74589204788208\n",
            "Training Iteration 2984, Loss: 4.731819152832031\n",
            "Training Iteration 2985, Loss: 1.9023500680923462\n",
            "Training Iteration 2986, Loss: 3.891089916229248\n",
            "Training Iteration 2987, Loss: 5.9748969078063965\n",
            "Training Iteration 2988, Loss: 3.5555219650268555\n",
            "Training Iteration 2989, Loss: 4.035604953765869\n",
            "Training Iteration 2990, Loss: 1.9916194677352905\n",
            "Training Iteration 2991, Loss: 2.9781241416931152\n",
            "Training Iteration 2992, Loss: 4.68947696685791\n",
            "Training Iteration 2993, Loss: 4.639288425445557\n",
            "Training Iteration 2994, Loss: 4.024788856506348\n",
            "Training Iteration 2995, Loss: 5.0212883949279785\n",
            "Training Iteration 2996, Loss: 4.17431640625\n",
            "Training Iteration 2997, Loss: 5.50148344039917\n",
            "Training Iteration 2998, Loss: 4.8446364402771\n",
            "Training Iteration 2999, Loss: 5.708336353302002\n",
            "Training Iteration 3000, Loss: 5.850467205047607\n",
            "Training Iteration 3001, Loss: 7.186120986938477\n",
            "Training Iteration 3002, Loss: 2.124398708343506\n",
            "Training Iteration 3003, Loss: 3.5803956985473633\n",
            "Training Iteration 3004, Loss: 3.9383490085601807\n",
            "Training Iteration 3005, Loss: 4.864016532897949\n",
            "Training Iteration 3006, Loss: 5.044764518737793\n",
            "Training Iteration 3007, Loss: 2.9095542430877686\n",
            "Training Iteration 3008, Loss: 2.535226345062256\n",
            "Training Iteration 3009, Loss: 2.9786763191223145\n",
            "Training Iteration 3010, Loss: 10.333568572998047\n",
            "Training Iteration 3011, Loss: 1.9833436012268066\n",
            "Training Iteration 3012, Loss: 5.77863073348999\n",
            "Training Iteration 3013, Loss: 4.039788722991943\n",
            "Training Iteration 3014, Loss: 4.131069660186768\n",
            "Training Iteration 3015, Loss: 3.5529885292053223\n",
            "Training Iteration 3016, Loss: 5.118169784545898\n",
            "Training Iteration 3017, Loss: 6.152400970458984\n",
            "Training Iteration 3018, Loss: 7.950121879577637\n",
            "Training Iteration 3019, Loss: 4.285760402679443\n",
            "Training Iteration 3020, Loss: 5.059659481048584\n",
            "Training Iteration 3021, Loss: 4.19310998916626\n",
            "Training Iteration 3022, Loss: 3.2959518432617188\n",
            "Training Iteration 3023, Loss: 5.5786848068237305\n",
            "Training Iteration 3024, Loss: 3.898805618286133\n",
            "Training Iteration 3025, Loss: 4.63885498046875\n",
            "Training Iteration 3026, Loss: 2.845085620880127\n",
            "Training Iteration 3027, Loss: 5.656989574432373\n",
            "Training Iteration 3028, Loss: 1.6361370086669922\n",
            "Training Iteration 3029, Loss: 4.884588241577148\n",
            "Training Iteration 3030, Loss: 4.495458602905273\n",
            "Training Iteration 3031, Loss: 2.3318495750427246\n",
            "Training Iteration 3032, Loss: 5.868988513946533\n",
            "Training Iteration 3033, Loss: 2.113316059112549\n",
            "Training Iteration 3034, Loss: 4.736386299133301\n",
            "Training Iteration 3035, Loss: 5.004519939422607\n",
            "Training Iteration 3036, Loss: 4.8267502784729\n",
            "Training Iteration 3037, Loss: 5.622621059417725\n",
            "Training Iteration 3038, Loss: 2.9264016151428223\n",
            "Training Iteration 3039, Loss: 4.944886207580566\n",
            "Training Iteration 3040, Loss: 4.614409446716309\n",
            "Training Iteration 3041, Loss: 3.2413952350616455\n",
            "Training Iteration 3042, Loss: 4.85932731628418\n",
            "Training Iteration 3043, Loss: 1.4706801176071167\n",
            "Training Iteration 3044, Loss: 6.049041748046875\n",
            "Training Iteration 3045, Loss: 3.009120225906372\n",
            "Training Iteration 3046, Loss: 4.9766669273376465\n",
            "Training Iteration 3047, Loss: 4.679637432098389\n",
            "Training Iteration 3048, Loss: 3.617151975631714\n",
            "Training Iteration 3049, Loss: 4.465677261352539\n",
            "Training Iteration 3050, Loss: 4.713631629943848\n",
            "Training Iteration 3051, Loss: 4.0348358154296875\n",
            "Training Iteration 3052, Loss: 4.158123970031738\n",
            "Training Iteration 3053, Loss: 6.756631851196289\n",
            "Training Iteration 3054, Loss: 4.735001087188721\n",
            "Training Iteration 3055, Loss: 2.9442014694213867\n",
            "Training Iteration 3056, Loss: 2.09417462348938\n",
            "Training Iteration 3057, Loss: 5.157230377197266\n",
            "Training Iteration 3058, Loss: 5.921996593475342\n",
            "Training Iteration 3059, Loss: 2.08219051361084\n",
            "Training Iteration 3060, Loss: 3.5351221561431885\n",
            "Training Iteration 3061, Loss: 3.8241524696350098\n",
            "Training Iteration 3062, Loss: 5.682333946228027\n",
            "Training Iteration 3063, Loss: 5.4018940925598145\n",
            "Training Iteration 3064, Loss: 2.137556791305542\n",
            "Training Iteration 3065, Loss: 6.255448341369629\n",
            "Training Iteration 3066, Loss: 2.166635036468506\n",
            "Training Iteration 3067, Loss: 4.257251739501953\n",
            "Training Iteration 3068, Loss: 3.5102458000183105\n",
            "Training Iteration 3069, Loss: 5.218245983123779\n",
            "Training Iteration 3070, Loss: 4.4949140548706055\n",
            "Training Iteration 3071, Loss: 4.276634693145752\n",
            "Training Iteration 3072, Loss: 1.514841914176941\n",
            "Training Iteration 3073, Loss: 4.176697254180908\n",
            "Training Iteration 3074, Loss: 2.384915590286255\n",
            "Training Iteration 3075, Loss: 1.3703850507736206\n",
            "Training Iteration 3076, Loss: 3.261763095855713\n",
            "Training Iteration 3077, Loss: 3.6355137825012207\n",
            "Training Iteration 3078, Loss: 5.465240955352783\n",
            "Training Iteration 3079, Loss: 2.495072364807129\n",
            "Training Iteration 3080, Loss: 4.390460014343262\n",
            "Training Iteration 3081, Loss: 1.5139973163604736\n",
            "Training Iteration 3082, Loss: 3.104038953781128\n",
            "Training Iteration 3083, Loss: 3.311600685119629\n",
            "Training Iteration 3084, Loss: 6.166600227355957\n",
            "Training Iteration 3085, Loss: 2.7455434799194336\n",
            "Training Iteration 3086, Loss: 5.090564250946045\n",
            "Training Iteration 3087, Loss: 4.292944431304932\n",
            "Training Iteration 3088, Loss: 3.866816520690918\n",
            "Training Iteration 3089, Loss: 4.52862548828125\n",
            "Training Iteration 3090, Loss: 2.3637444972991943\n",
            "Training Iteration 3091, Loss: 3.1048707962036133\n",
            "Training Iteration 3092, Loss: 3.3275070190429688\n",
            "Training Iteration 3093, Loss: 3.8171939849853516\n",
            "Training Iteration 3094, Loss: 5.583642959594727\n",
            "Training Iteration 3095, Loss: 3.4579646587371826\n",
            "Training Iteration 3096, Loss: 3.1269631385803223\n",
            "Training Iteration 3097, Loss: 2.515401601791382\n",
            "Training Iteration 3098, Loss: 2.94535231590271\n",
            "Training Iteration 3099, Loss: 2.4270694255828857\n",
            "Training Iteration 3100, Loss: 6.154590129852295\n",
            "Training Iteration 3101, Loss: 5.3089165687561035\n",
            "Training Iteration 3102, Loss: 5.895650386810303\n",
            "Training Iteration 3103, Loss: 3.7610764503479004\n",
            "Training Iteration 3104, Loss: 2.491062641143799\n",
            "Training Iteration 3105, Loss: 4.395081996917725\n",
            "Training Iteration 3106, Loss: 4.98767614364624\n",
            "Training Iteration 3107, Loss: 5.4923481941223145\n",
            "Training Iteration 3108, Loss: 3.5852878093719482\n",
            "Training Iteration 3109, Loss: 3.658341407775879\n",
            "Training Iteration 3110, Loss: 5.914525985717773\n",
            "Training Iteration 3111, Loss: 3.454218626022339\n",
            "Training Iteration 3112, Loss: 3.9698753356933594\n",
            "Training Iteration 3113, Loss: 3.3549158573150635\n",
            "Training Iteration 3114, Loss: 4.311397075653076\n",
            "Training Iteration 3115, Loss: 4.195056438446045\n",
            "Training Iteration 3116, Loss: 2.172828197479248\n",
            "Training Iteration 3117, Loss: 7.036269664764404\n",
            "Training Iteration 3118, Loss: 3.18864107131958\n",
            "Training Iteration 3119, Loss: 3.169107675552368\n",
            "Training Iteration 3120, Loss: 3.9132208824157715\n",
            "Training Iteration 3121, Loss: 2.9327168464660645\n",
            "Training Iteration 3122, Loss: 2.8261618614196777\n",
            "Training Iteration 3123, Loss: 3.1722235679626465\n",
            "Training Iteration 3124, Loss: 2.9128806591033936\n",
            "Training Iteration 3125, Loss: 5.776895046234131\n",
            "Training Iteration 3126, Loss: 2.04097318649292\n",
            "Training Iteration 3127, Loss: 6.382562637329102\n",
            "Training Iteration 3128, Loss: 4.946059703826904\n",
            "Training Iteration 3129, Loss: 2.727426767349243\n",
            "Training Iteration 3130, Loss: 7.781150817871094\n",
            "Training Iteration 3131, Loss: 6.773824214935303\n",
            "Training Iteration 3132, Loss: 4.460765838623047\n",
            "Training Iteration 3133, Loss: 5.061916828155518\n",
            "Training Iteration 3134, Loss: 3.593907117843628\n",
            "Training Iteration 3135, Loss: 4.643195152282715\n",
            "Training Iteration 3136, Loss: 4.355526447296143\n",
            "Training Iteration 3137, Loss: 3.5358078479766846\n",
            "Training Iteration 3138, Loss: 4.63709020614624\n",
            "Training Iteration 3139, Loss: 4.9586920738220215\n",
            "Training Iteration 3140, Loss: 2.928413152694702\n",
            "Training Iteration 3141, Loss: 2.5996336936950684\n",
            "Training Iteration 3142, Loss: 5.042571544647217\n",
            "Training Iteration 3143, Loss: 4.88135290145874\n",
            "Training Iteration 3144, Loss: 2.819866418838501\n",
            "Training Iteration 3145, Loss: 4.032559394836426\n",
            "Training Iteration 3146, Loss: 5.63453483581543\n",
            "Training Iteration 3147, Loss: 3.5083727836608887\n",
            "Training Iteration 3148, Loss: 7.613007545471191\n",
            "Training Iteration 3149, Loss: 4.235958099365234\n",
            "Training Iteration 3150, Loss: 2.8523967266082764\n",
            "Training Iteration 3151, Loss: 4.012241363525391\n",
            "Training Iteration 3152, Loss: 7.0630974769592285\n",
            "Training Iteration 3153, Loss: 12.647001266479492\n",
            "Training Iteration 3154, Loss: 1.978391170501709\n",
            "Training Iteration 3155, Loss: 3.195659637451172\n",
            "Training Iteration 3156, Loss: 1.9906988143920898\n",
            "Training Iteration 3157, Loss: 1.395483374595642\n",
            "Training Iteration 3158, Loss: 3.7454261779785156\n",
            "Training Iteration 3159, Loss: 6.675498008728027\n",
            "Training Iteration 3160, Loss: 4.5660881996154785\n",
            "Training Iteration 3161, Loss: 6.754258632659912\n",
            "Training Iteration 3162, Loss: 8.871841430664062\n",
            "Training Iteration 3163, Loss: 5.534378528594971\n",
            "Training Iteration 3164, Loss: 3.236733913421631\n",
            "Training Iteration 3165, Loss: 2.2020456790924072\n",
            "Training Iteration 3166, Loss: 3.480398178100586\n",
            "Training Iteration 3167, Loss: 3.700223445892334\n",
            "Training Iteration 3168, Loss: 5.601128578186035\n",
            "Training Iteration 3169, Loss: 3.190091609954834\n",
            "Training Iteration 3170, Loss: 3.614546775817871\n",
            "Training Iteration 3171, Loss: 3.798898696899414\n",
            "Training Iteration 3172, Loss: 3.3638105392456055\n",
            "Training Iteration 3173, Loss: 5.176905632019043\n",
            "Training Iteration 3174, Loss: 4.606513977050781\n",
            "Training Iteration 3175, Loss: 4.391958713531494\n",
            "Training Iteration 3176, Loss: 5.5164475440979\n",
            "Training Iteration 3177, Loss: 4.085335731506348\n",
            "Training Iteration 3178, Loss: 4.925425052642822\n",
            "Training Iteration 3179, Loss: 2.662928819656372\n",
            "Training Iteration 3180, Loss: 3.466235399246216\n",
            "Training Iteration 3181, Loss: 5.625789642333984\n",
            "Training Iteration 3182, Loss: 3.252636432647705\n",
            "Training Iteration 3183, Loss: 5.249821662902832\n",
            "Training Iteration 3184, Loss: 2.3348159790039062\n",
            "Training Iteration 3185, Loss: 2.806008815765381\n",
            "Training Iteration 3186, Loss: 2.791701316833496\n",
            "Training Iteration 3187, Loss: 4.608495235443115\n",
            "Training Iteration 3188, Loss: 4.643476486206055\n",
            "Training Iteration 3189, Loss: 4.158883094787598\n",
            "Training Iteration 3190, Loss: 2.824216842651367\n",
            "Training Iteration 3191, Loss: 5.632356643676758\n",
            "Training Iteration 3192, Loss: 6.047234535217285\n",
            "Training Iteration 3193, Loss: 3.7639200687408447\n",
            "Training Iteration 3194, Loss: 3.438976287841797\n",
            "Training Iteration 3195, Loss: 4.731400489807129\n",
            "Training Iteration 3196, Loss: 5.112423896789551\n",
            "Training Iteration 3197, Loss: 3.31894588470459\n",
            "Training Iteration 3198, Loss: 6.026864528656006\n",
            "Training Iteration 3199, Loss: 3.1309893131256104\n",
            "Training Iteration 3200, Loss: 3.353182554244995\n",
            "Training Iteration 3201, Loss: 3.2513785362243652\n",
            "Training Iteration 3202, Loss: 3.605440139770508\n",
            "Training Iteration 3203, Loss: 3.6857142448425293\n",
            "Training Iteration 3204, Loss: 2.119734048843384\n",
            "Training Iteration 3205, Loss: 4.535205364227295\n",
            "Training Iteration 3206, Loss: 6.275286674499512\n",
            "Training Iteration 3207, Loss: 5.069209098815918\n",
            "Training Iteration 3208, Loss: 3.478957414627075\n",
            "Training Iteration 3209, Loss: 5.060352802276611\n",
            "Training Iteration 3210, Loss: 3.457900047302246\n",
            "Training Iteration 3211, Loss: 5.097392559051514\n",
            "Training Iteration 3212, Loss: 7.437289714813232\n",
            "Training Iteration 3213, Loss: 4.874068737030029\n",
            "Training Iteration 3214, Loss: 4.9951887130737305\n",
            "Training Iteration 3215, Loss: 6.96133279800415\n",
            "Training Iteration 3216, Loss: 4.246876239776611\n",
            "Training Iteration 3217, Loss: 4.5605058670043945\n",
            "Training Iteration 3218, Loss: 4.474867343902588\n",
            "Training Iteration 3219, Loss: 3.3641915321350098\n",
            "Training Iteration 3220, Loss: 6.129822254180908\n",
            "Training Iteration 3221, Loss: 7.706297397613525\n",
            "Training Iteration 3222, Loss: 3.8627848625183105\n",
            "Training Iteration 3223, Loss: 3.3563034534454346\n",
            "Training Iteration 3224, Loss: 3.515962600708008\n",
            "Training Iteration 3225, Loss: 10.148641586303711\n",
            "Training Iteration 3226, Loss: 4.863371849060059\n",
            "Training Iteration 3227, Loss: 4.148319721221924\n",
            "Training Iteration 3228, Loss: 8.396574020385742\n",
            "Training Iteration 3229, Loss: 3.254190683364868\n",
            "Training Iteration 3230, Loss: 4.239480972290039\n",
            "Training Iteration 3231, Loss: 5.70054817199707\n",
            "Training Iteration 3232, Loss: 4.233284950256348\n",
            "Training Iteration 3233, Loss: 2.9769301414489746\n",
            "Training Iteration 3234, Loss: 2.9492783546447754\n",
            "Training Iteration 3235, Loss: 3.2395007610321045\n",
            "Training Iteration 3236, Loss: 4.493023872375488\n",
            "Training Iteration 3237, Loss: 9.43323802947998\n",
            "Training Iteration 3238, Loss: 6.215104579925537\n",
            "Training Iteration 3239, Loss: 3.7202019691467285\n",
            "Training Iteration 3240, Loss: 3.5194661617279053\n",
            "Training Iteration 3241, Loss: 5.821963787078857\n",
            "Training Iteration 3242, Loss: 4.4298505783081055\n",
            "Training Iteration 3243, Loss: 7.026425838470459\n",
            "Training Iteration 3244, Loss: 4.062831878662109\n",
            "Training Iteration 3245, Loss: 5.829766273498535\n",
            "Training Iteration 3246, Loss: 7.266115665435791\n",
            "Training Iteration 3247, Loss: 6.626450538635254\n",
            "Training Iteration 3248, Loss: 3.657136917114258\n",
            "Training Iteration 3249, Loss: 2.345716714859009\n",
            "Training Iteration 3250, Loss: 4.87907600402832\n",
            "Training Iteration 3251, Loss: 3.242047071456909\n",
            "Training Iteration 3252, Loss: 4.608785629272461\n",
            "Training Iteration 3253, Loss: 4.803999900817871\n",
            "Training Iteration 3254, Loss: 4.612131118774414\n",
            "Training Iteration 3255, Loss: 5.941738128662109\n",
            "Training Iteration 3256, Loss: 3.6078078746795654\n",
            "Training Iteration 3257, Loss: 2.6693239212036133\n",
            "Training Iteration 3258, Loss: 5.420843601226807\n",
            "Training Iteration 3259, Loss: 4.250097751617432\n",
            "Training Iteration 3260, Loss: 1.962938666343689\n",
            "Training Iteration 3261, Loss: 6.187405586242676\n",
            "Training Iteration 3262, Loss: 5.0860161781311035\n",
            "Training Iteration 3263, Loss: 3.747490882873535\n",
            "Training Iteration 3264, Loss: 4.665980815887451\n",
            "Training Iteration 3265, Loss: 4.938371658325195\n",
            "Training Iteration 3266, Loss: 6.032738208770752\n",
            "Training Iteration 3267, Loss: 3.3070318698883057\n",
            "Training Iteration 3268, Loss: 3.182039260864258\n",
            "Training Iteration 3269, Loss: 3.962128162384033\n",
            "Training Iteration 3270, Loss: 3.6283576488494873\n",
            "Training Iteration 3271, Loss: 2.9049601554870605\n",
            "Training Iteration 3272, Loss: 5.402626037597656\n",
            "Training Iteration 3273, Loss: 3.6892080307006836\n",
            "Training Iteration 3274, Loss: 2.864441394805908\n",
            "Training Iteration 3275, Loss: 3.3664872646331787\n",
            "Training Iteration 3276, Loss: 4.605756759643555\n",
            "Training Iteration 3277, Loss: 4.251031875610352\n",
            "Training Iteration 3278, Loss: 3.515789031982422\n",
            "Training Iteration 3279, Loss: 3.9168426990509033\n",
            "Training Iteration 3280, Loss: 9.428252220153809\n",
            "Training Iteration 3281, Loss: 3.2608590126037598\n",
            "Training Iteration 3282, Loss: 3.0938448905944824\n",
            "Training Iteration 3283, Loss: 3.4272372722625732\n",
            "Training Iteration 3284, Loss: 6.1845502853393555\n",
            "Training Iteration 3285, Loss: 5.1427788734436035\n",
            "Training Iteration 3286, Loss: 6.427840709686279\n",
            "Training Iteration 3287, Loss: 3.463805675506592\n",
            "Training Iteration 3288, Loss: 3.174473762512207\n",
            "Training Iteration 3289, Loss: 3.3130416870117188\n",
            "Training Iteration 3290, Loss: 3.327737808227539\n",
            "Training Iteration 3291, Loss: 5.317184925079346\n",
            "Training Iteration 3292, Loss: 4.262734413146973\n",
            "Training Iteration 3293, Loss: 5.0725789070129395\n",
            "Training Iteration 3294, Loss: 4.990238666534424\n",
            "Training Iteration 3295, Loss: 4.040056228637695\n",
            "Training Iteration 3296, Loss: 2.9551339149475098\n",
            "Training Iteration 3297, Loss: 5.124579429626465\n",
            "Training Iteration 3298, Loss: 3.382802963256836\n",
            "Training Iteration 3299, Loss: 6.2531962394714355\n",
            "Training Iteration 3300, Loss: 6.754770755767822\n",
            "Training Iteration 3301, Loss: 3.677774667739868\n",
            "Training Iteration 3302, Loss: 3.2455713748931885\n",
            "Training Iteration 3303, Loss: 2.764523506164551\n",
            "Training Iteration 3304, Loss: 4.113408088684082\n",
            "Training Iteration 3305, Loss: 3.354858636856079\n",
            "Training Iteration 3306, Loss: 2.951676845550537\n",
            "Training Iteration 3307, Loss: 3.775203227996826\n",
            "Training Iteration 3308, Loss: 3.0320138931274414\n",
            "Training Iteration 3309, Loss: 1.5422321557998657\n",
            "Training Iteration 3310, Loss: 3.948556423187256\n",
            "Training Iteration 3311, Loss: 6.025084495544434\n",
            "Training Iteration 3312, Loss: 3.128632068634033\n",
            "Training Iteration 3313, Loss: 2.646542549133301\n",
            "Training Iteration 3314, Loss: 2.8416781425476074\n",
            "Training Iteration 3315, Loss: 3.7534618377685547\n",
            "Training Iteration 3316, Loss: 10.105283737182617\n",
            "Training Iteration 3317, Loss: 3.9265098571777344\n",
            "Training Iteration 3318, Loss: 2.6039390563964844\n",
            "Training Iteration 3319, Loss: 4.31787633895874\n",
            "Training Iteration 3320, Loss: 3.250466823577881\n",
            "Training Iteration 3321, Loss: 4.130051612854004\n",
            "Training Iteration 3322, Loss: 2.410623550415039\n",
            "Training Iteration 3323, Loss: 2.681813955307007\n",
            "Training Iteration 3324, Loss: 3.0365078449249268\n",
            "Training Iteration 3325, Loss: 3.449005365371704\n",
            "Training Iteration 3326, Loss: 6.861640930175781\n",
            "Training Iteration 3327, Loss: 3.7313568592071533\n",
            "Training Iteration 3328, Loss: 8.334834098815918\n",
            "Training Iteration 3329, Loss: 4.520816326141357\n",
            "Training Iteration 3330, Loss: 6.763203144073486\n",
            "Training Iteration 3331, Loss: 3.298426389694214\n",
            "Training Iteration 3332, Loss: 4.457241058349609\n",
            "Training Iteration 3333, Loss: 6.09397554397583\n",
            "Training Iteration 3334, Loss: 6.585264205932617\n",
            "Training Iteration 3335, Loss: 7.170825481414795\n",
            "Training Iteration 3336, Loss: 4.4670610427856445\n",
            "Training Iteration 3337, Loss: 3.2594451904296875\n",
            "Training Iteration 3338, Loss: 4.865814208984375\n",
            "Training Iteration 3339, Loss: 3.2167131900787354\n",
            "Training Iteration 3340, Loss: 2.940708637237549\n",
            "Training Iteration 3341, Loss: 2.5648655891418457\n",
            "Training Iteration 3342, Loss: 6.858087062835693\n",
            "Training Iteration 3343, Loss: 3.856384038925171\n",
            "Training Iteration 3344, Loss: 5.255383491516113\n",
            "Training Iteration 3345, Loss: 1.7888516187667847\n",
            "Training Iteration 3346, Loss: 2.4552981853485107\n",
            "Training Iteration 3347, Loss: 2.331841468811035\n",
            "Training Iteration 3348, Loss: 5.667847156524658\n",
            "Training Iteration 3349, Loss: 0.6554089188575745\n",
            "Training Iteration 3350, Loss: 3.9578073024749756\n",
            "Training Iteration 3351, Loss: 6.127729892730713\n",
            "Training Iteration 3352, Loss: 4.9110612869262695\n",
            "Training Iteration 3353, Loss: 2.798154830932617\n",
            "Training Iteration 3354, Loss: 4.139725685119629\n",
            "Training Iteration 3355, Loss: 5.687421798706055\n",
            "Training Iteration 3356, Loss: 4.120489120483398\n",
            "Training Iteration 3357, Loss: 3.0561838150024414\n",
            "Training Iteration 3358, Loss: 6.594693183898926\n",
            "Training Iteration 3359, Loss: 3.871213436126709\n",
            "Training Iteration 3360, Loss: 4.420997619628906\n",
            "Training Iteration 3361, Loss: 4.3666253089904785\n",
            "Training Iteration 3362, Loss: 5.282857418060303\n",
            "Training Iteration 3363, Loss: 3.2171788215637207\n",
            "Training Iteration 3364, Loss: 2.5712239742279053\n",
            "Training Iteration 3365, Loss: 3.380227565765381\n",
            "Training Iteration 3366, Loss: 4.565247058868408\n",
            "Training Iteration 3367, Loss: 3.631486415863037\n",
            "Training Iteration 3368, Loss: 3.7260665893554688\n",
            "Training Iteration 3369, Loss: 8.317060470581055\n",
            "Training Iteration 3370, Loss: 6.3407464027404785\n",
            "Training Iteration 3371, Loss: 3.76843523979187\n",
            "Training Iteration 3372, Loss: 7.0665998458862305\n",
            "Training Iteration 3373, Loss: 4.513706684112549\n",
            "Training Iteration 3374, Loss: 2.904827356338501\n",
            "Training Iteration 3375, Loss: 2.6193439960479736\n",
            "Training Iteration 3376, Loss: 3.2546277046203613\n",
            "Training Iteration 3377, Loss: 3.7733681201934814\n",
            "Training Iteration 3378, Loss: 5.724666595458984\n",
            "Training Iteration 3379, Loss: 4.859288215637207\n",
            "Training Iteration 3380, Loss: 4.209201335906982\n",
            "Training Iteration 3381, Loss: 2.6674516201019287\n",
            "Training Iteration 3382, Loss: 3.988724708557129\n",
            "Training Iteration 3383, Loss: 2.6979382038116455\n",
            "Training Iteration 3384, Loss: 5.600148677825928\n",
            "Training Iteration 3385, Loss: 6.656345844268799\n",
            "Training Iteration 3386, Loss: 5.726301193237305\n",
            "Training Iteration 3387, Loss: 4.147647380828857\n",
            "Training Iteration 3388, Loss: 9.718438148498535\n",
            "Training Iteration 3389, Loss: 5.787394046783447\n",
            "Training Iteration 3390, Loss: 5.7796831130981445\n",
            "Training Iteration 3391, Loss: 3.3162171840667725\n",
            "Training Iteration 3392, Loss: 1.7753345966339111\n",
            "Training Iteration 3393, Loss: 2.1746087074279785\n",
            "Training Iteration 3394, Loss: 3.493640899658203\n",
            "Training Iteration 3395, Loss: 2.695085287094116\n",
            "Training Iteration 3396, Loss: 4.532261848449707\n",
            "Training Iteration 3397, Loss: 2.3087151050567627\n",
            "Training Iteration 3398, Loss: 5.625220775604248\n",
            "Training Iteration 3399, Loss: 5.186764240264893\n",
            "Training Iteration 3400, Loss: 2.316157341003418\n",
            "Training Iteration 3401, Loss: 2.4797515869140625\n",
            "Training Iteration 3402, Loss: 2.937335252761841\n",
            "Training Iteration 3403, Loss: 2.2153208255767822\n",
            "Training Iteration 3404, Loss: 4.210822105407715\n",
            "Training Iteration 3405, Loss: 5.352709770202637\n",
            "Training Iteration 3406, Loss: 3.8893988132476807\n",
            "Training Iteration 3407, Loss: 3.241198778152466\n",
            "Training Iteration 3408, Loss: 5.124834060668945\n",
            "Training Iteration 3409, Loss: 4.273143768310547\n",
            "Training Iteration 3410, Loss: 4.792229175567627\n",
            "Training Iteration 3411, Loss: 4.827836990356445\n",
            "Training Iteration 3412, Loss: 5.2063727378845215\n",
            "Training Iteration 3413, Loss: 3.175879716873169\n",
            "Training Iteration 3414, Loss: 3.620850086212158\n",
            "Training Iteration 3415, Loss: 6.313759803771973\n",
            "Training Iteration 3416, Loss: 4.411226272583008\n",
            "Training Iteration 3417, Loss: 2.177483320236206\n",
            "Training Iteration 3418, Loss: 2.473670482635498\n",
            "Training Iteration 3419, Loss: 5.467098236083984\n",
            "Training Iteration 3420, Loss: 3.2397587299346924\n",
            "Training Iteration 3421, Loss: 6.8449625968933105\n",
            "Training Iteration 3422, Loss: 4.075986862182617\n",
            "Training Iteration 3423, Loss: 1.1358428001403809\n",
            "Training Iteration 3424, Loss: 7.195248603820801\n",
            "Training Iteration 3425, Loss: 3.2045156955718994\n",
            "Training Iteration 3426, Loss: 2.8799448013305664\n",
            "Training Iteration 3427, Loss: 6.448962211608887\n",
            "Training Iteration 3428, Loss: 5.336893081665039\n",
            "Training Iteration 3429, Loss: 2.6460647583007812\n",
            "Training Iteration 3430, Loss: 3.251455783843994\n",
            "Training Iteration 3431, Loss: 4.96624231338501\n",
            "Training Iteration 3432, Loss: 3.858752965927124\n",
            "Training Iteration 3433, Loss: 5.479998588562012\n",
            "Training Iteration 3434, Loss: 3.1300296783447266\n",
            "Training Iteration 3435, Loss: 4.3346428871154785\n",
            "Training Iteration 3436, Loss: 6.718280792236328\n",
            "Training Iteration 3437, Loss: 6.475882053375244\n",
            "Training Iteration 3438, Loss: 3.7666916847229004\n",
            "Training Iteration 3439, Loss: 8.607828140258789\n",
            "Training Iteration 3440, Loss: 3.5724599361419678\n",
            "Training Iteration 3441, Loss: 5.687746524810791\n",
            "Training Iteration 3442, Loss: 4.711586952209473\n",
            "Training Iteration 3443, Loss: 4.781101703643799\n",
            "Training Iteration 3444, Loss: 4.615142822265625\n",
            "Training Iteration 3445, Loss: 4.188201904296875\n",
            "Training Iteration 3446, Loss: 5.2796502113342285\n",
            "Training Iteration 3447, Loss: 4.3897857666015625\n",
            "Training Iteration 3448, Loss: 3.5772194862365723\n",
            "Training Iteration 3449, Loss: 3.36129093170166\n",
            "Training Iteration 3450, Loss: 3.6652917861938477\n",
            "Training Iteration 3451, Loss: 5.206188201904297\n",
            "Training Iteration 3452, Loss: 2.7479493618011475\n",
            "Training Iteration 3453, Loss: 2.86011004447937\n",
            "Training Iteration 3454, Loss: 5.898026943206787\n",
            "Training Iteration 3455, Loss: 5.991518020629883\n",
            "Training Iteration 3456, Loss: 3.274529457092285\n",
            "Training Iteration 3457, Loss: 4.062403202056885\n",
            "Training Iteration 3458, Loss: 4.980192184448242\n",
            "Training Iteration 3459, Loss: 3.850184202194214\n",
            "Training Iteration 3460, Loss: 4.03629207611084\n",
            "Training Iteration 3461, Loss: 4.221240043640137\n",
            "Training Iteration 3462, Loss: 4.427245616912842\n",
            "Training Iteration 3463, Loss: 5.065877437591553\n",
            "Training Iteration 3464, Loss: 2.665867805480957\n",
            "Training Iteration 3465, Loss: 5.738763809204102\n",
            "Training Iteration 3466, Loss: 4.158878326416016\n",
            "Training Iteration 3467, Loss: 2.094979763031006\n",
            "Training Iteration 3468, Loss: 4.048553943634033\n",
            "Training Iteration 3469, Loss: 4.982978343963623\n",
            "Training Iteration 3470, Loss: 4.87786865234375\n",
            "Training Iteration 3471, Loss: 6.738559722900391\n",
            "Training Iteration 3472, Loss: 2.730740785598755\n",
            "Training Iteration 3473, Loss: 2.9139814376831055\n",
            "Training Iteration 3474, Loss: 4.450275421142578\n",
            "Training Iteration 3475, Loss: 3.6702868938446045\n",
            "Training Iteration 3476, Loss: 2.3893532752990723\n",
            "Training Iteration 3477, Loss: 6.760960578918457\n",
            "Training Iteration 3478, Loss: 5.070980072021484\n",
            "Training Iteration 3479, Loss: 7.518509387969971\n",
            "Training Iteration 3480, Loss: 2.6222195625305176\n",
            "Training Iteration 3481, Loss: 5.115419387817383\n",
            "Training Iteration 3482, Loss: 6.881826400756836\n",
            "Training Iteration 3483, Loss: 3.8451406955718994\n",
            "Training Iteration 3484, Loss: 3.074096202850342\n",
            "Training Iteration 3485, Loss: 8.483458518981934\n",
            "Training Iteration 3486, Loss: 4.516473770141602\n",
            "Training Iteration 3487, Loss: 5.99549674987793\n",
            "Training Iteration 3488, Loss: 3.005174160003662\n",
            "Training Iteration 3489, Loss: 8.844430923461914\n",
            "Training Iteration 3490, Loss: 8.135357856750488\n",
            "Training Iteration 3491, Loss: 3.2508983612060547\n",
            "Training Iteration 3492, Loss: 8.26375675201416\n",
            "Training Iteration 3493, Loss: 4.158034324645996\n",
            "Training Iteration 3494, Loss: 3.2652838230133057\n",
            "Training Iteration 3495, Loss: 4.209031581878662\n",
            "Training Iteration 3496, Loss: 4.744322299957275\n",
            "Training Iteration 3497, Loss: 4.246369361877441\n",
            "Training Iteration 3498, Loss: 6.385148525238037\n",
            "Training Iteration 3499, Loss: 4.8828582763671875\n",
            "Training Iteration 3500, Loss: 3.063467025756836\n",
            "Training Iteration 3501, Loss: 4.022010803222656\n",
            "Training Iteration 3502, Loss: 4.249288082122803\n",
            "Training Iteration 3503, Loss: 4.191027641296387\n",
            "Training Iteration 3504, Loss: 3.742340564727783\n",
            "Training Iteration 3505, Loss: 3.4570088386535645\n",
            "Training Iteration 3506, Loss: 3.355548143386841\n",
            "Training Iteration 3507, Loss: 4.262507438659668\n",
            "Training Iteration 3508, Loss: 8.041765213012695\n",
            "Training Iteration 3509, Loss: 3.754549503326416\n",
            "Training Iteration 3510, Loss: 3.771059989929199\n",
            "Training Iteration 3511, Loss: 5.345224857330322\n",
            "Training Iteration 3512, Loss: 3.63836407661438\n",
            "Training Iteration 3513, Loss: 4.878282070159912\n",
            "Training Iteration 3514, Loss: 3.488494873046875\n",
            "Training Iteration 3515, Loss: 2.722899913787842\n",
            "Training Iteration 3516, Loss: 3.347686767578125\n",
            "Training Iteration 3517, Loss: 3.7350070476531982\n",
            "Training Iteration 3518, Loss: 8.10567569732666\n",
            "Training Iteration 3519, Loss: 4.884227752685547\n",
            "Training Iteration 3520, Loss: 3.715590715408325\n",
            "Training Iteration 3521, Loss: 1.5071231126785278\n",
            "Training Iteration 3522, Loss: 1.7946207523345947\n",
            "Training Iteration 3523, Loss: 4.274735450744629\n",
            "Training Iteration 3524, Loss: 3.3303122520446777\n",
            "Training Iteration 3525, Loss: 4.541215896606445\n",
            "Training Iteration 3526, Loss: 2.9497368335723877\n",
            "Training Iteration 3527, Loss: 2.318781852722168\n",
            "Training Iteration 3528, Loss: 7.034759521484375\n",
            "Training Iteration 3529, Loss: 6.184863090515137\n",
            "Training Iteration 3530, Loss: 4.330016136169434\n",
            "Training Iteration 3531, Loss: 3.4433765411376953\n",
            "Training Iteration 3532, Loss: 5.578301429748535\n",
            "Training Iteration 3533, Loss: 6.430642127990723\n",
            "Training Iteration 3534, Loss: 4.611474514007568\n",
            "Training Iteration 3535, Loss: 3.801147937774658\n",
            "Training Iteration 3536, Loss: 5.069854736328125\n",
            "Training Iteration 3537, Loss: 4.450157642364502\n",
            "Training Iteration 3538, Loss: 4.826495170593262\n",
            "Training Iteration 3539, Loss: 4.926758766174316\n",
            "Training Iteration 3540, Loss: 7.778003692626953\n",
            "Training Iteration 3541, Loss: 2.213632345199585\n",
            "Training Iteration 3542, Loss: 2.7768290042877197\n",
            "Training Iteration 3543, Loss: 3.3184988498687744\n",
            "Training Iteration 3544, Loss: 2.512255907058716\n",
            "Training Iteration 3545, Loss: 3.7550323009490967\n",
            "Training Iteration 3546, Loss: 3.006019115447998\n",
            "Training Iteration 3547, Loss: 2.532818078994751\n",
            "Training Iteration 3548, Loss: 2.2003333568573\n",
            "Training Iteration 3549, Loss: 3.948171377182007\n",
            "Training Iteration 3550, Loss: 3.009842872619629\n",
            "Training Iteration 3551, Loss: 4.598013877868652\n",
            "Training Iteration 3552, Loss: 3.813077688217163\n",
            "Training Iteration 3553, Loss: 5.351733684539795\n",
            "Training Iteration 3554, Loss: 2.8345775604248047\n",
            "Training Iteration 3555, Loss: 3.672321081161499\n",
            "Training Iteration 3556, Loss: 1.3310246467590332\n",
            "Training Iteration 3557, Loss: 6.402771949768066\n",
            "Training Iteration 3558, Loss: 4.3765668869018555\n",
            "Training Iteration 3559, Loss: 2.8743748664855957\n",
            "Training Iteration 3560, Loss: 3.0039854049682617\n",
            "Training Iteration 3561, Loss: 3.1443583965301514\n",
            "Training Iteration 3562, Loss: 6.392075538635254\n",
            "Training Iteration 3563, Loss: 6.6286797523498535\n",
            "Training Iteration 3564, Loss: 7.229907989501953\n",
            "Training Iteration 3565, Loss: 2.9377706050872803\n",
            "Training Iteration 3566, Loss: 3.191795587539673\n",
            "Training Iteration 3567, Loss: 4.008029937744141\n",
            "Training Iteration 3568, Loss: 6.002364158630371\n",
            "Training Iteration 3569, Loss: 3.5902886390686035\n",
            "Training Iteration 3570, Loss: 5.684419631958008\n",
            "Training Iteration 3571, Loss: 7.2484588623046875\n",
            "Training Iteration 3572, Loss: 4.065943241119385\n",
            "Training Iteration 3573, Loss: 8.676552772521973\n",
            "Training Iteration 3574, Loss: 5.632237911224365\n",
            "Training Iteration 3575, Loss: 3.855825662612915\n",
            "Training Iteration 3576, Loss: 8.039200782775879\n",
            "Training Iteration 3577, Loss: 6.697290420532227\n",
            "Training Iteration 3578, Loss: 3.905374526977539\n",
            "Training Iteration 3579, Loss: 3.9781646728515625\n",
            "Training Iteration 3580, Loss: 3.1978065967559814\n",
            "Training Iteration 3581, Loss: 2.392805337905884\n",
            "Training Iteration 3582, Loss: 3.109466552734375\n",
            "Training Iteration 3583, Loss: 5.160888195037842\n",
            "Training Iteration 3584, Loss: 3.9744725227355957\n",
            "Training Iteration 3585, Loss: 6.219908237457275\n",
            "Training Iteration 3586, Loss: 5.43560791015625\n",
            "Training Iteration 3587, Loss: 2.8084545135498047\n",
            "Training Iteration 3588, Loss: 2.9869656562805176\n",
            "Training Iteration 3589, Loss: 2.758305072784424\n",
            "Training Iteration 3590, Loss: 4.029115200042725\n",
            "Training Iteration 3591, Loss: 5.887730598449707\n",
            "Training Iteration 3592, Loss: 4.2068352699279785\n",
            "Training Iteration 3593, Loss: 5.370502471923828\n",
            "Training Iteration 3594, Loss: 2.530388355255127\n",
            "Training Iteration 3595, Loss: 4.5088419914245605\n",
            "Training Iteration 3596, Loss: 4.241302967071533\n",
            "Training Iteration 3597, Loss: 5.493703365325928\n",
            "Training Iteration 3598, Loss: 7.073644161224365\n",
            "Training Iteration 3599, Loss: 3.4208192825317383\n",
            "Training Iteration 3600, Loss: 4.5295233726501465\n",
            "Training Iteration 3601, Loss: 4.029786109924316\n",
            "Training Iteration 3602, Loss: 3.843642234802246\n",
            "Training Iteration 3603, Loss: 2.3156065940856934\n",
            "Training Iteration 3604, Loss: 4.607550621032715\n",
            "Training Iteration 3605, Loss: 3.7995314598083496\n",
            "Training Iteration 3606, Loss: 4.272848606109619\n",
            "Training Iteration 3607, Loss: 6.210635662078857\n",
            "Training Iteration 3608, Loss: 5.501485824584961\n",
            "Training Iteration 3609, Loss: 5.446159839630127\n",
            "Training Iteration 3610, Loss: 3.867551326751709\n",
            "Training Iteration 3611, Loss: 4.815025329589844\n",
            "Training Iteration 3612, Loss: 4.149681091308594\n",
            "Training Iteration 3613, Loss: 2.956559419631958\n",
            "Training Iteration 3614, Loss: 5.843121528625488\n",
            "Training Iteration 3615, Loss: 10.417003631591797\n",
            "Training Iteration 3616, Loss: 5.108736038208008\n",
            "Training Iteration 3617, Loss: 6.1241841316223145\n",
            "Training Iteration 3618, Loss: 9.500896453857422\n",
            "Training Iteration 3619, Loss: 6.181734561920166\n",
            "Training Iteration 3620, Loss: 4.462543487548828\n",
            "Training Iteration 3621, Loss: 7.244715690612793\n",
            "Training Iteration 3622, Loss: 3.406334400177002\n",
            "Training Iteration 3623, Loss: 4.395136833190918\n",
            "Training Iteration 3624, Loss: 3.388652801513672\n",
            "Training Iteration 3625, Loss: 5.087192535400391\n",
            "Training Iteration 3626, Loss: 1.9417684078216553\n",
            "Training Iteration 3627, Loss: 6.072012901306152\n",
            "Training Iteration 3628, Loss: 3.20497989654541\n",
            "Training Iteration 3629, Loss: 2.520437240600586\n",
            "Training Iteration 3630, Loss: 2.191596746444702\n",
            "Training Iteration 3631, Loss: 6.624134540557861\n",
            "Training Iteration 3632, Loss: 5.134295463562012\n",
            "Training Iteration 3633, Loss: 3.3551511764526367\n",
            "Training Iteration 3634, Loss: 2.52329158782959\n",
            "Training Iteration 3635, Loss: 3.9716603755950928\n",
            "Training Iteration 3636, Loss: 2.0890605449676514\n",
            "Training Iteration 3637, Loss: 4.800145149230957\n",
            "Training Iteration 3638, Loss: 3.4798858165740967\n",
            "Training Iteration 3639, Loss: 4.732623100280762\n",
            "Training Iteration 3640, Loss: 4.243322372436523\n",
            "Training Iteration 3641, Loss: 4.1919403076171875\n",
            "Training Iteration 3642, Loss: 3.457608222961426\n",
            "Training Iteration 3643, Loss: 3.3479833602905273\n",
            "Training Iteration 3644, Loss: 2.4722750186920166\n",
            "Training Iteration 3645, Loss: 6.741329193115234\n",
            "Training Iteration 3646, Loss: 4.028953552246094\n",
            "Training Iteration 3647, Loss: 2.13891863822937\n",
            "Training Iteration 3648, Loss: 6.826733589172363\n",
            "Training Iteration 3649, Loss: 2.2991321086883545\n",
            "Training Iteration 3650, Loss: 5.386758327484131\n",
            "Training Iteration 3651, Loss: 6.632808208465576\n",
            "Training Iteration 3652, Loss: 5.479903697967529\n",
            "Training Iteration 3653, Loss: 3.5333733558654785\n",
            "Training Iteration 3654, Loss: 4.0073018074035645\n",
            "Training Iteration 3655, Loss: 2.703984498977661\n",
            "Training Iteration 3656, Loss: 2.9887287616729736\n",
            "Training Iteration 3657, Loss: 5.018314361572266\n",
            "Training Iteration 3658, Loss: 6.355048179626465\n",
            "Training Iteration 3659, Loss: 5.2558441162109375\n",
            "Training Iteration 3660, Loss: 7.005619049072266\n",
            "Training Iteration 3661, Loss: 3.7126429080963135\n",
            "Training Iteration 3662, Loss: 3.399540901184082\n",
            "Training Iteration 3663, Loss: 6.159229278564453\n",
            "Training Iteration 3664, Loss: 3.6086933612823486\n",
            "Training Iteration 3665, Loss: 5.0559258460998535\n",
            "Training Iteration 3666, Loss: 6.101139068603516\n",
            "Training Iteration 3667, Loss: 2.2738759517669678\n",
            "Training Iteration 3668, Loss: 7.179709434509277\n",
            "Training Iteration 3669, Loss: 4.477574825286865\n",
            "Training Iteration 3670, Loss: 5.599818229675293\n",
            "Training Iteration 3671, Loss: 4.390588760375977\n",
            "Training Iteration 3672, Loss: 4.2781662940979\n",
            "Training Iteration 3673, Loss: 4.804243564605713\n",
            "Training Iteration 3674, Loss: 7.02436637878418\n",
            "Training Iteration 3675, Loss: 4.523958206176758\n",
            "Training Iteration 3676, Loss: 8.535269737243652\n",
            "Training Iteration 3677, Loss: 3.06697416305542\n",
            "Training Iteration 3678, Loss: 8.282599449157715\n",
            "Training Iteration 3679, Loss: 7.178413391113281\n",
            "Training Iteration 3680, Loss: 3.2924208641052246\n",
            "Training Iteration 3681, Loss: 9.067755699157715\n",
            "Training Iteration 3682, Loss: 5.8208489418029785\n",
            "Training Iteration 3683, Loss: 4.232702255249023\n",
            "Training Iteration 3684, Loss: 5.388677597045898\n",
            "Training Iteration 3685, Loss: 4.244695663452148\n",
            "Training Iteration 3686, Loss: 3.733889102935791\n",
            "Training Iteration 3687, Loss: 4.377551078796387\n",
            "Training Iteration 3688, Loss: 4.0598273277282715\n",
            "Training Iteration 3689, Loss: 3.8037567138671875\n",
            "Training Iteration 3690, Loss: 3.3164827823638916\n",
            "Training Iteration 3691, Loss: 3.4994425773620605\n",
            "Training Iteration 3692, Loss: 3.3022234439849854\n",
            "Training Iteration 3693, Loss: 3.462082624435425\n",
            "Training Iteration 3694, Loss: 3.5399467945098877\n",
            "Training Iteration 3695, Loss: 5.135354995727539\n",
            "Training Iteration 3696, Loss: 6.124200344085693\n",
            "Training Iteration 3697, Loss: 3.0945446491241455\n",
            "Training Iteration 3698, Loss: 5.034426689147949\n",
            "Training Iteration 3699, Loss: 2.8597421646118164\n",
            "Training Iteration 3700, Loss: 2.3699183464050293\n",
            "Training Iteration 3701, Loss: 3.7480673789978027\n",
            "Training Iteration 3702, Loss: 2.6609268188476562\n",
            "Training Iteration 3703, Loss: 1.9350930452346802\n",
            "Training Iteration 3704, Loss: 3.550264835357666\n",
            "Training Iteration 3705, Loss: 3.669024705886841\n",
            "Training Iteration 3706, Loss: 2.5913844108581543\n",
            "Training Iteration 3707, Loss: 5.914430618286133\n",
            "Training Iteration 3708, Loss: 3.226806640625\n",
            "Training Iteration 3709, Loss: 5.717127799987793\n",
            "Training Iteration 3710, Loss: 6.5366740226745605\n",
            "Training Iteration 3711, Loss: 4.4364118576049805\n",
            "Training Iteration 3712, Loss: 6.261993885040283\n",
            "Training Iteration 3713, Loss: 8.045578002929688\n",
            "Training Iteration 3714, Loss: 4.737179279327393\n",
            "Training Iteration 3715, Loss: 4.277745246887207\n",
            "Training Iteration 3716, Loss: 3.439152717590332\n",
            "Training Iteration 3717, Loss: 4.217258930206299\n",
            "Training Iteration 3718, Loss: 3.679899215698242\n",
            "Training Iteration 3719, Loss: 2.7916979789733887\n",
            "Training Iteration 3720, Loss: 2.0119054317474365\n",
            "Training Iteration 3721, Loss: 6.141678810119629\n",
            "Training Iteration 3722, Loss: 7.92555570602417\n",
            "Training Iteration 3723, Loss: 4.2807817459106445\n",
            "Training Iteration 3724, Loss: 3.0725040435791016\n",
            "Training Iteration 3725, Loss: 2.792605400085449\n",
            "Training Iteration 3726, Loss: 5.356478214263916\n",
            "Training Iteration 3727, Loss: 9.064157485961914\n",
            "Training Iteration 3728, Loss: 1.9241583347320557\n",
            "Training Iteration 3729, Loss: 2.9532623291015625\n",
            "Training Iteration 3730, Loss: 4.316669464111328\n",
            "Training Iteration 3731, Loss: 4.271389007568359\n",
            "Training Iteration 3732, Loss: 4.905261039733887\n",
            "Training Iteration 3733, Loss: 5.867539882659912\n",
            "Training Iteration 3734, Loss: 4.007711887359619\n",
            "Training Iteration 3735, Loss: 3.230363607406616\n",
            "Training Iteration 3736, Loss: 5.170372486114502\n",
            "Training Iteration 3737, Loss: 8.162871360778809\n",
            "Training Iteration 3738, Loss: 2.603516101837158\n",
            "Training Iteration 3739, Loss: 5.698084831237793\n",
            "Training Iteration 3740, Loss: 2.024184226989746\n",
            "Training Iteration 3741, Loss: 4.496185302734375\n",
            "Training Iteration 3742, Loss: 8.809240341186523\n",
            "Training Iteration 3743, Loss: 3.2213079929351807\n",
            "Training Iteration 3744, Loss: 3.9069290161132812\n",
            "Training Iteration 3745, Loss: 3.264357089996338\n",
            "Training Iteration 3746, Loss: 3.8431644439697266\n",
            "Training Iteration 3747, Loss: 3.048200845718384\n",
            "Training Iteration 3748, Loss: 3.458130359649658\n",
            "Training Iteration 3749, Loss: 5.80956506729126\n",
            "Training Iteration 3750, Loss: 4.1348876953125\n",
            "Training Iteration 3751, Loss: 3.821101188659668\n",
            "Training Iteration 3752, Loss: 3.2638485431671143\n",
            "Training Iteration 3753, Loss: 4.567537307739258\n",
            "Training Iteration 3754, Loss: 5.138649940490723\n",
            "Training Iteration 3755, Loss: 4.341831207275391\n",
            "Training Iteration 3756, Loss: 2.140151262283325\n",
            "Training Iteration 3757, Loss: 2.680294990539551\n",
            "Training Iteration 3758, Loss: 2.7139694690704346\n",
            "Training Iteration 3759, Loss: 6.723377227783203\n",
            "Training Iteration 3760, Loss: 4.437689781188965\n",
            "Training Iteration 3761, Loss: 6.009435653686523\n",
            "Training Iteration 3762, Loss: 3.3220670223236084\n",
            "Training Iteration 3763, Loss: 4.314258575439453\n",
            "Training Iteration 3764, Loss: 2.574273109436035\n",
            "Training Iteration 3765, Loss: 5.630644798278809\n",
            "Training Iteration 3766, Loss: 2.7423689365386963\n",
            "Training Iteration 3767, Loss: 5.850179672241211\n",
            "Training Iteration 3768, Loss: 4.751443862915039\n",
            "Training Iteration 3769, Loss: 5.226499557495117\n",
            "Training Iteration 3770, Loss: 4.476898670196533\n",
            "Training Iteration 3771, Loss: 2.9209959506988525\n",
            "Training Iteration 3772, Loss: 2.963776111602783\n",
            "Training Iteration 3773, Loss: 5.81418514251709\n",
            "Training Iteration 3774, Loss: 4.465528964996338\n",
            "Training Iteration 3775, Loss: 4.6714582443237305\n",
            "Training Iteration 3776, Loss: 4.989863395690918\n",
            "Training Iteration 3777, Loss: 5.884296417236328\n",
            "Training Iteration 3778, Loss: 5.04290246963501\n",
            "Training Iteration 3779, Loss: 2.698948860168457\n",
            "Training Iteration 3780, Loss: 5.318525314331055\n",
            "Training Iteration 3781, Loss: 3.5945751667022705\n",
            "Training Iteration 3782, Loss: 5.323736190795898\n",
            "Training Iteration 3783, Loss: 5.342504978179932\n",
            "Training Iteration 3784, Loss: 3.657792091369629\n",
            "Training Iteration 3785, Loss: 3.5421786308288574\n",
            "Training Iteration 3786, Loss: 5.945278644561768\n",
            "Training Iteration 3787, Loss: 5.243895053863525\n",
            "Training Iteration 3788, Loss: 7.011065483093262\n",
            "Training Iteration 3789, Loss: 5.917575836181641\n",
            "Training Iteration 3790, Loss: 4.815184116363525\n",
            "Training Iteration 3791, Loss: 5.029639720916748\n",
            "Training Iteration 3792, Loss: 4.961219310760498\n",
            "Training Iteration 3793, Loss: 5.867844581604004\n",
            "Training Iteration 3794, Loss: 7.9406023025512695\n",
            "Training Iteration 3795, Loss: 7.979763507843018\n",
            "Training Iteration 3796, Loss: 8.191226959228516\n",
            "Training Iteration 3797, Loss: 5.855250358581543\n",
            "Training Iteration 3798, Loss: 4.604486465454102\n",
            "Training Iteration 3799, Loss: 7.043429374694824\n",
            "Training Iteration 3800, Loss: 6.852509498596191\n",
            "Training Iteration 3801, Loss: 5.976953506469727\n",
            "Training Iteration 3802, Loss: 4.309129238128662\n",
            "Training Iteration 3803, Loss: 4.139489650726318\n",
            "Training Iteration 3804, Loss: 3.451524257659912\n",
            "Training Iteration 3805, Loss: 4.1526055335998535\n",
            "Training Iteration 3806, Loss: 3.7571563720703125\n",
            "Training Iteration 3807, Loss: 4.006893634796143\n",
            "Training Iteration 3808, Loss: 3.316777467727661\n",
            "Training Iteration 3809, Loss: 2.179515838623047\n",
            "Training Iteration 3810, Loss: 6.059343338012695\n",
            "Training Iteration 3811, Loss: 5.427466869354248\n",
            "Training Iteration 3812, Loss: 5.3954339027404785\n",
            "Training Iteration 3813, Loss: 4.794161796569824\n",
            "Training Iteration 3814, Loss: 3.9068593978881836\n",
            "Training Iteration 3815, Loss: 3.8472700119018555\n",
            "Training Iteration 3816, Loss: 2.436560869216919\n",
            "Training Iteration 3817, Loss: 11.638725280761719\n",
            "Training Iteration 3818, Loss: 4.71633243560791\n",
            "Training Iteration 3819, Loss: 5.036245346069336\n",
            "Training Iteration 3820, Loss: 2.495605230331421\n",
            "Training Iteration 3821, Loss: 3.3652000427246094\n",
            "Training Iteration 3822, Loss: 3.6330995559692383\n",
            "Training Iteration 3823, Loss: 8.744779586791992\n",
            "Training Iteration 3824, Loss: 8.221341133117676\n",
            "Training Iteration 3825, Loss: 3.3950135707855225\n",
            "Training Iteration 3826, Loss: 3.3384695053100586\n",
            "Training Iteration 3827, Loss: 3.7970662117004395\n",
            "Training Iteration 3828, Loss: 8.132494926452637\n",
            "Training Iteration 3829, Loss: 5.100886821746826\n",
            "Training Iteration 3830, Loss: 1.8185620307922363\n",
            "Training Iteration 3831, Loss: 4.546648979187012\n",
            "Training Iteration 3832, Loss: 4.0197601318359375\n",
            "Training Iteration 3833, Loss: 3.263115882873535\n",
            "Training Iteration 3834, Loss: 6.785324573516846\n",
            "Training Iteration 3835, Loss: 7.984975337982178\n",
            "Training Iteration 3836, Loss: 4.822760105133057\n",
            "Training Iteration 3837, Loss: 4.489502906799316\n",
            "Training Iteration 3838, Loss: 4.4943742752075195\n",
            "Training Iteration 3839, Loss: 2.79158878326416\n",
            "Training Iteration 3840, Loss: 4.575150966644287\n",
            "Training Iteration 3841, Loss: 4.753851890563965\n",
            "Training Iteration 3842, Loss: 4.909817695617676\n",
            "Training Iteration 3843, Loss: 4.637972354888916\n",
            "Training Iteration 3844, Loss: 2.981935977935791\n",
            "Training Iteration 3845, Loss: 3.2332334518432617\n",
            "Training Iteration 3846, Loss: 4.173266410827637\n",
            "Training Iteration 3847, Loss: 4.055233001708984\n",
            "Training Iteration 3848, Loss: 5.141571044921875\n",
            "Training Iteration 3849, Loss: 6.492455005645752\n",
            "Training Iteration 3850, Loss: 4.940771579742432\n",
            "Training Iteration 3851, Loss: 3.8371224403381348\n",
            "Training Iteration 3852, Loss: 3.7951884269714355\n",
            "Training Iteration 3853, Loss: 4.4053521156311035\n",
            "Training Iteration 3854, Loss: 3.899691104888916\n",
            "Training Iteration 3855, Loss: 1.7047653198242188\n",
            "Training Iteration 3856, Loss: 3.2832844257354736\n",
            "Training Iteration 3857, Loss: 2.774000644683838\n",
            "Training Iteration 3858, Loss: 3.0064258575439453\n",
            "Training Iteration 3859, Loss: 3.931793451309204\n",
            "Training Iteration 3860, Loss: 4.055959701538086\n",
            "Training Iteration 3861, Loss: 5.4120707511901855\n",
            "Training Iteration 3862, Loss: 4.955826282501221\n",
            "Training Iteration 3863, Loss: 2.1439802646636963\n",
            "Training Iteration 3864, Loss: 2.965461254119873\n",
            "Training Iteration 3865, Loss: 6.572958469390869\n",
            "Training Iteration 3866, Loss: 3.657097339630127\n",
            "Training Iteration 3867, Loss: 5.237009048461914\n",
            "Training Iteration 3868, Loss: 4.287714958190918\n",
            "Training Iteration 3869, Loss: 4.418734550476074\n",
            "Training Iteration 3870, Loss: 3.2514901161193848\n",
            "Training Iteration 3871, Loss: 8.101420402526855\n",
            "Training Iteration 3872, Loss: 3.276353120803833\n",
            "Training Iteration 3873, Loss: 1.6784957647323608\n",
            "Training Iteration 3874, Loss: 4.475502014160156\n",
            "Training Iteration 3875, Loss: 2.3777153491973877\n",
            "Training Iteration 3876, Loss: 4.127894401550293\n",
            "Training Iteration 3877, Loss: 1.5734050273895264\n",
            "Training Iteration 3878, Loss: 6.526787281036377\n",
            "Training Iteration 3879, Loss: 1.6371994018554688\n",
            "Training Iteration 3880, Loss: 7.3777079582214355\n",
            "Training Iteration 3881, Loss: 3.189197301864624\n",
            "Training Iteration 3882, Loss: 2.076826572418213\n",
            "Training Iteration 3883, Loss: 1.9609516859054565\n",
            "Training Iteration 3884, Loss: 5.473328113555908\n",
            "Training Iteration 3885, Loss: 5.335725784301758\n",
            "Training Iteration 3886, Loss: 4.1517815589904785\n",
            "Training Iteration 3887, Loss: 2.72226619720459\n",
            "Training Iteration 3888, Loss: 5.35610294342041\n",
            "Training Iteration 3889, Loss: 5.089136600494385\n",
            "Training Iteration 3890, Loss: 3.878613233566284\n",
            "Training Iteration 3891, Loss: 4.97088623046875\n",
            "Training Iteration 3892, Loss: 2.4235317707061768\n",
            "Training Iteration 3893, Loss: 3.353519916534424\n",
            "Training Iteration 3894, Loss: 5.729355335235596\n",
            "Training Iteration 3895, Loss: 2.617534875869751\n",
            "Training Iteration 3896, Loss: 8.399524688720703\n",
            "Training Iteration 3897, Loss: 5.052624225616455\n",
            "Training Iteration 3898, Loss: 3.1201982498168945\n",
            "Training Iteration 3899, Loss: 5.409533500671387\n",
            "Training Iteration 3900, Loss: 4.517496109008789\n",
            "Training Iteration 3901, Loss: 4.134974002838135\n",
            "Training Iteration 3902, Loss: 1.8431411981582642\n",
            "Training Iteration 3903, Loss: 4.689945220947266\n",
            "Training Iteration 3904, Loss: 5.2730913162231445\n",
            "Training Iteration 3905, Loss: 3.653122663497925\n",
            "Training Iteration 3906, Loss: 5.5836896896362305\n",
            "Training Iteration 3907, Loss: 3.96895694732666\n",
            "Training Iteration 3908, Loss: 3.9158592224121094\n",
            "Training Iteration 3909, Loss: 3.3554160594940186\n",
            "Training Iteration 3910, Loss: 6.724397659301758\n",
            "Training Iteration 3911, Loss: 6.022085666656494\n",
            "Training Iteration 3912, Loss: 8.829994201660156\n",
            "Training Iteration 3913, Loss: 5.136046409606934\n",
            "Training Iteration 3914, Loss: 3.850426435470581\n",
            "Training Iteration 3915, Loss: 3.734041213989258\n",
            "Training Iteration 3916, Loss: 4.915375709533691\n",
            "Training Iteration 3917, Loss: 5.224008560180664\n",
            "Training Iteration 3918, Loss: 1.9348859786987305\n",
            "Training Iteration 3919, Loss: 3.186835289001465\n",
            "Training Iteration 3920, Loss: 3.00944447517395\n",
            "Training Iteration 3921, Loss: 4.54383659362793\n",
            "Training Iteration 3922, Loss: 3.625027656555176\n",
            "Training Iteration 3923, Loss: 4.4504828453063965\n",
            "Training Iteration 3924, Loss: 2.07317852973938\n",
            "Training Iteration 3925, Loss: 2.481945514678955\n",
            "Training Iteration 3926, Loss: 4.650731563568115\n",
            "Training Iteration 3927, Loss: 2.8655550479888916\n",
            "Training Iteration 3928, Loss: 2.1477110385894775\n",
            "Training Iteration 3929, Loss: 2.3670499324798584\n",
            "Training Iteration 3930, Loss: 9.329658508300781\n",
            "Training Iteration 3931, Loss: 7.492339134216309\n",
            "Training Iteration 3932, Loss: 6.069370746612549\n",
            "Training Iteration 3933, Loss: 7.263392448425293\n",
            "Training Iteration 3934, Loss: 5.936110496520996\n",
            "Training Iteration 3935, Loss: 1.9444464445114136\n",
            "Training Iteration 3936, Loss: 2.5536208152770996\n",
            "Training Iteration 3937, Loss: 4.501446723937988\n",
            "Training Iteration 3938, Loss: 3.9734249114990234\n",
            "Training Iteration 3939, Loss: 1.9303877353668213\n",
            "Training Iteration 3940, Loss: 6.793118953704834\n",
            "Training Iteration 3941, Loss: 2.9561471939086914\n",
            "Training Iteration 3942, Loss: 5.377726078033447\n",
            "Training Iteration 3943, Loss: 3.2182419300079346\n",
            "Training Iteration 3944, Loss: 4.175739765167236\n",
            "Training Iteration 3945, Loss: 3.0497498512268066\n",
            "Training Iteration 3946, Loss: 6.431352615356445\n",
            "Training Iteration 3947, Loss: 6.04289436340332\n",
            "Training Iteration 3948, Loss: 3.0566964149475098\n",
            "Training Iteration 3949, Loss: 4.614590167999268\n",
            "Training Iteration 3950, Loss: 6.874309539794922\n",
            "Training Iteration 3951, Loss: 6.7058868408203125\n",
            "Training Iteration 3952, Loss: 3.6499719619750977\n",
            "Training Iteration 3953, Loss: 5.867010593414307\n",
            "Training Iteration 3954, Loss: 4.3692498207092285\n",
            "Training Iteration 3955, Loss: 3.1424150466918945\n",
            "Training Iteration 3956, Loss: 2.5253357887268066\n",
            "Training Iteration 3957, Loss: 4.439311981201172\n",
            "Training Iteration 3958, Loss: 3.8168246746063232\n",
            "Training Iteration 3959, Loss: 4.860929012298584\n",
            "Training Iteration 3960, Loss: 6.469104290008545\n",
            "Training Iteration 3961, Loss: 6.550321578979492\n",
            "Training Iteration 3962, Loss: 4.938569068908691\n",
            "Training Iteration 3963, Loss: 2.7436282634735107\n",
            "Training Iteration 3964, Loss: 3.8908090591430664\n",
            "Training Iteration 3965, Loss: 5.6349287033081055\n",
            "Training Iteration 3966, Loss: 4.1394944190979\n",
            "Training Iteration 3967, Loss: 2.399414300918579\n",
            "Training Iteration 3968, Loss: 7.716302871704102\n",
            "Training Iteration 3969, Loss: 4.484376907348633\n",
            "Training Iteration 3970, Loss: 5.074409484863281\n",
            "Training Iteration 3971, Loss: 6.163446426391602\n",
            "Training Iteration 3972, Loss: 6.116709232330322\n",
            "Training Iteration 3973, Loss: 4.133984088897705\n",
            "Training Iteration 3974, Loss: 4.364720821380615\n",
            "Training Iteration 3975, Loss: 5.661360740661621\n",
            "Training Iteration 3976, Loss: 2.855238914489746\n",
            "Training Iteration 3977, Loss: 4.325017929077148\n",
            "Training Iteration 3978, Loss: 6.3109450340271\n",
            "Training Iteration 3979, Loss: 4.006937026977539\n",
            "Training Iteration 3980, Loss: 2.925952672958374\n",
            "Training Iteration 3981, Loss: 5.068014621734619\n",
            "Training Iteration 3982, Loss: 6.680131435394287\n",
            "Training Iteration 3983, Loss: 4.751479148864746\n",
            "Training Iteration 3984, Loss: 6.5823774337768555\n",
            "Training Iteration 3985, Loss: 5.358977794647217\n",
            "Training Iteration 3986, Loss: 5.851898193359375\n",
            "Training Iteration 3987, Loss: 4.534083366394043\n",
            "Training Iteration 3988, Loss: 4.235150337219238\n",
            "Training Iteration 3989, Loss: 6.59684944152832\n",
            "Training Iteration 3990, Loss: 2.360205888748169\n",
            "Training Iteration 3991, Loss: 6.065253734588623\n",
            "Training Iteration 3992, Loss: 4.945084095001221\n",
            "Training Iteration 3993, Loss: 4.1652069091796875\n",
            "Training Iteration 3994, Loss: 4.408104419708252\n",
            "Training Iteration 3995, Loss: 6.031658172607422\n",
            "Training Iteration 3996, Loss: 8.410085678100586\n",
            "Training Iteration 3997, Loss: 3.987468719482422\n",
            "Training Iteration 3998, Loss: 2.938782215118408\n",
            "Training Iteration 3999, Loss: 4.568316459655762\n",
            "Training Iteration 4000, Loss: 4.498509407043457\n",
            "Training Iteration 4001, Loss: 6.842831134796143\n",
            "Training Iteration 4002, Loss: 4.293747425079346\n",
            "Training Iteration 4003, Loss: 3.042705535888672\n",
            "Training Iteration 4004, Loss: 1.2725507020950317\n",
            "Training Iteration 4005, Loss: 2.61772084236145\n",
            "Training Iteration 4006, Loss: 4.213926792144775\n",
            "Training Iteration 4007, Loss: 2.8346989154815674\n",
            "Training Iteration 4008, Loss: 2.7999377250671387\n",
            "Training Iteration 4009, Loss: 6.681065082550049\n",
            "Training Iteration 4010, Loss: 6.0585618019104\n",
            "Training Iteration 4011, Loss: 6.070849418640137\n",
            "Training Iteration 4012, Loss: 9.837102890014648\n",
            "Training Iteration 4013, Loss: 4.357083797454834\n",
            "Training Iteration 4014, Loss: 4.820291042327881\n",
            "Training Iteration 4015, Loss: 3.360358953475952\n",
            "Training Iteration 4016, Loss: 4.5231709480285645\n",
            "Training Iteration 4017, Loss: 3.522969961166382\n",
            "Training Iteration 4018, Loss: 2.505530834197998\n",
            "Training Iteration 4019, Loss: 4.518467426300049\n",
            "Training Iteration 4020, Loss: 5.438483238220215\n",
            "Training Iteration 4021, Loss: 6.372749328613281\n",
            "Training Iteration 4022, Loss: 3.8681936264038086\n",
            "Training Iteration 4023, Loss: 4.154221534729004\n",
            "Training Iteration 4024, Loss: 5.240113735198975\n",
            "Training Iteration 4025, Loss: 4.713223934173584\n",
            "Training Iteration 4026, Loss: 5.389870643615723\n",
            "Training Iteration 4027, Loss: 4.952133655548096\n",
            "Training Iteration 4028, Loss: 2.9840798377990723\n",
            "Training Iteration 4029, Loss: 5.579500675201416\n",
            "Training Iteration 4030, Loss: 5.820415496826172\n",
            "Training Iteration 4031, Loss: 6.017356872558594\n",
            "Training Iteration 4032, Loss: 2.874772071838379\n",
            "Training Iteration 4033, Loss: 2.8240435123443604\n",
            "Training Iteration 4034, Loss: 3.2418744564056396\n",
            "Training Iteration 4035, Loss: 4.939047336578369\n",
            "Training Iteration 4036, Loss: 5.792935848236084\n",
            "Training Iteration 4037, Loss: 5.481759071350098\n",
            "Training Iteration 4038, Loss: 8.317120552062988\n",
            "Training Iteration 4039, Loss: 3.930110454559326\n",
            "Training Iteration 4040, Loss: 2.1324989795684814\n",
            "Training Iteration 4041, Loss: 4.22114896774292\n",
            "Training Iteration 4042, Loss: 2.028052568435669\n",
            "Training Iteration 4043, Loss: 3.367687225341797\n",
            "Training Iteration 4044, Loss: 4.142022132873535\n",
            "Training Iteration 4045, Loss: 4.890065670013428\n",
            "Training Iteration 4046, Loss: 4.88681697845459\n",
            "Training Iteration 4047, Loss: 6.201478958129883\n",
            "Training Iteration 4048, Loss: 5.220920085906982\n",
            "Training Iteration 4049, Loss: 6.445723533630371\n",
            "Training Iteration 4050, Loss: 7.9784674644470215\n",
            "Training Iteration 4051, Loss: 5.62391996383667\n",
            "Training Iteration 4052, Loss: 3.4688799381256104\n",
            "Training Iteration 4053, Loss: 6.873941421508789\n",
            "Training Iteration 4054, Loss: 6.954259872436523\n",
            "Training Iteration 4055, Loss: 3.806244134902954\n",
            "Training Iteration 4056, Loss: 6.502514839172363\n",
            "Training Iteration 4057, Loss: 3.9465866088867188\n",
            "Training Iteration 4058, Loss: 4.2742228507995605\n",
            "Training Iteration 4059, Loss: 4.52800989151001\n",
            "Training Iteration 4060, Loss: 4.971756458282471\n",
            "Training Iteration 4061, Loss: 3.501777410507202\n",
            "Training Iteration 4062, Loss: 4.269245147705078\n",
            "Training Iteration 4063, Loss: 4.349567890167236\n",
            "Training Iteration 4064, Loss: 5.2375335693359375\n",
            "Training Iteration 4065, Loss: 1.604378342628479\n",
            "Training Iteration 4066, Loss: 2.0749685764312744\n",
            "Training Iteration 4067, Loss: 4.244803428649902\n",
            "Training Iteration 4068, Loss: 3.838704824447632\n",
            "Training Iteration 4069, Loss: 1.908378005027771\n",
            "Training Iteration 4070, Loss: 5.209443092346191\n",
            "Training Iteration 4071, Loss: 5.91517972946167\n",
            "Training Iteration 4072, Loss: 3.167987823486328\n",
            "Training Iteration 4073, Loss: 3.667782783508301\n",
            "Training Iteration 4074, Loss: 4.1826934814453125\n",
            "Training Iteration 4075, Loss: 1.987214207649231\n",
            "Training Iteration 4076, Loss: 3.8975119590759277\n",
            "Training Iteration 4077, Loss: 3.0891988277435303\n",
            "Training Iteration 4078, Loss: 4.085336685180664\n",
            "Training Iteration 4079, Loss: 3.4482667446136475\n",
            "Training Iteration 4080, Loss: 4.4790568351745605\n",
            "Training Iteration 4081, Loss: 3.3373923301696777\n",
            "Training Iteration 4082, Loss: 6.20521354675293\n",
            "Training Iteration 4083, Loss: 6.01161527633667\n",
            "Training Iteration 4084, Loss: 3.7392635345458984\n",
            "Training Iteration 4085, Loss: 5.840047836303711\n",
            "Training Iteration 4086, Loss: 4.545308589935303\n",
            "Training Iteration 4087, Loss: 6.6037492752075195\n",
            "Training Iteration 4088, Loss: 4.00939416885376\n",
            "Training Iteration 4089, Loss: 5.096430778503418\n",
            "Training Iteration 4090, Loss: 6.732684135437012\n",
            "Training Iteration 4091, Loss: 7.7230634689331055\n",
            "Training Iteration 4092, Loss: 5.176978588104248\n",
            "Training Iteration 4093, Loss: 2.1760313510894775\n",
            "Training Iteration 4094, Loss: 6.577877998352051\n",
            "Training Iteration 4095, Loss: 8.462700843811035\n",
            "Training Iteration 4096, Loss: 6.858808517456055\n",
            "Training Iteration 4097, Loss: 7.705036163330078\n",
            "Training Iteration 4098, Loss: 3.2903807163238525\n",
            "Training Iteration 4099, Loss: 4.300968647003174\n",
            "Training Iteration 4100, Loss: 5.500184059143066\n",
            "Training Iteration 4101, Loss: 4.010054588317871\n",
            "Training Iteration 4102, Loss: 3.9805445671081543\n",
            "Training Iteration 4103, Loss: 3.932302474975586\n",
            "Training Iteration 4104, Loss: 5.217013835906982\n",
            "Training Iteration 4105, Loss: 3.9905686378479004\n",
            "Training Iteration 4106, Loss: 4.987751483917236\n",
            "Training Iteration 4107, Loss: 6.199594974517822\n",
            "Training Iteration 4108, Loss: 4.625365257263184\n",
            "Training Iteration 4109, Loss: 3.738732099533081\n",
            "Training Iteration 4110, Loss: 2.395059585571289\n",
            "Training Iteration 4111, Loss: 6.655604839324951\n",
            "Training Iteration 4112, Loss: 7.045413970947266\n",
            "Training Iteration 4113, Loss: 2.377054452896118\n",
            "Training Iteration 4114, Loss: 5.799840927124023\n",
            "Training Iteration 4115, Loss: 6.795522689819336\n",
            "Training Iteration 4116, Loss: 6.6387104988098145\n",
            "Training Iteration 4117, Loss: 5.583636283874512\n",
            "Training Iteration 4118, Loss: 5.0614094734191895\n",
            "Training Iteration 4119, Loss: 4.756085395812988\n",
            "Training Iteration 4120, Loss: 4.123195171356201\n",
            "Training Iteration 4121, Loss: 2.9076716899871826\n",
            "Training Iteration 4122, Loss: 3.8166215419769287\n",
            "Training Iteration 4123, Loss: 8.191590309143066\n",
            "Training Iteration 4124, Loss: 3.170747756958008\n",
            "Training Iteration 4125, Loss: 6.226612091064453\n",
            "Training Iteration 4126, Loss: 4.378552436828613\n",
            "Training Iteration 4127, Loss: 5.677979946136475\n",
            "Training Iteration 4128, Loss: 1.476730465888977\n",
            "Training Iteration 4129, Loss: 2.9363834857940674\n",
            "Training Iteration 4130, Loss: 5.869283676147461\n",
            "Training Iteration 4131, Loss: 3.213275671005249\n",
            "Training Iteration 4132, Loss: 5.65330696105957\n",
            "Training Iteration 4133, Loss: 4.650699615478516\n",
            "Training Iteration 4134, Loss: 5.003762245178223\n",
            "Training Iteration 4135, Loss: 5.308484077453613\n",
            "Training Iteration 4136, Loss: 7.271780490875244\n",
            "Training Iteration 4137, Loss: 6.263843536376953\n",
            "Training Iteration 4138, Loss: 1.6800127029418945\n",
            "Training Iteration 4139, Loss: 4.5598649978637695\n",
            "Training Iteration 4140, Loss: 3.3240694999694824\n",
            "Training Iteration 4141, Loss: 3.1094744205474854\n",
            "Training Iteration 4142, Loss: 5.837630271911621\n",
            "Training Iteration 4143, Loss: 4.474879264831543\n",
            "Training Iteration 4144, Loss: 5.048270225524902\n",
            "Training Iteration 4145, Loss: 1.9646042585372925\n",
            "Training Iteration 4146, Loss: 6.414727210998535\n",
            "Training Iteration 4147, Loss: 3.5867624282836914\n",
            "Training Iteration 4148, Loss: 6.791642189025879\n",
            "Training Iteration 4149, Loss: 4.998049259185791\n",
            "Training Iteration 4150, Loss: 3.069391965866089\n",
            "Training Iteration 4151, Loss: 3.325796365737915\n",
            "Training Iteration 4152, Loss: 5.7150115966796875\n",
            "Training Iteration 4153, Loss: 6.719658374786377\n",
            "Training Iteration 4154, Loss: 5.992294788360596\n",
            "Training Iteration 4155, Loss: 4.259308338165283\n",
            "Training Iteration 4156, Loss: 3.1736135482788086\n",
            "Training Iteration 4157, Loss: 5.29466438293457\n",
            "Training Iteration 4158, Loss: 5.481943130493164\n",
            "Training Iteration 4159, Loss: 6.144380569458008\n",
            "Training Iteration 4160, Loss: 6.352079391479492\n",
            "Training Iteration 4161, Loss: 3.4796621799468994\n",
            "Training Iteration 4162, Loss: 9.688488006591797\n",
            "Training Iteration 4163, Loss: 2.0277438163757324\n",
            "Training Iteration 4164, Loss: 4.416112899780273\n",
            "Training Iteration 4165, Loss: 3.3113954067230225\n",
            "Training Iteration 4166, Loss: 7.307708740234375\n",
            "Training Iteration 4167, Loss: 4.483075141906738\n",
            "Training Iteration 4168, Loss: 5.825367450714111\n",
            "Training Iteration 4169, Loss: 4.913122177124023\n",
            "Training Iteration 4170, Loss: 6.805901527404785\n",
            "Training Iteration 4171, Loss: 1.3077852725982666\n",
            "Training Iteration 4172, Loss: 2.8578906059265137\n",
            "Training Iteration 4173, Loss: 3.83380126953125\n",
            "Training Iteration 4174, Loss: 5.289649486541748\n",
            "Training Iteration 4175, Loss: 4.967127799987793\n",
            "Training Iteration 4176, Loss: 6.60723352432251\n",
            "Training Iteration 4177, Loss: 4.419893741607666\n",
            "Training Iteration 4178, Loss: 6.325008869171143\n",
            "Training Iteration 4179, Loss: 1.2181264162063599\n",
            "Training Iteration 4180, Loss: 5.27040433883667\n",
            "Training Iteration 4181, Loss: 6.810593605041504\n",
            "Training Iteration 4182, Loss: 4.0116496086120605\n",
            "Training Iteration 4183, Loss: 4.477262496948242\n",
            "Training Iteration 4184, Loss: 4.292722225189209\n",
            "Training Iteration 4185, Loss: 5.214024543762207\n",
            "Training Iteration 4186, Loss: 4.907372951507568\n",
            "Training Iteration 4187, Loss: 2.4586057662963867\n",
            "Training Iteration 4188, Loss: 5.8789472579956055\n",
            "Training Iteration 4189, Loss: 6.464349746704102\n",
            "Training Iteration 4190, Loss: 4.371806621551514\n",
            "Training Iteration 4191, Loss: 4.763916015625\n",
            "Training Iteration 4192, Loss: 6.180217742919922\n",
            "Training Iteration 4193, Loss: 5.915159225463867\n",
            "Training Iteration 4194, Loss: 2.7448322772979736\n",
            "Training Iteration 4195, Loss: 4.30949592590332\n",
            "Training Iteration 4196, Loss: 8.107327461242676\n",
            "Training Iteration 4197, Loss: 4.340796947479248\n",
            "Training Iteration 4198, Loss: 5.48495626449585\n",
            "Training Iteration 4199, Loss: 3.1287412643432617\n",
            "Training Iteration 4200, Loss: 2.517096519470215\n",
            "Training Iteration 4201, Loss: 3.0129361152648926\n",
            "Training Iteration 4202, Loss: 6.338155269622803\n",
            "Training Iteration 4203, Loss: 2.3488759994506836\n",
            "Training Iteration 4204, Loss: 8.940059661865234\n",
            "Training Iteration 4205, Loss: 4.245098114013672\n",
            "Training Iteration 4206, Loss: 5.535912990570068\n",
            "Training Iteration 4207, Loss: 2.213646411895752\n",
            "Training Iteration 4208, Loss: 3.646437883377075\n",
            "Training Iteration 4209, Loss: 5.162506580352783\n",
            "Training Iteration 4210, Loss: 7.502175331115723\n",
            "Training Iteration 4211, Loss: 4.2124528884887695\n",
            "Training Iteration 4212, Loss: 8.85821533203125\n",
            "Training Iteration 4213, Loss: 6.513397216796875\n",
            "Training Iteration 4214, Loss: 5.5815043449401855\n",
            "Training Iteration 4215, Loss: 3.7333767414093018\n",
            "Training Iteration 4216, Loss: 5.595188140869141\n",
            "Training Iteration 4217, Loss: 5.427487850189209\n",
            "Training Iteration 4218, Loss: 5.895846366882324\n",
            "Training Iteration 4219, Loss: 5.140881538391113\n",
            "Training Iteration 4220, Loss: 6.922820091247559\n",
            "Training Iteration 4221, Loss: 4.356588840484619\n",
            "Training Iteration 4222, Loss: 5.772810459136963\n",
            "Training Iteration 4223, Loss: 3.181145668029785\n",
            "Training Iteration 4224, Loss: 2.7627317905426025\n",
            "Training Iteration 4225, Loss: 2.7359554767608643\n",
            "Training Iteration 4226, Loss: 4.435405731201172\n",
            "Training Iteration 4227, Loss: 5.367345809936523\n",
            "Training Iteration 4228, Loss: 6.939905166625977\n",
            "Training Iteration 4229, Loss: 4.859070301055908\n",
            "Training Iteration 4230, Loss: 3.946695566177368\n",
            "Training Iteration 4231, Loss: 4.539694786071777\n",
            "Training Iteration 4232, Loss: 4.2498345375061035\n",
            "Training Iteration 4233, Loss: 2.3059401512145996\n",
            "Training Iteration 4234, Loss: 3.4067866802215576\n",
            "Training Iteration 4235, Loss: 3.475590705871582\n",
            "Training Iteration 4236, Loss: 3.824251890182495\n",
            "Training Iteration 4237, Loss: 3.640671491622925\n",
            "Training Iteration 4238, Loss: 5.88771390914917\n",
            "Training Iteration 4239, Loss: 6.376834392547607\n",
            "Training Iteration 4240, Loss: 2.4914557933807373\n",
            "Training Iteration 4241, Loss: 5.225701332092285\n",
            "Training Iteration 4242, Loss: 4.406017780303955\n",
            "Training Iteration 4243, Loss: 4.131940841674805\n",
            "Training Iteration 4244, Loss: 4.289220333099365\n",
            "Training Iteration 4245, Loss: 5.294907093048096\n",
            "Training Iteration 4246, Loss: 7.0786452293396\n",
            "Training Iteration 4247, Loss: 4.821717262268066\n",
            "Training Iteration 4248, Loss: 5.662714958190918\n",
            "Training Iteration 4249, Loss: 3.6565468311309814\n",
            "Training Iteration 4250, Loss: 2.539936065673828\n",
            "Training Iteration 4251, Loss: 3.1732184886932373\n",
            "Training Iteration 4252, Loss: 9.164244651794434\n",
            "Training Iteration 4253, Loss: 1.3845813274383545\n",
            "Training Iteration 4254, Loss: 5.502307891845703\n",
            "Training Iteration 4255, Loss: 3.195713520050049\n",
            "Training Iteration 4256, Loss: 3.6571192741394043\n",
            "Training Iteration 4257, Loss: 3.998709201812744\n",
            "Training Iteration 4258, Loss: 3.3847668170928955\n",
            "Training Iteration 4259, Loss: 4.301055908203125\n",
            "Training Iteration 4260, Loss: 6.761546611785889\n",
            "Training Iteration 4261, Loss: 5.125263214111328\n",
            "Training Iteration 4262, Loss: 3.28969669342041\n",
            "Training Iteration 4263, Loss: 4.213088512420654\n",
            "Training Iteration 4264, Loss: 2.519977331161499\n",
            "Training Iteration 4265, Loss: 7.407294750213623\n",
            "Training Iteration 4266, Loss: 7.0900678634643555\n",
            "Training Iteration 4267, Loss: 4.009377956390381\n",
            "Training Iteration 4268, Loss: 3.9637653827667236\n",
            "Training Iteration 4269, Loss: 1.9347610473632812\n",
            "Training Iteration 4270, Loss: 5.398311138153076\n",
            "Training Iteration 4271, Loss: 5.4610772132873535\n",
            "Training Iteration 4272, Loss: 3.200193405151367\n",
            "Training Iteration 4273, Loss: 6.459566593170166\n",
            "Training Iteration 4274, Loss: 4.246613025665283\n",
            "Training Iteration 4275, Loss: 5.9695820808410645\n",
            "Training Iteration 4276, Loss: 2.6265766620635986\n",
            "Training Iteration 4277, Loss: 5.143758773803711\n",
            "Training Iteration 4278, Loss: 4.929306983947754\n",
            "Training Iteration 4279, Loss: 4.01436185836792\n",
            "Training Iteration 4280, Loss: 6.633669853210449\n",
            "Training Iteration 4281, Loss: 4.221053600311279\n",
            "Training Iteration 4282, Loss: 9.427297592163086\n",
            "Training Iteration 4283, Loss: 3.310112714767456\n",
            "Training Iteration 4284, Loss: 4.854694366455078\n",
            "Training Iteration 4285, Loss: 5.379236221313477\n",
            "Training Iteration 4286, Loss: 7.644415855407715\n",
            "Training Iteration 4287, Loss: 8.687796592712402\n",
            "Training Iteration 4288, Loss: 6.774035930633545\n",
            "Training Iteration 4289, Loss: 6.066544532775879\n",
            "Training Iteration 4290, Loss: 3.7434206008911133\n",
            "Training Iteration 4291, Loss: 3.477799654006958\n",
            "Training Iteration 4292, Loss: 1.7677490711212158\n",
            "Training Iteration 4293, Loss: 4.398087501525879\n",
            "Training Iteration 4294, Loss: 7.4269022941589355\n",
            "Training Iteration 4295, Loss: 4.898033142089844\n",
            "Training Iteration 4296, Loss: 7.675394058227539\n",
            "Training Iteration 4297, Loss: 3.966719150543213\n",
            "Training Iteration 4298, Loss: 7.220938205718994\n",
            "Training Iteration 4299, Loss: 4.846401214599609\n",
            "Training Iteration 4300, Loss: 3.166785717010498\n",
            "Training Iteration 4301, Loss: 2.4824440479278564\n",
            "Training Iteration 4302, Loss: 8.191378593444824\n",
            "Training Iteration 4303, Loss: 4.083944797515869\n",
            "Training Iteration 4304, Loss: 7.0092973709106445\n",
            "Training Iteration 4305, Loss: 5.572808265686035\n",
            "Training Iteration 4306, Loss: 2.9826865196228027\n",
            "Training Iteration 4307, Loss: 5.805025100708008\n",
            "Training Iteration 4308, Loss: 3.5628621578216553\n",
            "Training Iteration 4309, Loss: 5.217793941497803\n",
            "Training Iteration 4310, Loss: 3.204674482345581\n",
            "Training Iteration 4311, Loss: 2.887826681137085\n",
            "Training Iteration 4312, Loss: 7.799195289611816\n",
            "Training Iteration 4313, Loss: 2.74117112159729\n",
            "Training Iteration 4314, Loss: 4.05418586730957\n",
            "Training Iteration 4315, Loss: 5.640872955322266\n",
            "Training Iteration 4316, Loss: 5.9156646728515625\n",
            "Training Iteration 4317, Loss: 2.363494634628296\n",
            "Training Iteration 4318, Loss: 2.5258634090423584\n",
            "Training Iteration 4319, Loss: 8.384714126586914\n",
            "Training Iteration 4320, Loss: 4.050947189331055\n",
            "Training Iteration 4321, Loss: 4.051712989807129\n",
            "Training Iteration 4322, Loss: 3.030911922454834\n",
            "Training Iteration 4323, Loss: 2.376812219619751\n",
            "Training Iteration 4324, Loss: 3.442002296447754\n",
            "Training Iteration 4325, Loss: 3.9223392009735107\n",
            "Training Iteration 4326, Loss: 3.9008147716522217\n",
            "Training Iteration 4327, Loss: 3.3897409439086914\n",
            "Training Iteration 4328, Loss: 4.700107097625732\n",
            "Training Iteration 4329, Loss: 2.698444128036499\n",
            "Training Iteration 4330, Loss: 4.026045799255371\n",
            "Training Iteration 4331, Loss: 4.49955940246582\n",
            "Training Iteration 4332, Loss: 3.8980486392974854\n",
            "Training Iteration 4333, Loss: 3.8161861896514893\n",
            "Training Iteration 4334, Loss: 4.158594131469727\n",
            "Training Iteration 4335, Loss: 4.008526802062988\n",
            "Training Iteration 4336, Loss: 3.5841665267944336\n",
            "Training Iteration 4337, Loss: 3.5070157051086426\n",
            "Training Iteration 4338, Loss: 7.473237991333008\n",
            "Training Iteration 4339, Loss: 3.748293399810791\n",
            "Training Iteration 4340, Loss: 3.81272029876709\n",
            "Training Iteration 4341, Loss: 3.6120412349700928\n",
            "Training Iteration 4342, Loss: 4.193489074707031\n",
            "Training Iteration 4343, Loss: 2.576997756958008\n",
            "Training Iteration 4344, Loss: 2.2656147480010986\n",
            "Training Iteration 4345, Loss: 4.713724613189697\n",
            "Training Iteration 4346, Loss: 5.608188152313232\n",
            "Training Iteration 4347, Loss: 4.431960582733154\n",
            "Training Iteration 4348, Loss: 3.0955793857574463\n",
            "Training Iteration 4349, Loss: 3.6107616424560547\n",
            "Training Iteration 4350, Loss: 7.963171482086182\n",
            "Training Iteration 4351, Loss: 3.0922698974609375\n",
            "Training Iteration 4352, Loss: 4.3749895095825195\n",
            "Training Iteration 4353, Loss: 4.018069267272949\n",
            "Training Iteration 4354, Loss: 4.552550315856934\n",
            "Training Iteration 4355, Loss: 6.69332218170166\n",
            "Training Iteration 4356, Loss: 2.264178991317749\n",
            "Training Iteration 4357, Loss: 4.943265438079834\n",
            "Training Iteration 4358, Loss: 4.522710800170898\n",
            "Training Iteration 4359, Loss: 4.353949546813965\n",
            "Training Iteration 4360, Loss: 4.8628315925598145\n",
            "Training Iteration 4361, Loss: 3.841916799545288\n",
            "Training Iteration 4362, Loss: 4.973536491394043\n",
            "Training Iteration 4363, Loss: 2.359200954437256\n",
            "Training Iteration 4364, Loss: 4.097862243652344\n",
            "Training Iteration 4365, Loss: 6.191348552703857\n",
            "Training Iteration 4366, Loss: 4.928313732147217\n",
            "Training Iteration 4367, Loss: 6.634843826293945\n",
            "Training Iteration 4368, Loss: 4.827762126922607\n",
            "Training Iteration 4369, Loss: 3.6399948596954346\n",
            "Training Iteration 4370, Loss: 3.4953272342681885\n",
            "Training Iteration 4371, Loss: 2.4890825748443604\n",
            "Training Iteration 4372, Loss: 4.478426933288574\n",
            "Training Iteration 4373, Loss: 5.21730899810791\n",
            "Training Iteration 4374, Loss: 5.811276912689209\n",
            "Training Iteration 4375, Loss: 8.17137336730957\n",
            "Training Iteration 4376, Loss: 2.22436261177063\n",
            "Training Iteration 4377, Loss: 4.129506587982178\n",
            "Training Iteration 4378, Loss: 3.2394614219665527\n",
            "Training Iteration 4379, Loss: 2.848609447479248\n",
            "Training Iteration 4380, Loss: 2.7792716026306152\n",
            "Training Iteration 4381, Loss: 6.501917362213135\n",
            "Training Iteration 4382, Loss: 6.681511402130127\n",
            "Training Iteration 4383, Loss: 4.9482221603393555\n",
            "Training Iteration 4384, Loss: 3.691133737564087\n",
            "Training Iteration 4385, Loss: 5.373165607452393\n",
            "Training Iteration 4386, Loss: 5.659649848937988\n",
            "Training Iteration 4387, Loss: 4.039651870727539\n",
            "Training Iteration 4388, Loss: 3.115412950515747\n",
            "Training Iteration 4389, Loss: 4.213723182678223\n",
            "Training Iteration 4390, Loss: 5.02491569519043\n",
            "Training Iteration 4391, Loss: 4.035635471343994\n",
            "Training Iteration 4392, Loss: 4.155852317810059\n",
            "Training Iteration 4393, Loss: 6.114058494567871\n",
            "Training Iteration 4394, Loss: 8.139148712158203\n",
            "Training Iteration 4395, Loss: 5.89304256439209\n",
            "Training Iteration 4396, Loss: 12.158554077148438\n",
            "Training Iteration 4397, Loss: 4.690400123596191\n",
            "Training Iteration 4398, Loss: 2.7889459133148193\n",
            "Training Iteration 4399, Loss: 4.0728254318237305\n",
            "Training Iteration 4400, Loss: 2.844350814819336\n",
            "Training Iteration 4401, Loss: 2.15244722366333\n",
            "Training Iteration 4402, Loss: 5.264865875244141\n",
            "Training Iteration 4403, Loss: 5.778162956237793\n",
            "Training Iteration 4404, Loss: 4.5090131759643555\n",
            "Training Iteration 4405, Loss: 6.762558937072754\n",
            "Training Iteration 4406, Loss: 6.910790920257568\n",
            "Training Iteration 4407, Loss: 4.81484317779541\n",
            "Training Iteration 4408, Loss: 3.845278739929199\n",
            "Training Iteration 4409, Loss: 4.081527233123779\n",
            "Training Iteration 4410, Loss: 2.2639975547790527\n",
            "Training Iteration 4411, Loss: 7.605095863342285\n",
            "Training Iteration 4412, Loss: 6.195037364959717\n",
            "Training Iteration 4413, Loss: 5.897196292877197\n",
            "Training Iteration 4414, Loss: 3.547302484512329\n",
            "Training Iteration 4415, Loss: 2.018519401550293\n",
            "Training Iteration 4416, Loss: 1.3591294288635254\n",
            "Training Iteration 4417, Loss: 2.379981517791748\n",
            "Training Iteration 4418, Loss: 3.6663928031921387\n",
            "Training Iteration 4419, Loss: 2.792755365371704\n",
            "Training Iteration 4420, Loss: 3.4531126022338867\n",
            "Training Iteration 4421, Loss: 8.13156795501709\n",
            "Training Iteration 4422, Loss: 9.084775924682617\n",
            "Training Iteration 4423, Loss: 7.83790397644043\n",
            "Training Iteration 4424, Loss: 3.6209282875061035\n",
            "Training Iteration 4425, Loss: 4.573521137237549\n",
            "Training Iteration 4426, Loss: 6.259077548980713\n",
            "Training Iteration 4427, Loss: 4.638783931732178\n",
            "Training Iteration 4428, Loss: 6.453741073608398\n",
            "Training Iteration 4429, Loss: 9.797119140625\n",
            "Training Iteration 4430, Loss: 6.337608337402344\n",
            "Training Iteration 4431, Loss: 6.778339385986328\n",
            "Training Iteration 4432, Loss: 7.6600823402404785\n",
            "Training Iteration 4433, Loss: 5.628419399261475\n",
            "Training Iteration 4434, Loss: 6.042932033538818\n",
            "Training Iteration 4435, Loss: 5.141908168792725\n",
            "Training Iteration 4436, Loss: 2.763826370239258\n",
            "Training Iteration 4437, Loss: 3.1552388668060303\n",
            "Training Iteration 4438, Loss: 5.646622180938721\n",
            "Training Iteration 4439, Loss: 7.353814125061035\n",
            "Training Iteration 4440, Loss: 3.681314706802368\n",
            "Training Iteration 4441, Loss: 5.499961853027344\n",
            "Training Iteration 4442, Loss: 5.178251266479492\n",
            "Training Iteration 4443, Loss: 7.297366142272949\n",
            "Training Iteration 4444, Loss: 2.2714052200317383\n",
            "Training Iteration 4445, Loss: 4.200788497924805\n",
            "Training Iteration 4446, Loss: 2.3656206130981445\n",
            "Training Iteration 4447, Loss: 8.78337574005127\n",
            "Training Iteration 4448, Loss: 4.506767749786377\n",
            "Training Iteration 4449, Loss: 5.189116954803467\n",
            "Training Iteration 4450, Loss: 7.5743513107299805\n",
            "Training Iteration 4451, Loss: 3.1570982933044434\n",
            "Training Iteration 4452, Loss: 3.2311415672302246\n",
            "Training Iteration 4453, Loss: 3.083094835281372\n",
            "Training Iteration 4454, Loss: 1.495105266571045\n",
            "Training Iteration 4455, Loss: 4.761489391326904\n",
            "Training Iteration 4456, Loss: 6.530503273010254\n",
            "Training Iteration 4457, Loss: 4.039323806762695\n",
            "Training Iteration 4458, Loss: 4.0846991539001465\n",
            "Training Iteration 4459, Loss: 9.398283004760742\n",
            "Training Iteration 4460, Loss: 3.0783374309539795\n",
            "Training Iteration 4461, Loss: 3.6584718227386475\n",
            "Training Iteration 4462, Loss: 3.3561184406280518\n",
            "Training Iteration 4463, Loss: 2.941394090652466\n",
            "Training Iteration 4464, Loss: 6.622906684875488\n",
            "Training Iteration 4465, Loss: 5.337184429168701\n",
            "Training Iteration 4466, Loss: 6.020243167877197\n",
            "Training Iteration 4467, Loss: 9.2592134475708\n",
            "Training Iteration 4468, Loss: 3.261444091796875\n",
            "Training Iteration 4469, Loss: 9.801122665405273\n",
            "Training Iteration 4470, Loss: 7.7535552978515625\n",
            "Training Iteration 4471, Loss: 4.35345983505249\n",
            "Training Iteration 4472, Loss: 8.710756301879883\n",
            "Training Iteration 4473, Loss: 4.3346147537231445\n",
            "Training Iteration 4474, Loss: 5.613931655883789\n",
            "Training Iteration 4475, Loss: 2.4112813472747803\n",
            "Training Iteration 4476, Loss: 4.122045516967773\n",
            "Training Iteration 4477, Loss: 3.862736701965332\n",
            "Training Iteration 4478, Loss: 2.6399855613708496\n",
            "Training Iteration 4479, Loss: 4.7460503578186035\n",
            "Training Iteration 4480, Loss: 7.688443183898926\n",
            "Training Iteration 4481, Loss: 4.931331634521484\n",
            "Training Iteration 4482, Loss: 3.9149551391601562\n",
            "Training Iteration 4483, Loss: 1.4064140319824219\n",
            "Training Iteration 4484, Loss: 3.956404209136963\n",
            "Training Iteration 4485, Loss: 6.740477085113525\n",
            "Training Iteration 4486, Loss: 4.069227695465088\n",
            "Training Iteration 4487, Loss: 3.919167995452881\n",
            "Training Iteration 4488, Loss: 1.875032901763916\n",
            "Training Iteration 4489, Loss: 5.227965354919434\n",
            "Training Iteration 4490, Loss: 4.807507514953613\n",
            "Training Iteration 4491, Loss: 4.973804473876953\n",
            "Training Iteration 4492, Loss: 6.15796422958374\n",
            "Training Iteration 4493, Loss: 4.761437892913818\n",
            "Training Iteration 4494, Loss: 4.654911994934082\n",
            "Training Iteration 4495, Loss: 5.4700927734375\n",
            "Training Iteration 4496, Loss: 6.02280855178833\n",
            "Training Iteration 4497, Loss: 6.21625280380249\n",
            "Training Iteration 4498, Loss: 6.850627422332764\n",
            "Training Iteration 4499, Loss: 1.9541336297988892\n",
            "Training Iteration 4500, Loss: 4.218840599060059\n",
            "Training Iteration 4501, Loss: 2.271272659301758\n",
            "Training Iteration 4502, Loss: 5.018479824066162\n",
            "Training Iteration 4503, Loss: 2.952930212020874\n",
            "Training Iteration 4504, Loss: 3.0430397987365723\n",
            "Training Iteration 4505, Loss: 3.365361213684082\n",
            "Training Iteration 4506, Loss: 4.28684139251709\n",
            "Training Iteration 4507, Loss: 7.49605131149292\n",
            "Training Iteration 4508, Loss: 6.098901271820068\n",
            "Training Iteration 4509, Loss: 1.8827444314956665\n",
            "Training Iteration 4510, Loss: 1.4421782493591309\n",
            "Training Iteration 4511, Loss: 6.454739570617676\n",
            "Training Iteration 4512, Loss: 1.8158960342407227\n",
            "Training Iteration 4513, Loss: 4.012304782867432\n",
            "Training Iteration 4514, Loss: 4.297488212585449\n",
            "Training Iteration 4515, Loss: 4.6244025230407715\n",
            "Training Iteration 4516, Loss: 3.1769261360168457\n",
            "Training Iteration 4517, Loss: 2.05074143409729\n",
            "Training Iteration 4518, Loss: 5.653919219970703\n",
            "Training Iteration 4519, Loss: 3.397030830383301\n",
            "Training Iteration 4520, Loss: 5.688082695007324\n",
            "Training Iteration 4521, Loss: 4.913669109344482\n",
            "Training Iteration 4522, Loss: 4.056056976318359\n",
            "Training Iteration 4523, Loss: 2.157266855239868\n",
            "Training Iteration 4524, Loss: 7.209346294403076\n",
            "Training Iteration 4525, Loss: 3.618030309677124\n",
            "Training Iteration 4526, Loss: 5.489329814910889\n",
            "Training Iteration 4527, Loss: 3.8581902980804443\n",
            "Training Iteration 4528, Loss: 5.66252326965332\n",
            "Training Iteration 4529, Loss: 3.25886869430542\n",
            "Training Iteration 4530, Loss: 6.716501235961914\n",
            "Training Iteration 4531, Loss: 7.453237056732178\n",
            "Training Iteration 4532, Loss: 3.008155345916748\n",
            "Training Iteration 4533, Loss: 6.502654552459717\n",
            "Training Iteration 4534, Loss: 4.227344036102295\n",
            "Training Iteration 4535, Loss: 2.9606680870056152\n",
            "Training Iteration 4536, Loss: 2.5042765140533447\n",
            "Training Iteration 4537, Loss: 5.245972156524658\n",
            "Training Iteration 4538, Loss: 6.626828193664551\n",
            "Training Iteration 4539, Loss: 4.120043754577637\n",
            "Training Iteration 4540, Loss: 4.618100643157959\n",
            "Training Iteration 4541, Loss: 10.160032272338867\n",
            "Training Iteration 4542, Loss: 5.431900978088379\n",
            "Training Iteration 4543, Loss: 3.805474042892456\n",
            "Training Iteration 4544, Loss: 5.223637580871582\n",
            "Training Iteration 4545, Loss: 3.39217472076416\n",
            "Training Iteration 4546, Loss: 5.14730167388916\n",
            "Training Iteration 4547, Loss: 4.402482986450195\n",
            "Training Iteration 4548, Loss: 2.9078311920166016\n",
            "Training Iteration 4549, Loss: 3.9868273735046387\n",
            "Training Iteration 4550, Loss: 4.800479412078857\n",
            "Training Iteration 4551, Loss: 5.026992321014404\n",
            "Training Iteration 4552, Loss: 4.53078556060791\n",
            "Training Iteration 4553, Loss: 4.363564491271973\n",
            "Training Iteration 4554, Loss: 3.6013481616973877\n",
            "Training Iteration 4555, Loss: 4.2529730796813965\n",
            "Training Iteration 4556, Loss: 10.060851097106934\n",
            "Training Iteration 4557, Loss: 5.388391494750977\n",
            "Training Iteration 4558, Loss: 3.8560824394226074\n",
            "Training Iteration 4559, Loss: 6.56702995300293\n",
            "Training Iteration 4560, Loss: 3.7314279079437256\n",
            "Training Iteration 4561, Loss: 6.915323257446289\n",
            "Training Iteration 4562, Loss: 4.438304901123047\n",
            "Training Iteration 4563, Loss: 4.881440162658691\n",
            "Training Iteration 4564, Loss: 3.3831567764282227\n",
            "Training Iteration 4565, Loss: 4.89708948135376\n",
            "Training Iteration 4566, Loss: 5.735723495483398\n",
            "Training Iteration 4567, Loss: 4.110589027404785\n",
            "Training Iteration 4568, Loss: 8.578063011169434\n",
            "Training Iteration 4569, Loss: 4.669986724853516\n",
            "Training Iteration 4570, Loss: 2.5987462997436523\n",
            "Training Iteration 4571, Loss: 6.0150957107543945\n",
            "Training Iteration 4572, Loss: 1.6723560094833374\n",
            "Training Iteration 4573, Loss: 3.0636179447174072\n",
            "Training Iteration 4574, Loss: 4.31168794631958\n",
            "Training Iteration 4575, Loss: 4.558812141418457\n",
            "Training Iteration 4576, Loss: 6.370401382446289\n",
            "Training Iteration 4577, Loss: 6.585009574890137\n",
            "Training Iteration 4578, Loss: 5.873952388763428\n",
            "Training Iteration 4579, Loss: 2.9064536094665527\n",
            "Training Iteration 4580, Loss: 3.262348175048828\n",
            "Training Iteration 4581, Loss: 3.1921892166137695\n",
            "Training Iteration 4582, Loss: 3.6346945762634277\n",
            "Training Iteration 4583, Loss: 6.451612949371338\n",
            "Training Iteration 4584, Loss: 1.7763851881027222\n",
            "Training Iteration 4585, Loss: 2.4646008014678955\n",
            "Training Iteration 4586, Loss: 3.8219499588012695\n",
            "Training Iteration 4587, Loss: 4.893017768859863\n",
            "Training Iteration 4588, Loss: 4.676732063293457\n",
            "Training Iteration 4589, Loss: 5.866039752960205\n",
            "Training Iteration 4590, Loss: 7.102846145629883\n",
            "Training Iteration 4591, Loss: 5.194730758666992\n",
            "Training Iteration 4592, Loss: 4.207938194274902\n",
            "Training Iteration 4593, Loss: 2.682645797729492\n",
            "Training Iteration 4594, Loss: 4.078428745269775\n",
            "Training Iteration 4595, Loss: 3.348712921142578\n",
            "Training Iteration 4596, Loss: 2.8304834365844727\n",
            "Training Iteration 4597, Loss: 5.344593048095703\n",
            "Training Iteration 4598, Loss: 3.67427134513855\n",
            "Training Iteration 4599, Loss: 4.628434181213379\n",
            "Training Iteration 4600, Loss: 4.761379718780518\n",
            "Training Iteration 4601, Loss: 6.793845176696777\n",
            "Training Iteration 4602, Loss: 2.994239568710327\n",
            "Training Iteration 4603, Loss: 4.755553245544434\n",
            "Training Iteration 4604, Loss: 4.913755893707275\n",
            "Training Iteration 4605, Loss: 7.699635028839111\n",
            "Training Iteration 4606, Loss: 5.766201972961426\n",
            "Training Iteration 4607, Loss: 3.2265853881835938\n",
            "Training Iteration 4608, Loss: 4.36483907699585\n",
            "Training Iteration 4609, Loss: 4.400435447692871\n",
            "Training Iteration 4610, Loss: 7.134825706481934\n",
            "Training Iteration 4611, Loss: 1.8898602724075317\n",
            "Training Iteration 4612, Loss: 2.766080379486084\n",
            "Training Iteration 4613, Loss: 2.8077237606048584\n",
            "Training Iteration 4614, Loss: 6.224422454833984\n",
            "Training Iteration 4615, Loss: 4.396492958068848\n",
            "Training Iteration 4616, Loss: 8.078312873840332\n",
            "Training Iteration 4617, Loss: 7.238715648651123\n",
            "Training Iteration 4618, Loss: 6.181654930114746\n",
            "Training Iteration 4619, Loss: 4.805893421173096\n",
            "Training Iteration 4620, Loss: 2.629102945327759\n",
            "Training Iteration 4621, Loss: 3.7989373207092285\n",
            "Training Iteration 4622, Loss: 5.274123191833496\n",
            "Training Iteration 4623, Loss: 6.229074478149414\n",
            "Training Iteration 4624, Loss: 3.2339389324188232\n",
            "Training Iteration 4625, Loss: 4.722271919250488\n",
            "Training Iteration 4626, Loss: 2.92516827583313\n",
            "Training Iteration 4627, Loss: 4.267746925354004\n",
            "Training Iteration 4628, Loss: 5.114349842071533\n",
            "Training Iteration 4629, Loss: 4.487950801849365\n",
            "Training Iteration 4630, Loss: 1.9926036596298218\n",
            "Training Iteration 4631, Loss: 2.7800345420837402\n",
            "Training Iteration 4632, Loss: 5.665103912353516\n",
            "Training Iteration 4633, Loss: 4.8522796630859375\n",
            "Training Iteration 4634, Loss: 4.45603084564209\n",
            "Training Iteration 4635, Loss: 3.5127310752868652\n",
            "Training Iteration 4636, Loss: 3.522318124771118\n",
            "Training Iteration 4637, Loss: 2.866049289703369\n",
            "Training Iteration 4638, Loss: 2.577373743057251\n",
            "Training Iteration 4639, Loss: 7.326496601104736\n",
            "Training Iteration 4640, Loss: 4.427605628967285\n",
            "Training Iteration 4641, Loss: 4.669558525085449\n",
            "Training Iteration 4642, Loss: 5.296067237854004\n",
            "Training Iteration 4643, Loss: 3.422431707382202\n",
            "Training Iteration 4644, Loss: 4.496382236480713\n",
            "Training Iteration 4645, Loss: 5.813516616821289\n",
            "Training Iteration 4646, Loss: 4.276146411895752\n",
            "Training Iteration 4647, Loss: 4.735767841339111\n",
            "Training Iteration 4648, Loss: 6.3789167404174805\n",
            "Training Iteration 4649, Loss: 4.276665210723877\n",
            "Training Iteration 4650, Loss: 3.6223769187927246\n",
            "Training Iteration 4651, Loss: 2.2921812534332275\n",
            "Training Iteration 4652, Loss: 5.677633285522461\n",
            "Training Iteration 4653, Loss: 5.708077907562256\n",
            "Training Iteration 4654, Loss: 4.65266752243042\n",
            "Training Iteration 4655, Loss: 3.6926300525665283\n",
            "Training Iteration 4656, Loss: 5.257065773010254\n",
            "Training Iteration 4657, Loss: 7.207208633422852\n",
            "Training Iteration 4658, Loss: 6.010866641998291\n",
            "Training Iteration 4659, Loss: 7.702715873718262\n",
            "Training Iteration 4660, Loss: 5.502357006072998\n",
            "Training Iteration 4661, Loss: 9.341200828552246\n",
            "Training Iteration 4662, Loss: 4.766465187072754\n",
            "Training Iteration 4663, Loss: 6.181052207946777\n",
            "Training Iteration 4664, Loss: 5.474292755126953\n",
            "Training Iteration 4665, Loss: 7.041304588317871\n",
            "Training Iteration 4666, Loss: 3.913510799407959\n",
            "Training Iteration 4667, Loss: 1.9750161170959473\n",
            "Training Iteration 4668, Loss: 5.9062347412109375\n",
            "Training Iteration 4669, Loss: 4.835285663604736\n",
            "Training Iteration 4670, Loss: 6.025279998779297\n",
            "Training Iteration 4671, Loss: 4.725985527038574\n",
            "Training Iteration 4672, Loss: 8.345884323120117\n",
            "Training Iteration 4673, Loss: 3.442356586456299\n",
            "Training Iteration 4674, Loss: 3.6755528450012207\n",
            "Training Iteration 4675, Loss: 2.5000109672546387\n",
            "Training Iteration 4676, Loss: 1.9066919088363647\n",
            "Training Iteration 4677, Loss: 6.657279014587402\n",
            "Training Iteration 4678, Loss: 7.0312700271606445\n",
            "Training Iteration 4679, Loss: 3.5327799320220947\n",
            "Training Iteration 4680, Loss: 3.98252534866333\n",
            "Training Iteration 4681, Loss: 4.475795269012451\n",
            "Training Iteration 4682, Loss: 5.613091945648193\n",
            "Training Iteration 4683, Loss: 3.6916139125823975\n",
            "Training Iteration 4684, Loss: 5.100337028503418\n",
            "Training Iteration 4685, Loss: 4.559848308563232\n",
            "Training Iteration 4686, Loss: 8.427358627319336\n",
            "Training Iteration 4687, Loss: 6.1152448654174805\n",
            "Training Iteration 4688, Loss: 4.511872291564941\n",
            "Training Iteration 4689, Loss: 4.693981647491455\n",
            "Training Iteration 4690, Loss: 7.15178918838501\n",
            "Training Iteration 4691, Loss: 4.089297771453857\n",
            "Training Iteration 4692, Loss: 10.385497093200684\n",
            "Training Iteration 4693, Loss: 6.620837211608887\n",
            "Training Iteration 4694, Loss: 7.146534442901611\n",
            "Training Iteration 4695, Loss: 6.233593940734863\n",
            "Training Iteration 4696, Loss: 3.6078953742980957\n",
            "Training Iteration 4697, Loss: 7.772251129150391\n",
            "Training Iteration 4698, Loss: 5.42488956451416\n",
            "Training Iteration 4699, Loss: 5.3432207107543945\n",
            "Training Iteration 4700, Loss: 4.470317363739014\n",
            "Training Iteration 4701, Loss: 8.035371780395508\n",
            "Training Iteration 4702, Loss: 6.575223445892334\n",
            "Training Iteration 4703, Loss: 4.512510776519775\n",
            "Training Iteration 4704, Loss: 2.570608139038086\n",
            "Training Iteration 4705, Loss: 3.315114736557007\n",
            "Training Iteration 4706, Loss: 4.308248043060303\n",
            "Training Iteration 4707, Loss: 7.437496185302734\n",
            "Training Iteration 4708, Loss: 5.284826278686523\n",
            "Training Iteration 4709, Loss: 8.378108978271484\n",
            "Training Iteration 4710, Loss: 8.056679725646973\n",
            "Training Iteration 4711, Loss: 7.124619483947754\n",
            "Training Iteration 4712, Loss: 4.264983654022217\n",
            "Training Iteration 4713, Loss: 4.154378890991211\n",
            "Training Iteration 4714, Loss: 7.036562919616699\n",
            "Training Iteration 4715, Loss: 5.580643177032471\n",
            "Training Iteration 4716, Loss: 1.9346022605895996\n",
            "Training Iteration 4717, Loss: 4.916257381439209\n",
            "Training Iteration 4718, Loss: 4.549760341644287\n",
            "Training Iteration 4719, Loss: 4.9563140869140625\n",
            "Training Iteration 4720, Loss: 4.364880561828613\n",
            "Training Iteration 4721, Loss: 9.646590232849121\n",
            "Training Iteration 4722, Loss: 9.299067497253418\n",
            "Training Iteration 4723, Loss: 4.359532833099365\n",
            "Training Iteration 4724, Loss: 8.36245059967041\n",
            "Training Iteration 4725, Loss: 7.4219441413879395\n",
            "Training Iteration 4726, Loss: 3.803852081298828\n",
            "Training Iteration 4727, Loss: 4.888359546661377\n",
            "Training Iteration 4728, Loss: 3.625694751739502\n",
            "Training Iteration 4729, Loss: 5.623432636260986\n",
            "Training Iteration 4730, Loss: 4.173132419586182\n",
            "Training Iteration 4731, Loss: 5.5895280838012695\n",
            "Training Iteration 4732, Loss: 3.7133402824401855\n",
            "Training Iteration 4733, Loss: 5.436725616455078\n",
            "Training Iteration 4734, Loss: 5.397111415863037\n",
            "Training Iteration 4735, Loss: 2.086655616760254\n",
            "Training Iteration 4736, Loss: 4.535296440124512\n",
            "Training Iteration 4737, Loss: 4.23916482925415\n",
            "Training Iteration 4738, Loss: 4.063745498657227\n",
            "Training Iteration 4739, Loss: 5.654177188873291\n",
            "Training Iteration 4740, Loss: 5.116038799285889\n",
            "Training Iteration 4741, Loss: 6.720290660858154\n",
            "Training Iteration 4742, Loss: 4.71150541305542\n",
            "Training Iteration 4743, Loss: 5.167198181152344\n",
            "Training Iteration 4744, Loss: 4.099149703979492\n",
            "Training Iteration 4745, Loss: 4.50578498840332\n",
            "Training Iteration 4746, Loss: 2.7578325271606445\n",
            "Training Iteration 4747, Loss: 4.25166130065918\n",
            "Training Iteration 4748, Loss: 3.136439323425293\n",
            "Training Iteration 4749, Loss: 3.4345180988311768\n",
            "Training Iteration 4750, Loss: 5.378606796264648\n",
            "Training Iteration 4751, Loss: 4.874736785888672\n",
            "Training Iteration 4752, Loss: 7.533710956573486\n",
            "Training Iteration 4753, Loss: 3.1432695388793945\n",
            "Training Iteration 4754, Loss: 3.8034703731536865\n",
            "Training Iteration 4755, Loss: 7.481205940246582\n",
            "Training Iteration 4756, Loss: 3.487842559814453\n",
            "Training Iteration 4757, Loss: 6.030717372894287\n",
            "Training Iteration 4758, Loss: 2.233034610748291\n",
            "Training Iteration 4759, Loss: 5.04526424407959\n",
            "Training Iteration 4760, Loss: 2.909815549850464\n",
            "Training Iteration 4761, Loss: 4.608260154724121\n",
            "Training Iteration 4762, Loss: 8.324378967285156\n",
            "Training Iteration 4763, Loss: 9.19841194152832\n",
            "Training Iteration 4764, Loss: 8.946542739868164\n",
            "Training Iteration 4765, Loss: 1.9663375616073608\n",
            "Training Iteration 4766, Loss: 2.8795576095581055\n",
            "Training Iteration 4767, Loss: 5.469301700592041\n",
            "Training Iteration 4768, Loss: 6.250905990600586\n",
            "Training Iteration 4769, Loss: 4.033227443695068\n",
            "Training Iteration 4770, Loss: 7.081672191619873\n",
            "Training Iteration 4771, Loss: 5.523194313049316\n",
            "Training Iteration 4772, Loss: 5.301703453063965\n",
            "Training Iteration 4773, Loss: 2.5064029693603516\n",
            "Training Iteration 4774, Loss: 5.284707069396973\n",
            "Training Iteration 4775, Loss: 6.586678981781006\n",
            "Training Iteration 4776, Loss: 5.186432361602783\n",
            "Training Iteration 4777, Loss: 1.095855951309204\n",
            "Training Iteration 4778, Loss: 5.725289344787598\n",
            "Training Iteration 4779, Loss: 5.8072309494018555\n",
            "Training Iteration 4780, Loss: 5.117496490478516\n",
            "Training Iteration 4781, Loss: 8.463754653930664\n",
            "Training Iteration 4782, Loss: 5.245138645172119\n",
            "Training Iteration 4783, Loss: 4.363046646118164\n",
            "Training Iteration 4784, Loss: 4.869038105010986\n",
            "Training Iteration 4785, Loss: 6.393654823303223\n",
            "Training Iteration 4786, Loss: 4.862584114074707\n",
            "Training Iteration 4787, Loss: 4.884927272796631\n",
            "Training Iteration 4788, Loss: 7.782060623168945\n",
            "Training Iteration 4789, Loss: 5.869057655334473\n",
            "Training Iteration 4790, Loss: 6.631535053253174\n",
            "Training Iteration 4791, Loss: 7.537432670593262\n",
            "Training Iteration 4792, Loss: 4.102156162261963\n",
            "Training Iteration 4793, Loss: 6.721674919128418\n",
            "Training Iteration 4794, Loss: 5.896652698516846\n",
            "Training Iteration 4795, Loss: 5.316697120666504\n",
            "Training Iteration 4796, Loss: 4.991630554199219\n",
            "Training Iteration 4797, Loss: 3.764345169067383\n",
            "Training Iteration 4798, Loss: 4.424795627593994\n",
            "Training Iteration 4799, Loss: 4.926323890686035\n",
            "Training Iteration 4800, Loss: 3.5707271099090576\n",
            "Training Iteration 4801, Loss: 4.199843406677246\n",
            "Training Iteration 4802, Loss: 4.48803186416626\n",
            "Training Iteration 4803, Loss: 6.472483158111572\n",
            "Training Iteration 4804, Loss: 5.443933486938477\n",
            "Training Iteration 4805, Loss: 4.194612503051758\n",
            "Training Iteration 4806, Loss: 2.143590211868286\n",
            "Training Iteration 4807, Loss: 4.981219291687012\n",
            "Training Iteration 4808, Loss: 5.523310661315918\n",
            "Training Iteration 4809, Loss: 4.278477191925049\n",
            "Training Iteration 4810, Loss: 4.162993907928467\n",
            "Training Iteration 4811, Loss: 3.1625661849975586\n",
            "Training Iteration 4812, Loss: 2.857616424560547\n",
            "Training Iteration 4813, Loss: 5.388525485992432\n",
            "Training Iteration 4814, Loss: 5.975480079650879\n",
            "Training Iteration 4815, Loss: 3.227457046508789\n",
            "Training Iteration 4816, Loss: 3.331233024597168\n",
            "Training Iteration 4817, Loss: 2.973517656326294\n",
            "Training Iteration 4818, Loss: 3.179490327835083\n",
            "Training Iteration 4819, Loss: 7.757325649261475\n",
            "Training Iteration 4820, Loss: 8.072364807128906\n",
            "Training Iteration 4821, Loss: 4.920192718505859\n",
            "Training Iteration 4822, Loss: 4.378672122955322\n",
            "Training Iteration 4823, Loss: 2.9279367923736572\n",
            "Training Iteration 4824, Loss: 5.289309501647949\n",
            "Training Iteration 4825, Loss: 4.558022499084473\n",
            "Training Iteration 4826, Loss: 3.565545082092285\n",
            "Training Iteration 4827, Loss: 6.779878616333008\n",
            "Training Iteration 4828, Loss: 6.636940002441406\n",
            "Training Iteration 4829, Loss: 4.166200637817383\n",
            "Training Iteration 4830, Loss: 5.49263858795166\n",
            "Training Iteration 4831, Loss: 5.902425289154053\n",
            "Training Iteration 4832, Loss: 3.7439966201782227\n",
            "Training Iteration 4833, Loss: 3.813788890838623\n",
            "Training Iteration 4834, Loss: 5.360566139221191\n",
            "Training Iteration 4835, Loss: 2.7591593265533447\n",
            "Training Iteration 4836, Loss: 3.8585407733917236\n",
            "Training Iteration 4837, Loss: 8.652381896972656\n",
            "Training Iteration 4838, Loss: 5.495867729187012\n",
            "Training Iteration 4839, Loss: 5.016636848449707\n",
            "Training Iteration 4840, Loss: 6.459985733032227\n",
            "Training Iteration 4841, Loss: 4.0669755935668945\n",
            "Training Iteration 4842, Loss: 7.260283946990967\n",
            "Training Iteration 4843, Loss: 3.436253070831299\n",
            "Training Iteration 4844, Loss: 2.0779809951782227\n",
            "Training Iteration 4845, Loss: 3.343527317047119\n",
            "Training Iteration 4846, Loss: 6.6070237159729\n",
            "Training Iteration 4847, Loss: 4.714711666107178\n",
            "Training Iteration 4848, Loss: 4.563802719116211\n",
            "Training Iteration 4849, Loss: 4.415794849395752\n",
            "Training Iteration 4850, Loss: 3.134124517440796\n",
            "Training Iteration 4851, Loss: 4.915441513061523\n",
            "Training Iteration 4852, Loss: 7.414017200469971\n",
            "Training Iteration 4853, Loss: 1.389194369316101\n",
            "Training Iteration 4854, Loss: 2.87241268157959\n",
            "Training Iteration 4855, Loss: 4.951023578643799\n",
            "Training Iteration 4856, Loss: 7.877902030944824\n",
            "Training Iteration 4857, Loss: 8.676279067993164\n",
            "Training Iteration 4858, Loss: 6.615686416625977\n",
            "Training Iteration 4859, Loss: 7.764455795288086\n",
            "Training Iteration 4860, Loss: 3.140324592590332\n",
            "Training Iteration 4861, Loss: 3.2483174800872803\n",
            "Training Iteration 4862, Loss: 6.580465316772461\n",
            "Training Iteration 4863, Loss: 7.977428913116455\n",
            "Training Iteration 4864, Loss: 7.26278018951416\n",
            "Training Iteration 4865, Loss: 5.629106044769287\n",
            "Training Iteration 4866, Loss: 6.425872802734375\n",
            "Training Iteration 4867, Loss: 5.493605613708496\n",
            "Training Iteration 4868, Loss: 3.2234044075012207\n",
            "Training Iteration 4869, Loss: 4.588813781738281\n",
            "Training Iteration 4870, Loss: 6.828436851501465\n",
            "Training Iteration 4871, Loss: 4.256894588470459\n",
            "Training Iteration 4872, Loss: 5.8751654624938965\n",
            "Training Iteration 4873, Loss: 5.268255710601807\n",
            "Training Iteration 4874, Loss: 2.361926794052124\n",
            "Training Iteration 4875, Loss: 3.4874563217163086\n",
            "Training Iteration 4876, Loss: 7.546933650970459\n",
            "Training Iteration 4877, Loss: 3.7577219009399414\n",
            "Training Iteration 4878, Loss: 7.194308280944824\n",
            "Training Iteration 4879, Loss: 3.5937740802764893\n",
            "Training Iteration 4880, Loss: 3.273974657058716\n",
            "Training Iteration 4881, Loss: 7.4712042808532715\n",
            "Training Iteration 4882, Loss: 2.635871171951294\n",
            "Training Iteration 4883, Loss: 3.273423671722412\n",
            "Training Iteration 4884, Loss: 6.916612148284912\n",
            "Training Iteration 4885, Loss: 6.618350028991699\n",
            "Training Iteration 4886, Loss: 5.866885662078857\n",
            "Training Iteration 4887, Loss: 2.7720980644226074\n",
            "Training Iteration 4888, Loss: 3.166189193725586\n",
            "Training Iteration 4889, Loss: 4.227580547332764\n",
            "Training Iteration 4890, Loss: 3.444516181945801\n",
            "Training Iteration 4891, Loss: 3.6440887451171875\n",
            "Training Iteration 4892, Loss: 7.755920886993408\n",
            "Training Iteration 4893, Loss: 5.900629043579102\n",
            "Training Iteration 4894, Loss: 3.7217767238616943\n",
            "Training Iteration 4895, Loss: 5.440349578857422\n",
            "Training Iteration 4896, Loss: 4.953732490539551\n",
            "Training Iteration 4897, Loss: 7.37725830078125\n",
            "Training Iteration 4898, Loss: 4.237213134765625\n",
            "Training Iteration 4899, Loss: 4.578800678253174\n",
            "Training Iteration 4900, Loss: 2.599811553955078\n",
            "Training Iteration 4901, Loss: 4.377981662750244\n",
            "Training Iteration 4902, Loss: 4.867587089538574\n",
            "Training Iteration 4903, Loss: 4.5140814781188965\n",
            "Training Iteration 4904, Loss: 6.630912780761719\n",
            "Training Iteration 4905, Loss: 1.5987054109573364\n",
            "Training Iteration 4906, Loss: 8.093843460083008\n",
            "Training Iteration 4907, Loss: 5.510198593139648\n",
            "Training Iteration 4908, Loss: 1.9457706212997437\n",
            "Training Iteration 4909, Loss: 3.755397319793701\n",
            "Training Iteration 4910, Loss: 6.972131729125977\n",
            "Training Iteration 4911, Loss: 5.035376071929932\n",
            "Training Iteration 4912, Loss: 5.185487270355225\n",
            "Training Iteration 4913, Loss: 5.486920356750488\n",
            "Training Iteration 4914, Loss: 3.9346611499786377\n",
            "Training Iteration 4915, Loss: 2.4513015747070312\n",
            "Training Iteration 4916, Loss: 6.2923407554626465\n",
            "Training Iteration 4917, Loss: 7.860065460205078\n",
            "Training Iteration 4918, Loss: 7.630842685699463\n",
            "Training Iteration 4919, Loss: 1.9837238788604736\n",
            "Training Iteration 4920, Loss: 4.223328590393066\n",
            "Training Iteration 4921, Loss: 5.444248199462891\n",
            "Training Iteration 4922, Loss: 4.032946586608887\n",
            "Training Iteration 4923, Loss: 6.944257736206055\n",
            "Training Iteration 4924, Loss: 1.4728683233261108\n",
            "Training Iteration 4925, Loss: 4.981198787689209\n",
            "Training Iteration 4926, Loss: 5.766984939575195\n",
            "Training Iteration 4927, Loss: 4.590030193328857\n",
            "Training Iteration 4928, Loss: 2.7333431243896484\n",
            "Training Iteration 4929, Loss: 3.6432337760925293\n",
            "Training Iteration 4930, Loss: 2.546271800994873\n",
            "Training Iteration 4931, Loss: 3.900588274002075\n",
            "Training Iteration 4932, Loss: 2.5146827697753906\n",
            "Training Iteration 4933, Loss: 4.23048210144043\n",
            "Training Iteration 4934, Loss: 2.8950107097625732\n",
            "Training Iteration 4935, Loss: 1.6918249130249023\n",
            "Training Iteration 4936, Loss: 3.7833073139190674\n",
            "Training Iteration 4937, Loss: 5.008448600769043\n",
            "Training Iteration 4938, Loss: 3.8400962352752686\n",
            "Training Iteration 4939, Loss: 4.1029438972473145\n",
            "Training Iteration 4940, Loss: 3.02719783782959\n",
            "Training Iteration 4941, Loss: 3.191596508026123\n",
            "Training Iteration 4942, Loss: 3.6735715866088867\n",
            "Training Iteration 4943, Loss: 3.3889389038085938\n",
            "Training Iteration 4944, Loss: 3.2346842288970947\n",
            "Training Iteration 4945, Loss: 3.3446624279022217\n",
            "Training Iteration 4946, Loss: 2.9498045444488525\n",
            "Training Iteration 4947, Loss: 6.029539585113525\n",
            "Training Iteration 4948, Loss: 4.812413692474365\n",
            "Training Iteration 4949, Loss: 6.1128644943237305\n",
            "Training Iteration 4950, Loss: 5.20256233215332\n",
            "Training Iteration 4951, Loss: 5.177504062652588\n",
            "Training Iteration 4952, Loss: 2.8898839950561523\n",
            "Training Iteration 4953, Loss: 3.3991384506225586\n",
            "Training Iteration 4954, Loss: 2.5284006595611572\n",
            "Training Iteration 4955, Loss: 3.9703919887542725\n",
            "Training Iteration 4956, Loss: 3.0335187911987305\n",
            "Training Iteration 4957, Loss: 4.634487628936768\n",
            "Training Iteration 4958, Loss: 3.869445562362671\n",
            "Training Iteration 4959, Loss: 2.6512162685394287\n",
            "Training Iteration 4960, Loss: 5.1309614181518555\n",
            "Training Iteration 4961, Loss: 6.791148662567139\n",
            "Training Iteration 4962, Loss: 4.525544166564941\n",
            "Training Iteration 4963, Loss: 4.361484527587891\n",
            "Training Iteration 4964, Loss: 4.120014667510986\n",
            "Training Iteration 4965, Loss: 3.9494271278381348\n",
            "Training Iteration 4966, Loss: 3.402623176574707\n",
            "Training Iteration 4967, Loss: 5.189746856689453\n",
            "Training Iteration 4968, Loss: 4.1512675285339355\n",
            "Training Iteration 4969, Loss: 3.3127965927124023\n",
            "Training Iteration 4970, Loss: 2.431797981262207\n",
            "Training Iteration 4971, Loss: 5.832180023193359\n",
            "Training Iteration 4972, Loss: 4.048596382141113\n",
            "Training Iteration 4973, Loss: 4.249197959899902\n",
            "Training Iteration 4974, Loss: 2.1871559619903564\n",
            "Training Iteration 4975, Loss: 4.754635810852051\n",
            "Training Iteration 4976, Loss: 3.4078943729400635\n",
            "Training Iteration 4977, Loss: 3.500716209411621\n",
            "Training Iteration 4978, Loss: 4.586198329925537\n",
            "Training Iteration 4979, Loss: 3.0439839363098145\n",
            "Training Iteration 4980, Loss: 8.336662292480469\n",
            "Training Iteration 4981, Loss: 3.8923492431640625\n",
            "Training Iteration 4982, Loss: 0.995090663433075\n",
            "Training Iteration 4983, Loss: 4.2235565185546875\n",
            "Training Iteration 4984, Loss: 6.340996265411377\n",
            "Training Iteration 4985, Loss: 4.227634906768799\n",
            "Training Iteration 4986, Loss: 5.202540874481201\n",
            "Training Iteration 4987, Loss: 6.590664386749268\n",
            "Training Iteration 4988, Loss: 8.600726127624512\n",
            "Training Iteration 4989, Loss: 7.711732864379883\n",
            "Training Iteration 4990, Loss: 8.309812545776367\n",
            "Training Iteration 4991, Loss: 5.959135055541992\n",
            "Training Iteration 4992, Loss: 8.000951766967773\n",
            "Training Iteration 4993, Loss: 5.180911064147949\n",
            "Training Iteration 4994, Loss: 5.51123046875\n",
            "Training Iteration 4995, Loss: 5.694282054901123\n",
            "Training Iteration 4996, Loss: 4.702341079711914\n",
            "Training Iteration 4997, Loss: 4.129275798797607\n",
            "Training Iteration 4998, Loss: 4.227650165557861\n",
            "Training Iteration 4999, Loss: 2.989449977874756\n",
            "Training Iteration 5000, Loss: 1.7944130897521973\n",
            "Training Iteration 5001, Loss: 5.4765729904174805\n",
            "Training Iteration 5002, Loss: 6.882453918457031\n",
            "Training Iteration 5003, Loss: 4.174525737762451\n",
            "Training Iteration 5004, Loss: 7.093684673309326\n",
            "Training Iteration 5005, Loss: 5.557650089263916\n",
            "Training Iteration 5006, Loss: 6.330183982849121\n",
            "Training Iteration 5007, Loss: 3.807084321975708\n",
            "Training Iteration 5008, Loss: 3.2235217094421387\n",
            "Training Iteration 5009, Loss: 4.632327079772949\n",
            "Training Iteration 5010, Loss: 3.876708507537842\n",
            "Training Iteration 5011, Loss: 3.4117836952209473\n",
            "Training Iteration 5012, Loss: 3.732480049133301\n",
            "Training Iteration 5013, Loss: 4.494397163391113\n",
            "Training Iteration 5014, Loss: 5.786242485046387\n",
            "Training Iteration 5015, Loss: 3.242170810699463\n",
            "Training Iteration 5016, Loss: 2.3128275871276855\n",
            "Training Iteration 5017, Loss: 4.689165115356445\n",
            "Training Iteration 5018, Loss: 5.931529998779297\n",
            "Training Iteration 5019, Loss: 3.6941685676574707\n",
            "Training Iteration 5020, Loss: 7.031375885009766\n",
            "Training Iteration 5021, Loss: 5.3315863609313965\n",
            "Training Iteration 5022, Loss: 4.030878067016602\n",
            "Training Iteration 5023, Loss: 1.449491262435913\n",
            "Training Iteration 5024, Loss: 6.212075233459473\n",
            "Training Iteration 5025, Loss: 3.1776182651519775\n",
            "Training Iteration 5026, Loss: 4.9906415939331055\n",
            "Training Iteration 5027, Loss: 5.209197998046875\n",
            "Training Iteration 5028, Loss: 7.859267711639404\n",
            "Training Iteration 5029, Loss: 4.15400505065918\n",
            "Training Iteration 5030, Loss: 5.084136486053467\n",
            "Training Iteration 5031, Loss: 6.8260931968688965\n",
            "Training Iteration 5032, Loss: 6.538813591003418\n",
            "Training Iteration 5033, Loss: 3.861363410949707\n",
            "Training Iteration 5034, Loss: 2.420915126800537\n",
            "Training Iteration 5035, Loss: 4.978076457977295\n",
            "Training Iteration 5036, Loss: 2.0455856323242188\n",
            "Training Iteration 5037, Loss: 4.834064960479736\n",
            "Training Iteration 5038, Loss: 9.111785888671875\n",
            "Training Iteration 5039, Loss: 5.679831027984619\n",
            "Training Iteration 5040, Loss: 6.309614181518555\n",
            "Training Iteration 5041, Loss: 3.9915504455566406\n",
            "Training Iteration 5042, Loss: 2.906569004058838\n",
            "Training Iteration 5043, Loss: 3.465243101119995\n",
            "Training Iteration 5044, Loss: 4.829161167144775\n",
            "Training Iteration 5045, Loss: 3.2938148975372314\n",
            "Training Iteration 5046, Loss: 4.2734599113464355\n",
            "Training Iteration 5047, Loss: 4.015991687774658\n",
            "Training Iteration 5048, Loss: 1.9394898414611816\n",
            "Training Iteration 5049, Loss: 6.95867395401001\n",
            "Training Iteration 5050, Loss: 3.729229211807251\n",
            "Training Iteration 5051, Loss: 3.6944093704223633\n",
            "Training Iteration 5052, Loss: 5.461186408996582\n",
            "Training Iteration 5053, Loss: 5.245314598083496\n",
            "Training Iteration 5054, Loss: 2.6283023357391357\n",
            "Training Iteration 5055, Loss: 5.6716766357421875\n",
            "Training Iteration 5056, Loss: 6.165679454803467\n",
            "Training Iteration 5057, Loss: 4.833318710327148\n",
            "Training Iteration 5058, Loss: 4.472743034362793\n",
            "Training Iteration 5059, Loss: 3.8386752605438232\n",
            "Training Iteration 5060, Loss: 2.7093288898468018\n",
            "Training Iteration 5061, Loss: 4.146713733673096\n",
            "Training Iteration 5062, Loss: 6.582489013671875\n",
            "Training Iteration 5063, Loss: 3.680298328399658\n",
            "Training Iteration 5064, Loss: 7.354646682739258\n",
            "Training Iteration 5065, Loss: 4.380322456359863\n",
            "Training Iteration 5066, Loss: 5.5039520263671875\n",
            "Training Iteration 5067, Loss: 6.376603603363037\n",
            "Training Iteration 5068, Loss: 4.1990742683410645\n",
            "Training Iteration 5069, Loss: 1.9489840269088745\n",
            "Training Iteration 5070, Loss: 2.21362042427063\n",
            "Training Iteration 5071, Loss: 6.795443058013916\n",
            "Training Iteration 5072, Loss: 6.012382507324219\n",
            "Training Iteration 5073, Loss: 4.473390579223633\n",
            "Training Iteration 5074, Loss: 4.339230537414551\n",
            "Training Iteration 5075, Loss: 4.0256805419921875\n",
            "Training Iteration 5076, Loss: 4.367462158203125\n",
            "Training Iteration 5077, Loss: 5.991837024688721\n",
            "Training Iteration 5078, Loss: 6.078096389770508\n",
            "Training Iteration 5079, Loss: 5.797160625457764\n",
            "Training Iteration 5080, Loss: 3.4955286979675293\n",
            "Training Iteration 5081, Loss: 2.8019979000091553\n",
            "Training Iteration 5082, Loss: 4.325266361236572\n",
            "Training Iteration 5083, Loss: 6.4409379959106445\n",
            "Training Iteration 5084, Loss: 2.445096254348755\n",
            "Training Iteration 5085, Loss: 6.897333145141602\n",
            "Training Iteration 5086, Loss: 4.312660217285156\n",
            "Training Iteration 5087, Loss: 5.584279537200928\n",
            "Training Iteration 5088, Loss: 5.2735700607299805\n",
            "Training Iteration 5089, Loss: 4.817817211151123\n",
            "Training Iteration 5090, Loss: 4.911484241485596\n",
            "Training Iteration 5091, Loss: 3.285233736038208\n",
            "Training Iteration 5092, Loss: 3.9231631755828857\n",
            "Training Iteration 5093, Loss: 1.7557611465454102\n",
            "Training Iteration 5094, Loss: 3.667224645614624\n",
            "Training Iteration 5095, Loss: 6.7925238609313965\n",
            "Training Iteration 5096, Loss: 5.078761100769043\n",
            "Training Iteration 5097, Loss: 6.194299697875977\n",
            "Training Iteration 5098, Loss: 5.342313766479492\n",
            "Training Iteration 5099, Loss: 1.4832416772842407\n",
            "Training Iteration 5100, Loss: 7.304815769195557\n",
            "Training Iteration 5101, Loss: 3.3482890129089355\n",
            "Training Iteration 5102, Loss: 8.23869800567627\n",
            "Training Iteration 5103, Loss: 2.2270419597625732\n",
            "Training Iteration 5104, Loss: 6.416265964508057\n",
            "Training Iteration 5105, Loss: 4.551257133483887\n",
            "Training Iteration 5106, Loss: 4.852488994598389\n",
            "Training Iteration 5107, Loss: 8.55916976928711\n",
            "Training Iteration 5108, Loss: 4.107434272766113\n",
            "Training Iteration 5109, Loss: 4.091137409210205\n",
            "Training Iteration 5110, Loss: 1.4756652116775513\n",
            "Training Iteration 5111, Loss: 4.577657699584961\n",
            "Training Iteration 5112, Loss: 3.8826630115509033\n",
            "Training Iteration 5113, Loss: 5.050154685974121\n",
            "Training Iteration 5114, Loss: 7.056751251220703\n",
            "Training Iteration 5115, Loss: 5.652840614318848\n",
            "Training Iteration 5116, Loss: 4.014355659484863\n",
            "Training Iteration 5117, Loss: 2.445392608642578\n",
            "Training Iteration 5118, Loss: 2.560842514038086\n",
            "Training Iteration 5119, Loss: 6.2056193351745605\n",
            "Training Iteration 5120, Loss: 3.935844659805298\n",
            "Training Iteration 5121, Loss: 5.267441749572754\n",
            "Training Iteration 5122, Loss: 5.751642227172852\n",
            "Training Iteration 5123, Loss: 4.426365852355957\n",
            "Training Iteration 5124, Loss: 4.047253131866455\n",
            "Training Iteration 5125, Loss: 2.563478946685791\n",
            "Training Iteration 5126, Loss: 4.103409290313721\n",
            "Training Iteration 5127, Loss: 3.529665470123291\n",
            "Training Iteration 5128, Loss: 1.1004363298416138\n",
            "Training Iteration 5129, Loss: 4.753235816955566\n",
            "Training Iteration 5130, Loss: 3.997144937515259\n",
            "Training Iteration 5131, Loss: 3.5812034606933594\n",
            "Training Iteration 5132, Loss: 4.987523078918457\n",
            "Training Iteration 5133, Loss: 6.685909748077393\n",
            "Training Iteration 5134, Loss: 6.272500038146973\n",
            "Training Iteration 5135, Loss: 5.213647365570068\n",
            "Training Iteration 5136, Loss: 4.253787040710449\n",
            "Training Iteration 5137, Loss: 5.130210876464844\n",
            "Training Iteration 5138, Loss: 6.734759330749512\n",
            "Training Iteration 5139, Loss: 4.215336322784424\n",
            "Training Iteration 5140, Loss: 5.982131004333496\n",
            "Training Iteration 5141, Loss: 4.925206184387207\n",
            "Training Iteration 5142, Loss: 4.151119232177734\n",
            "Training Iteration 5143, Loss: 2.83829927444458\n",
            "Training Iteration 5144, Loss: 6.638551235198975\n",
            "Training Iteration 5145, Loss: 4.559167385101318\n",
            "Training Iteration 5146, Loss: 2.99568510055542\n",
            "Training Iteration 5147, Loss: 7.160587310791016\n",
            "Training Iteration 5148, Loss: 3.134290933609009\n",
            "Training Iteration 5149, Loss: 5.936832904815674\n",
            "Training Iteration 5150, Loss: 6.590209007263184\n",
            "Training Iteration 5151, Loss: 6.433051586151123\n",
            "Training Iteration 5152, Loss: 3.161799430847168\n",
            "Training Iteration 5153, Loss: 5.759077072143555\n",
            "Training Iteration 5154, Loss: 4.875202655792236\n",
            "Training Iteration 5155, Loss: 4.970099925994873\n",
            "Training Iteration 5156, Loss: 6.530238151550293\n",
            "Training Iteration 5157, Loss: 4.0636701583862305\n",
            "Training Iteration 5158, Loss: 3.273484945297241\n",
            "Training Iteration 5159, Loss: 2.955092191696167\n",
            "Training Iteration 5160, Loss: 7.457934379577637\n",
            "Training Iteration 5161, Loss: 4.292571067810059\n",
            "Training Iteration 5162, Loss: 5.599284648895264\n",
            "Training Iteration 5163, Loss: 7.292730331420898\n",
            "Training Iteration 5164, Loss: 10.72054672241211\n",
            "Training Iteration 5165, Loss: 2.1650843620300293\n",
            "Training Iteration 5166, Loss: 3.7125866413116455\n",
            "Training Iteration 5167, Loss: 3.5255227088928223\n",
            "Training Iteration 5168, Loss: 2.7148165702819824\n",
            "Training Iteration 5169, Loss: 3.4775564670562744\n",
            "Training Iteration 5170, Loss: 3.9687767028808594\n",
            "Training Iteration 5171, Loss: 2.326852798461914\n",
            "Training Iteration 5172, Loss: 6.546975135803223\n",
            "Training Iteration 5173, Loss: 6.212864398956299\n",
            "Training Iteration 5174, Loss: 3.768662452697754\n",
            "Training Iteration 5175, Loss: 3.453857660293579\n",
            "Training Iteration 5176, Loss: 6.063562393188477\n",
            "Training Iteration 5177, Loss: 6.4084014892578125\n",
            "Training Iteration 5178, Loss: 2.7425057888031006\n",
            "Training Iteration 5179, Loss: 6.53510856628418\n",
            "Training Iteration 5180, Loss: 4.099839210510254\n",
            "Training Iteration 5181, Loss: 5.6441121101379395\n",
            "Training Iteration 5182, Loss: 5.906310081481934\n",
            "Training Iteration 5183, Loss: 4.277442455291748\n",
            "Training Iteration 5184, Loss: 2.719726324081421\n",
            "Training Iteration 5185, Loss: 5.028404712677002\n",
            "Training Iteration 5186, Loss: 7.212738513946533\n",
            "Training Iteration 5187, Loss: 11.293539047241211\n",
            "Training Iteration 5188, Loss: 5.439572334289551\n",
            "Training Iteration 5189, Loss: 11.358405113220215\n",
            "Training Iteration 5190, Loss: 7.211446285247803\n",
            "Training Iteration 5191, Loss: 6.2917256355285645\n",
            "Training Iteration 5192, Loss: 4.4337263107299805\n",
            "Training Iteration 5193, Loss: 7.87432336807251\n",
            "Training Iteration 5194, Loss: 7.08943510055542\n",
            "Training Iteration 5195, Loss: 6.528904914855957\n",
            "Training Iteration 5196, Loss: 12.655830383300781\n",
            "Training Iteration 5197, Loss: 7.380368709564209\n",
            "Training Iteration 5198, Loss: 3.5683906078338623\n",
            "Training Iteration 5199, Loss: 5.112950325012207\n",
            "Training Iteration 5200, Loss: 3.3992316722869873\n",
            "Training Iteration 5201, Loss: 7.835865497589111\n",
            "Training Iteration 5202, Loss: 5.437756538391113\n",
            "Training Iteration 5203, Loss: 6.212878704071045\n",
            "Training Iteration 5204, Loss: 1.3373416662216187\n",
            "Training Iteration 5205, Loss: 4.172279357910156\n",
            "Training Iteration 5206, Loss: 6.533150672912598\n",
            "Training Iteration 5207, Loss: 10.733664512634277\n",
            "Training Iteration 5208, Loss: 7.229806423187256\n",
            "Training Iteration 5209, Loss: 4.9833526611328125\n",
            "Training Iteration 5210, Loss: 5.0635666847229\n",
            "Training Iteration 5211, Loss: 3.985781669616699\n",
            "Training Iteration 5212, Loss: 3.982612133026123\n",
            "Training Iteration 5213, Loss: 4.570022106170654\n",
            "Training Iteration 5214, Loss: 4.798609256744385\n",
            "Training Iteration 5215, Loss: 3.653536558151245\n",
            "Training Iteration 5216, Loss: 5.5165815353393555\n",
            "Training Iteration 5217, Loss: 4.118676662445068\n",
            "Training Iteration 5218, Loss: 5.102122783660889\n",
            "Training Iteration 5219, Loss: 6.270397186279297\n",
            "Training Iteration 5220, Loss: 8.095563888549805\n",
            "Training Iteration 5221, Loss: 7.251588821411133\n",
            "Training Iteration 5222, Loss: 9.997308731079102\n",
            "Training Iteration 5223, Loss: 7.943919658660889\n",
            "Training Iteration 5224, Loss: 8.32657527923584\n",
            "Training Iteration 5225, Loss: 12.840337753295898\n",
            "Training Iteration 5226, Loss: 9.405570030212402\n",
            "Training Iteration 5227, Loss: 7.010890483856201\n",
            "Training Iteration 5228, Loss: 6.039041519165039\n",
            "Training Iteration 5229, Loss: 5.4521894454956055\n",
            "Training Iteration 5230, Loss: 6.127267360687256\n",
            "Training Iteration 5231, Loss: 8.693757057189941\n",
            "Training Iteration 5232, Loss: 6.045223236083984\n",
            "Training Iteration 5233, Loss: 6.179646015167236\n",
            "Training Iteration 5234, Loss: 9.416171073913574\n",
            "Training Iteration 5235, Loss: 8.05242919921875\n",
            "Training Iteration 5236, Loss: 9.5563325881958\n",
            "Training Iteration 5237, Loss: 4.211163520812988\n",
            "Training Iteration 5238, Loss: 1.311951994895935\n",
            "Training Iteration 5239, Loss: 5.141718864440918\n",
            "Training Iteration 5240, Loss: 4.126077651977539\n",
            "Training Iteration 5241, Loss: 2.645211696624756\n",
            "Training Iteration 5242, Loss: 2.5024354457855225\n",
            "Training Iteration 5243, Loss: 3.586627960205078\n",
            "Training Iteration 5244, Loss: 3.479588508605957\n",
            "Training Iteration 5245, Loss: 3.4560911655426025\n",
            "Training Iteration 5246, Loss: 2.498251438140869\n",
            "Training Iteration 5247, Loss: 4.452926158905029\n",
            "Training Iteration 5248, Loss: 5.701737880706787\n",
            "Training Iteration 5249, Loss: 6.667422771453857\n",
            "Training Iteration 5250, Loss: 6.256477355957031\n",
            "Training Iteration 5251, Loss: 3.4745733737945557\n",
            "Training Iteration 5252, Loss: 3.7266733646392822\n",
            "Training Iteration 5253, Loss: 7.099670886993408\n",
            "Training Iteration 5254, Loss: 9.246655464172363\n",
            "Training Iteration 5255, Loss: 6.709303855895996\n",
            "Training Iteration 5256, Loss: 5.9203948974609375\n",
            "Training Iteration 5257, Loss: 4.7073869705200195\n",
            "Training Iteration 5258, Loss: 6.044372081756592\n",
            "Training Iteration 5259, Loss: 5.0872697830200195\n",
            "Training Iteration 5260, Loss: 4.549764633178711\n",
            "Training Iteration 5261, Loss: 4.587278842926025\n",
            "Training Iteration 5262, Loss: 5.313785076141357\n",
            "Training Iteration 5263, Loss: 5.3857645988464355\n",
            "Training Iteration 5264, Loss: 5.058885097503662\n",
            "Training Iteration 5265, Loss: 2.517040252685547\n",
            "Training Iteration 5266, Loss: 9.27557373046875\n",
            "Training Iteration 5267, Loss: 5.3464508056640625\n",
            "Training Iteration 5268, Loss: 10.251510620117188\n",
            "Training Iteration 5269, Loss: 7.826179504394531\n",
            "Training Iteration 5270, Loss: 6.899701118469238\n",
            "Training Iteration 5271, Loss: 8.436507225036621\n",
            "Training Iteration 5272, Loss: 5.085716247558594\n",
            "Training Iteration 5273, Loss: 4.62138032913208\n",
            "Training Iteration 5274, Loss: 5.564236164093018\n",
            "Training Iteration 5275, Loss: 6.035257339477539\n",
            "Training Iteration 5276, Loss: 7.399492263793945\n",
            "Training Iteration 5277, Loss: 2.4096462726593018\n",
            "Training Iteration 5278, Loss: 3.521280288696289\n",
            "Training Iteration 5279, Loss: 3.162702798843384\n",
            "Training Iteration 5280, Loss: 7.301140308380127\n",
            "Training Iteration 5281, Loss: 5.863543510437012\n",
            "Training Iteration 5282, Loss: 4.7619524002075195\n",
            "Training Iteration 5283, Loss: 3.5726094245910645\n",
            "Training Iteration 5284, Loss: 3.8051133155822754\n",
            "Training Iteration 5285, Loss: 4.822056770324707\n",
            "Training Iteration 5286, Loss: 4.417235374450684\n",
            "Training Iteration 5287, Loss: 5.025923728942871\n",
            "Training Iteration 5288, Loss: 8.00399398803711\n",
            "Training Iteration 5289, Loss: 3.666475534439087\n",
            "Training Iteration 5290, Loss: 2.728001356124878\n",
            "Training Iteration 5291, Loss: 3.596003293991089\n",
            "Training Iteration 5292, Loss: 2.891417980194092\n",
            "Training Iteration 5293, Loss: 2.5665595531463623\n",
            "Training Iteration 5294, Loss: 2.1784183979034424\n",
            "Training Iteration 5295, Loss: 3.424795150756836\n",
            "Training Iteration 5296, Loss: 10.641007423400879\n",
            "Training Iteration 5297, Loss: 3.645756483078003\n",
            "Training Iteration 5298, Loss: 5.83536434173584\n",
            "Training Iteration 5299, Loss: 2.334385871887207\n",
            "Training Iteration 5300, Loss: 4.777399063110352\n",
            "Training Iteration 5301, Loss: 5.13795804977417\n",
            "Training Iteration 5302, Loss: 5.700674057006836\n",
            "Training Iteration 5303, Loss: 3.010066509246826\n",
            "Training Iteration 5304, Loss: 4.937157154083252\n",
            "Training Iteration 5305, Loss: 4.021127223968506\n",
            "Training Iteration 5306, Loss: 4.221261024475098\n",
            "Training Iteration 5307, Loss: 3.7674877643585205\n",
            "Training Iteration 5308, Loss: 3.2568538188934326\n",
            "Training Iteration 5309, Loss: 7.184474945068359\n",
            "Training Iteration 5310, Loss: 4.668005466461182\n",
            "Training Iteration 5311, Loss: 3.6621391773223877\n",
            "Training Iteration 5312, Loss: 4.2381510734558105\n",
            "Training Iteration 5313, Loss: 3.735103130340576\n",
            "Training Iteration 5314, Loss: 4.997625350952148\n",
            "Training Iteration 5315, Loss: 4.340399742126465\n",
            "Training Iteration 5316, Loss: 4.758480072021484\n",
            "Training Iteration 5317, Loss: 1.7988865375518799\n",
            "Training Iteration 5318, Loss: 2.57708740234375\n",
            "Training Iteration 5319, Loss: 4.8255157470703125\n",
            "Training Iteration 5320, Loss: 3.7246270179748535\n",
            "Training Iteration 5321, Loss: 3.8889000415802\n",
            "Training Iteration 5322, Loss: 3.3864474296569824\n",
            "Training Iteration 5323, Loss: 8.511610984802246\n",
            "Training Iteration 5324, Loss: 5.154449462890625\n",
            "Training Iteration 5325, Loss: 4.070466995239258\n",
            "Training Iteration 5326, Loss: 5.615113258361816\n",
            "Training Iteration 5327, Loss: 4.116201877593994\n",
            "Training Iteration 5328, Loss: 2.779529333114624\n",
            "Training Iteration 5329, Loss: 5.7342143058776855\n",
            "Training Iteration 5330, Loss: 4.05777645111084\n",
            "Training Iteration 5331, Loss: 3.5221407413482666\n",
            "Training Iteration 5332, Loss: 4.500944137573242\n",
            "Training Iteration 5333, Loss: 1.7207412719726562\n",
            "Training Iteration 5334, Loss: 3.5060439109802246\n",
            "Training Iteration 5335, Loss: 3.2941293716430664\n",
            "Training Iteration 5336, Loss: 6.522504806518555\n",
            "Training Iteration 5337, Loss: 5.047048568725586\n",
            "Training Iteration 5338, Loss: 4.599953651428223\n",
            "Training Iteration 5339, Loss: 3.1314759254455566\n",
            "Training Iteration 5340, Loss: 4.262999057769775\n",
            "Training Iteration 5341, Loss: 4.157678604125977\n",
            "Training Iteration 5342, Loss: 6.071867942810059\n",
            "Training Iteration 5343, Loss: 3.3569283485412598\n",
            "Training Iteration 5344, Loss: 1.7201803922653198\n",
            "Training Iteration 5345, Loss: 6.9505615234375\n",
            "Training Iteration 5346, Loss: 3.0553464889526367\n",
            "Training Iteration 5347, Loss: 4.446812152862549\n",
            "Training Iteration 5348, Loss: 7.43122673034668\n",
            "Training Iteration 5349, Loss: 3.483802080154419\n",
            "Training Iteration 5350, Loss: 3.917133331298828\n",
            "Training Iteration 5351, Loss: 3.429589033126831\n",
            "Training Iteration 5352, Loss: 7.213967323303223\n",
            "Training Iteration 5353, Loss: 6.114811897277832\n",
            "Training Iteration 5354, Loss: 6.372995376586914\n",
            "Training Iteration 5355, Loss: 2.8644232749938965\n",
            "Training Iteration 5356, Loss: 6.333861351013184\n",
            "Training Iteration 5357, Loss: 3.0453643798828125\n",
            "Training Iteration 5358, Loss: 3.7115466594696045\n",
            "Training Iteration 5359, Loss: 3.111205816268921\n",
            "Training Iteration 5360, Loss: 5.623431205749512\n",
            "Training Iteration 5361, Loss: 1.147873044013977\n",
            "Training Iteration 5362, Loss: 8.614211082458496\n",
            "Training Iteration 5363, Loss: 8.125069618225098\n",
            "Training Iteration 5364, Loss: 3.916738748550415\n",
            "Training Iteration 5365, Loss: 2.448184013366699\n",
            "Training Iteration 5366, Loss: 3.040346145629883\n",
            "Training Iteration 5367, Loss: 6.021575927734375\n",
            "Training Iteration 5368, Loss: 4.679699897766113\n",
            "Training Iteration 5369, Loss: 11.936767578125\n",
            "Training Iteration 5370, Loss: 4.431920528411865\n",
            "Training Iteration 5371, Loss: 4.126850128173828\n",
            "Training Iteration 5372, Loss: 4.553598880767822\n",
            "Training Iteration 5373, Loss: 3.122574806213379\n",
            "Training Iteration 5374, Loss: 3.9751744270324707\n",
            "Training Iteration 5375, Loss: 6.892540454864502\n",
            "Training Iteration 5376, Loss: 4.358058452606201\n",
            "Training Iteration 5377, Loss: 9.41137409210205\n",
            "Training Iteration 5378, Loss: 9.01782512664795\n",
            "Training Iteration 5379, Loss: 4.2484846115112305\n",
            "Training Iteration 5380, Loss: 4.578312873840332\n",
            "Training Iteration 5381, Loss: 6.4394307136535645\n",
            "Training Iteration 5382, Loss: 4.070762634277344\n",
            "Training Iteration 5383, Loss: 7.599289894104004\n",
            "Training Iteration 5384, Loss: 5.229317665100098\n",
            "Training Iteration 5385, Loss: 4.920571327209473\n",
            "Training Iteration 5386, Loss: 5.8049092292785645\n",
            "Training Iteration 5387, Loss: 6.740686893463135\n",
            "Training Iteration 5388, Loss: 6.8217453956604\n",
            "Training Iteration 5389, Loss: 5.660618305206299\n",
            "Training Iteration 5390, Loss: 4.549205303192139\n",
            "Training Iteration 5391, Loss: 4.7955451011657715\n",
            "Training Iteration 5392, Loss: 5.1091108322143555\n",
            "Training Iteration 5393, Loss: 2.6203126907348633\n",
            "Training Iteration 5394, Loss: 9.343749046325684\n",
            "Training Iteration 5395, Loss: 4.493290424346924\n",
            "Training Iteration 5396, Loss: 4.689002990722656\n",
            "Training Iteration 5397, Loss: 4.128350257873535\n",
            "Training Iteration 5398, Loss: 5.918758392333984\n",
            "Training Iteration 5399, Loss: 3.076312303543091\n",
            "Training Iteration 5400, Loss: 5.215617656707764\n",
            "Training Iteration 5401, Loss: 3.7284657955169678\n",
            "Training Iteration 5402, Loss: 4.450324058532715\n",
            "Training Iteration 5403, Loss: 3.6144444942474365\n",
            "Training Iteration 5404, Loss: 1.3029166460037231\n",
            "Training Iteration 5405, Loss: 4.519069671630859\n",
            "Training Iteration 5406, Loss: 2.241751194000244\n",
            "Training Iteration 5407, Loss: 5.768836975097656\n",
            "Training Iteration 5408, Loss: 5.350438594818115\n",
            "Training Iteration 5409, Loss: 3.9469072818756104\n",
            "Training Iteration 5410, Loss: 2.110053777694702\n",
            "Training Iteration 5411, Loss: 6.533695220947266\n",
            "Training Iteration 5412, Loss: 3.6358389854431152\n",
            "Training Iteration 5413, Loss: 2.0869996547698975\n",
            "Training Iteration 5414, Loss: 6.270129203796387\n",
            "Training Iteration 5415, Loss: 5.438110828399658\n",
            "Training Iteration 5416, Loss: 3.384474992752075\n",
            "Training Iteration 5417, Loss: 4.087082862854004\n",
            "Training Iteration 5418, Loss: 5.1424479484558105\n",
            "Training Iteration 5419, Loss: 2.4831089973449707\n",
            "Training Iteration 5420, Loss: 3.6269447803497314\n",
            "Training Iteration 5421, Loss: 5.948319435119629\n",
            "Training Iteration 5422, Loss: 3.5231895446777344\n",
            "Training Iteration 5423, Loss: 6.83375358581543\n",
            "Training Iteration 5424, Loss: 3.7669596672058105\n",
            "Training Iteration 5425, Loss: 6.698599338531494\n",
            "Training Iteration 5426, Loss: 3.6276261806488037\n",
            "Training Iteration 5427, Loss: 4.486851215362549\n",
            "Training Iteration 5428, Loss: 5.021845817565918\n",
            "Training Iteration 5429, Loss: 5.175490379333496\n",
            "Training Iteration 5430, Loss: 6.060500621795654\n",
            "Training Iteration 5431, Loss: 6.884340286254883\n",
            "Training Iteration 5432, Loss: 4.0723114013671875\n",
            "Training Iteration 5433, Loss: 6.402655601501465\n",
            "Training Iteration 5434, Loss: 4.902588367462158\n",
            "Training Iteration 5435, Loss: 8.650165557861328\n",
            "Training Iteration 5436, Loss: 4.881278991699219\n",
            "Training Iteration 5437, Loss: 4.5485734939575195\n",
            "Training Iteration 5438, Loss: 2.693333148956299\n",
            "Training Iteration 5439, Loss: 7.421823501586914\n",
            "Training Iteration 5440, Loss: 3.2868590354919434\n",
            "Training Iteration 5441, Loss: 6.872289180755615\n",
            "Training Iteration 5442, Loss: 9.96292781829834\n",
            "Training Iteration 5443, Loss: 3.7627289295196533\n",
            "Training Iteration 5444, Loss: 4.7579731941223145\n",
            "Training Iteration 5445, Loss: 3.2402939796447754\n",
            "Training Iteration 5446, Loss: 6.4467549324035645\n",
            "Training Iteration 5447, Loss: 3.112323760986328\n",
            "Training Iteration 5448, Loss: 3.435918092727661\n",
            "Training Iteration 5449, Loss: 2.983956813812256\n",
            "Training Iteration 5450, Loss: 5.055329322814941\n",
            "Training Iteration 5451, Loss: 4.0385637283325195\n",
            "Training Iteration 5452, Loss: 5.397708415985107\n",
            "Training Iteration 5453, Loss: 2.3942947387695312\n",
            "Training Iteration 5454, Loss: 2.52142333984375\n",
            "Training Iteration 5455, Loss: 3.8428354263305664\n",
            "Training Iteration 5456, Loss: 2.4818341732025146\n",
            "Training Iteration 5457, Loss: 3.671030282974243\n",
            "Training Iteration 5458, Loss: 4.421946048736572\n",
            "Training Iteration 5459, Loss: 2.3350625038146973\n",
            "Training Iteration 5460, Loss: 8.18871784210205\n",
            "Training Iteration 5461, Loss: 8.061470031738281\n",
            "Training Iteration 5462, Loss: 3.374370574951172\n",
            "Training Iteration 5463, Loss: 2.7257285118103027\n",
            "Training Iteration 5464, Loss: 2.6926939487457275\n",
            "Training Iteration 5465, Loss: 7.901662826538086\n",
            "Training Iteration 5466, Loss: 6.0991339683532715\n",
            "Training Iteration 5467, Loss: 5.470139503479004\n",
            "Training Iteration 5468, Loss: 4.754530429840088\n",
            "Training Iteration 5469, Loss: 2.183706045150757\n",
            "Training Iteration 5470, Loss: 5.656283378601074\n",
            "Training Iteration 5471, Loss: 4.335052013397217\n",
            "Training Iteration 5472, Loss: 4.3280816078186035\n",
            "Training Iteration 5473, Loss: 3.780965566635132\n",
            "Training Iteration 5474, Loss: 4.166861534118652\n",
            "Training Iteration 5475, Loss: 7.619163513183594\n",
            "Training Iteration 5476, Loss: 2.9922730922698975\n",
            "Training Iteration 5477, Loss: 4.436840057373047\n",
            "Training Iteration 5478, Loss: 2.9371588230133057\n",
            "Training Iteration 5479, Loss: 4.691253662109375\n",
            "Training Iteration 5480, Loss: 5.3078413009643555\n",
            "Training Iteration 5481, Loss: 4.793731689453125\n",
            "Training Iteration 5482, Loss: 1.537327527999878\n",
            "Training Iteration 5483, Loss: 2.5374515056610107\n",
            "Training Iteration 5484, Loss: 4.1801300048828125\n",
            "Training Iteration 5485, Loss: 3.126035213470459\n",
            "Training Iteration 5486, Loss: 5.863171577453613\n",
            "Training Iteration 5487, Loss: 5.770698547363281\n",
            "Training Iteration 5488, Loss: 2.1293022632598877\n",
            "Training Iteration 5489, Loss: 1.9863982200622559\n",
            "Training Iteration 5490, Loss: 2.5900754928588867\n",
            "Training Iteration 5491, Loss: 5.377140998840332\n",
            "Training Iteration 5492, Loss: 3.418405055999756\n",
            "Training Iteration 5493, Loss: 2.743380546569824\n",
            "Training Iteration 5494, Loss: 4.753719806671143\n",
            "Training Iteration 5495, Loss: 4.1981096267700195\n",
            "Training Iteration 5496, Loss: 4.206137657165527\n",
            "Training Iteration 5497, Loss: 1.8257782459259033\n",
            "Training Iteration 5498, Loss: 6.674286365509033\n",
            "Training Iteration 5499, Loss: 4.207793712615967\n",
            "Training Iteration 5500, Loss: 5.268958568572998\n",
            "Training Iteration 5501, Loss: 2.6487789154052734\n",
            "Training Iteration 5502, Loss: 3.405914783477783\n",
            "Training Iteration 5503, Loss: 8.037031173706055\n",
            "Training Iteration 5504, Loss: 5.421816825866699\n",
            "Training Iteration 5505, Loss: 8.865771293640137\n",
            "Training Iteration 5506, Loss: 7.000982284545898\n",
            "Training Iteration 5507, Loss: 7.880668640136719\n",
            "Training Iteration 5508, Loss: 4.504419326782227\n",
            "Training Iteration 5509, Loss: 3.0512735843658447\n",
            "Training Iteration 5510, Loss: 4.376855850219727\n",
            "Training Iteration 5511, Loss: 9.672941207885742\n",
            "Training Iteration 5512, Loss: 7.646089553833008\n",
            "Training Iteration 5513, Loss: 8.405130386352539\n",
            "Training Iteration 5514, Loss: 8.730860710144043\n",
            "Training Iteration 5515, Loss: 5.963028430938721\n",
            "Training Iteration 5516, Loss: 1.936180830001831\n",
            "Training Iteration 5517, Loss: 3.967315673828125\n",
            "Training Iteration 5518, Loss: 3.3481884002685547\n",
            "Training Iteration 5519, Loss: 5.058078765869141\n",
            "Training Iteration 5520, Loss: 2.8173041343688965\n",
            "Training Iteration 5521, Loss: 6.591418743133545\n",
            "Training Iteration 5522, Loss: 3.7713425159454346\n",
            "Training Iteration 5523, Loss: 5.082081317901611\n",
            "Training Iteration 5524, Loss: 6.495120048522949\n",
            "Training Iteration 5525, Loss: 3.9675021171569824\n",
            "Training Iteration 5526, Loss: 2.9038212299346924\n",
            "Training Iteration 5527, Loss: 4.614137172698975\n",
            "Training Iteration 5528, Loss: 3.939100742340088\n",
            "Training Iteration 5529, Loss: 4.321376323699951\n",
            "Training Iteration 5530, Loss: 5.291090965270996\n",
            "Training Iteration 5531, Loss: 6.362377643585205\n",
            "Training Iteration 5532, Loss: 5.405761241912842\n",
            "Training Iteration 5533, Loss: 4.990843772888184\n",
            "Training Iteration 5534, Loss: 5.233806133270264\n",
            "Training Iteration 5535, Loss: 2.516083002090454\n",
            "Training Iteration 5536, Loss: 7.571328639984131\n",
            "Training Iteration 5537, Loss: 5.603944778442383\n",
            "Training Iteration 5538, Loss: 2.982712507247925\n",
            "Training Iteration 5539, Loss: 5.830432415008545\n",
            "Training Iteration 5540, Loss: 7.606916427612305\n",
            "Training Iteration 5541, Loss: 6.8468523025512695\n",
            "Training Iteration 5542, Loss: 5.0620198249816895\n",
            "Training Iteration 5543, Loss: 3.833544969558716\n",
            "Training Iteration 5544, Loss: 6.069950103759766\n",
            "Training Iteration 5545, Loss: 4.997962474822998\n",
            "Training Iteration 5546, Loss: 3.372058391571045\n",
            "Training Iteration 5547, Loss: 4.969578266143799\n",
            "Training Iteration 5548, Loss: 8.814001083374023\n",
            "Training Iteration 5549, Loss: 7.312803745269775\n",
            "Training Iteration 5550, Loss: 9.03709602355957\n",
            "Training Iteration 5551, Loss: 3.6853652000427246\n",
            "Training Iteration 5552, Loss: 2.775034189224243\n",
            "Training Iteration 5553, Loss: 4.6050591468811035\n",
            "Training Iteration 5554, Loss: 4.37952995300293\n",
            "Training Iteration 5555, Loss: 4.525516510009766\n",
            "Training Iteration 5556, Loss: 4.416867733001709\n",
            "Training Iteration 5557, Loss: 3.5566341876983643\n",
            "Training Iteration 5558, Loss: 4.781449794769287\n",
            "Training Iteration 5559, Loss: 4.868659496307373\n",
            "Training Iteration 5560, Loss: 4.6248040199279785\n",
            "Training Iteration 5561, Loss: 7.476754665374756\n",
            "Training Iteration 5562, Loss: 4.098141670227051\n",
            "Training Iteration 5563, Loss: 1.8979322910308838\n",
            "Training Iteration 5564, Loss: 5.973731994628906\n",
            "Training Iteration 5565, Loss: 6.037264823913574\n",
            "Training Iteration 5566, Loss: 4.040783882141113\n",
            "Training Iteration 5567, Loss: 3.0259556770324707\n",
            "Training Iteration 5568, Loss: 4.557254791259766\n",
            "Training Iteration 5569, Loss: 4.693601131439209\n",
            "Training Iteration 5570, Loss: 7.255073547363281\n",
            "Training Iteration 5571, Loss: 4.172271728515625\n",
            "Training Iteration 5572, Loss: 7.4008378982543945\n",
            "Training Iteration 5573, Loss: 6.494222640991211\n",
            "Training Iteration 5574, Loss: 9.636039733886719\n",
            "Training Iteration 5575, Loss: 3.5221176147460938\n",
            "Training Iteration 5576, Loss: 4.433385848999023\n",
            "Training Iteration 5577, Loss: 3.195488452911377\n",
            "Training Iteration 5578, Loss: 4.859306335449219\n",
            "Training Iteration 5579, Loss: 3.9220781326293945\n",
            "Training Iteration 5580, Loss: 1.9158365726470947\n",
            "Training Iteration 5581, Loss: 7.247820854187012\n",
            "Training Iteration 5582, Loss: 2.852348804473877\n",
            "Training Iteration 5583, Loss: 7.671517848968506\n",
            "Training Iteration 5584, Loss: 8.63965892791748\n",
            "Training Iteration 5585, Loss: 9.309953689575195\n",
            "Training Iteration 5586, Loss: 4.107593059539795\n",
            "Training Iteration 5587, Loss: 2.4299144744873047\n",
            "Training Iteration 5588, Loss: 3.252941608428955\n",
            "Training Iteration 5589, Loss: 5.576620578765869\n",
            "Training Iteration 5590, Loss: 3.5443577766418457\n",
            "Training Iteration 5591, Loss: 4.933224201202393\n",
            "Training Iteration 5592, Loss: 9.785889625549316\n",
            "Training Iteration 5593, Loss: 4.806373596191406\n",
            "Training Iteration 5594, Loss: 5.285477638244629\n",
            "Training Iteration 5595, Loss: 3.343839645385742\n",
            "Training Iteration 5596, Loss: 2.9252378940582275\n",
            "Training Iteration 5597, Loss: 7.140510559082031\n",
            "Training Iteration 5598, Loss: 5.6156816482543945\n",
            "Training Iteration 5599, Loss: 4.931990146636963\n",
            "Training Iteration 5600, Loss: 3.6162161827087402\n",
            "Training Iteration 5601, Loss: 3.95902681350708\n",
            "Training Iteration 5602, Loss: 3.394047737121582\n",
            "Training Iteration 5603, Loss: 3.2361884117126465\n",
            "Training Iteration 5604, Loss: 3.75907301902771\n",
            "Training Iteration 5605, Loss: 3.767275810241699\n",
            "Training Iteration 5606, Loss: 2.624652147293091\n",
            "Training Iteration 5607, Loss: 5.700582981109619\n",
            "Training Iteration 5608, Loss: 3.16068959236145\n",
            "Training Iteration 5609, Loss: 7.007274627685547\n",
            "Training Iteration 5610, Loss: 3.197843074798584\n",
            "Training Iteration 5611, Loss: 2.5479063987731934\n",
            "Training Iteration 5612, Loss: 5.460424900054932\n",
            "Training Iteration 5613, Loss: 3.81581711769104\n",
            "Training Iteration 5614, Loss: 6.5255937576293945\n",
            "Training Iteration 5615, Loss: 5.802783012390137\n",
            "Training Iteration 5616, Loss: 6.061773300170898\n",
            "Training Iteration 5617, Loss: 5.8438401222229\n",
            "Training Iteration 5618, Loss: 4.640690803527832\n",
            "Training Iteration 5619, Loss: 1.7265233993530273\n",
            "Training Iteration 5620, Loss: 5.129331588745117\n",
            "Training Iteration 5621, Loss: 3.3665151596069336\n",
            "Training Iteration 5622, Loss: 4.09674072265625\n",
            "Training Iteration 5623, Loss: 5.2851762771606445\n",
            "Training Iteration 5624, Loss: 4.137230396270752\n",
            "Training Iteration 5625, Loss: 5.574907302856445\n",
            "Training Iteration 5626, Loss: 6.125993728637695\n",
            "Training Iteration 5627, Loss: 3.9076688289642334\n",
            "Training Iteration 5628, Loss: 4.354875564575195\n",
            "Training Iteration 5629, Loss: 5.8394389152526855\n",
            "Training Iteration 5630, Loss: 4.642992973327637\n",
            "Training Iteration 5631, Loss: 7.370553016662598\n",
            "Training Iteration 5632, Loss: 4.3841166496276855\n",
            "Training Iteration 5633, Loss: 4.3087687492370605\n",
            "Training Iteration 5634, Loss: 5.767793655395508\n",
            "Training Iteration 5635, Loss: 4.45212459564209\n",
            "Training Iteration 5636, Loss: 5.907483100891113\n",
            "Training Iteration 5637, Loss: 5.142887115478516\n",
            "Training Iteration 5638, Loss: 7.6441121101379395\n",
            "Training Iteration 5639, Loss: 4.879584789276123\n",
            "Training Iteration 5640, Loss: 4.054285049438477\n",
            "Training Iteration 5641, Loss: 5.513796806335449\n",
            "Training Iteration 5642, Loss: 5.2615509033203125\n",
            "Training Iteration 5643, Loss: 4.690453052520752\n",
            "Training Iteration 5644, Loss: 2.8234078884124756\n",
            "Training Iteration 5645, Loss: 7.653186798095703\n",
            "Training Iteration 5646, Loss: 4.055544376373291\n",
            "Training Iteration 5647, Loss: 5.76862096786499\n",
            "Training Iteration 5648, Loss: 3.193185806274414\n",
            "Training Iteration 5649, Loss: 4.196296691894531\n",
            "Training Iteration 5650, Loss: 7.550689697265625\n",
            "Training Iteration 5651, Loss: 3.877640724182129\n",
            "Training Iteration 5652, Loss: 4.21461296081543\n",
            "Training Iteration 5653, Loss: 4.140636920928955\n",
            "Training Iteration 5654, Loss: 4.917545795440674\n",
            "Training Iteration 5655, Loss: 6.751215934753418\n",
            "Training Iteration 5656, Loss: 14.437052726745605\n",
            "Training Iteration 5657, Loss: 3.2352294921875\n",
            "Training Iteration 5658, Loss: 5.722801685333252\n",
            "Training Iteration 5659, Loss: 4.748181343078613\n",
            "Training Iteration 5660, Loss: 6.820003986358643\n",
            "Training Iteration 5661, Loss: 4.2784833908081055\n",
            "Training Iteration 5662, Loss: 4.798705101013184\n",
            "Training Iteration 5663, Loss: 7.122171878814697\n",
            "Training Iteration 5664, Loss: 5.336238861083984\n",
            "Training Iteration 5665, Loss: 5.519264221191406\n",
            "Training Iteration 5666, Loss: 6.967552661895752\n",
            "Training Iteration 5667, Loss: 5.170875072479248\n",
            "Training Iteration 5668, Loss: 4.492028713226318\n",
            "Training Iteration 5669, Loss: 3.2358241081237793\n",
            "Training Iteration 5670, Loss: 4.135138988494873\n",
            "Training Iteration 5671, Loss: 7.78012752532959\n",
            "Training Iteration 5672, Loss: 2.457921028137207\n",
            "Training Iteration 5673, Loss: 3.964930295944214\n",
            "Training Iteration 5674, Loss: 4.0096001625061035\n",
            "Training Iteration 5675, Loss: 5.785332202911377\n",
            "Training Iteration 5676, Loss: 7.348728656768799\n",
            "Training Iteration 5677, Loss: 6.585162162780762\n",
            "Training Iteration 5678, Loss: 5.830264568328857\n",
            "Training Iteration 5679, Loss: 2.84257435798645\n",
            "Training Iteration 5680, Loss: 1.8295135498046875\n",
            "Training Iteration 5681, Loss: 6.270345687866211\n",
            "Training Iteration 5682, Loss: 5.061525821685791\n",
            "Training Iteration 5683, Loss: 5.412674903869629\n",
            "Training Iteration 5684, Loss: 5.2158355712890625\n",
            "Training Iteration 5685, Loss: 5.635846138000488\n",
            "Training Iteration 5686, Loss: 3.7177371978759766\n",
            "Training Iteration 5687, Loss: 4.82721471786499\n",
            "Training Iteration 5688, Loss: 3.558077335357666\n",
            "Training Iteration 5689, Loss: 4.574339866638184\n",
            "Training Iteration 5690, Loss: 3.6609644889831543\n",
            "Training Iteration 5691, Loss: 4.026860237121582\n",
            "Training Iteration 5692, Loss: 5.015380382537842\n",
            "Training Iteration 5693, Loss: 4.809859752655029\n",
            "Training Iteration 5694, Loss: 6.863102912902832\n",
            "Training Iteration 5695, Loss: 4.946168899536133\n",
            "Training Iteration 5696, Loss: 2.998997449874878\n",
            "Training Iteration 5697, Loss: 4.702437400817871\n",
            "Training Iteration 5698, Loss: 4.548854827880859\n",
            "Training Iteration 5699, Loss: 3.592501640319824\n",
            "Training Iteration 5700, Loss: 10.18520450592041\n",
            "Training Iteration 5701, Loss: 7.260045528411865\n",
            "Training Iteration 5702, Loss: 5.590951442718506\n",
            "Training Iteration 5703, Loss: 2.740108013153076\n",
            "Training Iteration 5704, Loss: 1.849043846130371\n",
            "Training Iteration 5705, Loss: 4.547977447509766\n",
            "Training Iteration 5706, Loss: 4.2341718673706055\n",
            "Training Iteration 5707, Loss: 5.19009256362915\n",
            "Training Iteration 5708, Loss: 3.3561813831329346\n",
            "Training Iteration 5709, Loss: 1.0257760286331177\n",
            "Training Iteration 5710, Loss: 2.7884647846221924\n",
            "Training Iteration 5711, Loss: 5.941583156585693\n",
            "Training Iteration 5712, Loss: 5.486616134643555\n",
            "Training Iteration 5713, Loss: 5.493483543395996\n",
            "Training Iteration 5714, Loss: 4.779199123382568\n",
            "Training Iteration 5715, Loss: 2.421905517578125\n",
            "Training Iteration 5716, Loss: 2.975858449935913\n",
            "Training Iteration 5717, Loss: 3.3300414085388184\n",
            "Training Iteration 5718, Loss: 4.432438850402832\n",
            "Training Iteration 5719, Loss: 4.217616558074951\n",
            "Training Iteration 5720, Loss: 3.7901976108551025\n",
            "Training Iteration 5721, Loss: 7.096478462219238\n",
            "Training Iteration 5722, Loss: 7.309953689575195\n",
            "Training Iteration 5723, Loss: 3.3525466918945312\n",
            "Training Iteration 5724, Loss: 5.223550796508789\n",
            "Training Iteration 5725, Loss: 5.1632981300354\n",
            "Training Iteration 5726, Loss: 8.178773880004883\n",
            "Training Iteration 5727, Loss: 2.8867685794830322\n",
            "Training Iteration 5728, Loss: 9.646482467651367\n",
            "Training Iteration 5729, Loss: 4.847414016723633\n",
            "Training Iteration 5730, Loss: 6.0780768394470215\n",
            "Training Iteration 5731, Loss: 6.854893684387207\n",
            "Training Iteration 5732, Loss: 4.472968578338623\n",
            "Training Iteration 5733, Loss: 3.5946762561798096\n",
            "Training Iteration 5734, Loss: 3.633681535720825\n",
            "Training Iteration 5735, Loss: 3.0948991775512695\n",
            "Training Iteration 5736, Loss: 4.621942520141602\n",
            "Training Iteration 5737, Loss: 4.277344226837158\n",
            "Training Iteration 5738, Loss: 3.2479910850524902\n",
            "Training Iteration 5739, Loss: 3.8452329635620117\n",
            "Training Iteration 5740, Loss: 6.6417155265808105\n",
            "Training Iteration 5741, Loss: 5.105913162231445\n",
            "Training Iteration 5742, Loss: 4.286761283874512\n",
            "Training Iteration 5743, Loss: 4.5940632820129395\n",
            "Training Iteration 5744, Loss: 6.270082950592041\n",
            "Training Iteration 5745, Loss: 4.276717662811279\n",
            "Training Iteration 5746, Loss: 4.6311516761779785\n",
            "Training Iteration 5747, Loss: 9.386927604675293\n",
            "Training Iteration 5748, Loss: 4.545224666595459\n",
            "Training Iteration 5749, Loss: 3.101792812347412\n",
            "Training Iteration 5750, Loss: 4.57579231262207\n",
            "Training Iteration 5751, Loss: 5.067548751831055\n",
            "Training Iteration 5752, Loss: 3.9884982109069824\n",
            "Training Iteration 5753, Loss: 4.298830032348633\n",
            "Training Iteration 5754, Loss: 2.827434778213501\n",
            "Training Iteration 5755, Loss: 3.053072690963745\n",
            "Training Iteration 5756, Loss: 6.007165908813477\n",
            "Training Iteration 5757, Loss: 2.827549457550049\n",
            "Training Iteration 5758, Loss: 3.427788257598877\n",
            "Training Iteration 5759, Loss: 6.9867262840271\n",
            "Training Iteration 5760, Loss: 7.67353630065918\n",
            "Training Iteration 5761, Loss: 2.2950031757354736\n",
            "Training Iteration 5762, Loss: 2.765615940093994\n",
            "Training Iteration 5763, Loss: 3.913294792175293\n",
            "Training Iteration 5764, Loss: 4.81397008895874\n",
            "Training Iteration 5765, Loss: 4.017989635467529\n",
            "Training Iteration 5766, Loss: 7.731147289276123\n",
            "Training Iteration 5767, Loss: 4.4043989181518555\n",
            "Training Iteration 5768, Loss: 3.5726919174194336\n",
            "Training Iteration 5769, Loss: 4.84079122543335\n",
            "Training Iteration 5770, Loss: 4.816864013671875\n",
            "Training Iteration 5771, Loss: 3.4606313705444336\n",
            "Training Iteration 5772, Loss: 4.252227783203125\n",
            "Training Iteration 5773, Loss: 3.466639995574951\n",
            "Training Iteration 5774, Loss: 5.0572614669799805\n",
            "Training Iteration 5775, Loss: 3.083395481109619\n",
            "Training Iteration 5776, Loss: 4.309752464294434\n",
            "Training Iteration 5777, Loss: 4.388824462890625\n",
            "Training Iteration 5778, Loss: 8.23470687866211\n",
            "Training Iteration 5779, Loss: 6.897920608520508\n",
            "Training Iteration 5780, Loss: 5.902132034301758\n",
            "Training Iteration 5781, Loss: 3.8152918815612793\n",
            "Training Iteration 5782, Loss: 6.372688293457031\n",
            "Training Iteration 5783, Loss: 2.4501631259918213\n",
            "Training Iteration 5784, Loss: 3.474494457244873\n",
            "Training Iteration 5785, Loss: 2.7388453483581543\n",
            "Training Iteration 5786, Loss: 4.91451358795166\n",
            "Training Iteration 5787, Loss: 4.619964122772217\n",
            "Training Iteration 5788, Loss: 2.84466552734375\n",
            "Training Iteration 5789, Loss: 2.3479232788085938\n",
            "Training Iteration 5790, Loss: 3.795053243637085\n",
            "Training Iteration 5791, Loss: 7.257834434509277\n",
            "Training Iteration 5792, Loss: 4.566338539123535\n",
            "Training Iteration 5793, Loss: 3.2953879833221436\n",
            "Training Iteration 5794, Loss: 4.682633399963379\n",
            "Training Iteration 5795, Loss: 6.783663749694824\n",
            "Training Iteration 5796, Loss: 4.002291679382324\n",
            "Training Iteration 5797, Loss: 6.7935261726379395\n",
            "Training Iteration 5798, Loss: 5.342677116394043\n",
            "Training Iteration 5799, Loss: 4.362747669219971\n",
            "Training Iteration 5800, Loss: 2.0350160598754883\n",
            "Training Iteration 5801, Loss: 3.0743842124938965\n",
            "Training Iteration 5802, Loss: 5.0107316970825195\n",
            "Training Iteration 5803, Loss: 4.443394184112549\n",
            "Training Iteration 5804, Loss: 3.7514188289642334\n",
            "Training Iteration 5805, Loss: 6.395505428314209\n",
            "Training Iteration 5806, Loss: 2.7936971187591553\n",
            "Training Iteration 5807, Loss: 9.322477340698242\n",
            "Training Iteration 5808, Loss: 4.0202717781066895\n",
            "Training Iteration 5809, Loss: 4.765716552734375\n",
            "Training Iteration 5810, Loss: 5.720613479614258\n",
            "Training Iteration 5811, Loss: 3.7493896484375\n",
            "Training Iteration 5812, Loss: 5.161409854888916\n",
            "Training Iteration 5813, Loss: 4.992338180541992\n",
            "Training Iteration 5814, Loss: 2.2020068168640137\n",
            "Training Iteration 5815, Loss: 3.3826098442077637\n",
            "Training Iteration 5816, Loss: 3.287816286087036\n",
            "Training Iteration 5817, Loss: 1.6302767992019653\n",
            "Training Iteration 5818, Loss: 4.933538913726807\n",
            "Training Iteration 5819, Loss: 4.177629470825195\n",
            "Training Iteration 5820, Loss: 4.544471740722656\n",
            "Training Iteration 5821, Loss: 3.11395263671875\n",
            "Training Iteration 5822, Loss: 4.410879611968994\n",
            "Training Iteration 5823, Loss: 3.573294162750244\n",
            "Training Iteration 5824, Loss: 2.752805471420288\n",
            "Training Iteration 5825, Loss: 7.212718486785889\n",
            "Training Iteration 5826, Loss: 4.549635410308838\n",
            "Training Iteration 5827, Loss: 4.837547779083252\n",
            "Training Iteration 5828, Loss: 6.936983585357666\n",
            "Training Iteration 5829, Loss: 5.22401237487793\n",
            "Training Iteration 5830, Loss: 0.9210138320922852\n",
            "Training Iteration 5831, Loss: 2.7778196334838867\n",
            "Training Iteration 5832, Loss: 3.679610252380371\n",
            "Training Iteration 5833, Loss: 4.331929683685303\n",
            "Training Iteration 5834, Loss: 5.747145652770996\n",
            "Training Iteration 5835, Loss: 3.3516311645507812\n",
            "Training Iteration 5836, Loss: 4.247609615325928\n",
            "Training Iteration 5837, Loss: 2.715787410736084\n",
            "Training Iteration 5838, Loss: 3.984523057937622\n",
            "Training Iteration 5839, Loss: 3.261925458908081\n",
            "Training Iteration 5840, Loss: 8.441507339477539\n",
            "Training Iteration 5841, Loss: 2.742614269256592\n",
            "Training Iteration 5842, Loss: 3.900822162628174\n",
            "Training Iteration 5843, Loss: 8.999813079833984\n",
            "Training Iteration 5844, Loss: 4.746516227722168\n",
            "Training Iteration 5845, Loss: 6.069031238555908\n",
            "Training Iteration 5846, Loss: 3.7078592777252197\n",
            "Training Iteration 5847, Loss: 2.877838611602783\n",
            "Training Iteration 5848, Loss: 2.5886733531951904\n",
            "Training Iteration 5849, Loss: 6.3357696533203125\n",
            "Training Iteration 5850, Loss: 3.8660333156585693\n",
            "Training Iteration 5851, Loss: 4.1781511306762695\n",
            "Training Iteration 5852, Loss: 6.4267168045043945\n",
            "Training Iteration 5853, Loss: 2.1936583518981934\n",
            "Training Iteration 5854, Loss: 6.866399765014648\n",
            "Training Iteration 5855, Loss: 2.916745901107788\n",
            "Training Iteration 5856, Loss: 5.456899166107178\n",
            "Training Iteration 5857, Loss: 9.620616912841797\n",
            "Training Iteration 5858, Loss: 2.858772039413452\n",
            "Training Iteration 5859, Loss: 3.063950300216675\n",
            "Training Iteration 5860, Loss: 8.74962329864502\n",
            "Training Iteration 5861, Loss: 7.0842204093933105\n",
            "Training Iteration 5862, Loss: 6.078912258148193\n",
            "Training Iteration 5863, Loss: 3.4270009994506836\n",
            "Training Iteration 5864, Loss: 2.942955255508423\n",
            "Training Iteration 5865, Loss: 3.6509861946105957\n",
            "Training Iteration 5866, Loss: 3.9775514602661133\n",
            "Training Iteration 5867, Loss: 4.61240816116333\n",
            "Training Iteration 5868, Loss: 5.692154407501221\n",
            "Training Iteration 5869, Loss: 6.257330417633057\n",
            "Training Iteration 5870, Loss: 4.2132415771484375\n",
            "Training Iteration 5871, Loss: 2.7071533203125\n",
            "Training Iteration 5872, Loss: 6.369575500488281\n",
            "Training Iteration 5873, Loss: 4.643665313720703\n",
            "Training Iteration 5874, Loss: 3.434337854385376\n",
            "Training Iteration 5875, Loss: 2.734041213989258\n",
            "Training Iteration 5876, Loss: 6.134588718414307\n",
            "Training Iteration 5877, Loss: 6.253159523010254\n",
            "Training Iteration 5878, Loss: 2.2301673889160156\n",
            "Training Iteration 5879, Loss: 5.038520336151123\n",
            "Training Iteration 5880, Loss: 3.4000580310821533\n",
            "Training Iteration 5881, Loss: 3.760411500930786\n",
            "Training Iteration 5882, Loss: 2.2147624492645264\n",
            "Training Iteration 5883, Loss: 3.364682674407959\n",
            "Training Iteration 5884, Loss: 4.00601863861084\n",
            "Training Iteration 5885, Loss: 6.647149085998535\n",
            "Training Iteration 5886, Loss: 5.738205432891846\n",
            "Training Iteration 5887, Loss: 4.169093608856201\n",
            "Training Iteration 5888, Loss: 4.002171993255615\n",
            "Training Iteration 5889, Loss: 4.495536804199219\n",
            "Training Iteration 5890, Loss: 4.755319595336914\n",
            "Training Iteration 5891, Loss: 4.666859149932861\n",
            "Training Iteration 5892, Loss: 4.067018985748291\n",
            "Training Iteration 5893, Loss: 7.4147748947143555\n",
            "Training Iteration 5894, Loss: 4.049267768859863\n",
            "Training Iteration 5895, Loss: 4.798982620239258\n",
            "Training Iteration 5896, Loss: 5.311701774597168\n",
            "Training Iteration 5897, Loss: 4.042389869689941\n",
            "Training Iteration 5898, Loss: 3.200732707977295\n",
            "Training Iteration 5899, Loss: 4.917314052581787\n",
            "Training Iteration 5900, Loss: 5.60983943939209\n",
            "Training Iteration 5901, Loss: 3.255760431289673\n",
            "Training Iteration 5902, Loss: 4.747541427612305\n",
            "Training Iteration 5903, Loss: 4.373959541320801\n",
            "Training Iteration 5904, Loss: 6.026078224182129\n",
            "Training Iteration 5905, Loss: 8.867897987365723\n",
            "Training Iteration 5906, Loss: 4.404500961303711\n",
            "Training Iteration 5907, Loss: 1.8799563646316528\n",
            "Training Iteration 5908, Loss: 7.588191986083984\n",
            "Training Iteration 5909, Loss: 3.7429752349853516\n",
            "Training Iteration 5910, Loss: 4.420595645904541\n",
            "Training Iteration 5911, Loss: 5.096490859985352\n",
            "Training Iteration 5912, Loss: 3.3748340606689453\n",
            "Training Iteration 5913, Loss: 5.042062282562256\n",
            "Training Iteration 5914, Loss: 8.220466613769531\n",
            "Training Iteration 5915, Loss: 2.518899440765381\n",
            "Training Iteration 5916, Loss: 3.6043362617492676\n",
            "Training Iteration 5917, Loss: 3.0185914039611816\n",
            "Training Iteration 5918, Loss: 5.504685401916504\n",
            "Training Iteration 5919, Loss: 5.4345173835754395\n",
            "Training Iteration 5920, Loss: 7.460182189941406\n",
            "Training Iteration 5921, Loss: 4.9026055335998535\n",
            "Training Iteration 5922, Loss: 7.518734931945801\n",
            "Training Iteration 5923, Loss: 5.14915657043457\n",
            "Training Iteration 5924, Loss: 6.685776710510254\n",
            "Training Iteration 5925, Loss: 5.388697624206543\n",
            "Training Iteration 5926, Loss: 4.275518894195557\n",
            "Training Iteration 5927, Loss: 6.526198387145996\n",
            "Training Iteration 5928, Loss: 5.837717056274414\n",
            "Training Iteration 5929, Loss: 5.208094596862793\n",
            "Training Iteration 5930, Loss: 3.454667806625366\n",
            "Training Iteration 5931, Loss: 1.1763567924499512\n",
            "Training Iteration 5932, Loss: 4.317241191864014\n",
            "Training Iteration 5933, Loss: 3.725757122039795\n",
            "Training Iteration 5934, Loss: 5.9497480392456055\n",
            "Training Iteration 5935, Loss: 3.1654560565948486\n",
            "Training Iteration 5936, Loss: 4.0063276290893555\n",
            "Training Iteration 5937, Loss: 4.613248825073242\n",
            "Training Iteration 5938, Loss: 5.185650825500488\n",
            "Training Iteration 5939, Loss: 5.6272406578063965\n",
            "Training Iteration 5940, Loss: 5.565694332122803\n",
            "Training Iteration 5941, Loss: 6.859884738922119\n",
            "Training Iteration 5942, Loss: 4.104056358337402\n",
            "Training Iteration 5943, Loss: 3.8298912048339844\n",
            "Training Iteration 5944, Loss: 3.978344678878784\n",
            "Training Iteration 5945, Loss: 5.057877063751221\n",
            "Training Iteration 5946, Loss: 5.151738166809082\n",
            "Training Iteration 5947, Loss: 4.394294261932373\n",
            "Training Iteration 5948, Loss: 6.5099687576293945\n",
            "Training Iteration 5949, Loss: 5.195643901824951\n",
            "Training Iteration 5950, Loss: 4.734706401824951\n",
            "Training Iteration 5951, Loss: 7.776972770690918\n",
            "Training Iteration 5952, Loss: 4.971042633056641\n",
            "Training Iteration 5953, Loss: 4.329296112060547\n",
            "Training Iteration 5954, Loss: 3.976020097732544\n",
            "Training Iteration 5955, Loss: 2.9072046279907227\n",
            "Training Iteration 5956, Loss: 6.282444953918457\n",
            "Training Iteration 5957, Loss: 3.4600698947906494\n",
            "Training Iteration 5958, Loss: 5.324963569641113\n",
            "Training Iteration 5959, Loss: 6.3636908531188965\n",
            "Training Iteration 5960, Loss: 2.5367467403411865\n",
            "Training Iteration 5961, Loss: 4.796278953552246\n",
            "Training Iteration 5962, Loss: 3.953670024871826\n",
            "Training Iteration 5963, Loss: 5.704160690307617\n",
            "Training Iteration 5964, Loss: 3.3183164596557617\n",
            "Training Iteration 5965, Loss: 1.7007701396942139\n",
            "Training Iteration 5966, Loss: 4.028682231903076\n",
            "Training Iteration 5967, Loss: 1.4372141361236572\n",
            "Training Iteration 5968, Loss: 2.639129638671875\n",
            "Training Iteration 5969, Loss: 8.280735969543457\n",
            "Training Iteration 5970, Loss: 7.265690803527832\n",
            "Training Iteration 5971, Loss: 5.490676403045654\n",
            "Training Iteration 5972, Loss: 6.405861854553223\n",
            "Training Iteration 5973, Loss: 3.86214017868042\n",
            "Training Iteration 5974, Loss: 4.568637847900391\n",
            "Training Iteration 5975, Loss: 2.129514455795288\n",
            "Training Iteration 5976, Loss: 7.730047225952148\n",
            "Training Iteration 5977, Loss: 4.162439346313477\n",
            "Training Iteration 5978, Loss: 2.0176572799682617\n",
            "Training Iteration 5979, Loss: 3.7108359336853027\n",
            "Training Iteration 5980, Loss: 2.9764668941497803\n",
            "Training Iteration 5981, Loss: 6.479125022888184\n",
            "Training Iteration 5982, Loss: 4.444238185882568\n",
            "Training Iteration 5983, Loss: 3.411965847015381\n",
            "Training Iteration 5984, Loss: 4.414519309997559\n",
            "Training Iteration 5985, Loss: 4.059138774871826\n",
            "Training Iteration 5986, Loss: 3.537903070449829\n",
            "Training Iteration 5987, Loss: 4.4426679611206055\n",
            "Training Iteration 5988, Loss: 4.095973968505859\n",
            "Training Iteration 5989, Loss: 3.6064400672912598\n",
            "Training Iteration 5990, Loss: 4.353262424468994\n",
            "Training Iteration 5991, Loss: 4.354990005493164\n",
            "Training Iteration 5992, Loss: 5.7047576904296875\n",
            "Training Iteration 5993, Loss: 4.88976526260376\n",
            "Training Iteration 5994, Loss: 6.191650867462158\n",
            "Training Iteration 5995, Loss: 3.8493001461029053\n",
            "Training Iteration 5996, Loss: 6.635040760040283\n",
            "Training Iteration 5997, Loss: 4.957127571105957\n",
            "Training Iteration 5998, Loss: 2.311918258666992\n",
            "Training Iteration 5999, Loss: 4.714993000030518\n",
            "Training Iteration 6000, Loss: 4.095556735992432\n",
            "Training Iteration 6001, Loss: 4.817108631134033\n",
            "Training Iteration 6002, Loss: 3.466217517852783\n",
            "Training Iteration 6003, Loss: 2.2456250190734863\n",
            "Training Iteration 6004, Loss: 4.79530143737793\n",
            "Training Iteration 6005, Loss: 7.621586799621582\n",
            "Training Iteration 6006, Loss: 7.337307453155518\n",
            "Training Iteration 6007, Loss: 0.5831220746040344\n",
            "Training Iteration 6008, Loss: 6.438459396362305\n",
            "Training Iteration 6009, Loss: 5.034790992736816\n",
            "Training Iteration 6010, Loss: 6.935954570770264\n",
            "Training Iteration 6011, Loss: 4.644040107727051\n",
            "Training Iteration 6012, Loss: 7.285955905914307\n",
            "Training Iteration 6013, Loss: 5.483854293823242\n",
            "Training Iteration 6014, Loss: 3.818972587585449\n",
            "Training Iteration 6015, Loss: 6.068670749664307\n",
            "Training Iteration 6016, Loss: 4.161319255828857\n",
            "Training Iteration 6017, Loss: 7.167835712432861\n",
            "Training Iteration 6018, Loss: 3.0504865646362305\n",
            "Training Iteration 6019, Loss: 4.439916610717773\n",
            "Training Iteration 6020, Loss: 8.196738243103027\n",
            "Training Iteration 6021, Loss: 4.446361064910889\n",
            "Training Iteration 6022, Loss: 3.584352970123291\n",
            "Training Iteration 6023, Loss: 4.8510894775390625\n",
            "Training Iteration 6024, Loss: 4.910505294799805\n",
            "Training Iteration 6025, Loss: 5.470673561096191\n",
            "Training Iteration 6026, Loss: 3.794614315032959\n",
            "Training Iteration 6027, Loss: 6.353429317474365\n",
            "Training Iteration 6028, Loss: 4.128050327301025\n",
            "Training Iteration 6029, Loss: 4.96328067779541\n",
            "Training Iteration 6030, Loss: 4.532271385192871\n",
            "Training Iteration 6031, Loss: 4.211556911468506\n",
            "Training Iteration 6032, Loss: 7.916565418243408\n",
            "Training Iteration 6033, Loss: 4.781989097595215\n",
            "Training Iteration 6034, Loss: 3.4337642192840576\n",
            "Training Iteration 6035, Loss: 4.363569736480713\n",
            "Training Iteration 6036, Loss: 2.226630687713623\n",
            "Training Iteration 6037, Loss: 4.001716613769531\n",
            "Training Iteration 6038, Loss: 3.019284963607788\n",
            "Training Iteration 6039, Loss: 4.471436500549316\n",
            "Training Iteration 6040, Loss: 5.969172954559326\n",
            "Training Iteration 6041, Loss: 5.288522720336914\n",
            "Training Iteration 6042, Loss: 2.104440212249756\n",
            "Training Iteration 6043, Loss: 3.503730535507202\n",
            "Training Iteration 6044, Loss: 2.4502646923065186\n",
            "Training Iteration 6045, Loss: 3.9157962799072266\n",
            "Training Iteration 6046, Loss: 3.9866719245910645\n",
            "Training Iteration 6047, Loss: 3.8756136894226074\n",
            "Training Iteration 6048, Loss: 4.895662784576416\n",
            "Training Iteration 6049, Loss: 5.252903461456299\n",
            "Training Iteration 6050, Loss: 5.567676067352295\n",
            "Training Iteration 6051, Loss: 4.077646255493164\n",
            "Training Iteration 6052, Loss: 8.458353996276855\n",
            "Training Iteration 6053, Loss: 4.320904731750488\n",
            "Training Iteration 6054, Loss: 2.881322145462036\n",
            "Training Iteration 6055, Loss: 3.400529623031616\n",
            "Training Iteration 6056, Loss: 4.065945148468018\n",
            "Training Iteration 6057, Loss: 5.661705017089844\n",
            "Training Iteration 6058, Loss: 5.793228626251221\n",
            "Training Iteration 6059, Loss: 4.21519136428833\n",
            "Training Iteration 6060, Loss: 3.8575234413146973\n",
            "Training Iteration 6061, Loss: 5.142913341522217\n",
            "Training Iteration 6062, Loss: 5.030807018280029\n",
            "Training Iteration 6063, Loss: 7.035684108734131\n",
            "Training Iteration 6064, Loss: 6.504519462585449\n",
            "Training Iteration 6065, Loss: 8.847813606262207\n",
            "Training Iteration 6066, Loss: 2.931128740310669\n",
            "Training Iteration 6067, Loss: 5.316008567810059\n",
            "Training Iteration 6068, Loss: 4.857937335968018\n",
            "Training Iteration 6069, Loss: 6.422754287719727\n",
            "Training Iteration 6070, Loss: 6.542814254760742\n",
            "Training Iteration 6071, Loss: 3.6004204750061035\n",
            "Training Iteration 6072, Loss: 3.93349027633667\n",
            "Training Iteration 6073, Loss: 5.382964134216309\n",
            "Training Iteration 6074, Loss: 6.2458271980285645\n",
            "Training Iteration 6075, Loss: 4.716917991638184\n",
            "Training Iteration 6076, Loss: 7.028425216674805\n",
            "Training Iteration 6077, Loss: 4.361598968505859\n",
            "Training Iteration 6078, Loss: 4.928153038024902\n",
            "Training Iteration 6079, Loss: 2.7221317291259766\n",
            "Training Iteration 6080, Loss: 4.785740375518799\n",
            "Training Iteration 6081, Loss: 4.364739418029785\n",
            "Training Iteration 6082, Loss: 8.075053215026855\n",
            "Training Iteration 6083, Loss: 5.126624584197998\n",
            "Training Iteration 6084, Loss: 5.209959983825684\n",
            "Training Iteration 6085, Loss: 5.3508453369140625\n",
            "Training Iteration 6086, Loss: 4.335519313812256\n",
            "Training Iteration 6087, Loss: 2.14202880859375\n",
            "Training Iteration 6088, Loss: 11.205946922302246\n",
            "Training Iteration 6089, Loss: 7.513086318969727\n",
            "Training Iteration 6090, Loss: 4.289651393890381\n",
            "Training Iteration 6091, Loss: 6.010371685028076\n",
            "Training Iteration 6092, Loss: 7.033050537109375\n",
            "Training Iteration 6093, Loss: 5.062344074249268\n",
            "Training Iteration 6094, Loss: 7.519575119018555\n",
            "Training Iteration 6095, Loss: 6.840179443359375\n",
            "Training Iteration 6096, Loss: 4.9086456298828125\n",
            "Training Iteration 6097, Loss: 1.529937982559204\n",
            "Training Iteration 6098, Loss: 4.821516036987305\n",
            "Training Iteration 6099, Loss: 3.6892616748809814\n",
            "Training Iteration 6100, Loss: 4.830333232879639\n",
            "Training Iteration 6101, Loss: 6.093319892883301\n",
            "Training Iteration 6102, Loss: 3.0853934288024902\n",
            "Training Iteration 6103, Loss: 5.501117706298828\n",
            "Training Iteration 6104, Loss: 6.162644386291504\n",
            "Training Iteration 6105, Loss: 4.011958599090576\n",
            "Training Iteration 6106, Loss: 6.8115715980529785\n",
            "Training Iteration 6107, Loss: 3.0758724212646484\n",
            "Training Iteration 6108, Loss: 3.558460235595703\n",
            "Training Iteration 6109, Loss: 3.5590121746063232\n",
            "Training Iteration 6110, Loss: 3.39449143409729\n",
            "Training Iteration 6111, Loss: 7.4060893058776855\n",
            "Training Iteration 6112, Loss: 3.101766347885132\n",
            "Training Iteration 6113, Loss: 3.8845975399017334\n",
            "Training Iteration 6114, Loss: 4.786929130554199\n",
            "Training Iteration 6115, Loss: 3.1265451908111572\n",
            "Training Iteration 6116, Loss: 3.622877597808838\n",
            "Training Iteration 6117, Loss: 6.214312553405762\n",
            "Training Iteration 6118, Loss: 4.315182209014893\n",
            "Training Iteration 6119, Loss: 3.492835521697998\n",
            "Training Iteration 6120, Loss: 6.908005714416504\n",
            "Training Iteration 6121, Loss: 3.315438985824585\n",
            "Training Iteration 6122, Loss: 4.072867393493652\n",
            "Training Iteration 6123, Loss: 6.266883850097656\n",
            "Training Iteration 6124, Loss: 4.966058731079102\n",
            "Training Iteration 6125, Loss: 3.7085437774658203\n",
            "Training Iteration 6126, Loss: 6.412650108337402\n",
            "Training Iteration 6127, Loss: 5.247715950012207\n",
            "Training Iteration 6128, Loss: 4.651787757873535\n",
            "Training Iteration 6129, Loss: 4.0157389640808105\n",
            "Training Iteration 6130, Loss: 4.672543048858643\n",
            "Training Iteration 6131, Loss: 7.1612138748168945\n",
            "Training Iteration 6132, Loss: 7.6324567794799805\n",
            "Training Iteration 6133, Loss: 6.966365814208984\n",
            "Training Iteration 6134, Loss: 5.365288257598877\n",
            "Training Iteration 6135, Loss: 2.789050579071045\n",
            "Training Iteration 6136, Loss: 2.694380760192871\n",
            "Training Iteration 6137, Loss: 3.197383403778076\n",
            "Training Iteration 6138, Loss: 2.633389949798584\n",
            "Training Iteration 6139, Loss: 1.898202657699585\n",
            "Training Iteration 6140, Loss: 4.9235734939575195\n",
            "Training Iteration 6141, Loss: 6.323997497558594\n",
            "Training Iteration 6142, Loss: 6.216818809509277\n",
            "Training Iteration 6143, Loss: 5.471041679382324\n",
            "Training Iteration 6144, Loss: 3.426279067993164\n",
            "Training Iteration 6145, Loss: 6.997668266296387\n",
            "Training Iteration 6146, Loss: 6.576437950134277\n",
            "Training Iteration 6147, Loss: 6.5349907875061035\n",
            "Training Iteration 6148, Loss: 4.4079766273498535\n",
            "Training Iteration 6149, Loss: 6.67475700378418\n",
            "Training Iteration 6150, Loss: 4.93620491027832\n",
            "Training Iteration 6151, Loss: 4.172874927520752\n",
            "Training Iteration 6152, Loss: 4.0723066329956055\n",
            "Training Iteration 6153, Loss: 4.1712846755981445\n",
            "Training Iteration 6154, Loss: 5.733141899108887\n",
            "Training Iteration 6155, Loss: 3.6705918312072754\n",
            "Training Iteration 6156, Loss: 2.7841649055480957\n",
            "Training Iteration 6157, Loss: 2.7480220794677734\n",
            "Training Iteration 6158, Loss: 4.4230637550354\n",
            "Training Iteration 6159, Loss: 7.755845069885254\n",
            "Training Iteration 6160, Loss: 3.380824327468872\n",
            "Training Iteration 6161, Loss: 3.8300364017486572\n",
            "Training Iteration 6162, Loss: 5.312870979309082\n",
            "Training Iteration 6163, Loss: 4.539439678192139\n",
            "Training Iteration 6164, Loss: 4.141620635986328\n",
            "Training Iteration 6165, Loss: 2.844029664993286\n",
            "Training Iteration 6166, Loss: 3.7313013076782227\n",
            "Training Iteration 6167, Loss: 2.8509702682495117\n",
            "Training Iteration 6168, Loss: 5.420572280883789\n",
            "Training Iteration 6169, Loss: 6.510792255401611\n",
            "Training Iteration 6170, Loss: 1.3845996856689453\n",
            "Training Iteration 6171, Loss: 3.4935150146484375\n",
            "Training Iteration 6172, Loss: 6.130453109741211\n",
            "Training Iteration 6173, Loss: 7.685283184051514\n",
            "Training Iteration 6174, Loss: 7.479737281799316\n",
            "Training Iteration 6175, Loss: 4.419238090515137\n",
            "Training Iteration 6176, Loss: 2.5079665184020996\n",
            "Training Iteration 6177, Loss: 5.926143169403076\n",
            "Training Iteration 6178, Loss: 3.8797519207000732\n",
            "Training Iteration 6179, Loss: 3.937286853790283\n",
            "Training Iteration 6180, Loss: 3.8626458644866943\n",
            "Training Iteration 6181, Loss: 8.240189552307129\n",
            "Training Iteration 6182, Loss: 3.7152342796325684\n",
            "Training Iteration 6183, Loss: 6.62799072265625\n",
            "Training Iteration 6184, Loss: 3.9136533737182617\n",
            "Training Iteration 6185, Loss: 3.9168145656585693\n",
            "Training Iteration 6186, Loss: 4.070753574371338\n",
            "Training Iteration 6187, Loss: 6.237078666687012\n",
            "Training Iteration 6188, Loss: 3.927924156188965\n",
            "Training Iteration 6189, Loss: 3.0613999366760254\n",
            "Training Iteration 6190, Loss: 3.440885543823242\n",
            "Training Iteration 6191, Loss: 5.109899997711182\n",
            "Training Iteration 6192, Loss: 2.788543224334717\n",
            "Training Iteration 6193, Loss: 4.584290027618408\n",
            "Training Iteration 6194, Loss: 4.8559980392456055\n",
            "Training Iteration 6195, Loss: 5.23962926864624\n",
            "Training Iteration 6196, Loss: 2.6292881965637207\n",
            "Training Iteration 6197, Loss: 2.2723441123962402\n",
            "Training Iteration 6198, Loss: 5.772745132446289\n",
            "Training Iteration 6199, Loss: 3.0700790882110596\n",
            "Training Iteration 6200, Loss: 3.052726984024048\n",
            "Training Iteration 6201, Loss: 4.258567810058594\n",
            "Training Iteration 6202, Loss: 5.658792495727539\n",
            "Training Iteration 6203, Loss: 6.422731399536133\n",
            "Training Iteration 6204, Loss: 5.540974140167236\n",
            "Training Iteration 6205, Loss: 5.006147861480713\n",
            "Training Iteration 6206, Loss: 6.440758228302002\n",
            "Training Iteration 6207, Loss: 3.639738082885742\n",
            "Training Iteration 6208, Loss: 2.6741676330566406\n",
            "Training Iteration 6209, Loss: 5.108244895935059\n",
            "Training Iteration 6210, Loss: 3.6265151500701904\n",
            "Training Iteration 6211, Loss: 6.705875873565674\n",
            "Training Iteration 6212, Loss: 7.144382476806641\n",
            "Training Iteration 6213, Loss: 3.1691677570343018\n",
            "Training Iteration 6214, Loss: 6.166607856750488\n",
            "Training Iteration 6215, Loss: 4.895092010498047\n",
            "Training Iteration 6216, Loss: 3.5671520233154297\n",
            "Training Iteration 6217, Loss: 2.4617881774902344\n",
            "Training Iteration 6218, Loss: 5.210254192352295\n",
            "Training Iteration 6219, Loss: 2.1615476608276367\n",
            "Training Iteration 6220, Loss: 9.090332984924316\n",
            "Training Iteration 6221, Loss: 4.721900939941406\n",
            "Training Iteration 6222, Loss: 4.067797660827637\n",
            "Training Iteration 6223, Loss: 2.761052131652832\n",
            "Training Iteration 6224, Loss: 2.5977046489715576\n",
            "Training Iteration 6225, Loss: 4.198139190673828\n",
            "Training Iteration 6226, Loss: 7.28572940826416\n",
            "Training Iteration 6227, Loss: 2.787684202194214\n",
            "Training Iteration 6228, Loss: 2.0712368488311768\n",
            "Training Iteration 6229, Loss: 3.206289291381836\n",
            "Training Iteration 6230, Loss: 6.410357475280762\n",
            "Training Iteration 6231, Loss: 4.320897102355957\n",
            "Training Iteration 6232, Loss: 2.045520305633545\n",
            "Training Iteration 6233, Loss: 2.781996250152588\n",
            "Training Iteration 6234, Loss: 4.499484539031982\n",
            "Training Iteration 6235, Loss: 4.414310455322266\n",
            "Training Iteration 6236, Loss: 1.7260737419128418\n",
            "Training Iteration 6237, Loss: 2.6503989696502686\n",
            "Training Iteration 6238, Loss: 3.9932005405426025\n",
            "Training Iteration 6239, Loss: 4.066437244415283\n",
            "Training Iteration 6240, Loss: 3.9507534503936768\n",
            "Training Iteration 6241, Loss: 1.65802001953125\n",
            "Training Iteration 6242, Loss: 6.985864162445068\n",
            "Training Iteration 6243, Loss: 2.651176929473877\n",
            "Training Iteration 6244, Loss: 3.3485641479492188\n",
            "Training Iteration 6245, Loss: 3.6297357082366943\n",
            "Training Iteration 6246, Loss: 5.386084079742432\n",
            "Training Iteration 6247, Loss: 3.608553171157837\n",
            "Training Iteration 6248, Loss: 3.310900926589966\n",
            "Training Iteration 6249, Loss: 3.5169105529785156\n",
            "Training Iteration 6250, Loss: 7.9042158126831055\n",
            "Training Iteration 6251, Loss: 4.624176979064941\n",
            "Training Iteration 6252, Loss: 2.536860466003418\n",
            "Training Iteration 6253, Loss: 8.057768821716309\n",
            "Training Iteration 6254, Loss: 5.892258644104004\n",
            "Training Iteration 6255, Loss: 3.772885799407959\n",
            "Training Iteration 6256, Loss: 3.4550459384918213\n",
            "Training Iteration 6257, Loss: 1.7539546489715576\n",
            "Training Iteration 6258, Loss: 3.8593156337738037\n",
            "Training Iteration 6259, Loss: 2.980762004852295\n",
            "Training Iteration 6260, Loss: 2.4103927612304688\n",
            "Training Iteration 6261, Loss: 5.183724403381348\n",
            "Training Iteration 6262, Loss: 1.483758807182312\n",
            "Training Iteration 6263, Loss: 4.893876075744629\n",
            "Training Iteration 6264, Loss: 4.778054237365723\n",
            "Training Iteration 6265, Loss: 5.011879920959473\n",
            "Training Iteration 6266, Loss: 3.289987564086914\n",
            "Training Iteration 6267, Loss: 4.14085054397583\n",
            "Training Iteration 6268, Loss: 2.618663787841797\n",
            "Training Iteration 6269, Loss: 4.099532604217529\n",
            "Training Iteration 6270, Loss: 4.863072395324707\n",
            "Training Iteration 6271, Loss: 6.160558700561523\n",
            "Training Iteration 6272, Loss: 4.415806293487549\n",
            "Training Iteration 6273, Loss: 5.201122760772705\n",
            "Training Iteration 6274, Loss: 2.5858511924743652\n",
            "Training Iteration 6275, Loss: 3.0715889930725098\n",
            "Training Iteration 6276, Loss: 2.916128158569336\n",
            "Training Iteration 6277, Loss: 2.757011890411377\n",
            "Training Iteration 6278, Loss: 5.922341346740723\n",
            "Training Iteration 6279, Loss: 2.5302557945251465\n",
            "Training Iteration 6280, Loss: 5.425440788269043\n",
            "Training Iteration 6281, Loss: 2.501190185546875\n",
            "Training Iteration 6282, Loss: 4.189736366271973\n",
            "Training Iteration 6283, Loss: 2.533778190612793\n",
            "Training Iteration 6284, Loss: 2.3166933059692383\n",
            "Training Iteration 6285, Loss: 6.58489465713501\n",
            "Training Iteration 6286, Loss: 2.3398571014404297\n",
            "Training Iteration 6287, Loss: 8.183847427368164\n",
            "Training Iteration 6288, Loss: 7.033692359924316\n",
            "Training Iteration 6289, Loss: 6.811809539794922\n",
            "Training Iteration 6290, Loss: 4.584948539733887\n",
            "Training Iteration 6291, Loss: 4.180335521697998\n",
            "Training Iteration 6292, Loss: 7.0317583084106445\n",
            "Training Iteration 6293, Loss: 2.141679048538208\n",
            "Training Iteration 6294, Loss: 7.714652061462402\n",
            "Training Iteration 6295, Loss: 5.849667072296143\n",
            "Training Iteration 6296, Loss: 6.352357864379883\n",
            "Training Iteration 6297, Loss: 6.854389190673828\n",
            "Training Iteration 6298, Loss: 5.3141632080078125\n",
            "Training Iteration 6299, Loss: 5.273757457733154\n",
            "Training Iteration 6300, Loss: 0.7945209741592407\n",
            "Training Iteration 6301, Loss: 7.801868915557861\n",
            "Training Iteration 6302, Loss: 6.056869983673096\n",
            "Training Iteration 6303, Loss: 4.843411445617676\n",
            "Training Iteration 6304, Loss: 6.763238906860352\n",
            "Training Iteration 6305, Loss: 4.176326751708984\n",
            "Training Iteration 6306, Loss: 4.928776741027832\n",
            "Training Iteration 6307, Loss: 6.041746616363525\n",
            "Training Iteration 6308, Loss: 4.306956768035889\n",
            "Training Iteration 6309, Loss: 4.292458534240723\n",
            "Training Iteration 6310, Loss: 5.259696960449219\n",
            "Training Iteration 6311, Loss: 1.9747779369354248\n",
            "Training Iteration 6312, Loss: 5.0233988761901855\n",
            "Training Iteration 6313, Loss: 4.472925186157227\n",
            "Training Iteration 6314, Loss: 3.443403959274292\n",
            "Training Iteration 6315, Loss: 5.165689945220947\n",
            "Training Iteration 6316, Loss: 4.162299156188965\n",
            "Training Iteration 6317, Loss: 4.5312933921813965\n",
            "Training Iteration 6318, Loss: 4.343692779541016\n",
            "Training Iteration 6319, Loss: 6.653942584991455\n",
            "Training Iteration 6320, Loss: 2.919057846069336\n",
            "Training Iteration 6321, Loss: 5.310890197753906\n",
            "Training Iteration 6322, Loss: 2.744586944580078\n",
            "Training Iteration 6323, Loss: 4.994839668273926\n",
            "Training Iteration 6324, Loss: 3.0151443481445312\n",
            "Training Iteration 6325, Loss: 6.2376627922058105\n",
            "Training Iteration 6326, Loss: 5.193503379821777\n",
            "Training Iteration 6327, Loss: 4.23047399520874\n",
            "Training Iteration 6328, Loss: 4.695605754852295\n",
            "Training Iteration 6329, Loss: 3.1208741664886475\n",
            "Training Iteration 6330, Loss: 5.725043773651123\n",
            "Training Iteration 6331, Loss: 4.930974006652832\n",
            "Training Iteration 6332, Loss: 3.895200252532959\n",
            "Training Iteration 6333, Loss: 7.693636894226074\n",
            "Training Iteration 6334, Loss: 5.933282375335693\n",
            "Training Iteration 6335, Loss: 8.624610900878906\n",
            "Training Iteration 6336, Loss: 6.045300006866455\n",
            "Training Iteration 6337, Loss: 10.70065689086914\n",
            "Training Iteration 6338, Loss: 5.216111660003662\n",
            "Training Iteration 6339, Loss: 4.380826473236084\n",
            "Training Iteration 6340, Loss: 3.810051441192627\n",
            "Training Iteration 6341, Loss: 5.348934650421143\n",
            "Training Iteration 6342, Loss: 4.591485023498535\n",
            "Training Iteration 6343, Loss: 7.982288360595703\n",
            "Training Iteration 6344, Loss: 4.583542823791504\n",
            "Training Iteration 6345, Loss: 2.236471652984619\n",
            "Training Iteration 6346, Loss: 2.844311237335205\n",
            "Training Iteration 6347, Loss: 1.554822564125061\n",
            "Training Iteration 6348, Loss: 4.945923328399658\n",
            "Training Iteration 6349, Loss: 1.8944389820098877\n",
            "Training Iteration 6350, Loss: 2.090726137161255\n",
            "Training Iteration 6351, Loss: 5.167214870452881\n",
            "Training Iteration 6352, Loss: 3.6680049896240234\n",
            "Training Iteration 6353, Loss: 2.5954809188842773\n",
            "Training Iteration 6354, Loss: 3.084494113922119\n",
            "Training Iteration 6355, Loss: 10.708577156066895\n",
            "Training Iteration 6356, Loss: 6.0566582679748535\n",
            "Training Iteration 6357, Loss: 6.586681365966797\n",
            "Training Iteration 6358, Loss: 3.197749137878418\n",
            "Training Iteration 6359, Loss: 2.5625686645507812\n",
            "Training Iteration 6360, Loss: 5.684366226196289\n",
            "Training Iteration 6361, Loss: 7.486028671264648\n",
            "Training Iteration 6362, Loss: 4.235228061676025\n",
            "Training Iteration 6363, Loss: 5.637191295623779\n",
            "Training Iteration 6364, Loss: 4.170906066894531\n",
            "Training Iteration 6365, Loss: 5.129388332366943\n",
            "Training Iteration 6366, Loss: 3.519315719604492\n",
            "Training Iteration 6367, Loss: 5.723038673400879\n",
            "Training Iteration 6368, Loss: 7.338152885437012\n",
            "Training Iteration 6369, Loss: 5.88844633102417\n",
            "Training Iteration 6370, Loss: 9.476357460021973\n",
            "Training Iteration 6371, Loss: 6.899439811706543\n",
            "Training Iteration 6372, Loss: 5.142977714538574\n",
            "Training Iteration 6373, Loss: 4.286481857299805\n",
            "Training Iteration 6374, Loss: 4.188408851623535\n",
            "Training Iteration 6375, Loss: 4.5756330490112305\n",
            "Training Iteration 6376, Loss: 2.8329803943634033\n",
            "Training Iteration 6377, Loss: 4.680929183959961\n",
            "Training Iteration 6378, Loss: 4.1575798988342285\n",
            "Training Iteration 6379, Loss: 4.704298973083496\n",
            "Training Iteration 6380, Loss: 3.63549542427063\n",
            "Training Iteration 6381, Loss: 2.964423179626465\n",
            "Training Iteration 6382, Loss: 3.638432741165161\n",
            "Training Iteration 6383, Loss: 8.795392990112305\n",
            "Training Iteration 6384, Loss: 5.7960591316223145\n",
            "Training Iteration 6385, Loss: 1.3105340003967285\n",
            "Training Iteration 6386, Loss: 3.30096173286438\n",
            "Training Iteration 6387, Loss: 9.77079963684082\n",
            "Training Iteration 6388, Loss: 3.651078462600708\n",
            "Training Iteration 6389, Loss: 5.142654895782471\n",
            "Training Iteration 6390, Loss: 4.2887396812438965\n",
            "Training Iteration 6391, Loss: 6.561721324920654\n",
            "Training Iteration 6392, Loss: 2.4663009643554688\n",
            "Training Iteration 6393, Loss: 5.2844085693359375\n",
            "Training Iteration 6394, Loss: 3.6264212131500244\n",
            "Training Iteration 6395, Loss: 7.607237339019775\n",
            "Training Iteration 6396, Loss: 5.069796085357666\n",
            "Training Iteration 6397, Loss: 5.790534496307373\n",
            "Training Iteration 6398, Loss: 2.3938910961151123\n",
            "Training Iteration 6399, Loss: 4.779806613922119\n",
            "Training Iteration 6400, Loss: 2.4979326725006104\n",
            "Training Iteration 6401, Loss: 7.7622222900390625\n",
            "Training Iteration 6402, Loss: 3.250277519226074\n",
            "Training Iteration 6403, Loss: 2.3043622970581055\n",
            "Training Iteration 6404, Loss: 6.412348747253418\n",
            "Training Iteration 6405, Loss: 4.659436225891113\n",
            "Training Iteration 6406, Loss: 2.9521188735961914\n",
            "Training Iteration 6407, Loss: 9.032210350036621\n",
            "Training Iteration 6408, Loss: 3.128412961959839\n",
            "Training Iteration 6409, Loss: 2.5055294036865234\n",
            "Training Iteration 6410, Loss: 7.086130142211914\n",
            "Training Iteration 6411, Loss: 4.321545124053955\n",
            "Training Iteration 6412, Loss: 3.818114995956421\n",
            "Training Iteration 6413, Loss: 2.560065984725952\n",
            "Training Iteration 6414, Loss: 6.084559917449951\n",
            "Training Iteration 6415, Loss: 5.906892776489258\n",
            "Training Iteration 6416, Loss: 4.02564001083374\n",
            "Training Iteration 6417, Loss: 2.860100746154785\n",
            "Training Iteration 6418, Loss: 4.326408863067627\n",
            "Training Iteration 6419, Loss: 6.360959053039551\n",
            "Training Iteration 6420, Loss: 6.362593650817871\n",
            "Training Iteration 6421, Loss: 5.563194274902344\n",
            "Training Iteration 6422, Loss: 3.3410069942474365\n",
            "Training Iteration 6423, Loss: 4.200751781463623\n",
            "Training Iteration 6424, Loss: 5.573155879974365\n",
            "Training Iteration 6425, Loss: 1.8048449754714966\n",
            "Training Iteration 6426, Loss: 5.72297477722168\n",
            "Training Iteration 6427, Loss: 7.084958076477051\n",
            "Training Iteration 6428, Loss: 4.251889228820801\n",
            "Training Iteration 6429, Loss: 4.440196514129639\n",
            "Training Iteration 6430, Loss: 2.9208459854125977\n",
            "Training Iteration 6431, Loss: 0.8177788853645325\n",
            "Training Iteration 6432, Loss: 7.135316848754883\n",
            "Training Iteration 6433, Loss: 5.347703456878662\n",
            "Training Iteration 6434, Loss: 6.016623497009277\n",
            "Training Iteration 6435, Loss: 3.998432159423828\n",
            "Training Iteration 6436, Loss: 3.6169259548187256\n",
            "Training Iteration 6437, Loss: 4.547577381134033\n",
            "Training Iteration 6438, Loss: 4.4969682693481445\n",
            "Training Iteration 6439, Loss: 3.0627827644348145\n",
            "Training Iteration 6440, Loss: 5.327725410461426\n",
            "Training Iteration 6441, Loss: 3.9460372924804688\n",
            "Training Iteration 6442, Loss: 2.149501085281372\n",
            "Training Iteration 6443, Loss: 4.662625312805176\n",
            "Training Iteration 6444, Loss: 5.72191047668457\n",
            "Training Iteration 6445, Loss: 1.973928451538086\n",
            "Training Iteration 6446, Loss: 5.476364612579346\n",
            "Training Iteration 6447, Loss: 4.547966003417969\n",
            "Training Iteration 6448, Loss: 3.4768733978271484\n",
            "Training Iteration 6449, Loss: 4.7827324867248535\n",
            "Training Iteration 6450, Loss: 3.853708267211914\n",
            "Training Iteration 6451, Loss: 4.246185779571533\n",
            "Training Iteration 6452, Loss: 4.827954292297363\n",
            "Training Iteration 6453, Loss: 4.293421745300293\n",
            "Training Iteration 6454, Loss: 3.0812273025512695\n",
            "Training Iteration 6455, Loss: 3.7739720344543457\n",
            "Training Iteration 6456, Loss: 3.165073871612549\n",
            "Training Iteration 6457, Loss: 2.599104166030884\n",
            "Training Iteration 6458, Loss: 2.1697239875793457\n",
            "Training Iteration 6459, Loss: 5.51240348815918\n",
            "Training Iteration 6460, Loss: 3.040274143218994\n",
            "Training Iteration 6461, Loss: 2.952000856399536\n",
            "Training Iteration 6462, Loss: 3.386465549468994\n",
            "Training Iteration 6463, Loss: 2.6111035346984863\n",
            "Training Iteration 6464, Loss: 4.3228936195373535\n",
            "Training Iteration 6465, Loss: 3.099273681640625\n",
            "Training Iteration 6466, Loss: 3.833108901977539\n",
            "Training Iteration 6467, Loss: 4.485392093658447\n",
            "Training Iteration 6468, Loss: 2.4584710597991943\n",
            "Training Iteration 6469, Loss: 5.503735065460205\n",
            "Training Iteration 6470, Loss: 6.117430210113525\n",
            "Training Iteration 6471, Loss: 4.688796043395996\n",
            "Training Iteration 6472, Loss: 3.409841775894165\n",
            "Training Iteration 6473, Loss: 5.280825614929199\n",
            "Training Iteration 6474, Loss: 5.295165538787842\n",
            "Training Iteration 6475, Loss: 6.883557319641113\n",
            "Training Iteration 6476, Loss: 7.217367172241211\n",
            "Training Iteration 6477, Loss: 6.005954742431641\n",
            "Training Iteration 6478, Loss: 4.104424953460693\n",
            "Training Iteration 6479, Loss: 4.585341930389404\n",
            "Training Iteration 6480, Loss: 4.7297444343566895\n",
            "Training Iteration 6481, Loss: 5.243535041809082\n",
            "Training Iteration 6482, Loss: 2.366448402404785\n",
            "Training Iteration 6483, Loss: 4.462125778198242\n",
            "Training Iteration 6484, Loss: 6.203157424926758\n",
            "Training Iteration 6485, Loss: 7.123438358306885\n",
            "Training Iteration 6486, Loss: 4.74569034576416\n",
            "Training Iteration 6487, Loss: 3.6768929958343506\n",
            "Training Iteration 6488, Loss: 2.0436601638793945\n",
            "Training Iteration 6489, Loss: 5.53892707824707\n",
            "Training Iteration 6490, Loss: 4.559698104858398\n",
            "Training Iteration 6491, Loss: 2.7129578590393066\n",
            "Training Iteration 6492, Loss: 4.815579891204834\n",
            "Training Iteration 6493, Loss: 4.818399906158447\n",
            "Training Iteration 6494, Loss: 5.879113674163818\n",
            "Training Iteration 6495, Loss: 3.4070558547973633\n",
            "Training Iteration 6496, Loss: 5.394052982330322\n",
            "Training Iteration 6497, Loss: 6.422645568847656\n",
            "Training Iteration 6498, Loss: 3.143975019454956\n",
            "Training Iteration 6499, Loss: 1.5560526847839355\n",
            "Training Iteration 6500, Loss: 7.346165180206299\n",
            "Training Iteration 6501, Loss: 9.718505859375\n",
            "Training Iteration 6502, Loss: 6.928913593292236\n",
            "Training Iteration 6503, Loss: 2.4798078536987305\n",
            "Training Iteration 6504, Loss: 4.504666328430176\n",
            "Training Iteration 6505, Loss: 1.5320731401443481\n",
            "Training Iteration 6506, Loss: 3.501607894897461\n",
            "Training Iteration 6507, Loss: 5.607361316680908\n",
            "Training Iteration 6508, Loss: 6.085210800170898\n",
            "Training Iteration 6509, Loss: 6.34942626953125\n",
            "Training Iteration 6510, Loss: 9.30385684967041\n",
            "Training Iteration 6511, Loss: 4.646760940551758\n",
            "Training Iteration 6512, Loss: 6.7002129554748535\n",
            "Training Iteration 6513, Loss: 2.2903809547424316\n",
            "Training Iteration 6514, Loss: 8.748443603515625\n",
            "Training Iteration 6515, Loss: 7.3577880859375\n",
            "Training Iteration 6516, Loss: 6.150057792663574\n",
            "Training Iteration 6517, Loss: 4.379186153411865\n",
            "Training Iteration 6518, Loss: 3.7929859161376953\n",
            "Training Iteration 6519, Loss: 5.119110584259033\n",
            "Training Iteration 6520, Loss: 3.8252930641174316\n",
            "Training Iteration 6521, Loss: 4.172264575958252\n",
            "Training Iteration 6522, Loss: 4.111223220825195\n",
            "Training Iteration 6523, Loss: 2.9989471435546875\n",
            "Training Iteration 6524, Loss: 2.856886386871338\n",
            "Training Iteration 6525, Loss: 5.436244964599609\n",
            "Training Iteration 6526, Loss: 5.394064903259277\n",
            "Training Iteration 6527, Loss: 5.07733154296875\n",
            "Training Iteration 6528, Loss: 2.460005044937134\n",
            "Training Iteration 6529, Loss: 3.7582693099975586\n",
            "Training Iteration 6530, Loss: 3.9217281341552734\n",
            "Training Iteration 6531, Loss: 4.393741130828857\n",
            "Training Iteration 6532, Loss: 3.3246138095855713\n",
            "Training Iteration 6533, Loss: 5.627307891845703\n",
            "Training Iteration 6534, Loss: 3.8265035152435303\n",
            "Training Iteration 6535, Loss: 6.097591400146484\n",
            "Training Iteration 6536, Loss: 2.432471752166748\n",
            "Training Iteration 6537, Loss: 5.619110107421875\n",
            "Training Iteration 6538, Loss: 1.85079824924469\n",
            "Training Iteration 6539, Loss: 2.6434271335601807\n",
            "Training Iteration 6540, Loss: 6.18286657333374\n",
            "Training Iteration 6541, Loss: 3.9080424308776855\n",
            "Training Iteration 6542, Loss: 5.918784141540527\n",
            "Training Iteration 6543, Loss: 4.348320960998535\n",
            "Training Iteration 6544, Loss: 4.086777210235596\n",
            "Training Iteration 6545, Loss: 4.743949890136719\n",
            "Training Iteration 6546, Loss: 4.924201965332031\n",
            "Training Iteration 6547, Loss: 5.252324104309082\n",
            "Training Iteration 6548, Loss: 5.497413158416748\n",
            "Training Iteration 6549, Loss: 4.236400604248047\n",
            "Training Iteration 6550, Loss: 6.963036060333252\n",
            "Training Iteration 6551, Loss: 4.871474266052246\n",
            "Training Iteration 6552, Loss: 6.515799045562744\n",
            "Training Iteration 6553, Loss: 3.423011064529419\n",
            "Training Iteration 6554, Loss: 2.580932140350342\n",
            "tensor([[3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        ...,\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06]])\n",
            "Training loss for epcoh 5: 2.1496907319279024\n",
            "Training accuracy for epoch 5: 0.21004844924274216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        ...,\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06],\n",
            "        [3.1731e-01, 1.0614e-01, 5.1673e-01, 4.7347e-02, 1.2469e-02, 4.5414e-06]])\n",
            "Validation loss for epcoh 5: 2.173406802349661\n",
            "Test accuracy for epoch 5: 0.20653086137178606\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch No: 6:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42cf010108004f9e9a749882344f9856"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Iteration 1, Loss: 4.894985675811768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 5.628978252410889\n",
            "Training Iteration 1565, Loss: 2.753277540206909\n",
            "Training Iteration 1566, Loss: 4.089610576629639\n",
            "Training Iteration 1567, Loss: 6.974461555480957\n",
            "Training Iteration 1568, Loss: 4.780025959014893\n",
            "Training Iteration 1569, Loss: 5.031686782836914\n",
            "Training Iteration 1570, Loss: 3.016155481338501\n",
            "Training Iteration 1571, Loss: 4.529686450958252\n",
            "Training Iteration 1572, Loss: 4.785595893859863\n",
            "Training Iteration 1573, Loss: 2.253812789916992\n",
            "Training Iteration 1574, Loss: 3.1708197593688965\n",
            "Training Iteration 1575, Loss: 5.106964111328125\n",
            "Training Iteration 1576, Loss: 5.183438301086426\n",
            "Training Iteration 1577, Loss: 2.1626696586608887\n",
            "Training Iteration 1578, Loss: 4.560771942138672\n",
            "Training Iteration 1579, Loss: 3.7334413528442383\n",
            "Training Iteration 1580, Loss: 4.377841472625732\n",
            "Training Iteration 1581, Loss: 4.026440143585205\n",
            "Training Iteration 1582, Loss: 3.5200607776641846\n",
            "Training Iteration 1583, Loss: 3.520968198776245\n",
            "Training Iteration 1584, Loss: 2.400676727294922\n",
            "Training Iteration 1585, Loss: 3.6502444744110107\n",
            "Training Iteration 1586, Loss: 4.597914695739746\n",
            "Training Iteration 1587, Loss: 5.461819171905518\n",
            "Training Iteration 1588, Loss: 4.619076251983643\n",
            "Training Iteration 1589, Loss: 3.437983751296997\n",
            "Training Iteration 1590, Loss: 4.00694465637207\n",
            "Training Iteration 1591, Loss: 6.49429988861084\n",
            "Training Iteration 1592, Loss: 4.2479023933410645\n",
            "Training Iteration 1593, Loss: 2.630418062210083\n",
            "Training Iteration 1594, Loss: 3.2648541927337646\n",
            "Training Iteration 1595, Loss: 4.9355340003967285\n",
            "Training Iteration 1596, Loss: 5.337918758392334\n",
            "Training Iteration 1597, Loss: 6.7156219482421875\n",
            "Training Iteration 1598, Loss: 2.952653646469116\n",
            "Training Iteration 1599, Loss: 2.2578766345977783\n",
            "Training Iteration 1600, Loss: 6.235107421875\n",
            "Training Iteration 1601, Loss: 6.366425514221191\n",
            "Training Iteration 1602, Loss: 4.119356155395508\n",
            "Training Iteration 1603, Loss: 2.7713563442230225\n",
            "Training Iteration 1604, Loss: 7.718320846557617\n",
            "Training Iteration 1605, Loss: 4.391131401062012\n",
            "Training Iteration 1606, Loss: 4.722085952758789\n",
            "Training Iteration 1607, Loss: 6.944873809814453\n",
            "Training Iteration 1608, Loss: 4.690264701843262\n",
            "Training Iteration 1609, Loss: 5.084402561187744\n",
            "Training Iteration 1610, Loss: 5.092453479766846\n",
            "Training Iteration 1611, Loss: 4.97685432434082\n",
            "Training Iteration 1612, Loss: 4.059113502502441\n",
            "Training Iteration 1613, Loss: 0.8507261276245117\n",
            "Training Iteration 1614, Loss: 4.57780647277832\n",
            "Training Iteration 1615, Loss: 3.235642910003662\n",
            "Training Iteration 1616, Loss: 7.304372787475586\n",
            "Training Iteration 1617, Loss: 3.5907199382781982\n",
            "Training Iteration 1618, Loss: 6.852543354034424\n",
            "Training Iteration 1619, Loss: 7.725437641143799\n",
            "Training Iteration 1620, Loss: 5.709935665130615\n",
            "Training Iteration 1621, Loss: 3.361876964569092\n",
            "Training Iteration 1622, Loss: 4.860619068145752\n",
            "Training Iteration 1623, Loss: 3.546154022216797\n",
            "Training Iteration 1624, Loss: 5.238770484924316\n",
            "Training Iteration 1625, Loss: 6.162041664123535\n",
            "Training Iteration 1626, Loss: 2.898099899291992\n",
            "Training Iteration 1627, Loss: 4.962289333343506\n",
            "Training Iteration 1628, Loss: 5.927002906799316\n",
            "Training Iteration 1629, Loss: 6.535309791564941\n",
            "Training Iteration 1630, Loss: 4.828159332275391\n",
            "Training Iteration 1631, Loss: 3.975581169128418\n",
            "Training Iteration 1632, Loss: 6.274631500244141\n",
            "Training Iteration 1633, Loss: 3.7961373329162598\n",
            "Training Iteration 1634, Loss: 4.920725345611572\n",
            "Training Iteration 1635, Loss: 3.0968732833862305\n",
            "Training Iteration 1636, Loss: 3.6488804817199707\n",
            "Training Iteration 1637, Loss: 4.746362209320068\n",
            "Training Iteration 1638, Loss: 2.592672109603882\n",
            "Training Iteration 1639, Loss: 2.6022942066192627\n",
            "Training Iteration 1640, Loss: 5.378118991851807\n",
            "Training Iteration 1641, Loss: 3.82586932182312\n",
            "Training Iteration 1642, Loss: 5.1922688484191895\n",
            "Training Iteration 1643, Loss: 4.841002941131592\n",
            "Training Iteration 1644, Loss: 6.106396675109863\n",
            "Training Iteration 1645, Loss: 4.846586227416992\n",
            "Training Iteration 1646, Loss: 5.980556011199951\n",
            "Training Iteration 1647, Loss: 3.704888105392456\n",
            "Training Iteration 1648, Loss: 5.659157752990723\n",
            "Training Iteration 1649, Loss: 6.119681358337402\n",
            "Training Iteration 1650, Loss: 2.709075689315796\n",
            "Training Iteration 1651, Loss: 3.642388343811035\n",
            "Training Iteration 1652, Loss: 5.835156440734863\n",
            "Training Iteration 1653, Loss: 3.2118780612945557\n",
            "Training Iteration 1654, Loss: 3.8085522651672363\n",
            "Training Iteration 1655, Loss: 3.7141871452331543\n",
            "Training Iteration 1656, Loss: 3.4039814472198486\n",
            "Training Iteration 1657, Loss: 3.3360755443573\n",
            "Training Iteration 1658, Loss: 5.159059524536133\n",
            "Training Iteration 1659, Loss: 2.926837205886841\n",
            "Training Iteration 1660, Loss: 2.8399298191070557\n",
            "Training Iteration 1661, Loss: 2.8198578357696533\n",
            "Training Iteration 1662, Loss: 3.4972751140594482\n",
            "Training Iteration 1663, Loss: 4.540865898132324\n",
            "Training Iteration 1664, Loss: 5.66706657409668\n",
            "Training Iteration 1665, Loss: 7.507777214050293\n",
            "Training Iteration 1666, Loss: 4.026766300201416\n",
            "Training Iteration 1667, Loss: 3.410740852355957\n",
            "Training Iteration 1668, Loss: 3.6659884452819824\n",
            "Training Iteration 1669, Loss: 7.213998317718506\n",
            "Training Iteration 1670, Loss: 5.353339672088623\n",
            "Training Iteration 1671, Loss: 6.517381191253662\n",
            "Training Iteration 1672, Loss: 7.097318649291992\n",
            "Training Iteration 1673, Loss: 4.465327739715576\n",
            "Training Iteration 1674, Loss: 3.3575613498687744\n",
            "Training Iteration 1675, Loss: 3.5692226886749268\n",
            "Training Iteration 1676, Loss: 5.906669616699219\n",
            "Training Iteration 1677, Loss: 5.470478534698486\n",
            "Training Iteration 1678, Loss: 5.445993900299072\n",
            "Training Iteration 1679, Loss: 6.494311809539795\n",
            "Training Iteration 1680, Loss: 4.406004905700684\n",
            "Training Iteration 1681, Loss: 3.8656163215637207\n",
            "Training Iteration 1682, Loss: 3.8784003257751465\n",
            "Training Iteration 1683, Loss: 4.20042610168457\n",
            "Training Iteration 1684, Loss: 4.643477439880371\n",
            "Training Iteration 1685, Loss: 6.968724727630615\n",
            "Training Iteration 1686, Loss: 2.9763050079345703\n",
            "Training Iteration 1687, Loss: 2.505812168121338\n",
            "Training Iteration 1688, Loss: 3.879376173019409\n",
            "Training Iteration 1689, Loss: 3.2812857627868652\n",
            "Training Iteration 1690, Loss: 3.477931022644043\n",
            "Training Iteration 1691, Loss: 1.725354790687561\n",
            "Training Iteration 1692, Loss: 8.133757591247559\n",
            "Training Iteration 1693, Loss: 3.058624267578125\n",
            "Training Iteration 1694, Loss: 3.4519057273864746\n",
            "Training Iteration 1695, Loss: 5.874272346496582\n",
            "Training Iteration 1696, Loss: 1.5455633401870728\n",
            "Training Iteration 1697, Loss: 3.6543796062469482\n",
            "Training Iteration 1698, Loss: 4.40216064453125\n",
            "Training Iteration 1699, Loss: 6.562180519104004\n",
            "Training Iteration 1700, Loss: 3.7091217041015625\n",
            "Training Iteration 1701, Loss: 6.1728715896606445\n",
            "Training Iteration 1702, Loss: 5.909976005554199\n",
            "Training Iteration 1703, Loss: 2.2492220401763916\n",
            "Training Iteration 1704, Loss: 4.665467262268066\n",
            "Training Iteration 1705, Loss: 5.239943981170654\n",
            "Training Iteration 1706, Loss: 3.1738595962524414\n",
            "Training Iteration 1707, Loss: 4.689309120178223\n",
            "Training Iteration 1708, Loss: 5.5150957107543945\n",
            "Training Iteration 1709, Loss: 1.6544476747512817\n",
            "Training Iteration 1710, Loss: 5.5216755867004395\n",
            "Training Iteration 1711, Loss: 3.381714105606079\n",
            "Training Iteration 1712, Loss: 3.1993162631988525\n",
            "Training Iteration 1713, Loss: 6.139822959899902\n",
            "Training Iteration 1714, Loss: 5.5224761962890625\n",
            "Training Iteration 1715, Loss: 3.248887538909912\n",
            "Training Iteration 1716, Loss: 5.684262275695801\n",
            "Training Iteration 1717, Loss: 6.003790378570557\n",
            "Training Iteration 1718, Loss: 1.6989476680755615\n",
            "Training Iteration 1719, Loss: 1.9067585468292236\n",
            "Training Iteration 1720, Loss: 2.598202705383301\n",
            "Training Iteration 1721, Loss: 3.470710515975952\n",
            "Training Iteration 1722, Loss: 2.727179527282715\n",
            "Training Iteration 1723, Loss: 3.66890549659729\n",
            "Training Iteration 1724, Loss: 3.281546115875244\n",
            "Training Iteration 1725, Loss: 5.5601654052734375\n",
            "Training Iteration 1726, Loss: 3.543794631958008\n",
            "Training Iteration 1727, Loss: 2.5488474369049072\n",
            "Training Iteration 1728, Loss: 3.424774169921875\n",
            "Training Iteration 1729, Loss: 4.6366376876831055\n",
            "Training Iteration 1730, Loss: 2.6575801372528076\n",
            "Training Iteration 1731, Loss: 8.587152481079102\n",
            "Training Iteration 1732, Loss: 3.6766719818115234\n",
            "Training Iteration 1733, Loss: 3.5885534286499023\n",
            "Training Iteration 1734, Loss: 4.592726707458496\n",
            "Training Iteration 1735, Loss: 3.31528902053833\n",
            "Training Iteration 1736, Loss: 5.485785961151123\n",
            "Training Iteration 1737, Loss: 2.910353899002075\n",
            "Training Iteration 1738, Loss: 5.156625747680664\n",
            "Training Iteration 1739, Loss: 6.426389217376709\n",
            "Training Iteration 1740, Loss: 2.8462648391723633\n",
            "Training Iteration 1741, Loss: 3.0126535892486572\n",
            "Training Iteration 1742, Loss: 2.1332623958587646\n",
            "Training Iteration 1743, Loss: 3.8552956581115723\n",
            "Training Iteration 1744, Loss: 2.6397111415863037\n",
            "Training Iteration 1745, Loss: 5.132714748382568\n",
            "Training Iteration 1746, Loss: 4.808119297027588\n",
            "Training Iteration 1747, Loss: 3.755398750305176\n",
            "Training Iteration 1748, Loss: 4.654101371765137\n",
            "Training Iteration 1749, Loss: 3.2466001510620117\n",
            "Training Iteration 1750, Loss: 1.5889849662780762\n",
            "Training Iteration 1751, Loss: 3.9867565631866455\n",
            "Training Iteration 1752, Loss: 4.764811038970947\n",
            "Training Iteration 1753, Loss: 3.5668628215789795\n",
            "Training Iteration 1754, Loss: 4.772373199462891\n",
            "Training Iteration 1755, Loss: 7.099428653717041\n",
            "Training Iteration 1756, Loss: 6.212926864624023\n",
            "Training Iteration 1757, Loss: 3.488215684890747\n",
            "Training Iteration 1758, Loss: 8.096186637878418\n",
            "Training Iteration 1759, Loss: 5.47629976272583\n",
            "Training Iteration 1760, Loss: 5.1453046798706055\n",
            "Training Iteration 1761, Loss: 4.515822410583496\n",
            "Training Iteration 1762, Loss: 4.0591630935668945\n",
            "Training Iteration 1763, Loss: 4.617743015289307\n",
            "Training Iteration 1764, Loss: 3.645918130874634\n",
            "Training Iteration 1765, Loss: 3.5173888206481934\n",
            "Training Iteration 1766, Loss: 3.9542627334594727\n",
            "Training Iteration 1767, Loss: 4.980808258056641\n",
            "Training Iteration 1768, Loss: 3.685239315032959\n",
            "Training Iteration 1769, Loss: 4.3962602615356445\n",
            "Training Iteration 1770, Loss: 5.124908447265625\n",
            "Training Iteration 1771, Loss: 2.755040407180786\n",
            "Training Iteration 1772, Loss: 3.3468403816223145\n",
            "Training Iteration 1773, Loss: 2.902855157852173\n",
            "Training Iteration 1774, Loss: 5.6981120109558105\n",
            "Training Iteration 1775, Loss: 4.333136081695557\n",
            "Training Iteration 1776, Loss: 9.606012344360352\n",
            "Training Iteration 1777, Loss: 10.542617797851562\n",
            "Training Iteration 1778, Loss: 4.223756790161133\n",
            "Training Iteration 1779, Loss: 5.056521415710449\n",
            "Training Iteration 1780, Loss: 2.0708305835723877\n",
            "Training Iteration 1781, Loss: 5.545590400695801\n",
            "Training Iteration 1782, Loss: 7.4096903800964355\n",
            "Training Iteration 1783, Loss: 3.2197318077087402\n",
            "Training Iteration 1784, Loss: 5.242393970489502\n",
            "Training Iteration 1785, Loss: 3.6194443702697754\n",
            "Training Iteration 1786, Loss: 6.186257839202881\n",
            "Training Iteration 1787, Loss: 3.6886627674102783\n",
            "Training Iteration 1788, Loss: 6.109960079193115\n",
            "Training Iteration 1789, Loss: 4.4045729637146\n",
            "Training Iteration 1790, Loss: 4.42795991897583\n",
            "Training Iteration 1791, Loss: 4.435781955718994\n",
            "Training Iteration 1792, Loss: 3.3131814002990723\n",
            "Training Iteration 1793, Loss: 2.0875067710876465\n",
            "Training Iteration 1794, Loss: 2.6799042224884033\n",
            "Training Iteration 1795, Loss: 9.866616249084473\n",
            "Training Iteration 1796, Loss: 6.657959938049316\n",
            "Training Iteration 1797, Loss: 4.977924823760986\n",
            "Training Iteration 1798, Loss: 3.504735231399536\n",
            "Training Iteration 1799, Loss: 1.85525643825531\n",
            "Training Iteration 1800, Loss: 4.636689186096191\n",
            "Training Iteration 1801, Loss: 9.990108489990234\n",
            "Training Iteration 1802, Loss: 6.626440048217773\n",
            "Training Iteration 1803, Loss: 9.167608261108398\n",
            "Training Iteration 1804, Loss: 4.744550704956055\n",
            "Training Iteration 1805, Loss: 9.387200355529785\n",
            "Training Iteration 1806, Loss: 7.269996166229248\n",
            "Training Iteration 1807, Loss: 8.111021995544434\n",
            "Training Iteration 1808, Loss: 2.15683650970459\n",
            "Training Iteration 1809, Loss: 7.753389358520508\n",
            "Training Iteration 1810, Loss: 6.375577449798584\n",
            "Training Iteration 1811, Loss: 8.539488792419434\n",
            "Training Iteration 1812, Loss: 11.679929733276367\n",
            "Training Iteration 1813, Loss: 6.017196178436279\n",
            "Training Iteration 1814, Loss: 4.279191017150879\n",
            "Training Iteration 1815, Loss: 8.902132987976074\n",
            "Training Iteration 1816, Loss: 4.1916422843933105\n",
            "Training Iteration 1817, Loss: 5.357707977294922\n",
            "Training Iteration 1818, Loss: 2.9404144287109375\n",
            "Training Iteration 1819, Loss: 8.243685722351074\n",
            "Training Iteration 1820, Loss: 5.441310882568359\n",
            "Training Iteration 1821, Loss: 5.864017009735107\n",
            "Training Iteration 1822, Loss: 5.574962615966797\n",
            "Training Iteration 1823, Loss: 6.166238784790039\n",
            "Training Iteration 1824, Loss: 4.891384124755859\n",
            "Training Iteration 1825, Loss: 5.644002437591553\n",
            "Training Iteration 1826, Loss: 3.7115912437438965\n",
            "Training Iteration 1827, Loss: 3.521313190460205\n",
            "Training Iteration 1828, Loss: 4.774299621582031\n",
            "Training Iteration 1829, Loss: 4.0883469581604\n",
            "Training Iteration 1830, Loss: 4.4214301109313965\n",
            "Training Iteration 1831, Loss: 4.887667655944824\n",
            "Training Iteration 1832, Loss: 4.63006591796875\n",
            "Training Iteration 1833, Loss: 3.2926158905029297\n",
            "Training Iteration 1834, Loss: 3.588592290878296\n",
            "Training Iteration 1835, Loss: 4.339211940765381\n",
            "Training Iteration 1836, Loss: 5.4508185386657715\n",
            "Training Iteration 1837, Loss: 6.017348766326904\n",
            "Training Iteration 1838, Loss: 5.208767890930176\n",
            "Training Iteration 1839, Loss: 3.794111728668213\n",
            "Training Iteration 1840, Loss: 4.100225925445557\n",
            "Training Iteration 1841, Loss: 5.775247097015381\n",
            "Training Iteration 1842, Loss: 3.310885190963745\n",
            "Training Iteration 1843, Loss: 4.610970973968506\n",
            "Training Iteration 1844, Loss: 8.161409378051758\n",
            "Training Iteration 1845, Loss: 2.7327396869659424\n",
            "Training Iteration 1846, Loss: 4.151705265045166\n",
            "Training Iteration 1847, Loss: 1.3356306552886963\n",
            "Training Iteration 1848, Loss: 4.883645534515381\n",
            "Training Iteration 1849, Loss: 0.4839176535606384\n",
            "Training Iteration 1850, Loss: 3.877685546875\n",
            "Training Iteration 1851, Loss: 3.819500684738159\n",
            "Training Iteration 1852, Loss: 3.3168680667877197\n",
            "Training Iteration 1853, Loss: 2.5142879486083984\n",
            "Training Iteration 1854, Loss: 4.109806060791016\n",
            "Training Iteration 1855, Loss: 3.418123483657837\n",
            "Training Iteration 1856, Loss: 4.483389854431152\n",
            "Training Iteration 1857, Loss: 3.1522576808929443\n",
            "Training Iteration 1858, Loss: 4.032249450683594\n",
            "Training Iteration 1859, Loss: 2.5105597972869873\n",
            "Training Iteration 1860, Loss: 3.5958192348480225\n",
            "Training Iteration 1861, Loss: 3.2220306396484375\n",
            "Training Iteration 1862, Loss: 2.3550357818603516\n",
            "Training Iteration 1863, Loss: 8.057171821594238\n",
            "Training Iteration 1864, Loss: 5.45869255065918\n",
            "Training Iteration 1865, Loss: 3.5884549617767334\n",
            "Training Iteration 1866, Loss: 6.294093608856201\n",
            "Training Iteration 1867, Loss: 5.064724445343018\n",
            "Training Iteration 1868, Loss: 8.830592155456543\n",
            "Training Iteration 1869, Loss: 4.023029327392578\n",
            "Training Iteration 1870, Loss: 6.170401573181152\n",
            "Training Iteration 1871, Loss: 5.721478462219238\n",
            "Training Iteration 1872, Loss: 4.27226448059082\n",
            "Training Iteration 1873, Loss: 4.857687950134277\n",
            "Training Iteration 1874, Loss: 3.7672510147094727\n",
            "Training Iteration 1875, Loss: 3.2174577713012695\n",
            "Training Iteration 1876, Loss: 6.411789894104004\n",
            "Training Iteration 1877, Loss: 4.222993850708008\n",
            "Training Iteration 1878, Loss: 5.555392265319824\n",
            "Training Iteration 1879, Loss: 4.172841548919678\n",
            "Training Iteration 1880, Loss: 3.488497257232666\n",
            "Training Iteration 1881, Loss: 3.80942440032959\n",
            "Training Iteration 1882, Loss: 2.2964487075805664\n",
            "Training Iteration 1883, Loss: 4.559565544128418\n",
            "Training Iteration 1884, Loss: 3.51611065864563\n",
            "Training Iteration 1885, Loss: 4.959682464599609\n",
            "Training Iteration 1886, Loss: 3.0857737064361572\n",
            "Training Iteration 1887, Loss: 2.8418335914611816\n",
            "Training Iteration 1888, Loss: 2.819479465484619\n",
            "Training Iteration 1889, Loss: 3.020021915435791\n",
            "Training Iteration 1890, Loss: 5.784806728363037\n",
            "Training Iteration 1891, Loss: 6.320245742797852\n",
            "Training Iteration 1892, Loss: 4.57269287109375\n",
            "Training Iteration 1893, Loss: 3.2020699977874756\n",
            "Training Iteration 1894, Loss: 3.11212420463562\n",
            "Training Iteration 1895, Loss: 3.749880075454712\n",
            "Training Iteration 1896, Loss: 4.108748435974121\n",
            "Training Iteration 1897, Loss: 4.072941780090332\n",
            "Training Iteration 1898, Loss: 5.978244781494141\n",
            "Training Iteration 1899, Loss: 4.410948276519775\n",
            "Training Iteration 1900, Loss: 5.335794448852539\n",
            "Training Iteration 1901, Loss: 2.8609299659729004\n",
            "Training Iteration 1902, Loss: 5.021738052368164\n",
            "Training Iteration 1903, Loss: 4.099127292633057\n",
            "Training Iteration 1904, Loss: 1.2566229104995728\n",
            "Training Iteration 1905, Loss: 2.553340435028076\n",
            "Training Iteration 1906, Loss: 4.4873199462890625\n",
            "Training Iteration 1907, Loss: 5.5040998458862305\n",
            "Training Iteration 1908, Loss: 4.585303783416748\n",
            "Training Iteration 1909, Loss: 1.5180180072784424\n",
            "Training Iteration 1910, Loss: 5.156828880310059\n",
            "Training Iteration 1911, Loss: 2.3252079486846924\n",
            "Training Iteration 1912, Loss: 3.943401575088501\n",
            "Training Iteration 1913, Loss: 3.4494712352752686\n",
            "Training Iteration 1914, Loss: 7.386266708374023\n",
            "Training Iteration 1915, Loss: 4.1277899742126465\n",
            "Training Iteration 1916, Loss: 5.149808406829834\n",
            "Training Iteration 1917, Loss: 2.101494073867798\n",
            "Training Iteration 1918, Loss: 2.0399844646453857\n",
            "Training Iteration 1919, Loss: 4.274814605712891\n",
            "Training Iteration 1920, Loss: 4.517377853393555\n",
            "Training Iteration 1921, Loss: 4.872261047363281\n",
            "Training Iteration 1922, Loss: 4.7708964347839355\n",
            "Training Iteration 1923, Loss: 4.03155517578125\n",
            "Training Iteration 1924, Loss: 3.9605629444122314\n",
            "Training Iteration 1925, Loss: 4.982550621032715\n",
            "Training Iteration 1926, Loss: 5.277700424194336\n",
            "Training Iteration 1927, Loss: 4.846347332000732\n",
            "Training Iteration 1928, Loss: 8.205537796020508\n",
            "Training Iteration 1929, Loss: 3.573187828063965\n",
            "Training Iteration 1930, Loss: 7.4507646560668945\n",
            "Training Iteration 1931, Loss: 3.83101749420166\n",
            "Training Iteration 1932, Loss: 3.469841480255127\n",
            "Training Iteration 1933, Loss: 3.9869275093078613\n",
            "Training Iteration 1934, Loss: 3.350651979446411\n",
            "Training Iteration 1935, Loss: 3.2421395778656006\n",
            "Training Iteration 1936, Loss: 3.0293803215026855\n",
            "Training Iteration 1937, Loss: 3.2792537212371826\n",
            "Training Iteration 1938, Loss: 6.691887855529785\n",
            "Training Iteration 1939, Loss: 3.0382540225982666\n",
            "Training Iteration 1940, Loss: 3.6949267387390137\n",
            "Training Iteration 1941, Loss: 1.4645540714263916\n",
            "Training Iteration 1942, Loss: 4.135984897613525\n",
            "Training Iteration 1943, Loss: 5.4921770095825195\n",
            "Training Iteration 1944, Loss: 2.9407787322998047\n",
            "Training Iteration 1945, Loss: 2.379788875579834\n",
            "Training Iteration 1946, Loss: 3.69954252243042\n",
            "Training Iteration 1947, Loss: 4.544922828674316\n",
            "Training Iteration 1948, Loss: 3.976630449295044\n",
            "Training Iteration 1949, Loss: 3.8553037643432617\n",
            "Training Iteration 1950, Loss: 2.725229024887085\n",
            "Training Iteration 1951, Loss: 4.2416534423828125\n",
            "Training Iteration 1952, Loss: 3.496852159500122\n",
            "Training Iteration 1953, Loss: 2.3460640907287598\n",
            "Training Iteration 1954, Loss: 4.479996681213379\n",
            "Training Iteration 1955, Loss: 3.199876070022583\n",
            "Training Iteration 1956, Loss: 5.893950462341309\n",
            "Training Iteration 1957, Loss: 3.1178483963012695\n",
            "Training Iteration 1958, Loss: 2.0399084091186523\n",
            "Training Iteration 1959, Loss: 4.947493553161621\n",
            "Training Iteration 1960, Loss: 4.72452449798584\n",
            "Training Iteration 1961, Loss: 2.032167673110962\n",
            "Training Iteration 1962, Loss: 6.107529163360596\n",
            "Training Iteration 1963, Loss: 5.126372337341309\n",
            "Training Iteration 1964, Loss: 3.9566798210144043\n",
            "Training Iteration 1965, Loss: 4.8845415115356445\n",
            "Training Iteration 1966, Loss: 3.7474544048309326\n",
            "Training Iteration 1967, Loss: 4.23274040222168\n",
            "Training Iteration 1968, Loss: 3.5879344940185547\n",
            "Training Iteration 1969, Loss: 4.116170883178711\n",
            "Training Iteration 1970, Loss: 3.55657958984375\n",
            "Training Iteration 1971, Loss: 5.115549087524414\n",
            "Training Iteration 1972, Loss: 6.977208614349365\n",
            "Training Iteration 1973, Loss: 3.4142000675201416\n",
            "Training Iteration 1974, Loss: 1.626541018486023\n",
            "Training Iteration 1975, Loss: 4.846292972564697\n",
            "Training Iteration 1976, Loss: 4.65683126449585\n",
            "Training Iteration 1977, Loss: 1.4576094150543213\n",
            "Training Iteration 1978, Loss: 4.5657057762146\n",
            "Training Iteration 1979, Loss: 3.0594592094421387\n",
            "Training Iteration 1980, Loss: 4.816159725189209\n",
            "Training Iteration 1981, Loss: 4.376209735870361\n",
            "Training Iteration 1982, Loss: 6.2991156578063965\n",
            "Training Iteration 1983, Loss: 10.72601318359375\n",
            "Training Iteration 1984, Loss: 6.996715545654297\n",
            "Training Iteration 1985, Loss: 5.204109191894531\n",
            "Training Iteration 1986, Loss: 4.954686164855957\n",
            "Training Iteration 1987, Loss: 4.590635299682617\n",
            "Training Iteration 1988, Loss: 6.6531476974487305\n",
            "Training Iteration 1989, Loss: 2.888612747192383\n",
            "Training Iteration 1990, Loss: 8.637943267822266\n",
            "Training Iteration 1991, Loss: 4.111181259155273\n",
            "Training Iteration 1992, Loss: 4.394742012023926\n",
            "Training Iteration 1993, Loss: 5.751529693603516\n",
            "Training Iteration 1994, Loss: 2.981192111968994\n",
            "Training Iteration 1995, Loss: 3.883068084716797\n",
            "Training Iteration 1996, Loss: 5.809288501739502\n",
            "Training Iteration 1997, Loss: 8.832046508789062\n",
            "Training Iteration 1998, Loss: 5.161492347717285\n",
            "Training Iteration 1999, Loss: 3.873931884765625\n",
            "Training Iteration 2000, Loss: 5.148796081542969\n",
            "Training Iteration 2001, Loss: 2.4731624126434326\n",
            "Training Iteration 2002, Loss: 2.0632998943328857\n",
            "Training Iteration 2003, Loss: 4.995776653289795\n",
            "Training Iteration 2004, Loss: 8.872838973999023\n",
            "Training Iteration 2005, Loss: 4.212283611297607\n",
            "Training Iteration 2006, Loss: 8.250200271606445\n",
            "Training Iteration 2007, Loss: 3.226465940475464\n",
            "Training Iteration 2008, Loss: 7.090847969055176\n",
            "Training Iteration 2009, Loss: 6.092863082885742\n",
            "Training Iteration 2010, Loss: 4.877035140991211\n",
            "Training Iteration 2011, Loss: 3.9410910606384277\n",
            "Training Iteration 2012, Loss: 3.815883159637451\n",
            "Training Iteration 2013, Loss: 3.083739757537842\n",
            "Training Iteration 2014, Loss: 6.114373207092285\n",
            "Training Iteration 2015, Loss: 4.9831743240356445\n",
            "Training Iteration 2016, Loss: 7.51295280456543\n",
            "Training Iteration 2017, Loss: 5.44890022277832\n",
            "Training Iteration 2018, Loss: 9.625978469848633\n",
            "Training Iteration 2019, Loss: 4.178958415985107\n",
            "Training Iteration 2020, Loss: 5.571527004241943\n",
            "Training Iteration 2021, Loss: 6.876833438873291\n",
            "Training Iteration 2022, Loss: 3.4010605812072754\n",
            "Training Iteration 2023, Loss: 7.040389060974121\n",
            "Training Iteration 2024, Loss: 5.512755393981934\n",
            "Training Iteration 2025, Loss: 3.257805347442627\n",
            "Training Iteration 2026, Loss: 2.678887128829956\n",
            "Training Iteration 2027, Loss: 5.295555114746094\n",
            "Training Iteration 2028, Loss: 4.249969005584717\n",
            "Training Iteration 2029, Loss: 4.71235466003418\n",
            "Training Iteration 2030, Loss: 7.035387992858887\n",
            "Training Iteration 2031, Loss: 4.313847064971924\n",
            "Training Iteration 2032, Loss: 5.469071388244629\n",
            "Training Iteration 2033, Loss: 4.3235344886779785\n",
            "Training Iteration 2034, Loss: 6.523862361907959\n",
            "Training Iteration 2035, Loss: 7.416791915893555\n",
            "Training Iteration 2036, Loss: 4.124066352844238\n",
            "Training Iteration 2037, Loss: 6.893355846405029\n",
            "Training Iteration 2038, Loss: 4.537985324859619\n",
            "Training Iteration 2039, Loss: 6.613719463348389\n",
            "Training Iteration 2040, Loss: 2.2660391330718994\n",
            "Training Iteration 2041, Loss: 7.563223838806152\n",
            "Training Iteration 2042, Loss: 5.389822959899902\n",
            "Training Iteration 2043, Loss: 5.3438239097595215\n",
            "Training Iteration 2044, Loss: 4.512552738189697\n",
            "Training Iteration 2045, Loss: 4.191524505615234\n",
            "Training Iteration 2046, Loss: 5.674198150634766\n",
            "Training Iteration 2047, Loss: 3.611630439758301\n",
            "Training Iteration 2048, Loss: 3.331000804901123\n",
            "Training Iteration 2049, Loss: 3.197786808013916\n",
            "Training Iteration 2050, Loss: 3.2374331951141357\n",
            "Training Iteration 2051, Loss: 3.343136787414551\n",
            "Training Iteration 2052, Loss: 2.481674909591675\n",
            "Training Iteration 2053, Loss: 6.650474548339844\n",
            "Training Iteration 2054, Loss: 3.502345085144043\n",
            "Training Iteration 2055, Loss: 1.9589002132415771\n",
            "Training Iteration 2056, Loss: 4.512641906738281\n",
            "Training Iteration 2057, Loss: 6.047828674316406\n",
            "Training Iteration 2058, Loss: 6.030210018157959\n",
            "Training Iteration 2059, Loss: 8.421247482299805\n",
            "Training Iteration 2060, Loss: 2.101263999938965\n",
            "Training Iteration 2061, Loss: 6.559737682342529\n",
            "Training Iteration 2062, Loss: 3.9335033893585205\n",
            "Training Iteration 2063, Loss: 2.5238351821899414\n",
            "Training Iteration 2064, Loss: 5.011346817016602\n",
            "Training Iteration 2065, Loss: 2.094858169555664\n",
            "Training Iteration 2066, Loss: 7.535548686981201\n",
            "Training Iteration 2067, Loss: 2.060638904571533\n",
            "Training Iteration 2068, Loss: 3.154337167739868\n",
            "Training Iteration 2069, Loss: 2.570418357849121\n",
            "Training Iteration 2070, Loss: 2.817000150680542\n",
            "Training Iteration 2071, Loss: 3.936830520629883\n",
            "Training Iteration 2072, Loss: 1.5371266603469849\n",
            "Training Iteration 2073, Loss: 3.5078413486480713\n",
            "Training Iteration 2074, Loss: 3.2415053844451904\n",
            "Training Iteration 2075, Loss: 2.3162825107574463\n",
            "Training Iteration 2076, Loss: 7.676083564758301\n",
            "Training Iteration 2077, Loss: 3.874265193939209\n",
            "Training Iteration 2078, Loss: 5.289173126220703\n",
            "Training Iteration 2079, Loss: 3.183812141418457\n",
            "Training Iteration 2080, Loss: 6.477546691894531\n",
            "Training Iteration 2081, Loss: 2.3834428787231445\n",
            "Training Iteration 2082, Loss: 3.4674952030181885\n",
            "Training Iteration 2083, Loss: 3.09690523147583\n",
            "Training Iteration 2084, Loss: 7.299066543579102\n",
            "Training Iteration 2085, Loss: 2.6514945030212402\n",
            "Training Iteration 2086, Loss: 6.089043617248535\n",
            "Training Iteration 2087, Loss: 5.193979263305664\n",
            "Training Iteration 2088, Loss: 4.158123016357422\n",
            "Training Iteration 2089, Loss: 4.323313236236572\n",
            "Training Iteration 2090, Loss: 3.62355899810791\n",
            "Training Iteration 2091, Loss: 2.4850645065307617\n",
            "Training Iteration 2092, Loss: 4.691734790802002\n",
            "Training Iteration 2093, Loss: 4.009392738342285\n",
            "Training Iteration 2094, Loss: 2.3846335411071777\n",
            "Training Iteration 2095, Loss: 3.8576056957244873\n",
            "Training Iteration 2096, Loss: 5.670973300933838\n",
            "Training Iteration 2097, Loss: 4.073817729949951\n",
            "Training Iteration 2098, Loss: 5.524023056030273\n",
            "Training Iteration 2099, Loss: 3.009538173675537\n",
            "Training Iteration 2100, Loss: 4.672707557678223\n",
            "Training Iteration 2101, Loss: 5.656581878662109\n",
            "Training Iteration 2102, Loss: 3.7277534008026123\n",
            "Training Iteration 2103, Loss: 3.481583595275879\n",
            "Training Iteration 2104, Loss: 3.6766862869262695\n",
            "Training Iteration 2105, Loss: 4.38391637802124\n",
            "Training Iteration 2106, Loss: 3.786076307296753\n",
            "Training Iteration 2107, Loss: 4.114373683929443\n",
            "Training Iteration 2108, Loss: 4.756430625915527\n",
            "Training Iteration 2109, Loss: 5.406539440155029\n",
            "Training Iteration 2110, Loss: 4.9635748863220215\n",
            "Training Iteration 2111, Loss: 5.464442253112793\n",
            "Training Iteration 2112, Loss: 5.188005447387695\n",
            "Training Iteration 2113, Loss: 3.7039191722869873\n",
            "Training Iteration 2114, Loss: 8.011529922485352\n",
            "Training Iteration 2115, Loss: 7.5584211349487305\n",
            "Training Iteration 2116, Loss: 4.821465015411377\n",
            "Training Iteration 2117, Loss: 3.689342498779297\n",
            "Training Iteration 2118, Loss: 4.121275424957275\n",
            "Training Iteration 2119, Loss: 4.875171661376953\n",
            "Training Iteration 2120, Loss: 4.041482925415039\n",
            "Training Iteration 2121, Loss: 4.0131449699401855\n",
            "Training Iteration 2122, Loss: 5.855859279632568\n",
            "Training Iteration 2123, Loss: 5.841919898986816\n",
            "Training Iteration 2124, Loss: 6.631298065185547\n",
            "Training Iteration 2125, Loss: 2.6071746349334717\n",
            "Training Iteration 2126, Loss: 2.9828414916992188\n",
            "Training Iteration 2127, Loss: 5.286101818084717\n",
            "Training Iteration 2128, Loss: 6.2564191818237305\n",
            "Training Iteration 2129, Loss: 6.839019775390625\n",
            "Training Iteration 2130, Loss: 1.7687551975250244\n",
            "Training Iteration 2131, Loss: 5.975171089172363\n",
            "Training Iteration 2132, Loss: 2.7450671195983887\n",
            "Training Iteration 2133, Loss: 6.590113639831543\n",
            "Training Iteration 2134, Loss: 3.28995943069458\n",
            "Training Iteration 2135, Loss: 2.296001434326172\n",
            "Training Iteration 2136, Loss: 5.821259021759033\n",
            "Training Iteration 2137, Loss: 3.635871410369873\n",
            "Training Iteration 2138, Loss: 3.2157537937164307\n",
            "Training Iteration 2139, Loss: 4.757168292999268\n",
            "Training Iteration 2140, Loss: 4.405544757843018\n",
            "Training Iteration 2141, Loss: 6.347263813018799\n",
            "Training Iteration 2142, Loss: 7.950077533721924\n",
            "Training Iteration 2143, Loss: 3.554208278656006\n",
            "Training Iteration 2144, Loss: 4.782773971557617\n",
            "Training Iteration 2145, Loss: 4.20095157623291\n",
            "Training Iteration 2146, Loss: 3.2328896522521973\n",
            "Training Iteration 2147, Loss: 1.9773274660110474\n",
            "Training Iteration 2148, Loss: 5.133809566497803\n",
            "Training Iteration 2149, Loss: 6.245457649230957\n",
            "Training Iteration 2150, Loss: 4.555756092071533\n",
            "Training Iteration 2151, Loss: 5.171517848968506\n",
            "Training Iteration 2152, Loss: 5.889736652374268\n",
            "Training Iteration 2153, Loss: 5.352950096130371\n",
            "Training Iteration 2154, Loss: 4.611313343048096\n",
            "Training Iteration 2155, Loss: 3.006725788116455\n",
            "Training Iteration 2156, Loss: 2.9044671058654785\n",
            "Training Iteration 2157, Loss: 5.557502746582031\n",
            "Training Iteration 2158, Loss: 2.783754825592041\n",
            "Training Iteration 2159, Loss: 5.493844032287598\n",
            "Training Iteration 2160, Loss: 2.8779051303863525\n",
            "Training Iteration 2161, Loss: 5.108189105987549\n",
            "Training Iteration 2162, Loss: 6.477680206298828\n",
            "Training Iteration 2163, Loss: 3.9912970066070557\n",
            "Training Iteration 2164, Loss: 9.59500503540039\n",
            "Training Iteration 2165, Loss: 4.250995635986328\n",
            "Training Iteration 2166, Loss: 7.033599853515625\n",
            "Training Iteration 2167, Loss: 5.2436203956604\n",
            "Training Iteration 2168, Loss: 7.504271507263184\n",
            "Training Iteration 2169, Loss: 5.391574859619141\n",
            "Training Iteration 2170, Loss: 3.7851390838623047\n",
            "Training Iteration 2171, Loss: 4.178132057189941\n",
            "Training Iteration 2172, Loss: 4.659073829650879\n",
            "Training Iteration 2173, Loss: 3.1047606468200684\n",
            "Training Iteration 2174, Loss: 4.703980445861816\n",
            "Training Iteration 2175, Loss: 7.221912860870361\n",
            "Training Iteration 2176, Loss: 7.839714050292969\n",
            "Training Iteration 2177, Loss: 3.6934030055999756\n",
            "Training Iteration 2178, Loss: 6.104815483093262\n",
            "Training Iteration 2179, Loss: 5.641332149505615\n",
            "Training Iteration 2180, Loss: 3.404055118560791\n",
            "Training Iteration 2181, Loss: 4.303188800811768\n",
            "Training Iteration 2182, Loss: 7.7864484786987305\n",
            "Training Iteration 2183, Loss: 5.143218517303467\n",
            "Training Iteration 2184, Loss: 5.649717330932617\n",
            "Training Iteration 2185, Loss: 5.690313339233398\n",
            "Training Iteration 2186, Loss: 3.786329746246338\n",
            "Training Iteration 2187, Loss: 5.416208744049072\n",
            "Training Iteration 2188, Loss: 4.7230424880981445\n",
            "Training Iteration 2189, Loss: 3.5697672367095947\n",
            "Training Iteration 2190, Loss: 4.952010631561279\n",
            "Training Iteration 2191, Loss: 4.785240650177002\n",
            "Training Iteration 2192, Loss: 4.558838844299316\n",
            "Training Iteration 2193, Loss: 6.291019916534424\n",
            "Training Iteration 2194, Loss: 3.472943067550659\n",
            "Training Iteration 2195, Loss: 3.918381452560425\n",
            "Training Iteration 2196, Loss: 3.0883567333221436\n",
            "Training Iteration 2197, Loss: 5.772634983062744\n",
            "Training Iteration 2198, Loss: 8.214261054992676\n",
            "Training Iteration 2199, Loss: 4.544120788574219\n",
            "Training Iteration 2200, Loss: 7.336365222930908\n",
            "Training Iteration 2201, Loss: 5.2887959480285645\n",
            "Training Iteration 2202, Loss: 6.611668586730957\n",
            "Training Iteration 2203, Loss: 4.756834030151367\n",
            "Training Iteration 2204, Loss: 6.180478572845459\n",
            "Training Iteration 2205, Loss: 2.988280773162842\n",
            "Training Iteration 2206, Loss: 4.92427396774292\n",
            "Training Iteration 2207, Loss: 2.8263542652130127\n",
            "Training Iteration 2208, Loss: 2.3430614471435547\n",
            "Training Iteration 2209, Loss: 3.0787782669067383\n",
            "Training Iteration 2210, Loss: 4.058343887329102\n",
            "Training Iteration 2211, Loss: 3.1250722408294678\n",
            "Training Iteration 2212, Loss: 2.845970392227173\n",
            "Training Iteration 2213, Loss: 2.2432444095611572\n",
            "Training Iteration 2214, Loss: 4.098607540130615\n",
            "Training Iteration 2215, Loss: 3.6539134979248047\n",
            "Training Iteration 2216, Loss: 3.5943689346313477\n",
            "Training Iteration 2217, Loss: 2.0686447620391846\n",
            "Training Iteration 2218, Loss: 2.6782939434051514\n",
            "Training Iteration 2219, Loss: 4.786247253417969\n",
            "Training Iteration 2220, Loss: 4.417549133300781\n",
            "Training Iteration 2221, Loss: 1.7785420417785645\n",
            "Training Iteration 2222, Loss: 5.494815826416016\n",
            "Training Iteration 2223, Loss: 2.08901047706604\n",
            "Training Iteration 2224, Loss: 4.568957328796387\n",
            "Training Iteration 2225, Loss: 2.602111339569092\n",
            "Training Iteration 2226, Loss: 3.7927865982055664\n",
            "Training Iteration 2227, Loss: 4.89270544052124\n",
            "Training Iteration 2228, Loss: 6.474390983581543\n",
            "Training Iteration 2229, Loss: 3.7958593368530273\n",
            "Training Iteration 2230, Loss: 2.763373374938965\n",
            "Training Iteration 2231, Loss: 3.7555413246154785\n",
            "Training Iteration 2232, Loss: 4.764194488525391\n",
            "Training Iteration 2233, Loss: 7.968859672546387\n",
            "Training Iteration 2234, Loss: 3.9291417598724365\n",
            "Training Iteration 2235, Loss: 4.80168342590332\n",
            "Training Iteration 2236, Loss: 4.928761959075928\n",
            "Training Iteration 2237, Loss: 4.674343585968018\n",
            "Training Iteration 2238, Loss: 5.139984130859375\n",
            "Training Iteration 2239, Loss: 3.898289442062378\n",
            "Training Iteration 2240, Loss: 3.148503541946411\n",
            "Training Iteration 2241, Loss: 5.579287528991699\n",
            "Training Iteration 2242, Loss: 3.693297863006592\n",
            "Training Iteration 2243, Loss: 4.284763813018799\n",
            "Training Iteration 2244, Loss: 3.750314235687256\n",
            "Training Iteration 2245, Loss: 3.010472059249878\n",
            "Training Iteration 2246, Loss: 3.52908992767334\n",
            "Training Iteration 2247, Loss: 3.284471035003662\n",
            "Training Iteration 2248, Loss: 4.797676086425781\n",
            "Training Iteration 2249, Loss: 4.111507415771484\n",
            "Training Iteration 2250, Loss: 3.2660748958587646\n",
            "Training Iteration 2251, Loss: 3.3558778762817383\n",
            "Training Iteration 2252, Loss: 5.492616176605225\n",
            "Training Iteration 2253, Loss: 3.503746271133423\n",
            "Training Iteration 2254, Loss: 4.880138397216797\n",
            "Training Iteration 2255, Loss: 6.894210338592529\n",
            "Training Iteration 2256, Loss: 2.2826545238494873\n",
            "Training Iteration 2257, Loss: 6.382490634918213\n",
            "Training Iteration 2258, Loss: 5.968162536621094\n",
            "Training Iteration 2259, Loss: 4.892722129821777\n",
            "Training Iteration 2260, Loss: 3.840610980987549\n",
            "Training Iteration 2261, Loss: 2.6089589595794678\n",
            "Training Iteration 2262, Loss: 4.0838727951049805\n",
            "Training Iteration 2263, Loss: 3.0462448596954346\n",
            "Training Iteration 2264, Loss: 4.434109687805176\n",
            "Training Iteration 2265, Loss: 3.3373703956604004\n",
            "Training Iteration 2266, Loss: 4.357011795043945\n",
            "Training Iteration 2267, Loss: 4.844794273376465\n",
            "Training Iteration 2268, Loss: 3.7368974685668945\n",
            "Training Iteration 2269, Loss: 3.1304709911346436\n",
            "Training Iteration 2270, Loss: 5.152030944824219\n",
            "Training Iteration 2271, Loss: 5.506258010864258\n",
            "Training Iteration 2272, Loss: 5.710102558135986\n",
            "Training Iteration 2273, Loss: 2.3670668601989746\n",
            "Training Iteration 2274, Loss: 4.2907328605651855\n",
            "Training Iteration 2275, Loss: 3.607203483581543\n",
            "Training Iteration 2276, Loss: 3.8386545181274414\n",
            "Training Iteration 2277, Loss: 1.6540184020996094\n",
            "Training Iteration 2278, Loss: 2.6652779579162598\n",
            "Training Iteration 2279, Loss: 4.4568281173706055\n",
            "Training Iteration 2280, Loss: 3.231750965118408\n",
            "Training Iteration 2281, Loss: 4.000787258148193\n",
            "Training Iteration 2282, Loss: 3.2634763717651367\n",
            "Training Iteration 2283, Loss: 6.873022079467773\n",
            "Training Iteration 2284, Loss: 5.094395637512207\n",
            "Training Iteration 2285, Loss: 3.207970142364502\n",
            "Training Iteration 2286, Loss: 4.327860355377197\n",
            "Training Iteration 2287, Loss: 4.88547420501709\n",
            "Training Iteration 2288, Loss: 5.115382194519043\n",
            "Training Iteration 2289, Loss: 4.1932692527771\n",
            "Training Iteration 2290, Loss: 3.4117331504821777\n",
            "Training Iteration 2291, Loss: 3.103879928588867\n",
            "Training Iteration 2292, Loss: 4.12973165512085\n",
            "Training Iteration 2293, Loss: 2.1439168453216553\n",
            "Training Iteration 2294, Loss: 3.262207508087158\n",
            "Training Iteration 2295, Loss: 8.002626419067383\n",
            "Training Iteration 2296, Loss: 4.582817077636719\n",
            "Training Iteration 2297, Loss: 4.62445592880249\n",
            "Training Iteration 2298, Loss: 2.4708354473114014\n",
            "Training Iteration 2299, Loss: 2.610337257385254\n",
            "Training Iteration 2300, Loss: 6.492031097412109\n",
            "Training Iteration 2301, Loss: 8.4033842086792\n",
            "Training Iteration 2302, Loss: 8.178593635559082\n",
            "Training Iteration 2303, Loss: 3.761737585067749\n",
            "Training Iteration 2304, Loss: 6.4047136306762695\n",
            "Training Iteration 2305, Loss: 5.445157527923584\n",
            "Training Iteration 2306, Loss: 3.954288959503174\n",
            "Training Iteration 2307, Loss: 6.961369037628174\n",
            "Training Iteration 2308, Loss: 5.258012771606445\n",
            "Training Iteration 2309, Loss: 5.864577770233154\n",
            "Training Iteration 2310, Loss: 4.491828918457031\n",
            "Training Iteration 2311, Loss: 3.1965348720550537\n",
            "Training Iteration 2312, Loss: 5.670772075653076\n",
            "Training Iteration 2313, Loss: 4.955687999725342\n",
            "Training Iteration 2314, Loss: 3.379476308822632\n",
            "Training Iteration 2315, Loss: 5.1738386154174805\n",
            "Training Iteration 2316, Loss: 3.1896250247955322\n",
            "Training Iteration 2317, Loss: 8.900208473205566\n",
            "Training Iteration 2318, Loss: 5.329641342163086\n",
            "Training Iteration 2319, Loss: 5.729694366455078\n",
            "Training Iteration 2320, Loss: 3.3663864135742188\n",
            "Training Iteration 2321, Loss: 6.526487350463867\n",
            "Training Iteration 2322, Loss: 5.329012870788574\n",
            "Training Iteration 2323, Loss: 14.743749618530273\n",
            "Training Iteration 2324, Loss: 8.154197692871094\n",
            "Training Iteration 2325, Loss: 5.641909599304199\n",
            "Training Iteration 2326, Loss: 5.617067337036133\n",
            "Training Iteration 2327, Loss: 4.3290863037109375\n",
            "Training Iteration 2328, Loss: 2.978874921798706\n",
            "Training Iteration 2329, Loss: 3.7207131385803223\n",
            "Training Iteration 2330, Loss: 4.217000961303711\n",
            "Training Iteration 2331, Loss: 5.509191513061523\n",
            "Training Iteration 2332, Loss: 3.4266834259033203\n",
            "Training Iteration 2333, Loss: 5.296100616455078\n",
            "Training Iteration 2334, Loss: 7.09926700592041\n",
            "Training Iteration 2335, Loss: 9.003173828125\n",
            "Training Iteration 2336, Loss: 3.9931321144104004\n",
            "Training Iteration 2337, Loss: 7.0397725105285645\n",
            "Training Iteration 2338, Loss: 2.3593835830688477\n",
            "Training Iteration 2339, Loss: 5.108203887939453\n",
            "Training Iteration 2340, Loss: 8.983264923095703\n",
            "Training Iteration 2341, Loss: 3.098782777786255\n",
            "Training Iteration 2342, Loss: 7.272369384765625\n",
            "Training Iteration 2343, Loss: 7.9231858253479\n",
            "Training Iteration 2344, Loss: 4.5733232498168945\n",
            "Training Iteration 2345, Loss: 4.500314712524414\n",
            "Training Iteration 2346, Loss: 8.306190490722656\n",
            "Training Iteration 2347, Loss: 6.385172367095947\n",
            "Training Iteration 2348, Loss: 5.754055500030518\n",
            "Training Iteration 2349, Loss: 4.942410469055176\n",
            "Training Iteration 2350, Loss: 3.9687910079956055\n",
            "Training Iteration 2351, Loss: 1.6231884956359863\n",
            "Training Iteration 2352, Loss: 4.2586894035339355\n",
            "Training Iteration 2353, Loss: 2.9876105785369873\n",
            "Training Iteration 2354, Loss: 4.032004356384277\n",
            "Training Iteration 2355, Loss: 2.7071266174316406\n",
            "Training Iteration 2356, Loss: 2.9648008346557617\n",
            "Training Iteration 2357, Loss: 5.95889949798584\n",
            "Training Iteration 2358, Loss: 5.224109649658203\n",
            "Training Iteration 2359, Loss: 10.301803588867188\n",
            "Training Iteration 2360, Loss: 2.1488847732543945\n",
            "Training Iteration 2361, Loss: 7.18748664855957\n",
            "Training Iteration 2362, Loss: 6.762748718261719\n",
            "Training Iteration 2363, Loss: 6.669410228729248\n",
            "Training Iteration 2364, Loss: 2.4943227767944336\n",
            "Training Iteration 2365, Loss: 8.791193008422852\n",
            "Training Iteration 2366, Loss: 4.935731887817383\n",
            "Training Iteration 2367, Loss: 3.060816526412964\n",
            "Training Iteration 2368, Loss: 3.5930871963500977\n",
            "Training Iteration 2369, Loss: 2.082460880279541\n",
            "Training Iteration 2370, Loss: 5.257430553436279\n",
            "Training Iteration 2371, Loss: 3.8426055908203125\n",
            "Training Iteration 2372, Loss: 6.013023853302002\n",
            "Training Iteration 2373, Loss: 3.3815135955810547\n",
            "Training Iteration 2374, Loss: 5.030821323394775\n",
            "Training Iteration 2375, Loss: 5.204529762268066\n",
            "Training Iteration 2376, Loss: 3.4700284004211426\n",
            "Training Iteration 2377, Loss: 2.7921671867370605\n",
            "Training Iteration 2378, Loss: 5.468297958374023\n",
            "Training Iteration 2379, Loss: 3.8824281692504883\n",
            "Training Iteration 2380, Loss: 4.896615982055664\n",
            "Training Iteration 2381, Loss: 2.663240432739258\n",
            "Training Iteration 2382, Loss: 5.667434215545654\n",
            "Training Iteration 2383, Loss: 2.5193581581115723\n",
            "Training Iteration 2384, Loss: 1.814781665802002\n",
            "Training Iteration 2385, Loss: 4.049858570098877\n",
            "Training Iteration 2386, Loss: 7.058203220367432\n",
            "Training Iteration 2387, Loss: 4.05078649520874\n",
            "Training Iteration 2388, Loss: 6.037895679473877\n",
            "Training Iteration 2389, Loss: 4.697153091430664\n",
            "Training Iteration 2390, Loss: 7.2599945068359375\n",
            "Training Iteration 2391, Loss: 4.1819868087768555\n",
            "Training Iteration 2392, Loss: 2.7781896591186523\n",
            "Training Iteration 2393, Loss: 4.1295485496521\n",
            "Training Iteration 2394, Loss: 4.32671594619751\n",
            "Training Iteration 2395, Loss: 5.6927595138549805\n",
            "Training Iteration 2396, Loss: 3.8408467769622803\n",
            "Training Iteration 2397, Loss: 6.739860534667969\n",
            "Training Iteration 2398, Loss: 2.9115822315216064\n",
            "Training Iteration 2399, Loss: 4.327084541320801\n",
            "Training Iteration 2400, Loss: 4.726471900939941\n",
            "Training Iteration 2401, Loss: 9.066061019897461\n",
            "Training Iteration 2402, Loss: 3.6688895225524902\n",
            "Training Iteration 2403, Loss: 2.922776699066162\n",
            "Training Iteration 2404, Loss: 4.869058609008789\n",
            "Training Iteration 2405, Loss: 5.383295059204102\n",
            "Training Iteration 2406, Loss: 5.487000465393066\n",
            "Training Iteration 2407, Loss: 2.6374034881591797\n",
            "Training Iteration 2408, Loss: 4.143030643463135\n",
            "Training Iteration 2409, Loss: 5.846704959869385\n",
            "Training Iteration 2410, Loss: 2.481477975845337\n",
            "Training Iteration 2411, Loss: 7.560640811920166\n",
            "Training Iteration 2412, Loss: 4.682458877563477\n",
            "Training Iteration 2413, Loss: 5.452643394470215\n",
            "Training Iteration 2414, Loss: 3.517496109008789\n",
            "Training Iteration 2415, Loss: 4.066446781158447\n",
            "Training Iteration 2416, Loss: 5.912927627563477\n",
            "Training Iteration 2417, Loss: 7.521596431732178\n",
            "Training Iteration 2418, Loss: 5.100128650665283\n",
            "Training Iteration 2419, Loss: 3.910245656967163\n",
            "Training Iteration 2420, Loss: 3.6364030838012695\n",
            "Training Iteration 2421, Loss: 4.537273406982422\n",
            "Training Iteration 2422, Loss: 5.67120885848999\n",
            "Training Iteration 2423, Loss: 2.1375889778137207\n",
            "Training Iteration 2424, Loss: 3.7657387256622314\n",
            "Training Iteration 2425, Loss: 6.72553014755249\n",
            "Training Iteration 2426, Loss: 6.9865193367004395\n",
            "Training Iteration 2427, Loss: 4.7031450271606445\n",
            "Training Iteration 2428, Loss: 4.976037502288818\n",
            "Training Iteration 2429, Loss: 2.880019187927246\n",
            "Training Iteration 2430, Loss: 3.5706467628479004\n",
            "Training Iteration 2431, Loss: 4.486415863037109\n",
            "Training Iteration 2432, Loss: 5.525552749633789\n",
            "Training Iteration 2433, Loss: 3.617558240890503\n",
            "Training Iteration 2434, Loss: 4.146637916564941\n",
            "Training Iteration 2435, Loss: 6.222720146179199\n",
            "Training Iteration 2436, Loss: 2.2852845191955566\n",
            "Training Iteration 2437, Loss: 8.00967788696289\n",
            "Training Iteration 2438, Loss: 7.304559230804443\n",
            "Training Iteration 2439, Loss: 3.9072163105010986\n",
            "Training Iteration 2440, Loss: 4.009565830230713\n",
            "Training Iteration 2441, Loss: 2.2795443534851074\n",
            "Training Iteration 2442, Loss: 5.390955448150635\n",
            "Training Iteration 2443, Loss: 4.6852240562438965\n",
            "Training Iteration 2444, Loss: 5.699162483215332\n",
            "Training Iteration 2445, Loss: 5.551267147064209\n",
            "Training Iteration 2446, Loss: 5.609130382537842\n",
            "Training Iteration 2447, Loss: 4.414045810699463\n",
            "Training Iteration 2448, Loss: 2.321662187576294\n",
            "Training Iteration 2449, Loss: 2.7627718448638916\n",
            "Training Iteration 2450, Loss: 6.264959335327148\n",
            "Training Iteration 2451, Loss: 3.454211950302124\n",
            "Training Iteration 2452, Loss: 2.6863808631896973\n",
            "Training Iteration 2453, Loss: 4.202889919281006\n",
            "Training Iteration 2454, Loss: 3.467289447784424\n",
            "Training Iteration 2455, Loss: 5.94122838973999\n",
            "Training Iteration 2456, Loss: 4.363425254821777\n",
            "Training Iteration 2457, Loss: 3.2267065048217773\n",
            "Training Iteration 2458, Loss: 3.004713773727417\n",
            "Training Iteration 2459, Loss: 5.062042236328125\n",
            "Training Iteration 2460, Loss: 3.5557212829589844\n",
            "Training Iteration 2461, Loss: 4.428064346313477\n",
            "Training Iteration 2462, Loss: 4.28639030456543\n",
            "Training Iteration 2463, Loss: 3.6085493564605713\n",
            "Training Iteration 2464, Loss: 4.057035446166992\n",
            "Training Iteration 2465, Loss: 6.6629638671875\n",
            "Training Iteration 2466, Loss: 5.552059650421143\n",
            "Training Iteration 2467, Loss: 5.3818888664245605\n",
            "Training Iteration 2468, Loss: 2.292480230331421\n",
            "Training Iteration 2469, Loss: 6.478872776031494\n",
            "Training Iteration 2470, Loss: 4.110562801361084\n",
            "Training Iteration 2471, Loss: 4.851332187652588\n",
            "Training Iteration 2472, Loss: 5.869207382202148\n",
            "Training Iteration 2473, Loss: 4.84889030456543\n",
            "Training Iteration 2474, Loss: 5.805800914764404\n",
            "Training Iteration 2475, Loss: 6.530735969543457\n",
            "Training Iteration 2476, Loss: 4.34820032119751\n",
            "Training Iteration 2477, Loss: 3.361067771911621\n",
            "Training Iteration 2478, Loss: 5.402940273284912\n",
            "Training Iteration 2479, Loss: 2.5457379817962646\n",
            "Training Iteration 2480, Loss: 3.726928234100342\n",
            "Training Iteration 2481, Loss: 6.265383720397949\n",
            "Training Iteration 2482, Loss: 8.110381126403809\n",
            "Training Iteration 2483, Loss: 3.922243595123291\n",
            "Training Iteration 2484, Loss: 7.585803985595703\n",
            "Training Iteration 2485, Loss: 3.8997302055358887\n",
            "Training Iteration 2486, Loss: 5.341218948364258\n",
            "Training Iteration 2487, Loss: 6.541227340698242\n",
            "Training Iteration 2488, Loss: 3.5560171604156494\n",
            "Training Iteration 2489, Loss: 7.035905838012695\n",
            "Training Iteration 2490, Loss: 2.976090431213379\n",
            "Training Iteration 2491, Loss: 4.573567867279053\n",
            "Training Iteration 2492, Loss: 4.6573591232299805\n",
            "Training Iteration 2493, Loss: 2.9251840114593506\n",
            "Training Iteration 2494, Loss: 6.701127529144287\n",
            "Training Iteration 2495, Loss: 2.9507713317871094\n",
            "Training Iteration 2496, Loss: 2.137064218521118\n",
            "Training Iteration 2497, Loss: 2.7829041481018066\n",
            "Training Iteration 2498, Loss: 2.9450178146362305\n",
            "Training Iteration 2499, Loss: 7.5825653076171875\n",
            "Training Iteration 2500, Loss: 2.2955069541931152\n",
            "Training Iteration 2501, Loss: 4.4223175048828125\n",
            "Training Iteration 2502, Loss: 6.402314186096191\n",
            "Training Iteration 2503, Loss: 4.815667152404785\n",
            "Training Iteration 2504, Loss: 6.515965461730957\n",
            "Training Iteration 2505, Loss: 2.208672523498535\n",
            "Training Iteration 2506, Loss: 4.049674987792969\n",
            "Training Iteration 2507, Loss: 2.5909335613250732\n",
            "Training Iteration 2508, Loss: 2.9564194679260254\n",
            "Training Iteration 2509, Loss: 2.4445152282714844\n",
            "Training Iteration 2510, Loss: 4.981102466583252\n",
            "Training Iteration 2511, Loss: 7.020181179046631\n",
            "Training Iteration 2512, Loss: 5.344505786895752\n",
            "Training Iteration 2513, Loss: 5.422063827514648\n",
            "Training Iteration 2514, Loss: 3.762450695037842\n",
            "Training Iteration 2515, Loss: 6.897367477416992\n",
            "Training Iteration 2516, Loss: 5.300595760345459\n",
            "Training Iteration 2517, Loss: 6.083348751068115\n",
            "Training Iteration 2518, Loss: 3.735988140106201\n",
            "Training Iteration 2519, Loss: 4.393121719360352\n",
            "Training Iteration 2520, Loss: 3.429243326187134\n",
            "Training Iteration 2521, Loss: 3.9834322929382324\n",
            "Training Iteration 2522, Loss: 4.146402359008789\n",
            "Training Iteration 2523, Loss: 2.7584710121154785\n",
            "Training Iteration 2524, Loss: 5.304155349731445\n",
            "Training Iteration 2525, Loss: 4.220612525939941\n",
            "Training Iteration 2526, Loss: 8.534488677978516\n",
            "Training Iteration 2527, Loss: 3.588118076324463\n",
            "Training Iteration 2528, Loss: 3.6227729320526123\n",
            "Training Iteration 2529, Loss: 3.06907320022583\n",
            "Training Iteration 2530, Loss: 5.734867095947266\n",
            "Training Iteration 2531, Loss: 4.739387512207031\n",
            "Training Iteration 2532, Loss: 4.583780765533447\n",
            "Training Iteration 2533, Loss: 4.1125264167785645\n",
            "Training Iteration 2534, Loss: 4.541217803955078\n",
            "Training Iteration 2535, Loss: 4.476559638977051\n",
            "Training Iteration 2536, Loss: 3.7784078121185303\n",
            "Training Iteration 2537, Loss: 7.063786506652832\n",
            "Training Iteration 2538, Loss: 4.741116046905518\n",
            "Training Iteration 2539, Loss: 5.690617561340332\n",
            "Training Iteration 2540, Loss: 2.1484291553497314\n",
            "Training Iteration 2541, Loss: 4.820537090301514\n",
            "Training Iteration 2542, Loss: 5.057098388671875\n",
            "Training Iteration 2543, Loss: 3.035015344619751\n",
            "Training Iteration 2544, Loss: 2.808835983276367\n",
            "Training Iteration 2545, Loss: 3.9970874786376953\n",
            "Training Iteration 2546, Loss: 3.494922161102295\n",
            "Training Iteration 2547, Loss: 4.522492408752441\n",
            "Training Iteration 2548, Loss: 4.989936828613281\n",
            "Training Iteration 2549, Loss: 5.682492256164551\n",
            "Training Iteration 2550, Loss: 4.368146896362305\n",
            "Training Iteration 2551, Loss: 3.2322874069213867\n",
            "Training Iteration 2552, Loss: 5.3644232749938965\n",
            "Training Iteration 2553, Loss: 5.431638717651367\n",
            "Training Iteration 2554, Loss: 4.491921424865723\n",
            "Training Iteration 2555, Loss: 6.377620220184326\n",
            "Training Iteration 2556, Loss: 7.0008134841918945\n",
            "Training Iteration 2557, Loss: 4.7389912605285645\n",
            "Training Iteration 2558, Loss: 2.4601221084594727\n",
            "Training Iteration 2559, Loss: 2.765561103820801\n",
            "Training Iteration 2560, Loss: 4.1279120445251465\n",
            "Training Iteration 2561, Loss: 6.036964416503906\n",
            "Training Iteration 2562, Loss: 5.293238162994385\n",
            "Training Iteration 2563, Loss: 4.9663238525390625\n",
            "Training Iteration 2564, Loss: 2.650125503540039\n",
            "Training Iteration 2565, Loss: 5.232694149017334\n",
            "Training Iteration 2566, Loss: 1.4958573579788208\n",
            "Training Iteration 2567, Loss: 4.866376876831055\n",
            "Training Iteration 2568, Loss: 5.470644474029541\n",
            "Training Iteration 2569, Loss: 4.051785945892334\n",
            "Training Iteration 2570, Loss: 4.160526752471924\n",
            "Training Iteration 2571, Loss: 2.1654622554779053\n",
            "Training Iteration 2572, Loss: 1.8310459852218628\n",
            "Training Iteration 2573, Loss: 3.816702365875244\n",
            "Training Iteration 2574, Loss: 4.5441741943359375\n",
            "Training Iteration 2575, Loss: 4.5753583908081055\n",
            "Training Iteration 2576, Loss: 3.7532854080200195\n",
            "Training Iteration 2577, Loss: 6.697719097137451\n",
            "Training Iteration 2578, Loss: 2.714460849761963\n",
            "Training Iteration 2579, Loss: 5.076091289520264\n",
            "Training Iteration 2580, Loss: 5.274327754974365\n",
            "Training Iteration 2581, Loss: 2.288923978805542\n",
            "Training Iteration 2582, Loss: 6.065421104431152\n",
            "Training Iteration 2583, Loss: 7.8539814949035645\n",
            "Training Iteration 2584, Loss: 3.9582679271698\n",
            "Training Iteration 2585, Loss: 7.853232383728027\n",
            "Training Iteration 2586, Loss: 4.286040782928467\n",
            "Training Iteration 2587, Loss: 3.535973072052002\n",
            "Training Iteration 2588, Loss: 6.544037818908691\n",
            "Training Iteration 2589, Loss: 4.658437252044678\n",
            "Training Iteration 2590, Loss: 4.89490270614624\n",
            "Training Iteration 2591, Loss: 4.673476696014404\n",
            "Training Iteration 2592, Loss: 5.572121620178223\n",
            "Training Iteration 2593, Loss: 5.665809631347656\n",
            "Training Iteration 2594, Loss: 7.807210445404053\n",
            "Training Iteration 2595, Loss: 4.152194499969482\n",
            "Training Iteration 2596, Loss: 3.351377010345459\n",
            "Training Iteration 2597, Loss: 5.017001152038574\n",
            "Training Iteration 2598, Loss: 2.2937235832214355\n",
            "Training Iteration 2599, Loss: 7.028611183166504\n",
            "Training Iteration 2600, Loss: 7.983318328857422\n",
            "Training Iteration 2601, Loss: 5.06678581237793\n",
            "Training Iteration 2602, Loss: 6.490564346313477\n",
            "Training Iteration 2603, Loss: 5.494837760925293\n",
            "Training Iteration 2604, Loss: 3.3322901725769043\n",
            "Training Iteration 2605, Loss: 3.2778191566467285\n",
            "Training Iteration 2606, Loss: 1.9410851001739502\n",
            "Training Iteration 2607, Loss: 5.449001789093018\n",
            "Training Iteration 2608, Loss: 8.478147506713867\n",
            "Training Iteration 2609, Loss: 5.79644775390625\n",
            "Training Iteration 2610, Loss: 3.5880227088928223\n",
            "Training Iteration 2611, Loss: 3.2838196754455566\n",
            "Training Iteration 2612, Loss: 7.254079341888428\n",
            "Training Iteration 2613, Loss: 4.316249847412109\n",
            "Training Iteration 2614, Loss: 3.1605570316314697\n",
            "Training Iteration 2615, Loss: 4.067874908447266\n",
            "Training Iteration 2616, Loss: 4.431553363800049\n",
            "Training Iteration 2617, Loss: 3.151183843612671\n",
            "Training Iteration 2618, Loss: 3.219113349914551\n",
            "Training Iteration 2619, Loss: 3.7298429012298584\n",
            "Training Iteration 2620, Loss: 3.27356219291687\n",
            "Training Iteration 2621, Loss: 4.218811988830566\n",
            "Training Iteration 2622, Loss: 3.835106372833252\n",
            "Training Iteration 2623, Loss: 5.196727752685547\n",
            "Training Iteration 2624, Loss: 3.468346118927002\n",
            "Training Iteration 2625, Loss: 7.770027160644531\n",
            "Training Iteration 2626, Loss: 4.6982316970825195\n",
            "Training Iteration 2627, Loss: 6.058271884918213\n",
            "Training Iteration 2628, Loss: 4.96933650970459\n",
            "Training Iteration 2629, Loss: 3.884122610092163\n",
            "Training Iteration 2630, Loss: 7.631833553314209\n",
            "Training Iteration 2631, Loss: 3.9878673553466797\n",
            "Training Iteration 2632, Loss: 7.582677841186523\n",
            "Training Iteration 2633, Loss: 6.877630233764648\n",
            "Training Iteration 2634, Loss: 2.0826663970947266\n",
            "Training Iteration 2635, Loss: 3.143704414367676\n",
            "Training Iteration 2636, Loss: 7.25454044342041\n",
            "Training Iteration 2637, Loss: 3.894747734069824\n",
            "Training Iteration 2638, Loss: 5.484076023101807\n",
            "Training Iteration 2639, Loss: 5.591562747955322\n",
            "Training Iteration 2640, Loss: 8.91385269165039\n",
            "Training Iteration 2641, Loss: 4.477194786071777\n",
            "Training Iteration 2642, Loss: 2.812246561050415\n",
            "Training Iteration 2643, Loss: 4.638558864593506\n",
            "Training Iteration 2644, Loss: 4.642523765563965\n",
            "Training Iteration 2645, Loss: 3.4878034591674805\n",
            "Training Iteration 2646, Loss: 5.30540657043457\n",
            "Training Iteration 2647, Loss: 6.572237491607666\n",
            "Training Iteration 2648, Loss: 6.373317718505859\n",
            "Training Iteration 2649, Loss: 6.347350120544434\n",
            "Training Iteration 2650, Loss: 6.510005474090576\n",
            "Training Iteration 2651, Loss: 3.5270748138427734\n",
            "Training Iteration 2652, Loss: 10.288121223449707\n",
            "Training Iteration 2653, Loss: 6.125730991363525\n",
            "Training Iteration 2654, Loss: 4.028314590454102\n",
            "Training Iteration 2655, Loss: 4.402982711791992\n",
            "Training Iteration 2656, Loss: 2.1175918579101562\n",
            "Training Iteration 2657, Loss: 3.0774569511413574\n",
            "Training Iteration 2658, Loss: 6.103472709655762\n",
            "Training Iteration 2659, Loss: 3.6515378952026367\n",
            "Training Iteration 2660, Loss: 2.6933209896087646\n",
            "Training Iteration 2661, Loss: 5.891398906707764\n",
            "Training Iteration 2662, Loss: 2.462251901626587\n",
            "Training Iteration 2663, Loss: 4.139784336090088\n",
            "Training Iteration 2664, Loss: 4.099854946136475\n",
            "Training Iteration 2665, Loss: 5.5024495124816895\n",
            "Training Iteration 2666, Loss: 5.138573169708252\n",
            "Training Iteration 2667, Loss: 4.712177753448486\n",
            "Training Iteration 2668, Loss: 4.655638694763184\n",
            "Training Iteration 2669, Loss: 5.711523056030273\n",
            "Training Iteration 2670, Loss: 6.163079738616943\n",
            "Training Iteration 2671, Loss: 4.835469722747803\n",
            "Training Iteration 2672, Loss: 3.4936881065368652\n",
            "Training Iteration 2673, Loss: 4.543187618255615\n",
            "Training Iteration 2674, Loss: 3.1776187419891357\n",
            "Training Iteration 2675, Loss: 3.0951766967773438\n",
            "Training Iteration 2676, Loss: 5.841489315032959\n",
            "Training Iteration 2677, Loss: 5.359123229980469\n",
            "Training Iteration 2678, Loss: 6.229651927947998\n",
            "Training Iteration 2679, Loss: 6.647882461547852\n",
            "Training Iteration 2680, Loss: 5.613299369812012\n",
            "Training Iteration 2681, Loss: 4.600968360900879\n",
            "Training Iteration 2682, Loss: 2.500476598739624\n",
            "Training Iteration 2683, Loss: 7.081122398376465\n",
            "Training Iteration 2684, Loss: 8.362298011779785\n",
            "Training Iteration 2685, Loss: 6.9094109535217285\n",
            "Training Iteration 2686, Loss: 5.375985145568848\n",
            "Training Iteration 2687, Loss: 6.435981750488281\n",
            "Training Iteration 2688, Loss: 5.3743085861206055\n",
            "Training Iteration 2689, Loss: 5.943258285522461\n",
            "Training Iteration 2690, Loss: 4.715402126312256\n",
            "Training Iteration 2691, Loss: 4.548077583312988\n",
            "Training Iteration 2692, Loss: 3.201566457748413\n",
            "Training Iteration 2693, Loss: 5.525398254394531\n",
            "Training Iteration 2694, Loss: 3.413832664489746\n",
            "Training Iteration 2695, Loss: 5.744233131408691\n",
            "Training Iteration 2696, Loss: 5.163254737854004\n",
            "Training Iteration 2697, Loss: 4.089011192321777\n",
            "Training Iteration 2698, Loss: 2.9349284172058105\n",
            "Training Iteration 2699, Loss: 0.7905256748199463\n",
            "Training Iteration 2700, Loss: 3.468379497528076\n",
            "Training Iteration 2701, Loss: 2.9106736183166504\n",
            "Training Iteration 2702, Loss: 6.607137680053711\n",
            "Training Iteration 2703, Loss: 7.529219627380371\n",
            "Training Iteration 2704, Loss: 3.899453639984131\n",
            "Training Iteration 2705, Loss: 2.9232304096221924\n",
            "Training Iteration 2706, Loss: 4.994881629943848\n",
            "Training Iteration 2707, Loss: 6.8856048583984375\n",
            "Training Iteration 2708, Loss: 5.832619667053223\n",
            "Training Iteration 2709, Loss: 5.876758575439453\n",
            "Training Iteration 2710, Loss: 5.412132263183594\n",
            "Training Iteration 2711, Loss: 2.7935171127319336\n",
            "Training Iteration 2712, Loss: 3.4023427963256836\n",
            "Training Iteration 2713, Loss: 4.851959705352783\n",
            "Training Iteration 2714, Loss: 2.2122349739074707\n",
            "Training Iteration 2715, Loss: 4.122020721435547\n",
            "Training Iteration 2716, Loss: 3.7599847316741943\n",
            "Training Iteration 2717, Loss: 2.9996066093444824\n",
            "Training Iteration 2718, Loss: 4.90817928314209\n",
            "Training Iteration 2719, Loss: 2.760258436203003\n",
            "Training Iteration 2720, Loss: 5.166323184967041\n",
            "Training Iteration 2721, Loss: 5.988077163696289\n",
            "Training Iteration 2722, Loss: 3.811117172241211\n",
            "Training Iteration 2723, Loss: 3.92531418800354\n",
            "Training Iteration 2724, Loss: 4.522929668426514\n",
            "Training Iteration 2725, Loss: 3.7224960327148438\n",
            "Training Iteration 2726, Loss: 6.00750732421875\n",
            "Training Iteration 2727, Loss: 4.283816814422607\n",
            "Training Iteration 2728, Loss: 3.412334442138672\n",
            "Training Iteration 2729, Loss: 5.610185146331787\n",
            "Training Iteration 2730, Loss: 4.305627822875977\n",
            "Training Iteration 2731, Loss: 3.174391746520996\n",
            "Training Iteration 2732, Loss: 5.90951681137085\n",
            "Training Iteration 2733, Loss: 3.458174705505371\n",
            "Training Iteration 2734, Loss: 5.275997638702393\n",
            "Training Iteration 2735, Loss: 3.113203525543213\n",
            "Training Iteration 2736, Loss: 2.0673322677612305\n",
            "Training Iteration 2737, Loss: 5.007773399353027\n",
            "Training Iteration 2738, Loss: 5.000077247619629\n",
            "Training Iteration 2739, Loss: 3.9927380084991455\n",
            "Training Iteration 2740, Loss: 3.6206917762756348\n",
            "Training Iteration 2741, Loss: 3.035921096801758\n",
            "Training Iteration 2742, Loss: 6.214544773101807\n",
            "Training Iteration 2743, Loss: 4.1458024978637695\n",
            "Training Iteration 2744, Loss: 5.844188690185547\n",
            "Training Iteration 2745, Loss: 2.283557176589966\n",
            "Training Iteration 2746, Loss: 4.864917755126953\n",
            "Training Iteration 2747, Loss: 2.8028082847595215\n",
            "Training Iteration 2748, Loss: 3.1360549926757812\n",
            "Training Iteration 2749, Loss: 3.191725969314575\n",
            "Training Iteration 2750, Loss: 3.0242888927459717\n",
            "Training Iteration 2751, Loss: 3.15222430229187\n",
            "Training Iteration 2752, Loss: 4.825987339019775\n",
            "Training Iteration 2753, Loss: 6.474402904510498\n",
            "Training Iteration 2754, Loss: 2.186180591583252\n",
            "Training Iteration 2755, Loss: 5.403547763824463\n",
            "Training Iteration 2756, Loss: 2.6699275970458984\n",
            "Training Iteration 2757, Loss: 2.339869737625122\n",
            "Training Iteration 2758, Loss: 3.7583649158477783\n",
            "Training Iteration 2759, Loss: 2.9256677627563477\n",
            "Training Iteration 2760, Loss: 2.5499448776245117\n",
            "Training Iteration 2761, Loss: 4.718812465667725\n",
            "Training Iteration 2762, Loss: 7.580294609069824\n",
            "Training Iteration 2763, Loss: 6.799344539642334\n",
            "Training Iteration 2764, Loss: 2.053270101547241\n",
            "Training Iteration 2765, Loss: 6.265366554260254\n",
            "Training Iteration 2766, Loss: 2.685790538787842\n",
            "Training Iteration 2767, Loss: 3.5046470165252686\n",
            "Training Iteration 2768, Loss: 7.939093112945557\n",
            "Training Iteration 2769, Loss: 3.1154732704162598\n",
            "Training Iteration 2770, Loss: 7.048894882202148\n",
            "Training Iteration 2771, Loss: 3.0997653007507324\n",
            "Training Iteration 2772, Loss: 3.3249521255493164\n",
            "Training Iteration 2773, Loss: 4.744629383087158\n",
            "Training Iteration 2774, Loss: 1.6249698400497437\n",
            "Training Iteration 2775, Loss: 5.252193927764893\n",
            "Training Iteration 2776, Loss: 6.275918960571289\n",
            "Training Iteration 2777, Loss: 4.963493824005127\n",
            "Training Iteration 2778, Loss: 6.48841667175293\n",
            "Training Iteration 2779, Loss: 3.087780714035034\n",
            "Training Iteration 2780, Loss: 5.018817901611328\n",
            "Training Iteration 2781, Loss: 4.5263543128967285\n",
            "Training Iteration 2782, Loss: 5.045310974121094\n",
            "Training Iteration 2783, Loss: 8.331552505493164\n",
            "Training Iteration 2784, Loss: 5.443958759307861\n",
            "Training Iteration 2785, Loss: 6.52921199798584\n",
            "Training Iteration 2786, Loss: 4.872901439666748\n",
            "Training Iteration 2787, Loss: 4.882552146911621\n",
            "Training Iteration 2788, Loss: 2.7066004276275635\n",
            "Training Iteration 2789, Loss: 6.807276725769043\n",
            "Training Iteration 2790, Loss: 3.3085029125213623\n",
            "Training Iteration 2791, Loss: 3.1603260040283203\n",
            "Training Iteration 2792, Loss: 1.717809796333313\n",
            "Training Iteration 2793, Loss: 3.452136516571045\n",
            "Training Iteration 2794, Loss: 6.570775032043457\n",
            "Training Iteration 2795, Loss: 3.3110451698303223\n",
            "Training Iteration 2796, Loss: 3.509791851043701\n",
            "Training Iteration 2797, Loss: 7.7084503173828125\n",
            "Training Iteration 2798, Loss: 11.366342544555664\n",
            "Training Iteration 2799, Loss: 3.4219970703125\n",
            "Training Iteration 2800, Loss: 4.772070407867432\n",
            "Training Iteration 2801, Loss: 4.421891212463379\n",
            "Training Iteration 2802, Loss: 5.8577728271484375\n",
            "Training Iteration 2803, Loss: 6.068419933319092\n",
            "Training Iteration 2804, Loss: 3.1359519958496094\n",
            "Training Iteration 2805, Loss: 8.221369743347168\n",
            "Training Iteration 2806, Loss: 7.950909614562988\n",
            "Training Iteration 2807, Loss: 2.7171199321746826\n",
            "Training Iteration 2808, Loss: 2.553967237472534\n",
            "Training Iteration 2809, Loss: 3.829500198364258\n",
            "Training Iteration 2810, Loss: 2.9828407764434814\n",
            "Training Iteration 2811, Loss: 3.446181297302246\n",
            "Training Iteration 2812, Loss: 10.528300285339355\n",
            "Training Iteration 2813, Loss: 2.4669883251190186\n",
            "Training Iteration 2814, Loss: 4.2819647789001465\n",
            "Training Iteration 2815, Loss: 2.9068925380706787\n",
            "Training Iteration 2816, Loss: 7.430402755737305\n",
            "Training Iteration 2817, Loss: 8.686463356018066\n",
            "Training Iteration 2818, Loss: 3.18159818649292\n",
            "Training Iteration 2819, Loss: 5.8848795890808105\n",
            "Training Iteration 2820, Loss: 4.903493404388428\n",
            "Training Iteration 2821, Loss: 3.8228819370269775\n",
            "Training Iteration 2822, Loss: 2.236703634262085\n",
            "Training Iteration 2823, Loss: 6.319500923156738\n",
            "Training Iteration 2824, Loss: 4.035701274871826\n",
            "Training Iteration 2825, Loss: 5.050571918487549\n",
            "Training Iteration 2826, Loss: 5.2529988288879395\n",
            "Training Iteration 2827, Loss: 3.5577826499938965\n",
            "Training Iteration 2828, Loss: 4.322842597961426\n",
            "Training Iteration 2829, Loss: 3.888268232345581\n",
            "Training Iteration 2830, Loss: 3.4497954845428467\n",
            "Training Iteration 2831, Loss: 4.152719020843506\n",
            "Training Iteration 2832, Loss: 5.005446910858154\n",
            "Training Iteration 2833, Loss: 6.1500091552734375\n",
            "Training Iteration 2834, Loss: 3.994272232055664\n",
            "Training Iteration 2835, Loss: 2.7978975772857666\n",
            "Training Iteration 2836, Loss: 6.682962894439697\n",
            "Training Iteration 2837, Loss: 4.092134475708008\n",
            "Training Iteration 2838, Loss: 5.718297481536865\n",
            "Training Iteration 2839, Loss: 4.247195243835449\n",
            "Training Iteration 2840, Loss: 3.6340150833129883\n",
            "Training Iteration 2841, Loss: 5.271688461303711\n",
            "Training Iteration 2842, Loss: 5.297297954559326\n",
            "Training Iteration 2843, Loss: 3.924549102783203\n",
            "Training Iteration 2844, Loss: 4.160684108734131\n",
            "Training Iteration 2845, Loss: 6.319621562957764\n",
            "Training Iteration 2846, Loss: 4.142292499542236\n",
            "Training Iteration 2847, Loss: 7.4248046875\n",
            "Training Iteration 2848, Loss: 3.661137342453003\n",
            "Training Iteration 2849, Loss: 6.246407508850098\n",
            "Training Iteration 2850, Loss: 3.2890655994415283\n",
            "Training Iteration 2851, Loss: 3.4818949699401855\n",
            "Training Iteration 2852, Loss: 4.609030723571777\n",
            "Training Iteration 2853, Loss: 5.303188800811768\n",
            "Training Iteration 2854, Loss: 5.212085723876953\n",
            "Training Iteration 2855, Loss: 3.742661476135254\n",
            "Training Iteration 2856, Loss: 1.8060230016708374\n",
            "Training Iteration 2857, Loss: 3.590388536453247\n",
            "Training Iteration 2858, Loss: 3.024923801422119\n",
            "Training Iteration 2859, Loss: 2.909083127975464\n",
            "Training Iteration 2860, Loss: 5.25065803527832\n",
            "Training Iteration 2861, Loss: 4.021512508392334\n",
            "Training Iteration 2862, Loss: 1.8716942071914673\n",
            "Training Iteration 2863, Loss: 4.359659194946289\n",
            "Training Iteration 2864, Loss: 3.805190086364746\n",
            "Training Iteration 2865, Loss: 5.974030017852783\n",
            "Training Iteration 2866, Loss: 2.1412153244018555\n",
            "Training Iteration 2867, Loss: 3.4430580139160156\n",
            "Training Iteration 2868, Loss: 2.0485222339630127\n",
            "Training Iteration 2869, Loss: 4.0550537109375\n",
            "Training Iteration 2870, Loss: 3.583488702774048\n",
            "Training Iteration 2871, Loss: 3.3526039123535156\n",
            "Training Iteration 2872, Loss: 5.5199785232543945\n",
            "Training Iteration 2873, Loss: 3.5764784812927246\n",
            "Training Iteration 2874, Loss: 2.8818933963775635\n",
            "Training Iteration 2875, Loss: 3.320941925048828\n",
            "Training Iteration 2876, Loss: 3.1847331523895264\n",
            "Training Iteration 2877, Loss: 2.4645984172821045\n",
            "Training Iteration 2878, Loss: 4.0689005851745605\n",
            "Training Iteration 2879, Loss: 4.83030891418457\n",
            "Training Iteration 2880, Loss: 7.7316694259643555\n",
            "Training Iteration 2881, Loss: 6.523961544036865\n",
            "Training Iteration 2882, Loss: 6.189907550811768\n",
            "Training Iteration 2883, Loss: 4.551002502441406\n",
            "Training Iteration 2884, Loss: 3.7156622409820557\n",
            "Training Iteration 2885, Loss: 3.6411097049713135\n",
            "Training Iteration 2886, Loss: 4.419503211975098\n",
            "Training Iteration 2887, Loss: 4.462422847747803\n",
            "Training Iteration 2888, Loss: 2.2798304557800293\n",
            "Training Iteration 2889, Loss: 4.336993217468262\n",
            "Training Iteration 2890, Loss: 5.908324718475342\n",
            "Training Iteration 2891, Loss: 3.3674511909484863\n",
            "Training Iteration 2892, Loss: 3.2907235622406006\n",
            "Training Iteration 2893, Loss: 4.0774664878845215\n",
            "Training Iteration 2894, Loss: 3.3223090171813965\n",
            "Training Iteration 2895, Loss: 3.315164804458618\n",
            "Training Iteration 2896, Loss: 6.427273273468018\n",
            "Training Iteration 2897, Loss: 4.779965400695801\n",
            "Training Iteration 2898, Loss: 5.126732349395752\n",
            "Training Iteration 2899, Loss: 5.107622146606445\n",
            "Training Iteration 2900, Loss: 2.7570416927337646\n",
            "Training Iteration 2901, Loss: 2.0058228969573975\n",
            "Training Iteration 2902, Loss: 2.9853503704071045\n",
            "Training Iteration 2903, Loss: 3.24379825592041\n",
            "Training Iteration 2904, Loss: 5.908746242523193\n",
            "Training Iteration 2905, Loss: 3.452981948852539\n",
            "Training Iteration 2906, Loss: 4.644369125366211\n",
            "Training Iteration 2907, Loss: 3.7030723094940186\n",
            "Training Iteration 2908, Loss: 5.744344711303711\n",
            "Training Iteration 2909, Loss: 5.015156269073486\n",
            "Training Iteration 2910, Loss: 2.1381092071533203\n",
            "Training Iteration 2911, Loss: 3.9755303859710693\n",
            "Training Iteration 2912, Loss: 3.9673519134521484\n",
            "Training Iteration 2913, Loss: 4.226597785949707\n",
            "Training Iteration 2914, Loss: 7.167839050292969\n",
            "Training Iteration 2915, Loss: 4.736382007598877\n",
            "Training Iteration 2916, Loss: 4.875735759735107\n",
            "Training Iteration 2917, Loss: 5.100322246551514\n",
            "Training Iteration 2918, Loss: 3.9956347942352295\n",
            "Training Iteration 2919, Loss: 2.8426666259765625\n",
            "Training Iteration 2920, Loss: 3.379714012145996\n",
            "Training Iteration 2921, Loss: 2.670567035675049\n",
            "Training Iteration 2922, Loss: 5.948186874389648\n",
            "Training Iteration 2923, Loss: 2.4930272102355957\n",
            "Training Iteration 2924, Loss: 5.683260440826416\n",
            "Training Iteration 2925, Loss: 2.3281586170196533\n",
            "Training Iteration 2926, Loss: 3.1051278114318848\n",
            "Training Iteration 2927, Loss: 4.573376178741455\n",
            "Training Iteration 2928, Loss: 5.284734725952148\n",
            "Training Iteration 2929, Loss: 3.470848321914673\n",
            "Training Iteration 2930, Loss: 3.304109573364258\n",
            "Training Iteration 2931, Loss: 6.478744029998779\n",
            "Training Iteration 2932, Loss: 2.3475217819213867\n",
            "Training Iteration 2933, Loss: 5.001171112060547\n",
            "Training Iteration 2934, Loss: 4.819321632385254\n",
            "Training Iteration 2935, Loss: 4.238475322723389\n",
            "Training Iteration 2936, Loss: 3.3337109088897705\n",
            "Training Iteration 2937, Loss: 6.295887470245361\n",
            "Training Iteration 2938, Loss: 8.099961280822754\n",
            "Training Iteration 2939, Loss: 2.6445791721343994\n",
            "Training Iteration 2940, Loss: 3.80916690826416\n",
            "Training Iteration 2941, Loss: 3.8425307273864746\n",
            "Training Iteration 2942, Loss: 5.5516133308410645\n",
            "Training Iteration 2943, Loss: 3.3604235649108887\n",
            "Training Iteration 2944, Loss: 2.219209909439087\n",
            "Training Iteration 2945, Loss: 4.893091201782227\n",
            "Training Iteration 2946, Loss: 4.207652568817139\n",
            "Training Iteration 2947, Loss: 3.2614171504974365\n",
            "Training Iteration 2948, Loss: 4.567624568939209\n",
            "Training Iteration 2949, Loss: 3.7504749298095703\n",
            "Training Iteration 2950, Loss: 2.2033979892730713\n",
            "Training Iteration 2951, Loss: 8.2349271774292\n",
            "Training Iteration 2952, Loss: 4.544805526733398\n",
            "Training Iteration 2953, Loss: 4.278336524963379\n",
            "Training Iteration 2954, Loss: 4.698888301849365\n",
            "Training Iteration 2955, Loss: 3.5307698249816895\n",
            "Training Iteration 2956, Loss: 6.468221187591553\n",
            "Training Iteration 2957, Loss: 2.9837708473205566\n",
            "Training Iteration 2958, Loss: 2.1914360523223877\n",
            "Training Iteration 2959, Loss: 4.0419921875\n",
            "Training Iteration 2960, Loss: 5.3437042236328125\n",
            "Training Iteration 2961, Loss: 4.6252760887146\n",
            "Training Iteration 2962, Loss: 5.853081226348877\n",
            "Training Iteration 2963, Loss: 5.140523433685303\n",
            "Training Iteration 2964, Loss: 4.211088180541992\n",
            "Training Iteration 2965, Loss: 5.25422477722168\n",
            "Training Iteration 2966, Loss: 5.4589619636535645\n",
            "Training Iteration 2967, Loss: 4.117472171783447\n",
            "Training Iteration 2968, Loss: 6.989717483520508\n",
            "Training Iteration 2969, Loss: 4.828545093536377\n",
            "Training Iteration 2970, Loss: 3.8684263229370117\n",
            "Training Iteration 2971, Loss: 4.219938278198242\n",
            "Training Iteration 2972, Loss: 4.829449653625488\n",
            "Training Iteration 2973, Loss: 6.131188869476318\n",
            "Training Iteration 2974, Loss: 2.1830122470855713\n",
            "Training Iteration 2975, Loss: 3.85282826423645\n",
            "Training Iteration 2976, Loss: 2.4557056427001953\n",
            "Training Iteration 2977, Loss: 2.9102790355682373\n",
            "Training Iteration 2978, Loss: 7.8965349197387695\n",
            "Training Iteration 2979, Loss: 5.646600723266602\n",
            "Training Iteration 2980, Loss: 3.025986909866333\n",
            "Training Iteration 2981, Loss: 6.126824855804443\n",
            "Training Iteration 2982, Loss: 10.613824844360352\n",
            "Training Iteration 2983, Loss: 9.021711349487305\n",
            "Training Iteration 2984, Loss: 2.2491872310638428\n",
            "Training Iteration 2985, Loss: 5.65970516204834\n",
            "Training Iteration 2986, Loss: 3.486185073852539\n",
            "Training Iteration 2987, Loss: 4.547027587890625\n",
            "Training Iteration 2988, Loss: 4.841549396514893\n",
            "Training Iteration 2989, Loss: 5.5826849937438965\n",
            "Training Iteration 2990, Loss: 3.4184789657592773\n",
            "Training Iteration 2991, Loss: 4.2163166999816895\n",
            "Training Iteration 2992, Loss: 3.7148079872131348\n",
            "Training Iteration 2993, Loss: 4.129148483276367\n",
            "Training Iteration 2994, Loss: 6.141989707946777\n",
            "Training Iteration 2995, Loss: 3.6170895099639893\n",
            "Training Iteration 2996, Loss: 4.298671722412109\n",
            "Training Iteration 2997, Loss: 6.321619987487793\n",
            "Training Iteration 2998, Loss: 4.246929168701172\n",
            "Training Iteration 2999, Loss: 5.980623245239258\n",
            "Training Iteration 3000, Loss: 2.7176833152770996\n",
            "Training Iteration 3001, Loss: 2.0396180152893066\n",
            "Training Iteration 3002, Loss: 2.330500364303589\n",
            "Training Iteration 3003, Loss: 2.7553811073303223\n",
            "Training Iteration 3004, Loss: 4.9360671043396\n",
            "Training Iteration 3005, Loss: 3.712759494781494\n",
            "Training Iteration 3006, Loss: 5.499463081359863\n",
            "Training Iteration 3007, Loss: 4.698605537414551\n",
            "Training Iteration 3008, Loss: 5.474431037902832\n",
            "Training Iteration 3009, Loss: 2.9264063835144043\n",
            "Training Iteration 3010, Loss: 3.9524929523468018\n",
            "Training Iteration 3011, Loss: 7.080953121185303\n",
            "Training Iteration 3012, Loss: 5.6526665687561035\n",
            "Training Iteration 3013, Loss: 5.2464189529418945\n",
            "Training Iteration 3014, Loss: 5.442080020904541\n",
            "Training Iteration 3015, Loss: 2.9050357341766357\n",
            "Training Iteration 3016, Loss: 4.511958122253418\n",
            "Training Iteration 3017, Loss: 4.112363338470459\n",
            "Training Iteration 3018, Loss: 4.749354362487793\n",
            "Training Iteration 3019, Loss: 3.591193437576294\n",
            "Training Iteration 3020, Loss: 4.268150806427002\n",
            "Training Iteration 3021, Loss: 5.777771949768066\n",
            "Training Iteration 3022, Loss: 2.2100255489349365\n",
            "Training Iteration 3023, Loss: 6.934764385223389\n",
            "Training Iteration 3024, Loss: 5.340672016143799\n",
            "Training Iteration 3025, Loss: 6.5648298263549805\n",
            "Training Iteration 3026, Loss: 4.730310916900635\n",
            "Training Iteration 3027, Loss: 4.196842193603516\n",
            "Training Iteration 3028, Loss: 3.5288374423980713\n",
            "Training Iteration 3029, Loss: 4.914342403411865\n",
            "Training Iteration 3030, Loss: 2.7201051712036133\n",
            "Training Iteration 3031, Loss: 3.3764986991882324\n",
            "Training Iteration 3032, Loss: 5.057717323303223\n",
            "Training Iteration 3033, Loss: 5.149841785430908\n",
            "Training Iteration 3034, Loss: 5.491067409515381\n",
            "Training Iteration 3035, Loss: 3.979409694671631\n",
            "Training Iteration 3036, Loss: 3.685722589492798\n",
            "Training Iteration 3037, Loss: 5.715215682983398\n",
            "Training Iteration 3038, Loss: 7.57193660736084\n",
            "Training Iteration 3039, Loss: 3.7036073207855225\n",
            "Training Iteration 3040, Loss: 2.4557297229766846\n",
            "Training Iteration 3041, Loss: 5.624855995178223\n",
            "Training Iteration 3042, Loss: 0.83798748254776\n",
            "Training Iteration 3043, Loss: 3.4758217334747314\n",
            "Training Iteration 3044, Loss: 3.203685760498047\n",
            "Training Iteration 3045, Loss: 1.914718747138977\n",
            "Training Iteration 3046, Loss: 2.747880697250366\n",
            "Training Iteration 3047, Loss: 2.278561592102051\n",
            "Training Iteration 3048, Loss: 4.479616165161133\n",
            "Training Iteration 3049, Loss: 6.208968639373779\n",
            "Training Iteration 3050, Loss: 2.457089424133301\n",
            "Training Iteration 3051, Loss: 3.454899311065674\n",
            "Training Iteration 3052, Loss: 10.053865432739258\n",
            "Training Iteration 3053, Loss: 5.136227607727051\n",
            "Training Iteration 3054, Loss: 7.741992473602295\n",
            "Training Iteration 3055, Loss: 3.441258192062378\n",
            "Training Iteration 3056, Loss: 3.2684037685394287\n",
            "Training Iteration 3057, Loss: 6.755715847015381\n",
            "Training Iteration 3058, Loss: 4.573534965515137\n",
            "Training Iteration 3059, Loss: 3.760303497314453\n",
            "Training Iteration 3060, Loss: 3.518523693084717\n",
            "Training Iteration 3061, Loss: 4.549498558044434\n",
            "Training Iteration 3062, Loss: 5.0818047523498535\n",
            "Training Iteration 3063, Loss: 3.709580183029175\n",
            "Training Iteration 3064, Loss: 3.536006450653076\n",
            "Training Iteration 3065, Loss: 4.100281238555908\n",
            "Training Iteration 3066, Loss: 6.560893535614014\n",
            "Training Iteration 3067, Loss: 5.609204292297363\n",
            "Training Iteration 3068, Loss: 5.502784729003906\n",
            "Training Iteration 3069, Loss: 5.469810962677002\n",
            "Training Iteration 3070, Loss: 4.693923473358154\n",
            "Training Iteration 3071, Loss: 5.875250339508057\n",
            "Training Iteration 3072, Loss: 6.653830528259277\n",
            "Training Iteration 3073, Loss: 5.473320960998535\n",
            "Training Iteration 3074, Loss: 5.858489036560059\n",
            "Training Iteration 3075, Loss: 6.276768207550049\n",
            "Training Iteration 3076, Loss: 5.296386241912842\n",
            "Training Iteration 3077, Loss: 4.326643943786621\n",
            "Training Iteration 3078, Loss: 4.577090740203857\n",
            "Training Iteration 3079, Loss: 3.7061328887939453\n",
            "Training Iteration 3080, Loss: 4.2293853759765625\n",
            "Training Iteration 3081, Loss: 8.124018669128418\n",
            "Training Iteration 3082, Loss: 8.192830085754395\n",
            "Training Iteration 3083, Loss: 5.483213424682617\n",
            "Training Iteration 3084, Loss: 7.751382827758789\n",
            "Training Iteration 3085, Loss: 5.007420063018799\n",
            "Training Iteration 3086, Loss: 3.4657022953033447\n",
            "Training Iteration 3087, Loss: 2.206317186355591\n",
            "Training Iteration 3088, Loss: 5.57447624206543\n",
            "Training Iteration 3089, Loss: 5.161850452423096\n",
            "Training Iteration 3090, Loss: 3.746889591217041\n",
            "Training Iteration 3091, Loss: 7.766156196594238\n",
            "Training Iteration 3092, Loss: 5.692407131195068\n",
            "Training Iteration 3093, Loss: 3.993330478668213\n",
            "Training Iteration 3094, Loss: 4.469265460968018\n",
            "Training Iteration 3095, Loss: 3.9704437255859375\n",
            "Training Iteration 3096, Loss: 5.862033843994141\n",
            "Training Iteration 3097, Loss: 7.487380027770996\n",
            "Training Iteration 3098, Loss: 4.15753698348999\n",
            "Training Iteration 3099, Loss: 5.68625020980835\n",
            "Training Iteration 3100, Loss: 5.261085033416748\n",
            "Training Iteration 3101, Loss: 5.41464900970459\n",
            "Training Iteration 3102, Loss: 6.075459957122803\n",
            "Training Iteration 3103, Loss: 6.9518585205078125\n",
            "Training Iteration 3104, Loss: 4.79334831237793\n",
            "Training Iteration 3105, Loss: 4.276893615722656\n",
            "Training Iteration 3106, Loss: 2.294623851776123\n",
            "Training Iteration 3107, Loss: 4.371198654174805\n",
            "Training Iteration 3108, Loss: 9.745123863220215\n",
            "Training Iteration 3109, Loss: 5.805469989776611\n",
            "Training Iteration 3110, Loss: 2.04720401763916\n",
            "Training Iteration 3111, Loss: 2.870115280151367\n",
            "Training Iteration 3112, Loss: 2.489534378051758\n",
            "Training Iteration 3113, Loss: 2.7662534713745117\n",
            "Training Iteration 3114, Loss: 6.187178611755371\n",
            "Training Iteration 3115, Loss: 3.752096176147461\n",
            "Training Iteration 3116, Loss: 3.9502484798431396\n",
            "Training Iteration 3117, Loss: 3.398200511932373\n",
            "Training Iteration 3118, Loss: 5.3998517990112305\n",
            "Training Iteration 3119, Loss: 4.170872688293457\n",
            "Training Iteration 3120, Loss: 5.490147113800049\n",
            "Training Iteration 3121, Loss: 2.6780025959014893\n",
            "Training Iteration 3122, Loss: 3.9309070110321045\n",
            "Training Iteration 3123, Loss: 5.082758903503418\n",
            "Training Iteration 3124, Loss: 6.055615425109863\n",
            "Training Iteration 3125, Loss: 3.065099000930786\n",
            "Training Iteration 3126, Loss: 7.784895420074463\n",
            "Training Iteration 3127, Loss: 5.402396202087402\n",
            "Training Iteration 3128, Loss: 3.875413417816162\n",
            "Training Iteration 3129, Loss: 5.128884315490723\n",
            "Training Iteration 3130, Loss: 4.265573024749756\n",
            "Training Iteration 3131, Loss: 4.773125648498535\n",
            "Training Iteration 3132, Loss: 4.269898414611816\n",
            "Training Iteration 3133, Loss: 2.9232044219970703\n",
            "Training Iteration 3134, Loss: 2.8067963123321533\n",
            "Training Iteration 3135, Loss: 4.168660640716553\n",
            "Training Iteration 3136, Loss: 4.328204154968262\n",
            "Training Iteration 3137, Loss: 3.423173666000366\n",
            "Training Iteration 3138, Loss: 3.5563907623291016\n",
            "Training Iteration 3139, Loss: 4.087087154388428\n",
            "Training Iteration 3140, Loss: 2.577439069747925\n",
            "Training Iteration 3141, Loss: 4.247840881347656\n",
            "Training Iteration 3142, Loss: 4.702108860015869\n",
            "Training Iteration 3143, Loss: 6.15069055557251\n",
            "Training Iteration 3144, Loss: 7.511408805847168\n",
            "Training Iteration 3145, Loss: 5.670186519622803\n",
            "Training Iteration 3146, Loss: 3.590430736541748\n",
            "Training Iteration 3147, Loss: 4.221486568450928\n",
            "Training Iteration 3148, Loss: 5.435669898986816\n",
            "Training Iteration 3149, Loss: 3.1516082286834717\n",
            "Training Iteration 3150, Loss: 5.273901462554932\n",
            "Training Iteration 3151, Loss: 3.6491522789001465\n",
            "Training Iteration 3152, Loss: 2.6040899753570557\n",
            "Training Iteration 3153, Loss: 2.9090476036071777\n",
            "Training Iteration 3154, Loss: 2.9886362552642822\n",
            "Training Iteration 3155, Loss: 4.992363452911377\n",
            "Training Iteration 3156, Loss: 2.905694007873535\n",
            "Training Iteration 3157, Loss: 2.074867010116577\n",
            "Training Iteration 3158, Loss: 2.6158571243286133\n",
            "Training Iteration 3159, Loss: 1.6053615808486938\n",
            "Training Iteration 3160, Loss: 3.306018352508545\n",
            "Training Iteration 3161, Loss: 4.338944435119629\n",
            "Training Iteration 3162, Loss: 6.410797119140625\n",
            "Training Iteration 3163, Loss: 6.49687385559082\n",
            "Training Iteration 3164, Loss: 5.345139026641846\n",
            "Training Iteration 3165, Loss: 1.2961766719818115\n",
            "Training Iteration 3166, Loss: 6.286626815795898\n",
            "Training Iteration 3167, Loss: 7.158828258514404\n",
            "Training Iteration 3168, Loss: 3.7057065963745117\n",
            "Training Iteration 3169, Loss: 9.077329635620117\n",
            "Training Iteration 3170, Loss: 4.093708515167236\n",
            "Training Iteration 3171, Loss: 3.7898619174957275\n",
            "Training Iteration 3172, Loss: 7.0515360832214355\n",
            "Training Iteration 3173, Loss: 2.9016318321228027\n",
            "Training Iteration 3174, Loss: 5.246543884277344\n",
            "Training Iteration 3175, Loss: 4.238648891448975\n",
            "Training Iteration 3176, Loss: 7.340396881103516\n",
            "Training Iteration 3177, Loss: 5.0502519607543945\n",
            "Training Iteration 3178, Loss: 4.969715595245361\n",
            "Training Iteration 3179, Loss: 5.039468765258789\n",
            "Training Iteration 3180, Loss: 5.097601890563965\n",
            "Training Iteration 3181, Loss: 3.0253398418426514\n",
            "Training Iteration 3182, Loss: 2.594207763671875\n",
            "Training Iteration 3183, Loss: 3.117826461791992\n",
            "Training Iteration 3184, Loss: 4.615713119506836\n",
            "Training Iteration 3185, Loss: 2.6437597274780273\n",
            "Training Iteration 3186, Loss: 2.183711051940918\n",
            "Training Iteration 3187, Loss: 7.3050336837768555\n",
            "Training Iteration 3188, Loss: 5.380239963531494\n",
            "Training Iteration 3189, Loss: 3.9521608352661133\n",
            "Training Iteration 3190, Loss: 4.855851173400879\n",
            "Training Iteration 3191, Loss: 6.226964473724365\n",
            "Training Iteration 3192, Loss: 3.687967300415039\n",
            "Training Iteration 3193, Loss: 4.4835286140441895\n",
            "Training Iteration 3194, Loss: 6.005112171173096\n",
            "Training Iteration 3195, Loss: 3.808769702911377\n",
            "Training Iteration 3196, Loss: 3.8387856483459473\n",
            "Training Iteration 3197, Loss: 5.189359664916992\n",
            "Training Iteration 3198, Loss: 5.096990585327148\n",
            "Training Iteration 3199, Loss: 3.442305088043213\n",
            "Training Iteration 3200, Loss: 4.463386058807373\n",
            "Training Iteration 3201, Loss: 3.6369380950927734\n",
            "Training Iteration 3202, Loss: 5.052231311798096\n",
            "Training Iteration 3203, Loss: 5.146621227264404\n",
            "Training Iteration 3204, Loss: 11.452851295471191\n",
            "Training Iteration 3205, Loss: 2.649108409881592\n",
            "Training Iteration 3206, Loss: 4.763182163238525\n",
            "Training Iteration 3207, Loss: 3.200733184814453\n",
            "Training Iteration 3208, Loss: 3.5205461978912354\n",
            "Training Iteration 3209, Loss: 5.719676494598389\n",
            "Training Iteration 3210, Loss: 7.153709411621094\n",
            "Training Iteration 3211, Loss: 4.789373397827148\n",
            "Training Iteration 3212, Loss: 3.6176071166992188\n",
            "Training Iteration 3213, Loss: 3.565518379211426\n",
            "Training Iteration 3214, Loss: 2.6671302318573\n",
            "Training Iteration 3215, Loss: 8.236557006835938\n",
            "Training Iteration 3216, Loss: 1.578202486038208\n",
            "Training Iteration 3217, Loss: 3.1342580318450928\n",
            "Training Iteration 3218, Loss: 4.308467864990234\n",
            "Training Iteration 3219, Loss: 2.796588182449341\n",
            "Training Iteration 3220, Loss: 4.598346710205078\n",
            "Training Iteration 3221, Loss: 5.075222969055176\n",
            "Training Iteration 3222, Loss: 0.7920525074005127\n",
            "Training Iteration 3223, Loss: 6.1056413650512695\n",
            "Training Iteration 3224, Loss: 5.91859245300293\n",
            "Training Iteration 3225, Loss: 4.099865436553955\n",
            "Training Iteration 3226, Loss: 4.5318121910095215\n",
            "Training Iteration 3227, Loss: 3.9631664752960205\n",
            "Training Iteration 3228, Loss: 4.491936683654785\n",
            "Training Iteration 3229, Loss: 5.111673831939697\n",
            "Training Iteration 3230, Loss: 6.199338912963867\n",
            "Training Iteration 3231, Loss: 3.527169704437256\n",
            "Training Iteration 3232, Loss: 3.1944057941436768\n",
            "Training Iteration 3233, Loss: 3.970914125442505\n",
            "Training Iteration 3234, Loss: 6.540711402893066\n",
            "Training Iteration 3235, Loss: 4.549851417541504\n",
            "Training Iteration 3236, Loss: 3.650109052658081\n",
            "Training Iteration 3237, Loss: 10.943276405334473\n",
            "Training Iteration 3238, Loss: 4.5643630027771\n",
            "Training Iteration 3239, Loss: 8.263310432434082\n",
            "Training Iteration 3240, Loss: 4.7959885597229\n",
            "Training Iteration 3241, Loss: 4.941903114318848\n",
            "Training Iteration 3242, Loss: 2.951873302459717\n",
            "Training Iteration 3243, Loss: 3.897653341293335\n",
            "Training Iteration 3244, Loss: 5.5859222412109375\n",
            "Training Iteration 3245, Loss: 7.454660892486572\n",
            "Training Iteration 3246, Loss: 7.144053936004639\n",
            "Training Iteration 3247, Loss: 8.715738296508789\n",
            "Training Iteration 3248, Loss: 6.7013044357299805\n",
            "Training Iteration 3249, Loss: 8.348705291748047\n",
            "Training Iteration 3250, Loss: 5.765200138092041\n",
            "Training Iteration 3251, Loss: 9.18525218963623\n",
            "Training Iteration 3252, Loss: 4.767205238342285\n",
            "Training Iteration 3253, Loss: 4.0116963386535645\n",
            "Training Iteration 3254, Loss: 3.5344529151916504\n",
            "Training Iteration 3255, Loss: 3.795504570007324\n",
            "Training Iteration 3256, Loss: 5.419962406158447\n",
            "Training Iteration 3257, Loss: 6.751572608947754\n",
            "Training Iteration 3258, Loss: 6.847066879272461\n",
            "Training Iteration 3259, Loss: 10.602612495422363\n",
            "Training Iteration 3260, Loss: 5.456642150878906\n",
            "Training Iteration 3261, Loss: 10.4508056640625\n",
            "Training Iteration 3262, Loss: 3.9333362579345703\n",
            "Training Iteration 3263, Loss: 3.7133986949920654\n",
            "Training Iteration 3264, Loss: 2.5847482681274414\n",
            "Training Iteration 3265, Loss: 6.018649578094482\n",
            "Training Iteration 3266, Loss: 5.632773399353027\n",
            "Training Iteration 3267, Loss: 3.708988904953003\n",
            "Training Iteration 3268, Loss: 4.461328506469727\n",
            "Training Iteration 3269, Loss: 4.193436622619629\n",
            "Training Iteration 3270, Loss: 4.123086929321289\n",
            "Training Iteration 3271, Loss: 4.37407112121582\n",
            "Training Iteration 3272, Loss: 6.359957695007324\n",
            "Training Iteration 3273, Loss: 5.1944966316223145\n",
            "Training Iteration 3274, Loss: 2.601409673690796\n",
            "Training Iteration 3275, Loss: 6.189154148101807\n",
            "Training Iteration 3276, Loss: 4.3499226570129395\n",
            "Training Iteration 3277, Loss: 5.506689548492432\n",
            "Training Iteration 3278, Loss: 2.180941104888916\n",
            "Training Iteration 3279, Loss: 6.993234634399414\n",
            "Training Iteration 3280, Loss: 5.2385711669921875\n",
            "Training Iteration 3281, Loss: 5.789755821228027\n",
            "Training Iteration 3282, Loss: 3.9426121711730957\n",
            "Training Iteration 3283, Loss: 2.7899482250213623\n",
            "Training Iteration 3284, Loss: 3.546332836151123\n",
            "Training Iteration 3285, Loss: 4.955751419067383\n",
            "Training Iteration 3286, Loss: 2.9595706462860107\n",
            "Training Iteration 3287, Loss: 5.729918956756592\n",
            "Training Iteration 3288, Loss: 7.236265659332275\n",
            "Training Iteration 3289, Loss: 9.062265396118164\n",
            "Training Iteration 3290, Loss: 9.20701789855957\n",
            "Training Iteration 3291, Loss: 5.940164566040039\n",
            "Training Iteration 3292, Loss: 6.357400417327881\n",
            "Training Iteration 3293, Loss: 5.026385307312012\n",
            "Training Iteration 3294, Loss: 5.739534854888916\n",
            "Training Iteration 3295, Loss: 3.4427270889282227\n",
            "Training Iteration 3296, Loss: 6.071990966796875\n",
            "Training Iteration 3297, Loss: 4.0119242668151855\n",
            "Training Iteration 3298, Loss: 4.369897365570068\n",
            "Training Iteration 3299, Loss: 4.8172760009765625\n",
            "Training Iteration 3300, Loss: 3.641979694366455\n",
            "Training Iteration 3301, Loss: 4.094376564025879\n",
            "Training Iteration 3302, Loss: 3.8128857612609863\n",
            "Training Iteration 3303, Loss: 6.70189905166626\n",
            "Training Iteration 3304, Loss: 7.125474452972412\n",
            "Training Iteration 3305, Loss: 2.387033224105835\n",
            "Training Iteration 3306, Loss: 5.126154899597168\n",
            "Training Iteration 3307, Loss: 4.330818176269531\n",
            "Training Iteration 3308, Loss: 3.0560779571533203\n",
            "Training Iteration 3309, Loss: 6.169765472412109\n",
            "Training Iteration 3310, Loss: 6.010136604309082\n",
            "Training Iteration 3311, Loss: 3.0948939323425293\n",
            "Training Iteration 3312, Loss: 7.453962326049805\n",
            "Training Iteration 3313, Loss: 3.6686220169067383\n",
            "Training Iteration 3314, Loss: 4.617760181427002\n",
            "Training Iteration 3315, Loss: 2.5737462043762207\n",
            "Training Iteration 3316, Loss: 4.456192970275879\n",
            "Training Iteration 3317, Loss: 6.873188495635986\n",
            "Training Iteration 3318, Loss: 3.436209201812744\n",
            "Training Iteration 3319, Loss: 5.392624378204346\n",
            "Training Iteration 3320, Loss: 4.654630661010742\n",
            "Training Iteration 3321, Loss: 2.143376350402832\n",
            "Training Iteration 3322, Loss: 4.132068157196045\n",
            "Training Iteration 3323, Loss: 2.4906673431396484\n",
            "Training Iteration 3324, Loss: 3.3667891025543213\n",
            "Training Iteration 3325, Loss: 5.325490951538086\n",
            "Training Iteration 3326, Loss: 4.190504550933838\n",
            "Training Iteration 3327, Loss: 6.841117858886719\n",
            "Training Iteration 3328, Loss: 2.3457751274108887\n",
            "Training Iteration 3329, Loss: 3.630237579345703\n",
            "Training Iteration 3330, Loss: 5.230867385864258\n",
            "Training Iteration 3331, Loss: 5.428488254547119\n",
            "Training Iteration 3332, Loss: 4.982556343078613\n",
            "Training Iteration 3333, Loss: 4.261402606964111\n",
            "Training Iteration 3334, Loss: 2.9064371585845947\n",
            "Training Iteration 3335, Loss: 4.683360576629639\n",
            "Training Iteration 3336, Loss: 7.9678239822387695\n",
            "Training Iteration 3337, Loss: 1.6575610637664795\n",
            "Training Iteration 3338, Loss: 5.420053005218506\n",
            "Training Iteration 3339, Loss: 14.270798683166504\n",
            "Training Iteration 3340, Loss: 5.45417594909668\n",
            "Training Iteration 3341, Loss: 6.026721477508545\n",
            "Training Iteration 3342, Loss: 3.209728479385376\n",
            "Training Iteration 3343, Loss: 3.717050552368164\n",
            "Training Iteration 3344, Loss: 3.6386799812316895\n",
            "Training Iteration 3345, Loss: 5.890300750732422\n",
            "Training Iteration 3346, Loss: 4.964892864227295\n",
            "Training Iteration 3347, Loss: 2.0442042350769043\n",
            "Training Iteration 3348, Loss: 5.467913627624512\n",
            "Training Iteration 3349, Loss: 3.138641834259033\n",
            "Training Iteration 3350, Loss: 8.770978927612305\n",
            "Training Iteration 3351, Loss: 5.2378387451171875\n",
            "Training Iteration 3352, Loss: 6.260725498199463\n",
            "Training Iteration 3353, Loss: 4.225069999694824\n",
            "Training Iteration 3354, Loss: 3.477900981903076\n",
            "Training Iteration 3355, Loss: 4.879925727844238\n",
            "Training Iteration 3356, Loss: 4.147197723388672\n",
            "Training Iteration 3357, Loss: 3.974362850189209\n",
            "Training Iteration 3358, Loss: 4.318284034729004\n",
            "Training Iteration 3359, Loss: 3.8027002811431885\n",
            "Training Iteration 3360, Loss: 6.376739025115967\n",
            "Training Iteration 3361, Loss: 3.827625036239624\n",
            "Training Iteration 3362, Loss: 7.403011322021484\n",
            "Training Iteration 3363, Loss: 8.335552215576172\n",
            "Training Iteration 3364, Loss: 3.66772198677063\n",
            "Training Iteration 3365, Loss: 3.6127982139587402\n",
            "Training Iteration 3366, Loss: 2.6209793090820312\n",
            "Training Iteration 3367, Loss: 2.9382834434509277\n",
            "Training Iteration 3368, Loss: 7.991239070892334\n",
            "Training Iteration 3369, Loss: 3.139657735824585\n",
            "Training Iteration 3370, Loss: 5.334987163543701\n",
            "Training Iteration 3371, Loss: 5.680250644683838\n",
            "Training Iteration 3372, Loss: 4.106455326080322\n",
            "Training Iteration 3373, Loss: 5.0694193840026855\n",
            "Training Iteration 3374, Loss: 4.371087074279785\n",
            "Training Iteration 3375, Loss: 4.522429466247559\n",
            "Training Iteration 3376, Loss: 4.5182061195373535\n",
            "Training Iteration 3377, Loss: 2.3942489624023438\n",
            "Training Iteration 3378, Loss: 4.162722110748291\n",
            "Training Iteration 3379, Loss: 5.448333740234375\n",
            "Training Iteration 3380, Loss: 3.696986675262451\n",
            "Training Iteration 3381, Loss: 2.629909038543701\n",
            "Training Iteration 3382, Loss: 5.594663619995117\n",
            "Training Iteration 3383, Loss: 1.9972376823425293\n",
            "Training Iteration 3384, Loss: 3.981999158859253\n",
            "Training Iteration 3385, Loss: 4.929129600524902\n",
            "Training Iteration 3386, Loss: 6.21776008605957\n",
            "Training Iteration 3387, Loss: 2.579448699951172\n",
            "Training Iteration 3388, Loss: 4.252791404724121\n",
            "Training Iteration 3389, Loss: 9.566812515258789\n",
            "Training Iteration 3390, Loss: 4.853994846343994\n",
            "Training Iteration 3391, Loss: 6.714057922363281\n",
            "Training Iteration 3392, Loss: 7.245722770690918\n",
            "Training Iteration 3393, Loss: 4.8838653564453125\n",
            "Training Iteration 3394, Loss: 7.600170135498047\n",
            "Training Iteration 3395, Loss: 3.987725257873535\n",
            "Training Iteration 3396, Loss: 6.65925931930542\n",
            "Training Iteration 3397, Loss: 6.0231032371521\n",
            "Training Iteration 3398, Loss: 6.194977283477783\n",
            "Training Iteration 3399, Loss: 3.3301656246185303\n",
            "Training Iteration 3400, Loss: 1.9869327545166016\n",
            "Training Iteration 3401, Loss: 6.102452278137207\n",
            "Training Iteration 3402, Loss: 4.709904193878174\n",
            "Training Iteration 3403, Loss: 2.4957244396209717\n",
            "Training Iteration 3404, Loss: 6.5570831298828125\n",
            "Training Iteration 3405, Loss: 3.0057408809661865\n",
            "Training Iteration 3406, Loss: 5.066148281097412\n",
            "Training Iteration 3407, Loss: 3.6800074577331543\n",
            "Training Iteration 3408, Loss: 3.3658130168914795\n",
            "Training Iteration 3409, Loss: 5.367286682128906\n",
            "Training Iteration 3410, Loss: 6.2581658363342285\n",
            "Training Iteration 3411, Loss: 8.264314651489258\n",
            "Training Iteration 3412, Loss: 5.550225257873535\n",
            "Training Iteration 3413, Loss: 8.155837059020996\n",
            "Training Iteration 3414, Loss: 3.5698039531707764\n",
            "Training Iteration 3415, Loss: 5.4672698974609375\n",
            "Training Iteration 3416, Loss: 7.736764907836914\n",
            "Training Iteration 3417, Loss: 10.585103988647461\n",
            "Training Iteration 3418, Loss: 4.80754280090332\n",
            "Training Iteration 3419, Loss: 3.2449474334716797\n",
            "Training Iteration 3420, Loss: 6.426650047302246\n",
            "Training Iteration 3421, Loss: 3.1716482639312744\n",
            "Training Iteration 3422, Loss: 4.42010498046875\n",
            "Training Iteration 3423, Loss: 7.20610237121582\n",
            "Training Iteration 3424, Loss: 5.914032936096191\n",
            "Training Iteration 3425, Loss: 2.7903025150299072\n",
            "Training Iteration 3426, Loss: 9.332518577575684\n",
            "Training Iteration 3427, Loss: 5.205509662628174\n",
            "Training Iteration 3428, Loss: 4.757242202758789\n",
            "Training Iteration 3429, Loss: 6.3918843269348145\n",
            "Training Iteration 3430, Loss: 10.42697811126709\n",
            "Training Iteration 3431, Loss: 7.202319145202637\n",
            "Training Iteration 3432, Loss: 6.836578845977783\n",
            "Training Iteration 3433, Loss: 2.322408676147461\n",
            "Training Iteration 3434, Loss: 4.536455154418945\n",
            "Training Iteration 3435, Loss: 3.689096212387085\n",
            "Training Iteration 3436, Loss: 4.084285259246826\n",
            "Training Iteration 3437, Loss: 4.190676212310791\n",
            "Training Iteration 3438, Loss: 4.992059230804443\n",
            "Training Iteration 3439, Loss: 6.330924987792969\n",
            "Training Iteration 3440, Loss: 3.923187017440796\n",
            "Training Iteration 3441, Loss: 3.529684543609619\n",
            "Training Iteration 3442, Loss: 5.268877029418945\n",
            "Training Iteration 3443, Loss: 7.674892425537109\n",
            "Training Iteration 3444, Loss: 6.178602695465088\n",
            "Training Iteration 3445, Loss: 4.8916168212890625\n",
            "Training Iteration 3446, Loss: 4.085368633270264\n",
            "Training Iteration 3447, Loss: 2.1078834533691406\n",
            "Training Iteration 3448, Loss: 0.9431338310241699\n",
            "Training Iteration 3449, Loss: 4.096546173095703\n",
            "Training Iteration 3450, Loss: 6.48750638961792\n",
            "Training Iteration 3451, Loss: 3.190955400466919\n",
            "Training Iteration 3452, Loss: 2.7868540287017822\n",
            "Training Iteration 3453, Loss: 8.007410049438477\n",
            "Training Iteration 3454, Loss: 8.160318374633789\n",
            "Training Iteration 3455, Loss: 2.498772621154785\n",
            "Training Iteration 3456, Loss: 2.805415391921997\n",
            "Training Iteration 3457, Loss: 5.584056854248047\n",
            "Training Iteration 3458, Loss: 3.099613904953003\n",
            "Training Iteration 3459, Loss: 7.816288948059082\n",
            "Training Iteration 3460, Loss: 5.269525527954102\n",
            "Training Iteration 3461, Loss: 6.76019287109375\n",
            "Training Iteration 3462, Loss: 5.5554704666137695\n",
            "Training Iteration 3463, Loss: 3.9491450786590576\n",
            "Training Iteration 3464, Loss: 4.204418659210205\n",
            "Training Iteration 3465, Loss: 3.359919548034668\n",
            "Training Iteration 3466, Loss: 7.057365417480469\n",
            "Training Iteration 3467, Loss: 5.7745256423950195\n",
            "Training Iteration 3468, Loss: 4.173272132873535\n",
            "Training Iteration 3469, Loss: 6.854707717895508\n",
            "Training Iteration 3470, Loss: 5.527219295501709\n",
            "Training Iteration 3471, Loss: 4.440413475036621\n",
            "Training Iteration 3472, Loss: 6.986896991729736\n",
            "Training Iteration 3473, Loss: 3.8255813121795654\n",
            "Training Iteration 3474, Loss: 2.397885322570801\n",
            "Training Iteration 3475, Loss: 3.7469935417175293\n",
            "Training Iteration 3476, Loss: 2.5721046924591064\n",
            "Training Iteration 3477, Loss: 3.8920300006866455\n",
            "Training Iteration 3478, Loss: 5.01428747177124\n",
            "Training Iteration 3479, Loss: 5.062087059020996\n",
            "Training Iteration 3480, Loss: 3.653883934020996\n",
            "Training Iteration 3481, Loss: 3.442328929901123\n",
            "Training Iteration 3482, Loss: 3.933285713195801\n",
            "Training Iteration 3483, Loss: 3.0402631759643555\n",
            "Training Iteration 3484, Loss: 4.908956527709961\n",
            "Training Iteration 3485, Loss: 4.280948638916016\n",
            "Training Iteration 3486, Loss: 4.801081657409668\n",
            "Training Iteration 3487, Loss: 2.4685873985290527\n",
            "Training Iteration 3488, Loss: 6.866927146911621\n",
            "Training Iteration 3489, Loss: 1.7139225006103516\n",
            "Training Iteration 3490, Loss: 5.786595821380615\n",
            "Training Iteration 3491, Loss: 5.424920082092285\n",
            "Training Iteration 3492, Loss: 8.487759590148926\n",
            "Training Iteration 3493, Loss: 4.808647632598877\n",
            "Training Iteration 3494, Loss: 5.623603820800781\n",
            "Training Iteration 3495, Loss: 5.1166839599609375\n",
            "Training Iteration 3496, Loss: 4.772923946380615\n",
            "Training Iteration 3497, Loss: 5.7154951095581055\n",
            "Training Iteration 3498, Loss: 4.694177150726318\n",
            "Training Iteration 3499, Loss: 8.539977073669434\n",
            "Training Iteration 3500, Loss: 6.990163803100586\n",
            "Training Iteration 3501, Loss: 3.782533884048462\n",
            "Training Iteration 3502, Loss: 5.027198314666748\n",
            "Training Iteration 3503, Loss: 5.20971155166626\n",
            "Training Iteration 3504, Loss: 4.7414231300354\n",
            "Training Iteration 3505, Loss: 2.323288679122925\n",
            "Training Iteration 3506, Loss: 2.891860246658325\n",
            "Training Iteration 3507, Loss: 2.195207118988037\n",
            "Training Iteration 3508, Loss: 3.3564116954803467\n",
            "Training Iteration 3509, Loss: 5.114060878753662\n",
            "Training Iteration 3510, Loss: 3.7551040649414062\n",
            "Training Iteration 3511, Loss: 6.4050374031066895\n",
            "Training Iteration 3512, Loss: 5.089936256408691\n",
            "Training Iteration 3513, Loss: 2.854295253753662\n",
            "Training Iteration 3514, Loss: 2.1840505599975586\n",
            "Training Iteration 3515, Loss: 2.7632532119750977\n",
            "Training Iteration 3516, Loss: 7.7418975830078125\n",
            "Training Iteration 3517, Loss: 4.484864234924316\n",
            "Training Iteration 3518, Loss: 5.252901077270508\n",
            "Training Iteration 3519, Loss: 4.766878604888916\n",
            "Training Iteration 3520, Loss: 4.666658878326416\n",
            "Training Iteration 3521, Loss: 3.9362361431121826\n",
            "Training Iteration 3522, Loss: 5.464794158935547\n",
            "Training Iteration 3523, Loss: 5.890432357788086\n",
            "Training Iteration 3524, Loss: 8.485433578491211\n",
            "Training Iteration 3525, Loss: 4.204105854034424\n",
            "Training Iteration 3526, Loss: 1.8531570434570312\n",
            "Training Iteration 3527, Loss: 3.972620964050293\n",
            "Training Iteration 3528, Loss: 4.596183776855469\n",
            "Training Iteration 3529, Loss: 2.930572271347046\n",
            "Training Iteration 3530, Loss: 3.869110107421875\n",
            "Training Iteration 3531, Loss: 2.596627712249756\n",
            "Training Iteration 3532, Loss: 4.4063825607299805\n",
            "Training Iteration 3533, Loss: 4.743495941162109\n",
            "Training Iteration 3534, Loss: 5.051393508911133\n",
            "Training Iteration 3535, Loss: 5.63131856918335\n",
            "Training Iteration 3536, Loss: 1.7140331268310547\n",
            "Training Iteration 3537, Loss: 3.031822443008423\n",
            "Training Iteration 3538, Loss: 3.8890202045440674\n",
            "Training Iteration 3539, Loss: 3.004422187805176\n",
            "Training Iteration 3540, Loss: 2.587421417236328\n",
            "Training Iteration 3541, Loss: 3.2942698001861572\n",
            "Training Iteration 3542, Loss: 2.1810519695281982\n",
            "Training Iteration 3543, Loss: 5.543673038482666\n",
            "Training Iteration 3544, Loss: 3.99251651763916\n",
            "Training Iteration 3545, Loss: 3.1163692474365234\n",
            "Training Iteration 3546, Loss: 6.336851119995117\n",
            "Training Iteration 3547, Loss: 3.860532760620117\n",
            "Training Iteration 3548, Loss: 3.460597038269043\n",
            "Training Iteration 3549, Loss: 3.9153056144714355\n",
            "Training Iteration 3550, Loss: 2.605699300765991\n",
            "Training Iteration 3551, Loss: 4.19627571105957\n",
            "Training Iteration 3552, Loss: 4.104750633239746\n",
            "Training Iteration 3553, Loss: 2.6361405849456787\n",
            "Training Iteration 3554, Loss: 4.154272079467773\n",
            "Training Iteration 3555, Loss: 3.6581592559814453\n",
            "Training Iteration 3556, Loss: 4.825023651123047\n",
            "Training Iteration 3557, Loss: 3.5320162773132324\n",
            "Training Iteration 3558, Loss: 5.933986186981201\n",
            "Training Iteration 3559, Loss: 5.611306667327881\n",
            "Training Iteration 3560, Loss: 4.943840503692627\n",
            "Training Iteration 3561, Loss: 4.067999839782715\n",
            "Training Iteration 3562, Loss: 4.987236976623535\n",
            "Training Iteration 3563, Loss: 3.1725406646728516\n",
            "Training Iteration 3564, Loss: 5.100955486297607\n",
            "Training Iteration 3565, Loss: 4.156432628631592\n",
            "Training Iteration 3566, Loss: 4.24557638168335\n",
            "Training Iteration 3567, Loss: 7.3549113273620605\n",
            "Training Iteration 3568, Loss: 4.266973495483398\n",
            "Training Iteration 3569, Loss: 4.765888690948486\n",
            "Training Iteration 3570, Loss: 7.959328651428223\n",
            "Training Iteration 3571, Loss: 5.847609519958496\n",
            "Training Iteration 3572, Loss: 7.4070844650268555\n",
            "Training Iteration 3573, Loss: 5.7995405197143555\n",
            "Training Iteration 3574, Loss: 4.784989356994629\n",
            "Training Iteration 3575, Loss: 6.453887939453125\n",
            "Training Iteration 3576, Loss: 1.851870059967041\n",
            "Training Iteration 3577, Loss: 7.4559197425842285\n",
            "Training Iteration 3578, Loss: 5.812331199645996\n",
            "Training Iteration 3579, Loss: 2.953303337097168\n",
            "Training Iteration 3580, Loss: 4.06651496887207\n",
            "Training Iteration 3581, Loss: 4.080803871154785\n",
            "Training Iteration 3582, Loss: 4.232546329498291\n",
            "Training Iteration 3583, Loss: 5.95673942565918\n",
            "Training Iteration 3584, Loss: 6.387501239776611\n",
            "Training Iteration 3585, Loss: 3.890841245651245\n",
            "Training Iteration 3586, Loss: 5.1899847984313965\n",
            "Training Iteration 3587, Loss: 5.796485900878906\n",
            "Training Iteration 3588, Loss: 7.073705196380615\n",
            "Training Iteration 3589, Loss: 3.9853553771972656\n",
            "Training Iteration 3590, Loss: 0.574571430683136\n",
            "Training Iteration 3591, Loss: 5.428569793701172\n",
            "Training Iteration 3592, Loss: 3.7557621002197266\n",
            "Training Iteration 3593, Loss: 4.773629188537598\n",
            "Training Iteration 3594, Loss: 4.476005554199219\n",
            "Training Iteration 3595, Loss: 5.739950180053711\n",
            "Training Iteration 3596, Loss: 6.895111560821533\n",
            "Training Iteration 3597, Loss: 3.0759682655334473\n",
            "Training Iteration 3598, Loss: 3.1009020805358887\n",
            "Training Iteration 3599, Loss: 6.241692066192627\n",
            "Training Iteration 3600, Loss: 5.172816753387451\n",
            "Training Iteration 3601, Loss: 3.807671546936035\n",
            "Training Iteration 3602, Loss: 3.851961612701416\n",
            "Training Iteration 3603, Loss: 4.057234764099121\n",
            "Training Iteration 3604, Loss: 2.9645907878875732\n",
            "Training Iteration 3605, Loss: 5.094422340393066\n",
            "Training Iteration 3606, Loss: 4.5769758224487305\n",
            "Training Iteration 3607, Loss: 2.6066455841064453\n",
            "Training Iteration 3608, Loss: 6.8016486167907715\n",
            "Training Iteration 3609, Loss: 2.970139980316162\n",
            "Training Iteration 3610, Loss: 6.321303844451904\n",
            "Training Iteration 3611, Loss: 3.2641313076019287\n",
            "Training Iteration 3612, Loss: 6.909696102142334\n",
            "Training Iteration 3613, Loss: 3.2127153873443604\n",
            "Training Iteration 3614, Loss: 4.876882076263428\n",
            "Training Iteration 3615, Loss: 1.7656687498092651\n",
            "Training Iteration 3616, Loss: 4.970620632171631\n",
            "Training Iteration 3617, Loss: 3.8766047954559326\n",
            "Training Iteration 3618, Loss: 5.879696846008301\n",
            "Training Iteration 3619, Loss: 3.2671589851379395\n",
            "Training Iteration 3620, Loss: 3.6873421669006348\n",
            "Training Iteration 3621, Loss: 3.7312262058258057\n",
            "Training Iteration 3622, Loss: 3.833778142929077\n",
            "Training Iteration 3623, Loss: 4.186801910400391\n",
            "Training Iteration 3624, Loss: 5.188124656677246\n",
            "Training Iteration 3625, Loss: 2.783916711807251\n",
            "Training Iteration 3626, Loss: 1.1207871437072754\n",
            "Training Iteration 3627, Loss: 3.5022096633911133\n",
            "Training Iteration 3628, Loss: 4.397132873535156\n",
            "Training Iteration 3629, Loss: 4.888371467590332\n",
            "Training Iteration 3630, Loss: 4.231102466583252\n",
            "Training Iteration 3631, Loss: 3.787665843963623\n",
            "Training Iteration 3632, Loss: 2.949852228164673\n",
            "Training Iteration 3633, Loss: 7.451664924621582\n",
            "Training Iteration 3634, Loss: 6.167908191680908\n",
            "Training Iteration 3635, Loss: 1.8964407444000244\n",
            "Training Iteration 3636, Loss: 3.4751336574554443\n",
            "Training Iteration 3637, Loss: 3.5989794731140137\n",
            "Training Iteration 3638, Loss: 3.824204206466675\n",
            "Training Iteration 3639, Loss: 6.000848293304443\n",
            "Training Iteration 3640, Loss: 8.209161758422852\n",
            "Training Iteration 3641, Loss: 5.37019681930542\n",
            "Training Iteration 3642, Loss: 3.822944402694702\n",
            "Training Iteration 3643, Loss: 2.318403959274292\n",
            "Training Iteration 3644, Loss: 7.250319957733154\n",
            "Training Iteration 3645, Loss: 4.4881696701049805\n",
            "Training Iteration 3646, Loss: 5.89137077331543\n",
            "Training Iteration 3647, Loss: 2.573162078857422\n",
            "Training Iteration 3648, Loss: 3.696413040161133\n",
            "Training Iteration 3649, Loss: 3.4344325065612793\n",
            "Training Iteration 3650, Loss: 4.087110996246338\n",
            "Training Iteration 3651, Loss: 4.295094013214111\n",
            "Training Iteration 3652, Loss: 6.017423152923584\n",
            "Training Iteration 3653, Loss: 2.0303080081939697\n",
            "Training Iteration 3654, Loss: 4.63096809387207\n",
            "Training Iteration 3655, Loss: 3.6780447959899902\n",
            "Training Iteration 3656, Loss: 3.564613103866577\n",
            "Training Iteration 3657, Loss: 3.8269033432006836\n",
            "Training Iteration 3658, Loss: 5.074709892272949\n",
            "Training Iteration 3659, Loss: 3.9232022762298584\n",
            "Training Iteration 3660, Loss: 2.6441192626953125\n",
            "Training Iteration 3661, Loss: 4.160497188568115\n",
            "Training Iteration 3662, Loss: 4.5049967765808105\n",
            "Training Iteration 3663, Loss: 3.5054149627685547\n",
            "Training Iteration 3664, Loss: 1.5026687383651733\n",
            "Training Iteration 3665, Loss: 2.946195125579834\n",
            "Training Iteration 3666, Loss: 3.8492722511291504\n",
            "Training Iteration 3667, Loss: 4.020516872406006\n",
            "Training Iteration 3668, Loss: 6.137226581573486\n",
            "Training Iteration 3669, Loss: 3.0676674842834473\n",
            "Training Iteration 3670, Loss: 4.457484722137451\n",
            "Training Iteration 3671, Loss: 3.6156344413757324\n",
            "Training Iteration 3672, Loss: 4.298022270202637\n",
            "Training Iteration 3673, Loss: 4.052645206451416\n",
            "Training Iteration 3674, Loss: 5.216196060180664\n",
            "Training Iteration 3675, Loss: 4.428859710693359\n",
            "Training Iteration 3676, Loss: 3.899325132369995\n",
            "Training Iteration 3677, Loss: 6.473296165466309\n",
            "Training Iteration 3678, Loss: 5.557167053222656\n",
            "Training Iteration 3679, Loss: 4.301914215087891\n",
            "Training Iteration 3680, Loss: 4.398599147796631\n",
            "Training Iteration 3681, Loss: 3.6827850341796875\n",
            "Training Iteration 3682, Loss: 3.3036739826202393\n",
            "Training Iteration 3683, Loss: 4.2277445793151855\n",
            "Training Iteration 3684, Loss: 4.355926513671875\n",
            "Training Iteration 3685, Loss: 2.033984661102295\n",
            "Training Iteration 3686, Loss: 3.054842710494995\n",
            "Training Iteration 3687, Loss: 4.9348039627075195\n",
            "Training Iteration 3688, Loss: 4.17437219619751\n",
            "Training Iteration 3689, Loss: 7.596370697021484\n",
            "Training Iteration 3690, Loss: 4.144872188568115\n",
            "Training Iteration 3691, Loss: 4.9037394523620605\n",
            "Training Iteration 3692, Loss: 4.752166271209717\n",
            "Training Iteration 3693, Loss: 4.590631008148193\n",
            "Training Iteration 3694, Loss: 5.456872463226318\n",
            "Training Iteration 3695, Loss: 6.594260215759277\n",
            "Training Iteration 3696, Loss: 1.598981499671936\n",
            "Training Iteration 3697, Loss: 4.1485490798950195\n",
            "Training Iteration 3698, Loss: 3.247934341430664\n",
            "Training Iteration 3699, Loss: 5.660074710845947\n",
            "Training Iteration 3700, Loss: 5.690491676330566\n",
            "Training Iteration 3701, Loss: 3.773550510406494\n",
            "Training Iteration 3702, Loss: 4.540885925292969\n",
            "Training Iteration 3703, Loss: 4.276761054992676\n",
            "Training Iteration 3704, Loss: 7.43568229675293\n",
            "Training Iteration 3705, Loss: 5.633154392242432\n",
            "Training Iteration 3706, Loss: 5.170947551727295\n",
            "Training Iteration 3707, Loss: 5.452102184295654\n",
            "Training Iteration 3708, Loss: 5.17341423034668\n",
            "Training Iteration 3709, Loss: 4.172418117523193\n",
            "Training Iteration 3710, Loss: 6.192915916442871\n",
            "Training Iteration 3711, Loss: 7.634669780731201\n",
            "Training Iteration 3712, Loss: 6.727919101715088\n",
            "Training Iteration 3713, Loss: 5.792191505432129\n",
            "Training Iteration 3714, Loss: 4.312926292419434\n",
            "Training Iteration 3715, Loss: 3.7025089263916016\n",
            "Training Iteration 3716, Loss: 5.520959854125977\n",
            "Training Iteration 3717, Loss: 5.203906059265137\n",
            "Training Iteration 3718, Loss: 9.027504920959473\n",
            "Training Iteration 3719, Loss: 7.248697280883789\n",
            "Training Iteration 3720, Loss: 4.94478702545166\n",
            "Training Iteration 3721, Loss: 1.9489552974700928\n",
            "Training Iteration 3722, Loss: 6.1882476806640625\n",
            "Training Iteration 3723, Loss: 5.805333614349365\n",
            "Training Iteration 3724, Loss: 2.9702696800231934\n",
            "Training Iteration 3725, Loss: 9.915215492248535\n",
            "Training Iteration 3726, Loss: 4.462174892425537\n",
            "Training Iteration 3727, Loss: 6.509917259216309\n",
            "Training Iteration 3728, Loss: 3.430077075958252\n",
            "Training Iteration 3729, Loss: 5.430805683135986\n",
            "Training Iteration 3730, Loss: 5.441158294677734\n",
            "Training Iteration 3731, Loss: 2.8547682762145996\n",
            "Training Iteration 3732, Loss: 2.686037540435791\n",
            "Training Iteration 3733, Loss: 6.170266151428223\n",
            "Training Iteration 3734, Loss: 5.341551780700684\n",
            "Training Iteration 3735, Loss: 7.544375896453857\n",
            "Training Iteration 3736, Loss: 4.510612487792969\n",
            "Training Iteration 3737, Loss: 5.989628314971924\n",
            "Training Iteration 3738, Loss: 5.8345513343811035\n",
            "Training Iteration 3739, Loss: 7.5716400146484375\n",
            "Training Iteration 3740, Loss: 8.860749244689941\n",
            "Training Iteration 3741, Loss: 7.419389247894287\n",
            "Training Iteration 3742, Loss: 6.40001106262207\n",
            "Training Iteration 3743, Loss: 3.4273767471313477\n",
            "Training Iteration 3744, Loss: 4.513843536376953\n",
            "Training Iteration 3745, Loss: 5.996408939361572\n",
            "Training Iteration 3746, Loss: 7.1360578536987305\n",
            "Training Iteration 3747, Loss: 8.072636604309082\n",
            "Training Iteration 3748, Loss: 3.565112829208374\n",
            "Training Iteration 3749, Loss: 2.1070642471313477\n",
            "Training Iteration 3750, Loss: 3.2847180366516113\n",
            "Training Iteration 3751, Loss: 6.422739505767822\n",
            "Training Iteration 3752, Loss: 8.402219772338867\n",
            "Training Iteration 3753, Loss: 6.502607822418213\n",
            "Training Iteration 3754, Loss: 6.865922927856445\n",
            "Training Iteration 3755, Loss: 15.042988777160645\n",
            "Training Iteration 3756, Loss: 7.024833679199219\n",
            "Training Iteration 3757, Loss: 8.179386138916016\n",
            "Training Iteration 3758, Loss: 3.622288465499878\n",
            "Training Iteration 3759, Loss: 4.102626323699951\n",
            "Training Iteration 3760, Loss: 5.932120323181152\n",
            "Training Iteration 3761, Loss: 7.536363124847412\n",
            "Training Iteration 3762, Loss: 3.606874465942383\n",
            "Training Iteration 3763, Loss: 5.254990577697754\n",
            "Training Iteration 3764, Loss: 4.028831481933594\n",
            "Training Iteration 3765, Loss: 4.910367965698242\n",
            "Training Iteration 3766, Loss: 5.631161689758301\n",
            "Training Iteration 3767, Loss: 6.712701320648193\n",
            "Training Iteration 3768, Loss: 8.930538177490234\n",
            "Training Iteration 3769, Loss: 8.471588134765625\n",
            "Training Iteration 3770, Loss: 4.614869117736816\n",
            "Training Iteration 3771, Loss: 1.2752692699432373\n",
            "Training Iteration 3772, Loss: 4.095158100128174\n",
            "Training Iteration 3773, Loss: 4.83756685256958\n",
            "Training Iteration 3774, Loss: 2.1770236492156982\n",
            "Training Iteration 3775, Loss: 4.759397506713867\n",
            "Training Iteration 3776, Loss: 5.6795549392700195\n",
            "Training Iteration 3777, Loss: 5.72791862487793\n",
            "Training Iteration 3778, Loss: 3.6050264835357666\n",
            "Training Iteration 3779, Loss: 8.270295143127441\n",
            "Training Iteration 3780, Loss: 5.672262191772461\n",
            "Training Iteration 3781, Loss: 3.335675001144409\n",
            "Training Iteration 3782, Loss: 2.7581610679626465\n",
            "Training Iteration 3783, Loss: 3.3117527961730957\n",
            "Training Iteration 3784, Loss: 4.264313220977783\n",
            "Training Iteration 3785, Loss: 5.531874179840088\n",
            "Training Iteration 3786, Loss: 3.895725727081299\n",
            "Training Iteration 3787, Loss: 4.6175856590271\n",
            "Training Iteration 3788, Loss: 4.630387783050537\n",
            "Training Iteration 3789, Loss: 4.099626064300537\n",
            "Training Iteration 3790, Loss: 2.9767491817474365\n",
            "Training Iteration 3791, Loss: 4.192324638366699\n",
            "Training Iteration 3792, Loss: 4.747139930725098\n",
            "Training Iteration 3793, Loss: 2.595871925354004\n",
            "Training Iteration 3794, Loss: 4.270061492919922\n",
            "Training Iteration 3795, Loss: 4.509743690490723\n",
            "Training Iteration 3796, Loss: 5.402966022491455\n",
            "Training Iteration 3797, Loss: 3.7620558738708496\n",
            "Training Iteration 3798, Loss: 4.489348411560059\n",
            "Training Iteration 3799, Loss: 3.9738962650299072\n",
            "Training Iteration 3800, Loss: 3.0430312156677246\n",
            "Training Iteration 3801, Loss: 6.579904556274414\n",
            "Training Iteration 3802, Loss: 5.180465221405029\n",
            "Training Iteration 3803, Loss: 5.290665149688721\n",
            "Training Iteration 3804, Loss: 2.569760799407959\n",
            "Training Iteration 3805, Loss: 4.9787116050720215\n",
            "Training Iteration 3806, Loss: 5.096205234527588\n",
            "Training Iteration 3807, Loss: 3.6970415115356445\n",
            "Training Iteration 3808, Loss: 4.458626747131348\n",
            "Training Iteration 3809, Loss: 7.6680097579956055\n",
            "Training Iteration 3810, Loss: 8.299786567687988\n",
            "Training Iteration 3811, Loss: 3.4938392639160156\n",
            "Training Iteration 3812, Loss: 3.062830924987793\n",
            "Training Iteration 3813, Loss: 4.999445915222168\n",
            "Training Iteration 3814, Loss: 2.9637386798858643\n",
            "Training Iteration 3815, Loss: 7.934086322784424\n",
            "Training Iteration 3816, Loss: 5.6754560470581055\n",
            "Training Iteration 3817, Loss: 1.5898363590240479\n",
            "Training Iteration 3818, Loss: 5.786259174346924\n",
            "Training Iteration 3819, Loss: 5.6303324699401855\n",
            "Training Iteration 3820, Loss: 4.560704708099365\n",
            "Training Iteration 3821, Loss: 3.4752707481384277\n",
            "Training Iteration 3822, Loss: 6.138965606689453\n",
            "Training Iteration 3823, Loss: 2.5273451805114746\n",
            "Training Iteration 3824, Loss: 6.0901780128479\n",
            "Training Iteration 3825, Loss: 1.6357451677322388\n",
            "Training Iteration 3826, Loss: 2.394845724105835\n",
            "Training Iteration 3827, Loss: 4.216006755828857\n",
            "Training Iteration 3828, Loss: 5.872223854064941\n",
            "Training Iteration 3829, Loss: 3.3640644550323486\n",
            "Training Iteration 3830, Loss: 5.887913227081299\n",
            "Training Iteration 3831, Loss: 5.270472526550293\n",
            "Training Iteration 3832, Loss: 2.8517251014709473\n",
            "Training Iteration 3833, Loss: 6.80226469039917\n",
            "Training Iteration 3834, Loss: 5.177020072937012\n",
            "Training Iteration 3835, Loss: 4.321272373199463\n",
            "Training Iteration 3836, Loss: 4.952219486236572\n",
            "Training Iteration 3837, Loss: 3.9550273418426514\n",
            "Training Iteration 3838, Loss: 3.1273419857025146\n",
            "Training Iteration 3839, Loss: 4.171431064605713\n",
            "Training Iteration 3840, Loss: 3.3211331367492676\n",
            "Training Iteration 3841, Loss: 2.942692756652832\n",
            "Training Iteration 3842, Loss: 2.3566782474517822\n",
            "Training Iteration 3843, Loss: 4.663499355316162\n",
            "Training Iteration 3844, Loss: 4.007816791534424\n",
            "Training Iteration 3845, Loss: 5.303195953369141\n",
            "Training Iteration 3846, Loss: 5.3922648429870605\n",
            "Training Iteration 3847, Loss: 2.751960277557373\n",
            "Training Iteration 3848, Loss: 3.7115092277526855\n",
            "Training Iteration 3849, Loss: 2.884443998336792\n",
            "Training Iteration 3850, Loss: 2.7748632431030273\n",
            "Training Iteration 3851, Loss: 4.463622570037842\n",
            "Training Iteration 3852, Loss: 3.6003854274749756\n",
            "Training Iteration 3853, Loss: 3.2859854698181152\n",
            "Training Iteration 3854, Loss: 1.4906182289123535\n",
            "Training Iteration 3855, Loss: 8.172801971435547\n",
            "Training Iteration 3856, Loss: 5.824738025665283\n",
            "Training Iteration 3857, Loss: 5.294432640075684\n",
            "Training Iteration 3858, Loss: 2.521723747253418\n",
            "Training Iteration 3859, Loss: 5.280822277069092\n",
            "Training Iteration 3860, Loss: 4.537221431732178\n",
            "Training Iteration 3861, Loss: 4.9309234619140625\n",
            "Training Iteration 3862, Loss: 5.830206871032715\n",
            "Training Iteration 3863, Loss: 4.634765148162842\n",
            "Training Iteration 3864, Loss: 2.5657131671905518\n",
            "Training Iteration 3865, Loss: 4.67873477935791\n",
            "Training Iteration 3866, Loss: 5.722689151763916\n",
            "Training Iteration 3867, Loss: 3.531702756881714\n",
            "Training Iteration 3868, Loss: 3.281128168106079\n",
            "Training Iteration 3869, Loss: 6.345304489135742\n",
            "Training Iteration 3870, Loss: 4.42758846282959\n",
            "Training Iteration 3871, Loss: 5.967082977294922\n",
            "Training Iteration 3872, Loss: 2.847935676574707\n",
            "Training Iteration 3873, Loss: 2.516672134399414\n",
            "Training Iteration 3874, Loss: 5.840966701507568\n",
            "Training Iteration 3875, Loss: 3.6917991638183594\n",
            "Training Iteration 3876, Loss: 5.549458980560303\n",
            "Training Iteration 3877, Loss: 3.857269763946533\n",
            "Training Iteration 3878, Loss: 4.0135111808776855\n",
            "Training Iteration 3879, Loss: 3.51600980758667\n",
            "Training Iteration 3880, Loss: 1.2894068956375122\n",
            "Training Iteration 3881, Loss: 2.9446229934692383\n",
            "Training Iteration 3882, Loss: 3.314133405685425\n",
            "Training Iteration 3883, Loss: 3.3897838592529297\n",
            "Training Iteration 3884, Loss: 2.329814910888672\n",
            "Training Iteration 3885, Loss: 9.983406066894531\n",
            "Training Iteration 3886, Loss: 6.204580307006836\n",
            "Training Iteration 3887, Loss: 5.666590213775635\n",
            "Training Iteration 3888, Loss: 6.98982572555542\n",
            "Training Iteration 3889, Loss: 5.188047885894775\n",
            "Training Iteration 3890, Loss: 4.623733043670654\n",
            "Training Iteration 3891, Loss: 8.254286766052246\n",
            "Training Iteration 3892, Loss: 7.848038673400879\n",
            "Training Iteration 3893, Loss: 6.509090423583984\n",
            "Training Iteration 3894, Loss: 8.235795021057129\n",
            "Training Iteration 3895, Loss: 4.7533345222473145\n",
            "Training Iteration 3896, Loss: 2.687947988510132\n",
            "Training Iteration 3897, Loss: 6.878493785858154\n",
            "Training Iteration 3898, Loss: 7.893035411834717\n",
            "Training Iteration 3899, Loss: 3.66906476020813\n",
            "Training Iteration 3900, Loss: 4.804356575012207\n",
            "Training Iteration 3901, Loss: 5.719239234924316\n",
            "Training Iteration 3902, Loss: 3.664015293121338\n",
            "Training Iteration 3903, Loss: 5.107303619384766\n",
            "Training Iteration 3904, Loss: 4.826010227203369\n",
            "Training Iteration 3905, Loss: 5.479153633117676\n",
            "Training Iteration 3906, Loss: 3.0516510009765625\n",
            "Training Iteration 3907, Loss: 6.424488067626953\n",
            "Training Iteration 3908, Loss: 4.331719398498535\n",
            "Training Iteration 3909, Loss: 5.707274436950684\n",
            "Training Iteration 3910, Loss: 4.997549057006836\n",
            "Training Iteration 3911, Loss: 4.95298957824707\n",
            "Training Iteration 3912, Loss: 3.156935214996338\n",
            "Training Iteration 3913, Loss: 5.797273635864258\n",
            "Training Iteration 3914, Loss: 4.455901622772217\n",
            "Training Iteration 3915, Loss: 4.393895626068115\n",
            "Training Iteration 3916, Loss: 2.1878626346588135\n",
            "Training Iteration 3917, Loss: 6.546365737915039\n",
            "Training Iteration 3918, Loss: 5.648805141448975\n",
            "Training Iteration 3919, Loss: 4.909236907958984\n",
            "Training Iteration 3920, Loss: 5.028141021728516\n",
            "Training Iteration 3921, Loss: 4.345625400543213\n",
            "Training Iteration 3922, Loss: 7.149059295654297\n",
            "Training Iteration 3923, Loss: 3.6817970275878906\n",
            "Training Iteration 3924, Loss: 5.346658229827881\n",
            "Training Iteration 3925, Loss: 3.789675235748291\n",
            "Training Iteration 3926, Loss: 2.594953775405884\n",
            "Training Iteration 3927, Loss: 3.4083287715911865\n",
            "Training Iteration 3928, Loss: 5.815647602081299\n",
            "Training Iteration 3929, Loss: 4.710005760192871\n",
            "Training Iteration 3930, Loss: 7.253386497497559\n",
            "Training Iteration 3931, Loss: 4.434171199798584\n",
            "Training Iteration 3932, Loss: 5.654428482055664\n",
            "Training Iteration 3933, Loss: 5.134023666381836\n",
            "Training Iteration 3934, Loss: 5.421182632446289\n",
            "Training Iteration 3935, Loss: 6.531289100646973\n",
            "Training Iteration 3936, Loss: 4.850979804992676\n",
            "Training Iteration 3937, Loss: 5.903316497802734\n",
            "Training Iteration 3938, Loss: 4.1876420974731445\n",
            "Training Iteration 3939, Loss: 8.272516250610352\n",
            "Training Iteration 3940, Loss: 3.615340232849121\n",
            "Training Iteration 3941, Loss: 4.9576616287231445\n",
            "Training Iteration 3942, Loss: 5.005818843841553\n",
            "Training Iteration 3943, Loss: 3.9780712127685547\n",
            "Training Iteration 3944, Loss: 3.751081943511963\n",
            "Training Iteration 3945, Loss: 6.612811088562012\n",
            "Training Iteration 3946, Loss: 6.146010398864746\n",
            "Training Iteration 3947, Loss: 9.558568954467773\n",
            "Training Iteration 3948, Loss: 3.2641000747680664\n",
            "Training Iteration 3949, Loss: 8.342632293701172\n",
            "Training Iteration 3950, Loss: 3.4084513187408447\n",
            "Training Iteration 3951, Loss: 4.231170654296875\n",
            "Training Iteration 3952, Loss: 7.843674182891846\n",
            "Training Iteration 3953, Loss: 3.096538543701172\n",
            "Training Iteration 3954, Loss: 2.8232643604278564\n",
            "Training Iteration 3955, Loss: 3.824674606323242\n",
            "Training Iteration 3956, Loss: 4.255941867828369\n",
            "Training Iteration 3957, Loss: 4.517745494842529\n",
            "Training Iteration 3958, Loss: 4.299942970275879\n",
            "Training Iteration 3959, Loss: 3.317425012588501\n",
            "Training Iteration 3960, Loss: 4.99786376953125\n",
            "Training Iteration 3961, Loss: 3.7634289264678955\n",
            "Training Iteration 3962, Loss: 3.5138444900512695\n",
            "Training Iteration 3963, Loss: 3.4473659992218018\n",
            "Training Iteration 3964, Loss: 3.6608495712280273\n",
            "Training Iteration 3965, Loss: 3.7676944732666016\n",
            "Training Iteration 3966, Loss: 4.410412788391113\n",
            "Training Iteration 3967, Loss: 4.253509998321533\n",
            "Training Iteration 3968, Loss: 2.5597028732299805\n",
            "Training Iteration 3969, Loss: 5.5569682121276855\n",
            "Training Iteration 3970, Loss: 6.241157531738281\n",
            "Training Iteration 3971, Loss: 5.711292266845703\n",
            "Training Iteration 3972, Loss: 3.5020453929901123\n",
            "Training Iteration 3973, Loss: 5.491771697998047\n",
            "Training Iteration 3974, Loss: 3.8095498085021973\n",
            "Training Iteration 3975, Loss: 6.31423807144165\n",
            "Training Iteration 3976, Loss: 5.284815788269043\n",
            "Training Iteration 3977, Loss: 5.740674018859863\n",
            "Training Iteration 3978, Loss: 5.504554748535156\n",
            "Training Iteration 3979, Loss: 4.776366710662842\n",
            "Training Iteration 3980, Loss: 3.4374053478240967\n",
            "Training Iteration 3981, Loss: 3.5566978454589844\n",
            "Training Iteration 3982, Loss: 5.505557060241699\n",
            "Training Iteration 3983, Loss: 3.565399646759033\n",
            "Training Iteration 3984, Loss: 5.19955587387085\n",
            "Training Iteration 3985, Loss: 4.39655065536499\n",
            "Training Iteration 3986, Loss: 3.0658671855926514\n",
            "Training Iteration 3987, Loss: 7.22709846496582\n",
            "Training Iteration 3988, Loss: 2.8894124031066895\n",
            "Training Iteration 3989, Loss: 3.86700701713562\n",
            "Training Iteration 3990, Loss: 4.784512519836426\n",
            "Training Iteration 3991, Loss: 5.226855754852295\n",
            "Training Iteration 3992, Loss: 2.7042829990386963\n",
            "Training Iteration 3993, Loss: 4.089903831481934\n",
            "Training Iteration 3994, Loss: 3.7061967849731445\n",
            "Training Iteration 3995, Loss: 4.43762731552124\n",
            "Training Iteration 3996, Loss: 3.3029520511627197\n",
            "Training Iteration 3997, Loss: 4.624964714050293\n",
            "Training Iteration 3998, Loss: 5.838488578796387\n",
            "Training Iteration 3999, Loss: 6.714832305908203\n",
            "Training Iteration 4000, Loss: 3.3642005920410156\n",
            "Training Iteration 4001, Loss: 2.5049688816070557\n",
            "Training Iteration 4002, Loss: 3.8612120151519775\n",
            "Training Iteration 4003, Loss: 2.2862069606781006\n",
            "Training Iteration 4004, Loss: 5.831942558288574\n",
            "Training Iteration 4005, Loss: 8.11368179321289\n",
            "Training Iteration 4006, Loss: 9.499341011047363\n",
            "Training Iteration 4007, Loss: 5.279868125915527\n",
            "Training Iteration 4008, Loss: 5.873682975769043\n",
            "Training Iteration 4009, Loss: 5.7070770263671875\n",
            "Training Iteration 4010, Loss: 4.742243766784668\n",
            "Training Iteration 4011, Loss: 3.214050769805908\n",
            "Training Iteration 4012, Loss: 5.006994247436523\n",
            "Training Iteration 4013, Loss: 4.76754093170166\n",
            "Training Iteration 4014, Loss: 3.377197265625\n",
            "Training Iteration 4015, Loss: 4.2812652587890625\n",
            "Training Iteration 4016, Loss: 3.6407699584960938\n",
            "Training Iteration 4017, Loss: 2.463170289993286\n",
            "Training Iteration 4018, Loss: 2.796912670135498\n",
            "Training Iteration 4019, Loss: 4.240171432495117\n",
            "Training Iteration 4020, Loss: 4.314000606536865\n",
            "Training Iteration 4021, Loss: 8.864874839782715\n",
            "Training Iteration 4022, Loss: 4.815340518951416\n",
            "Training Iteration 4023, Loss: 2.0700087547302246\n",
            "Training Iteration 4024, Loss: 3.7761173248291016\n",
            "Training Iteration 4025, Loss: 1.6678962707519531\n",
            "Training Iteration 4026, Loss: 9.725028038024902\n",
            "Training Iteration 4027, Loss: 2.609959840774536\n",
            "Training Iteration 4028, Loss: 5.089077949523926\n",
            "Training Iteration 4029, Loss: 3.646965980529785\n",
            "Training Iteration 4030, Loss: 4.951270580291748\n",
            "Training Iteration 4031, Loss: 3.0848331451416016\n",
            "Training Iteration 4032, Loss: 2.304917335510254\n",
            "Training Iteration 4033, Loss: 3.4736199378967285\n",
            "Training Iteration 4034, Loss: 3.978505849838257\n",
            "Training Iteration 4035, Loss: 6.5926923751831055\n",
            "Training Iteration 4036, Loss: 4.49473237991333\n",
            "Training Iteration 4037, Loss: 3.2133820056915283\n",
            "Training Iteration 4038, Loss: 3.4402589797973633\n",
            "Training Iteration 4039, Loss: 5.180706024169922\n",
            "Training Iteration 4040, Loss: 2.816506862640381\n",
            "Training Iteration 4041, Loss: 3.535266160964966\n",
            "Training Iteration 4042, Loss: 4.4485087394714355\n",
            "Training Iteration 4043, Loss: 3.2961857318878174\n",
            "Training Iteration 4044, Loss: 9.78145694732666\n",
            "Training Iteration 4045, Loss: 3.612192392349243\n",
            "Training Iteration 4046, Loss: 5.901352405548096\n",
            "Training Iteration 4047, Loss: 6.726393222808838\n",
            "Training Iteration 4048, Loss: 2.4956493377685547\n",
            "Training Iteration 4049, Loss: 6.09264612197876\n",
            "Training Iteration 4050, Loss: 8.136661529541016\n",
            "Training Iteration 4051, Loss: 10.244874954223633\n",
            "Training Iteration 4052, Loss: 5.445788383483887\n",
            "Training Iteration 4053, Loss: 7.504443168640137\n",
            "Training Iteration 4054, Loss: 3.827080726623535\n",
            "Training Iteration 4055, Loss: 5.470040798187256\n",
            "Training Iteration 4056, Loss: 4.834239482879639\n",
            "Training Iteration 4057, Loss: 4.9113664627075195\n",
            "Training Iteration 4058, Loss: 4.4649200439453125\n",
            "Training Iteration 4059, Loss: 3.5363881587982178\n",
            "Training Iteration 4060, Loss: 6.348555088043213\n",
            "Training Iteration 4061, Loss: 5.078537464141846\n",
            "Training Iteration 4062, Loss: 5.201663970947266\n",
            "Training Iteration 4063, Loss: 4.375309467315674\n",
            "Training Iteration 4064, Loss: 6.195609092712402\n",
            "Training Iteration 4065, Loss: 4.898203372955322\n",
            "Training Iteration 4066, Loss: 5.599763870239258\n",
            "Training Iteration 4067, Loss: 4.482684135437012\n",
            "Training Iteration 4068, Loss: 6.386466979980469\n",
            "Training Iteration 4069, Loss: 6.664689540863037\n",
            "Training Iteration 4070, Loss: 6.585724353790283\n",
            "Training Iteration 4071, Loss: 5.609987735748291\n",
            "Training Iteration 4072, Loss: 8.079938888549805\n",
            "Training Iteration 4073, Loss: 3.0007898807525635\n",
            "Training Iteration 4074, Loss: 3.2901992797851562\n",
            "Training Iteration 4075, Loss: 11.24988842010498\n",
            "Training Iteration 4076, Loss: 4.839697360992432\n",
            "Training Iteration 4077, Loss: 8.60280990600586\n",
            "Training Iteration 4078, Loss: 7.752596855163574\n",
            "Training Iteration 4079, Loss: 5.79014778137207\n",
            "Training Iteration 4080, Loss: 3.6956303119659424\n",
            "Training Iteration 4081, Loss: 7.737800121307373\n",
            "Training Iteration 4082, Loss: 4.262025833129883\n",
            "Training Iteration 4083, Loss: 5.9693193435668945\n",
            "Training Iteration 4084, Loss: 7.962674140930176\n",
            "Training Iteration 4085, Loss: 3.4737179279327393\n",
            "Training Iteration 4086, Loss: 2.8357303142547607\n",
            "Training Iteration 4087, Loss: 2.9177117347717285\n",
            "Training Iteration 4088, Loss: 7.416336536407471\n",
            "Training Iteration 4089, Loss: 2.637104034423828\n",
            "Training Iteration 4090, Loss: 3.0260143280029297\n",
            "Training Iteration 4091, Loss: 8.193816184997559\n",
            "Training Iteration 4092, Loss: 7.436378479003906\n",
            "Training Iteration 4093, Loss: 9.557145118713379\n",
            "Training Iteration 4094, Loss: 8.02937126159668\n",
            "Training Iteration 4095, Loss: 4.4560394287109375\n",
            "Training Iteration 4096, Loss: 6.937709331512451\n",
            "Training Iteration 4097, Loss: 4.308782577514648\n",
            "Training Iteration 4098, Loss: 4.751774311065674\n",
            "Training Iteration 4099, Loss: 4.922688007354736\n",
            "Training Iteration 4100, Loss: 4.952247142791748\n",
            "Training Iteration 4101, Loss: 7.474498271942139\n",
            "Training Iteration 4102, Loss: 2.5501952171325684\n",
            "Training Iteration 4103, Loss: 2.7317113876342773\n",
            "Training Iteration 4104, Loss: 3.2453176975250244\n",
            "Training Iteration 4105, Loss: 4.390223503112793\n",
            "Training Iteration 4106, Loss: 4.604004383087158\n",
            "Training Iteration 4107, Loss: 4.420483589172363\n",
            "Training Iteration 4108, Loss: 4.421019554138184\n",
            "Training Iteration 4109, Loss: 7.144214630126953\n",
            "Training Iteration 4110, Loss: 6.489841461181641\n",
            "Training Iteration 4111, Loss: 3.5818848609924316\n",
            "Training Iteration 4112, Loss: 4.624211311340332\n",
            "Training Iteration 4113, Loss: 3.3702125549316406\n",
            "Training Iteration 4114, Loss: 3.7762465476989746\n",
            "Training Iteration 4115, Loss: 2.761725664138794\n",
            "Training Iteration 4116, Loss: 3.987912654876709\n",
            "Training Iteration 4117, Loss: 5.465453624725342\n",
            "Training Iteration 4118, Loss: 1.571761131286621\n",
            "Training Iteration 4119, Loss: 4.741166591644287\n",
            "Training Iteration 4120, Loss: 4.056631565093994\n",
            "Training Iteration 4121, Loss: 4.889721393585205\n",
            "Training Iteration 4122, Loss: 2.730341672897339\n",
            "Training Iteration 4123, Loss: 4.005604267120361\n",
            "Training Iteration 4124, Loss: 2.4045770168304443\n",
            "Training Iteration 4125, Loss: 4.379640102386475\n",
            "Training Iteration 4126, Loss: 6.166347503662109\n",
            "Training Iteration 4127, Loss: 2.871957540512085\n",
            "Training Iteration 4128, Loss: 2.931893825531006\n",
            "Training Iteration 4129, Loss: 4.763859748840332\n",
            "Training Iteration 4130, Loss: 3.2027499675750732\n",
            "Training Iteration 4131, Loss: 7.8848161697387695\n",
            "Training Iteration 4132, Loss: 4.12317419052124\n",
            "Training Iteration 4133, Loss: 4.310835361480713\n",
            "Training Iteration 4134, Loss: 2.3912839889526367\n",
            "Training Iteration 4135, Loss: 7.384979248046875\n",
            "Training Iteration 4136, Loss: 3.171884059906006\n",
            "Training Iteration 4137, Loss: 10.899053573608398\n",
            "Training Iteration 4138, Loss: 5.736865043640137\n",
            "Training Iteration 4139, Loss: 4.269675254821777\n",
            "Training Iteration 4140, Loss: 5.575464248657227\n",
            "Training Iteration 4141, Loss: 3.6818153858184814\n",
            "Training Iteration 4142, Loss: 6.7518839836120605\n",
            "Training Iteration 4143, Loss: 3.423475742340088\n",
            "Training Iteration 4144, Loss: 5.470088481903076\n",
            "Training Iteration 4145, Loss: 6.855865955352783\n",
            "Training Iteration 4146, Loss: 9.372177124023438\n",
            "Training Iteration 4147, Loss: 9.909570693969727\n",
            "Training Iteration 4148, Loss: 4.674423694610596\n",
            "Training Iteration 4149, Loss: 5.669672012329102\n",
            "Training Iteration 4150, Loss: 2.8940787315368652\n",
            "Training Iteration 4151, Loss: 5.399281978607178\n",
            "Training Iteration 4152, Loss: 8.113598823547363\n",
            "Training Iteration 4153, Loss: 4.7585554122924805\n",
            "Training Iteration 4154, Loss: 2.714630365371704\n",
            "Training Iteration 4155, Loss: 4.9299845695495605\n",
            "Training Iteration 4156, Loss: 5.388381481170654\n",
            "Training Iteration 4157, Loss: 3.421736240386963\n",
            "Training Iteration 4158, Loss: 3.2691023349761963\n",
            "Training Iteration 4159, Loss: 4.539178848266602\n",
            "Training Iteration 4160, Loss: 2.821620225906372\n",
            "Training Iteration 4161, Loss: 6.160422325134277\n",
            "Training Iteration 4162, Loss: 2.9242305755615234\n",
            "Training Iteration 4163, Loss: 5.667140007019043\n",
            "Training Iteration 4164, Loss: 3.759178638458252\n",
            "Training Iteration 4165, Loss: 2.756373405456543\n",
            "Training Iteration 4166, Loss: 5.750382423400879\n",
            "Training Iteration 4167, Loss: 3.5287137031555176\n",
            "Training Iteration 4168, Loss: 7.1084675788879395\n",
            "Training Iteration 4169, Loss: 6.086118221282959\n",
            "Training Iteration 4170, Loss: 2.9168806076049805\n",
            "Training Iteration 4171, Loss: 4.151433944702148\n",
            "Training Iteration 4172, Loss: 4.190924644470215\n",
            "Training Iteration 4173, Loss: 7.783222198486328\n",
            "Training Iteration 4174, Loss: 5.5054731369018555\n",
            "Training Iteration 4175, Loss: 3.684566020965576\n",
            "Training Iteration 4176, Loss: 7.304165363311768\n",
            "Training Iteration 4177, Loss: 4.007401466369629\n",
            "Training Iteration 4178, Loss: 3.3525500297546387\n",
            "Training Iteration 4179, Loss: 4.142548084259033\n",
            "Training Iteration 4180, Loss: 1.8186790943145752\n",
            "Training Iteration 4181, Loss: 3.1200411319732666\n",
            "Training Iteration 4182, Loss: 7.3650665283203125\n",
            "Training Iteration 4183, Loss: 6.104142189025879\n",
            "Training Iteration 4184, Loss: 8.005443572998047\n",
            "Training Iteration 4185, Loss: 10.341299057006836\n",
            "Training Iteration 4186, Loss: 2.1504931449890137\n",
            "Training Iteration 4187, Loss: 2.0510435104370117\n",
            "Training Iteration 4188, Loss: 4.107746124267578\n",
            "Training Iteration 4189, Loss: 3.9437694549560547\n",
            "Training Iteration 4190, Loss: 8.616883277893066\n",
            "Training Iteration 4191, Loss: 3.7559425830841064\n",
            "Training Iteration 4192, Loss: 5.229388236999512\n",
            "Training Iteration 4193, Loss: 4.0060200691223145\n",
            "Training Iteration 4194, Loss: 3.455900192260742\n",
            "Training Iteration 4195, Loss: 3.4660139083862305\n",
            "Training Iteration 4196, Loss: 2.636086940765381\n",
            "Training Iteration 4197, Loss: 2.8553764820098877\n",
            "Training Iteration 4198, Loss: 2.0174944400787354\n",
            "Training Iteration 4199, Loss: 3.554643392562866\n",
            "Training Iteration 4200, Loss: 2.68680477142334\n",
            "Training Iteration 4201, Loss: 5.737851142883301\n",
            "Training Iteration 4202, Loss: 5.634748458862305\n",
            "Training Iteration 4203, Loss: 4.990420818328857\n",
            "Training Iteration 4204, Loss: 3.595000743865967\n",
            "Training Iteration 4205, Loss: 7.631582260131836\n",
            "Training Iteration 4206, Loss: 3.181626081466675\n",
            "Training Iteration 4207, Loss: 5.227180480957031\n",
            "Training Iteration 4208, Loss: 5.7472429275512695\n",
            "Training Iteration 4209, Loss: 3.622518301010132\n",
            "Training Iteration 4210, Loss: 2.87552547454834\n",
            "Training Iteration 4211, Loss: 3.904615640640259\n",
            "Training Iteration 4212, Loss: 2.6972107887268066\n",
            "Training Iteration 4213, Loss: 3.196131944656372\n",
            "Training Iteration 4214, Loss: 2.5522713661193848\n",
            "Training Iteration 4215, Loss: 3.172139883041382\n",
            "Training Iteration 4216, Loss: 3.971266508102417\n",
            "Training Iteration 4217, Loss: 3.4067771434783936\n",
            "Training Iteration 4218, Loss: 6.8586859703063965\n",
            "Training Iteration 4219, Loss: 4.2352118492126465\n",
            "Training Iteration 4220, Loss: 3.9390666484832764\n",
            "Training Iteration 4221, Loss: 6.293863773345947\n",
            "Training Iteration 4222, Loss: 5.201589584350586\n",
            "Training Iteration 4223, Loss: 5.637073993682861\n",
            "Training Iteration 4224, Loss: 5.17744779586792\n",
            "Training Iteration 4225, Loss: 3.9059948921203613\n",
            "Training Iteration 4226, Loss: 5.329620361328125\n",
            "Training Iteration 4227, Loss: 6.809821605682373\n",
            "Training Iteration 4228, Loss: 2.9543657302856445\n",
            "Training Iteration 4229, Loss: 7.294582843780518\n",
            "Training Iteration 4230, Loss: 3.717808961868286\n",
            "Training Iteration 4231, Loss: 5.270677089691162\n",
            "Training Iteration 4232, Loss: 3.80051589012146\n",
            "Training Iteration 4233, Loss: 3.2524526119232178\n",
            "Training Iteration 4234, Loss: 2.7692718505859375\n",
            "Training Iteration 4235, Loss: 3.3983519077301025\n",
            "Training Iteration 4236, Loss: 3.385817527770996\n",
            "Training Iteration 4237, Loss: 4.0928120613098145\n",
            "Training Iteration 4238, Loss: 5.994222164154053\n",
            "Training Iteration 4239, Loss: 3.020501136779785\n",
            "Training Iteration 4240, Loss: 5.123954772949219\n",
            "Training Iteration 4241, Loss: 3.986043930053711\n",
            "Training Iteration 4242, Loss: 4.364328861236572\n",
            "Training Iteration 4243, Loss: 3.2097134590148926\n",
            "Training Iteration 4244, Loss: 3.9917612075805664\n",
            "Training Iteration 4245, Loss: 2.8625550270080566\n",
            "Training Iteration 4246, Loss: 7.076889991760254\n",
            "Training Iteration 4247, Loss: 4.6233720779418945\n",
            "Training Iteration 4248, Loss: 4.801869869232178\n",
            "Training Iteration 4249, Loss: 4.675583362579346\n",
            "Training Iteration 4250, Loss: 8.975107192993164\n",
            "Training Iteration 4251, Loss: 3.179779529571533\n",
            "Training Iteration 4252, Loss: 6.117488861083984\n",
            "Training Iteration 4253, Loss: 5.124029159545898\n",
            "Training Iteration 4254, Loss: 7.115697383880615\n",
            "Training Iteration 4255, Loss: 2.8201091289520264\n",
            "Training Iteration 4256, Loss: 1.8295406103134155\n",
            "Training Iteration 4257, Loss: 2.4850547313690186\n",
            "Training Iteration 4258, Loss: 4.611933708190918\n",
            "Training Iteration 4259, Loss: 6.680785655975342\n",
            "Training Iteration 4260, Loss: 4.533278942108154\n",
            "Training Iteration 4261, Loss: 5.887747764587402\n",
            "Training Iteration 4262, Loss: 3.4431493282318115\n",
            "Training Iteration 4263, Loss: 3.3445053100585938\n",
            "Training Iteration 4264, Loss: 5.461750507354736\n",
            "Training Iteration 4265, Loss: 1.80539870262146\n",
            "Training Iteration 4266, Loss: 5.4694132804870605\n",
            "Training Iteration 4267, Loss: 5.422677040100098\n",
            "Training Iteration 4268, Loss: 4.990257740020752\n",
            "Training Iteration 4269, Loss: 3.678910970687866\n",
            "Training Iteration 4270, Loss: 2.7331066131591797\n",
            "Training Iteration 4271, Loss: 3.2847843170166016\n",
            "Training Iteration 4272, Loss: 4.8550896644592285\n",
            "Training Iteration 4273, Loss: 7.511041164398193\n",
            "Training Iteration 4274, Loss: 6.199692726135254\n",
            "Training Iteration 4275, Loss: 3.4662437438964844\n",
            "Training Iteration 4276, Loss: 4.295282363891602\n",
            "Training Iteration 4277, Loss: 4.627363681793213\n",
            "Training Iteration 4278, Loss: 2.148528814315796\n",
            "Training Iteration 4279, Loss: 3.3046371936798096\n",
            "Training Iteration 4280, Loss: 2.223287343978882\n",
            "Training Iteration 4281, Loss: 3.051976203918457\n",
            "Training Iteration 4282, Loss: 2.9271156787872314\n",
            "Training Iteration 4283, Loss: 4.573947429656982\n",
            "Training Iteration 4284, Loss: 6.573234558105469\n",
            "Training Iteration 4285, Loss: 6.165124893188477\n",
            "Training Iteration 4286, Loss: 6.413578510284424\n",
            "Training Iteration 4287, Loss: 3.2890639305114746\n",
            "Training Iteration 4288, Loss: 1.9842872619628906\n",
            "Training Iteration 4289, Loss: 2.471008539199829\n",
            "Training Iteration 4290, Loss: 5.71353816986084\n",
            "Training Iteration 4291, Loss: 4.42595100402832\n",
            "Training Iteration 4292, Loss: 5.541045665740967\n",
            "Training Iteration 4293, Loss: 5.641764163970947\n",
            "Training Iteration 4294, Loss: 6.629108428955078\n",
            "Training Iteration 4295, Loss: 3.857349395751953\n",
            "Training Iteration 4296, Loss: 3.193087577819824\n",
            "Training Iteration 4297, Loss: 3.559412956237793\n",
            "Training Iteration 4298, Loss: 5.473714351654053\n",
            "Training Iteration 4299, Loss: 4.643413066864014\n",
            "Training Iteration 4300, Loss: 7.010229110717773\n",
            "Training Iteration 4301, Loss: 5.29965877532959\n",
            "Training Iteration 4302, Loss: 3.5831809043884277\n",
            "Training Iteration 4303, Loss: 2.732567071914673\n",
            "Training Iteration 4304, Loss: 4.333619594573975\n",
            "Training Iteration 4305, Loss: 1.476193904876709\n",
            "Training Iteration 4306, Loss: 3.4194071292877197\n",
            "Training Iteration 4307, Loss: 2.4612040519714355\n",
            "Training Iteration 4308, Loss: 3.4162709712982178\n",
            "Training Iteration 4309, Loss: 5.3734517097473145\n",
            "Training Iteration 4310, Loss: 3.3519794940948486\n",
            "Training Iteration 4311, Loss: 2.3401644229888916\n",
            "Training Iteration 4312, Loss: 1.2924447059631348\n",
            "Training Iteration 4313, Loss: 4.996340274810791\n",
            "Training Iteration 4314, Loss: 6.691054821014404\n",
            "Training Iteration 4315, Loss: 2.221951723098755\n",
            "Training Iteration 4316, Loss: 5.447883605957031\n",
            "Training Iteration 4317, Loss: 3.1831843852996826\n",
            "Training Iteration 4318, Loss: 2.7850656509399414\n",
            "Training Iteration 4319, Loss: 4.664862632751465\n",
            "Training Iteration 4320, Loss: 4.242361068725586\n",
            "Training Iteration 4321, Loss: 5.93908166885376\n",
            "Training Iteration 4322, Loss: 2.1551270484924316\n",
            "Training Iteration 4323, Loss: 2.7726991176605225\n",
            "Training Iteration 4324, Loss: 2.784752130508423\n",
            "Training Iteration 4325, Loss: 3.9125900268554688\n",
            "Training Iteration 4326, Loss: 1.720515251159668\n",
            "Training Iteration 4327, Loss: 2.9294047355651855\n",
            "Training Iteration 4328, Loss: 10.551837921142578\n",
            "Training Iteration 4329, Loss: 1.9553285837173462\n",
            "Training Iteration 4330, Loss: 4.278002738952637\n",
            "Training Iteration 4331, Loss: 4.633678436279297\n",
            "Training Iteration 4332, Loss: 3.662924289703369\n",
            "Training Iteration 4333, Loss: 5.751547813415527\n",
            "Training Iteration 4334, Loss: 6.544027805328369\n",
            "Training Iteration 4335, Loss: 3.124133825302124\n",
            "Training Iteration 4336, Loss: 3.5253090858459473\n",
            "Training Iteration 4337, Loss: 4.2390313148498535\n",
            "Training Iteration 4338, Loss: 2.8540985584259033\n",
            "Training Iteration 4339, Loss: 3.049898386001587\n",
            "Training Iteration 4340, Loss: 4.224296569824219\n",
            "Training Iteration 4341, Loss: 3.0849080085754395\n",
            "Training Iteration 4342, Loss: 3.6611576080322266\n",
            "Training Iteration 4343, Loss: 4.116389274597168\n",
            "Training Iteration 4344, Loss: 4.374932765960693\n",
            "Training Iteration 4345, Loss: 2.6540465354919434\n",
            "Training Iteration 4346, Loss: 3.2848594188690186\n",
            "Training Iteration 4347, Loss: 4.165103912353516\n",
            "Training Iteration 4348, Loss: 5.666615009307861\n",
            "Training Iteration 4349, Loss: 6.1947126388549805\n",
            "Training Iteration 4350, Loss: 5.476483345031738\n",
            "Training Iteration 4351, Loss: 3.3874340057373047\n",
            "Training Iteration 4352, Loss: 5.583952903747559\n",
            "Training Iteration 4353, Loss: 6.742650508880615\n",
            "Training Iteration 4354, Loss: 4.753307819366455\n",
            "Training Iteration 4355, Loss: 2.709895133972168\n",
            "Training Iteration 4356, Loss: 3.448387861251831\n",
            "Training Iteration 4357, Loss: 4.992349147796631\n",
            "Training Iteration 4358, Loss: 3.740007162094116\n",
            "Training Iteration 4359, Loss: 5.398108959197998\n",
            "Training Iteration 4360, Loss: 4.5298967361450195\n",
            "Training Iteration 4361, Loss: 4.222887992858887\n",
            "Training Iteration 4362, Loss: 6.47324275970459\n",
            "Training Iteration 4363, Loss: 3.2061119079589844\n",
            "Training Iteration 4364, Loss: 4.17606782913208\n",
            "Training Iteration 4365, Loss: 5.9118194580078125\n",
            "Training Iteration 4366, Loss: 7.310147285461426\n",
            "Training Iteration 4367, Loss: 3.470949649810791\n",
            "Training Iteration 4368, Loss: 3.4316246509552\n",
            "Training Iteration 4369, Loss: 6.8777971267700195\n",
            "Training Iteration 4370, Loss: 9.81890869140625\n",
            "Training Iteration 4371, Loss: 7.603645324707031\n",
            "Training Iteration 4372, Loss: 1.4941062927246094\n",
            "Training Iteration 4373, Loss: 5.144663333892822\n",
            "Training Iteration 4374, Loss: 4.420948505401611\n",
            "Training Iteration 4375, Loss: 3.342942953109741\n",
            "Training Iteration 4376, Loss: 3.8513810634613037\n",
            "Training Iteration 4377, Loss: 5.126077651977539\n",
            "Training Iteration 4378, Loss: 4.507399082183838\n",
            "Training Iteration 4379, Loss: 4.8465776443481445\n",
            "Training Iteration 4380, Loss: 4.177854061126709\n",
            "Training Iteration 4381, Loss: 5.365310192108154\n",
            "Training Iteration 4382, Loss: 7.376495361328125\n",
            "Training Iteration 4383, Loss: 3.7900071144104004\n",
            "Training Iteration 4384, Loss: 3.7468338012695312\n",
            "Training Iteration 4385, Loss: 5.023540496826172\n",
            "Training Iteration 4386, Loss: 5.150589466094971\n",
            "Training Iteration 4387, Loss: 3.8940515518188477\n",
            "Training Iteration 4388, Loss: 7.3432230949401855\n",
            "Training Iteration 4389, Loss: 2.121514320373535\n",
            "Training Iteration 4390, Loss: 3.243340492248535\n",
            "Training Iteration 4391, Loss: 2.2270379066467285\n",
            "Training Iteration 4392, Loss: 5.332860946655273\n",
            "Training Iteration 4393, Loss: 3.327425241470337\n",
            "Training Iteration 4394, Loss: 4.724905014038086\n",
            "Training Iteration 4395, Loss: 7.361883163452148\n",
            "Training Iteration 4396, Loss: 4.544938564300537\n",
            "Training Iteration 4397, Loss: 2.451930284500122\n",
            "Training Iteration 4398, Loss: 3.1429405212402344\n",
            "Training Iteration 4399, Loss: 3.4572460651397705\n",
            "Training Iteration 4400, Loss: 5.873446464538574\n",
            "Training Iteration 4401, Loss: 3.3002805709838867\n",
            "Training Iteration 4402, Loss: 4.345236301422119\n",
            "Training Iteration 4403, Loss: 3.6586127281188965\n",
            "Training Iteration 4404, Loss: 4.081662178039551\n",
            "Training Iteration 4405, Loss: 5.276979446411133\n",
            "Training Iteration 4406, Loss: 2.0134925842285156\n",
            "Training Iteration 4407, Loss: 6.334228038787842\n",
            "Training Iteration 4408, Loss: 3.840496063232422\n",
            "Training Iteration 4409, Loss: 6.040243148803711\n",
            "Training Iteration 4410, Loss: 2.60322642326355\n",
            "Training Iteration 4411, Loss: 4.650399208068848\n",
            "Training Iteration 4412, Loss: 3.853621244430542\n",
            "Training Iteration 4413, Loss: 7.335952281951904\n",
            "Training Iteration 4414, Loss: 6.052184581756592\n",
            "Training Iteration 4415, Loss: 3.4409122467041016\n",
            "Training Iteration 4416, Loss: 4.390215873718262\n",
            "Training Iteration 4417, Loss: 3.87461519241333\n",
            "Training Iteration 4418, Loss: 3.7249674797058105\n",
            "Training Iteration 4419, Loss: 5.435075283050537\n",
            "Training Iteration 4420, Loss: 5.434020042419434\n",
            "Training Iteration 4421, Loss: 5.129926681518555\n",
            "Training Iteration 4422, Loss: 2.673845052719116\n",
            "Training Iteration 4423, Loss: 4.19163703918457\n",
            "Training Iteration 4424, Loss: 3.9172985553741455\n",
            "Training Iteration 4425, Loss: 7.444872856140137\n",
            "Training Iteration 4426, Loss: 4.812718391418457\n",
            "Training Iteration 4427, Loss: 7.287129878997803\n",
            "Training Iteration 4428, Loss: 3.0914528369903564\n",
            "Training Iteration 4429, Loss: 2.822571039199829\n",
            "Training Iteration 4430, Loss: 3.910166025161743\n",
            "Training Iteration 4431, Loss: 6.258067607879639\n",
            "Training Iteration 4432, Loss: 3.8278133869171143\n",
            "Training Iteration 4433, Loss: 4.672695636749268\n",
            "Training Iteration 4434, Loss: 7.2834248542785645\n",
            "Training Iteration 4435, Loss: 4.013191223144531\n",
            "Training Iteration 4436, Loss: 5.188296318054199\n",
            "Training Iteration 4437, Loss: 4.7911481857299805\n",
            "Training Iteration 4438, Loss: 6.400093078613281\n",
            "Training Iteration 4439, Loss: 2.337608814239502\n",
            "Training Iteration 4440, Loss: 5.61466646194458\n",
            "Training Iteration 4441, Loss: 4.265524864196777\n",
            "Training Iteration 4442, Loss: 4.396655559539795\n",
            "Training Iteration 4443, Loss: 4.590348243713379\n",
            "Training Iteration 4444, Loss: 4.653753757476807\n",
            "Training Iteration 4445, Loss: 3.0934317111968994\n",
            "Training Iteration 4446, Loss: 4.3411970138549805\n",
            "Training Iteration 4447, Loss: 5.21097469329834\n",
            "Training Iteration 4448, Loss: 3.9355149269104004\n",
            "Training Iteration 4449, Loss: 3.702838897705078\n",
            "Training Iteration 4450, Loss: 5.427659034729004\n",
            "Training Iteration 4451, Loss: 5.374330997467041\n",
            "Training Iteration 4452, Loss: 1.0134429931640625\n",
            "Training Iteration 4453, Loss: 5.618931293487549\n",
            "Training Iteration 4454, Loss: 4.111513137817383\n",
            "Training Iteration 4455, Loss: 1.8329921960830688\n",
            "Training Iteration 4456, Loss: 5.0012335777282715\n",
            "Training Iteration 4457, Loss: 10.655256271362305\n",
            "Training Iteration 4458, Loss: 3.066457748413086\n",
            "Training Iteration 4459, Loss: 6.135054111480713\n",
            "Training Iteration 4460, Loss: 4.743475437164307\n",
            "Training Iteration 4461, Loss: 3.841658115386963\n",
            "Training Iteration 4462, Loss: 6.646530628204346\n",
            "Training Iteration 4463, Loss: 5.635303497314453\n",
            "Training Iteration 4464, Loss: 5.521882057189941\n",
            "Training Iteration 4465, Loss: 6.563577175140381\n",
            "Training Iteration 4466, Loss: 4.5534443855285645\n",
            "Training Iteration 4467, Loss: 4.123647689819336\n",
            "Training Iteration 4468, Loss: 4.794689655303955\n",
            "Training Iteration 4469, Loss: 4.115411758422852\n",
            "Training Iteration 4470, Loss: 6.115211009979248\n",
            "Training Iteration 4471, Loss: 4.05294132232666\n",
            "Training Iteration 4472, Loss: 3.770728588104248\n",
            "Training Iteration 4473, Loss: 5.837924957275391\n",
            "Training Iteration 4474, Loss: 2.510713815689087\n",
            "Training Iteration 4475, Loss: 3.291447877883911\n",
            "Training Iteration 4476, Loss: 4.104196548461914\n",
            "Training Iteration 4477, Loss: 1.724797010421753\n",
            "Training Iteration 4478, Loss: 8.188592910766602\n",
            "Training Iteration 4479, Loss: 3.566741943359375\n",
            "Training Iteration 4480, Loss: 2.9413201808929443\n",
            "Training Iteration 4481, Loss: 3.491098642349243\n",
            "Training Iteration 4482, Loss: 3.19297194480896\n",
            "Training Iteration 4483, Loss: 7.546248435974121\n",
            "Training Iteration 4484, Loss: 5.051904678344727\n",
            "Training Iteration 4485, Loss: 4.56024694442749\n",
            "Training Iteration 4486, Loss: 2.9430530071258545\n",
            "Training Iteration 4487, Loss: 2.748450517654419\n",
            "Training Iteration 4488, Loss: 2.9042060375213623\n",
            "Training Iteration 4489, Loss: 8.13142204284668\n",
            "Training Iteration 4490, Loss: 4.456331729888916\n",
            "Training Iteration 4491, Loss: 10.209514617919922\n",
            "Training Iteration 4492, Loss: 7.944140434265137\n",
            "Training Iteration 4493, Loss: 5.225456714630127\n",
            "Training Iteration 4494, Loss: 6.300248146057129\n",
            "Training Iteration 4495, Loss: 1.7453216314315796\n",
            "Training Iteration 4496, Loss: 6.387166976928711\n",
            "Training Iteration 4497, Loss: 4.029726028442383\n",
            "Training Iteration 4498, Loss: 3.5984902381896973\n",
            "Training Iteration 4499, Loss: 3.346130132675171\n",
            "Training Iteration 4500, Loss: 9.941055297851562\n",
            "Training Iteration 4501, Loss: 4.901267051696777\n",
            "Training Iteration 4502, Loss: 4.990190029144287\n",
            "Training Iteration 4503, Loss: 6.222120761871338\n",
            "Training Iteration 4504, Loss: 2.8722219467163086\n",
            "Training Iteration 4505, Loss: 6.511041641235352\n",
            "Training Iteration 4506, Loss: 5.283215522766113\n",
            "Training Iteration 4507, Loss: 5.901808261871338\n",
            "Training Iteration 4508, Loss: 5.56723165512085\n",
            "Training Iteration 4509, Loss: 5.0767598152160645\n",
            "Training Iteration 4510, Loss: 5.5637288093566895\n",
            "Training Iteration 4511, Loss: 4.593378067016602\n",
            "Training Iteration 4512, Loss: 4.429335117340088\n",
            "Training Iteration 4513, Loss: 5.2813873291015625\n",
            "Training Iteration 4514, Loss: 5.124538898468018\n",
            "Training Iteration 4515, Loss: 3.5687904357910156\n",
            "Training Iteration 4516, Loss: 6.950770854949951\n",
            "Training Iteration 4517, Loss: 4.657507419586182\n",
            "Training Iteration 4518, Loss: 6.520888805389404\n",
            "Training Iteration 4519, Loss: 6.558497428894043\n",
            "Training Iteration 4520, Loss: 4.2714385986328125\n",
            "Training Iteration 4521, Loss: 4.515024185180664\n",
            "Training Iteration 4522, Loss: 10.135952949523926\n",
            "Training Iteration 4523, Loss: 3.973135471343994\n",
            "Training Iteration 4524, Loss: 4.293700218200684\n",
            "Training Iteration 4525, Loss: 7.8526763916015625\n",
            "Training Iteration 4526, Loss: 7.65923547744751\n",
            "Training Iteration 4527, Loss: 5.026351451873779\n",
            "Training Iteration 4528, Loss: 4.093116760253906\n",
            "Training Iteration 4529, Loss: 2.8580875396728516\n",
            "Training Iteration 4530, Loss: 4.582335472106934\n",
            "Training Iteration 4531, Loss: 3.500730514526367\n",
            "Training Iteration 4532, Loss: 4.078058242797852\n",
            "Training Iteration 4533, Loss: 2.5775976181030273\n",
            "Training Iteration 4534, Loss: 3.2632360458374023\n",
            "Training Iteration 4535, Loss: 5.786520957946777\n",
            "Training Iteration 4536, Loss: 5.695923328399658\n",
            "Training Iteration 4537, Loss: 5.717234134674072\n",
            "Training Iteration 4538, Loss: 1.7440448999404907\n",
            "Training Iteration 4539, Loss: 4.837814807891846\n",
            "Training Iteration 4540, Loss: 3.370443105697632\n",
            "Training Iteration 4541, Loss: 3.338848114013672\n",
            "Training Iteration 4542, Loss: 9.717695236206055\n",
            "Training Iteration 4543, Loss: 3.7598438262939453\n",
            "Training Iteration 4544, Loss: 8.69919204711914\n",
            "Training Iteration 4545, Loss: 6.995319366455078\n",
            "Training Iteration 4546, Loss: 2.080767869949341\n",
            "Training Iteration 4547, Loss: 3.3660728931427\n",
            "Training Iteration 4548, Loss: 4.547482013702393\n",
            "Training Iteration 4549, Loss: 5.418146133422852\n",
            "Training Iteration 4550, Loss: 4.499802112579346\n",
            "Training Iteration 4551, Loss: 4.078976154327393\n",
            "Training Iteration 4552, Loss: 5.4898576736450195\n",
            "Training Iteration 4553, Loss: 3.5234315395355225\n",
            "Training Iteration 4554, Loss: 1.2696682214736938\n",
            "Training Iteration 4555, Loss: 2.8839101791381836\n",
            "Training Iteration 4556, Loss: 3.8233227729797363\n",
            "Training Iteration 4557, Loss: 5.4927978515625\n",
            "Training Iteration 4558, Loss: 4.04623556137085\n",
            "Training Iteration 4559, Loss: 4.925627708435059\n",
            "Training Iteration 4560, Loss: 3.372347831726074\n",
            "Training Iteration 4561, Loss: 4.5203776359558105\n",
            "Training Iteration 4562, Loss: 4.786912441253662\n",
            "Training Iteration 4563, Loss: 3.763744592666626\n",
            "Training Iteration 4564, Loss: 6.816874980926514\n",
            "Training Iteration 4565, Loss: 5.139459133148193\n",
            "Training Iteration 4566, Loss: 5.912606239318848\n",
            "Training Iteration 4567, Loss: 4.42298698425293\n",
            "Training Iteration 4568, Loss: 3.0375702381134033\n",
            "Training Iteration 4569, Loss: 4.50825309753418\n",
            "Training Iteration 4570, Loss: 3.811277389526367\n",
            "Training Iteration 4571, Loss: 6.259658336639404\n",
            "Training Iteration 4572, Loss: 2.515035629272461\n",
            "Training Iteration 4573, Loss: 3.849627733230591\n",
            "Training Iteration 4574, Loss: 2.6363329887390137\n",
            "Training Iteration 4575, Loss: 2.663658380508423\n",
            "Training Iteration 4576, Loss: 3.9620652198791504\n",
            "Training Iteration 4577, Loss: 2.5352139472961426\n",
            "Training Iteration 4578, Loss: 2.6121041774749756\n",
            "Training Iteration 4579, Loss: 5.425591945648193\n",
            "Training Iteration 4580, Loss: 3.9925503730773926\n",
            "Training Iteration 4581, Loss: 5.216465950012207\n",
            "Training Iteration 4582, Loss: 4.023210048675537\n",
            "Training Iteration 4583, Loss: 2.428400993347168\n",
            "Training Iteration 4584, Loss: 2.253824234008789\n",
            "Training Iteration 4585, Loss: 3.402757406234741\n",
            "Training Iteration 4586, Loss: 5.716505527496338\n",
            "Training Iteration 4587, Loss: 4.262537002563477\n",
            "Training Iteration 4588, Loss: 2.399975299835205\n",
            "Training Iteration 4589, Loss: 5.756072521209717\n",
            "Training Iteration 4590, Loss: 6.585291862487793\n",
            "Training Iteration 4591, Loss: 4.3592047691345215\n",
            "Training Iteration 4592, Loss: 6.319729804992676\n",
            "Training Iteration 4593, Loss: 3.061884641647339\n",
            "Training Iteration 4594, Loss: 3.538504123687744\n",
            "Training Iteration 4595, Loss: 8.28787612915039\n",
            "Training Iteration 4596, Loss: 5.2473883628845215\n",
            "Training Iteration 4597, Loss: 2.634948253631592\n",
            "Training Iteration 4598, Loss: 4.287359237670898\n",
            "Training Iteration 4599, Loss: 4.022025108337402\n",
            "Training Iteration 4600, Loss: 5.517910957336426\n",
            "Training Iteration 4601, Loss: 7.610565185546875\n",
            "Training Iteration 4602, Loss: 5.806077480316162\n",
            "Training Iteration 4603, Loss: 4.164692401885986\n",
            "Training Iteration 4604, Loss: 2.5633020401000977\n",
            "Training Iteration 4605, Loss: 6.73347282409668\n",
            "Training Iteration 4606, Loss: 4.986840724945068\n",
            "Training Iteration 4607, Loss: 4.602474689483643\n",
            "Training Iteration 4608, Loss: 5.732421398162842\n",
            "Training Iteration 4609, Loss: 4.095247745513916\n",
            "Training Iteration 4610, Loss: 2.291595697402954\n",
            "Training Iteration 4611, Loss: 6.646965503692627\n",
            "Training Iteration 4612, Loss: 5.143253326416016\n",
            "Training Iteration 4613, Loss: 5.555370807647705\n",
            "Training Iteration 4614, Loss: 3.6094768047332764\n",
            "Training Iteration 4615, Loss: 3.6106598377227783\n",
            "Training Iteration 4616, Loss: 3.228159189224243\n",
            "Training Iteration 4617, Loss: 3.7635271549224854\n",
            "Training Iteration 4618, Loss: 4.353476047515869\n",
            "Training Iteration 4619, Loss: 4.426623821258545\n",
            "Training Iteration 4620, Loss: 4.97052001953125\n",
            "Training Iteration 4621, Loss: 2.719756603240967\n",
            "Training Iteration 4622, Loss: 3.3210184574127197\n",
            "Training Iteration 4623, Loss: 6.048136234283447\n",
            "Training Iteration 4624, Loss: 3.2496047019958496\n",
            "Training Iteration 4625, Loss: 4.576169967651367\n",
            "Training Iteration 4626, Loss: 4.667109966278076\n",
            "Training Iteration 4627, Loss: 6.633543491363525\n",
            "Training Iteration 4628, Loss: 9.938901901245117\n",
            "Training Iteration 4629, Loss: 4.47938871383667\n",
            "Training Iteration 4630, Loss: 2.370206356048584\n",
            "Training Iteration 4631, Loss: 5.006582260131836\n",
            "Training Iteration 4632, Loss: 4.302133083343506\n",
            "Training Iteration 4633, Loss: 6.592796802520752\n",
            "Training Iteration 4634, Loss: 4.18271541595459\n",
            "Training Iteration 4635, Loss: 5.969693183898926\n",
            "Training Iteration 4636, Loss: 7.892345428466797\n",
            "Training Iteration 4637, Loss: 6.485779285430908\n",
            "Training Iteration 4638, Loss: 4.237423419952393\n",
            "Training Iteration 4639, Loss: 3.1571261882781982\n",
            "Training Iteration 4640, Loss: 3.808307647705078\n",
            "Training Iteration 4641, Loss: 7.957520961761475\n",
            "Training Iteration 4642, Loss: 4.493436813354492\n",
            "Training Iteration 4643, Loss: 2.843785285949707\n",
            "Training Iteration 4644, Loss: 2.9448018074035645\n",
            "Training Iteration 4645, Loss: 2.62701416015625\n",
            "Training Iteration 4646, Loss: 9.21834945678711\n",
            "Training Iteration 4647, Loss: 10.862171173095703\n",
            "Training Iteration 4648, Loss: 7.866677761077881\n",
            "Training Iteration 4649, Loss: 3.184908390045166\n",
            "Training Iteration 4650, Loss: 4.331831932067871\n",
            "Training Iteration 4651, Loss: 4.0341410636901855\n",
            "Training Iteration 4652, Loss: 3.570702075958252\n",
            "Training Iteration 4653, Loss: 3.51986026763916\n",
            "Training Iteration 4654, Loss: 4.143160820007324\n",
            "Training Iteration 4655, Loss: 6.203438758850098\n",
            "Training Iteration 4656, Loss: 3.591160297393799\n",
            "Training Iteration 4657, Loss: 10.607269287109375\n",
            "Training Iteration 4658, Loss: 5.760649681091309\n",
            "Training Iteration 4659, Loss: 2.570824384689331\n",
            "Training Iteration 4660, Loss: 5.202890396118164\n",
            "Training Iteration 4661, Loss: 2.1839632987976074\n",
            "Training Iteration 4662, Loss: 5.773440361022949\n",
            "Training Iteration 4663, Loss: 6.229395866394043\n",
            "Training Iteration 4664, Loss: 5.934816837310791\n",
            "Training Iteration 4665, Loss: 3.9857044219970703\n",
            "Training Iteration 4666, Loss: 6.649525165557861\n",
            "Training Iteration 4667, Loss: 5.7445197105407715\n",
            "Training Iteration 4668, Loss: 6.182123184204102\n",
            "Training Iteration 4669, Loss: 4.7841572761535645\n",
            "Training Iteration 4670, Loss: 6.554924964904785\n",
            "Training Iteration 4671, Loss: 6.189282417297363\n",
            "Training Iteration 4672, Loss: 4.150195121765137\n",
            "Training Iteration 4673, Loss: 1.7858903408050537\n",
            "Training Iteration 4674, Loss: 2.2185513973236084\n",
            "Training Iteration 4675, Loss: 5.606889247894287\n",
            "Training Iteration 4676, Loss: 6.307302951812744\n",
            "Training Iteration 4677, Loss: 5.840476989746094\n",
            "Training Iteration 4678, Loss: 5.332864284515381\n",
            "Training Iteration 4679, Loss: 3.535383939743042\n",
            "Training Iteration 4680, Loss: 6.348052978515625\n",
            "Training Iteration 4681, Loss: 2.7904393672943115\n",
            "Training Iteration 4682, Loss: 7.027392864227295\n",
            "Training Iteration 4683, Loss: 6.879544734954834\n",
            "Training Iteration 4684, Loss: 4.40911865234375\n",
            "Training Iteration 4685, Loss: 4.9961256980896\n",
            "Training Iteration 4686, Loss: 4.390048980712891\n",
            "Training Iteration 4687, Loss: 5.9393486976623535\n",
            "Training Iteration 4688, Loss: 5.0346784591674805\n",
            "Training Iteration 4689, Loss: 4.5850019454956055\n",
            "Training Iteration 4690, Loss: 6.264457702636719\n",
            "Training Iteration 4691, Loss: 4.961034297943115\n",
            "Training Iteration 4692, Loss: 3.960696220397949\n",
            "Training Iteration 4693, Loss: 4.925119400024414\n",
            "Training Iteration 4694, Loss: 3.8382925987243652\n",
            "Training Iteration 4695, Loss: 3.0744621753692627\n",
            "Training Iteration 4696, Loss: 4.01802921295166\n",
            "Training Iteration 4697, Loss: 4.293265342712402\n",
            "Training Iteration 4698, Loss: 6.430706977844238\n",
            "Training Iteration 4699, Loss: 2.486708164215088\n",
            "Training Iteration 4700, Loss: 4.162083625793457\n",
            "Training Iteration 4701, Loss: 2.7025516033172607\n",
            "Training Iteration 4702, Loss: 3.5254664421081543\n",
            "Training Iteration 4703, Loss: 6.081691741943359\n",
            "Training Iteration 4704, Loss: 5.362701892852783\n",
            "Training Iteration 4705, Loss: 5.341346740722656\n",
            "Training Iteration 4706, Loss: 6.275343894958496\n",
            "Training Iteration 4707, Loss: 1.8364514112472534\n",
            "Training Iteration 4708, Loss: 4.340775966644287\n",
            "Training Iteration 4709, Loss: 5.821385383605957\n",
            "Training Iteration 4710, Loss: 2.9859659671783447\n",
            "Training Iteration 4711, Loss: 3.574899435043335\n",
            "Training Iteration 4712, Loss: 3.3458657264709473\n",
            "Training Iteration 4713, Loss: 2.6166133880615234\n",
            "Training Iteration 4714, Loss: 2.9413697719573975\n",
            "Training Iteration 4715, Loss: 3.1339941024780273\n",
            "Training Iteration 4716, Loss: 2.8499059677124023\n",
            "Training Iteration 4717, Loss: 7.9589152336120605\n",
            "Training Iteration 4718, Loss: 3.2464897632598877\n",
            "Training Iteration 4719, Loss: 4.910203456878662\n",
            "Training Iteration 4720, Loss: 6.724496364593506\n",
            "Training Iteration 4721, Loss: 4.21259069442749\n",
            "Training Iteration 4722, Loss: 4.248228549957275\n",
            "Training Iteration 4723, Loss: 3.4683730602264404\n",
            "Training Iteration 4724, Loss: 4.955883502960205\n",
            "Training Iteration 4725, Loss: 5.094076156616211\n",
            "Training Iteration 4726, Loss: 5.172523021697998\n",
            "Training Iteration 4727, Loss: 3.293933391571045\n",
            "Training Iteration 4728, Loss: 3.763746738433838\n",
            "Training Iteration 4729, Loss: 2.561929941177368\n",
            "Training Iteration 4730, Loss: 5.498246192932129\n",
            "Training Iteration 4731, Loss: 7.994069576263428\n",
            "Training Iteration 4732, Loss: 4.0943756103515625\n",
            "Training Iteration 4733, Loss: 4.436481475830078\n",
            "Training Iteration 4734, Loss: 5.580820083618164\n",
            "Training Iteration 4735, Loss: 5.75081205368042\n",
            "Training Iteration 4736, Loss: 1.7012171745300293\n",
            "Training Iteration 4737, Loss: 4.086462497711182\n",
            "Training Iteration 4738, Loss: 5.498021602630615\n",
            "Training Iteration 4739, Loss: 3.2764976024627686\n",
            "Training Iteration 4740, Loss: 4.237612724304199\n",
            "Training Iteration 4741, Loss: 2.0307860374450684\n",
            "Training Iteration 4742, Loss: 3.5882792472839355\n",
            "Training Iteration 4743, Loss: 4.050591945648193\n",
            "Training Iteration 4744, Loss: 4.739151954650879\n",
            "Training Iteration 4745, Loss: 4.0698347091674805\n",
            "Training Iteration 4746, Loss: 3.5666913986206055\n",
            "Training Iteration 4747, Loss: 2.431131362915039\n",
            "Training Iteration 4748, Loss: 3.70322585105896\n",
            "Training Iteration 4749, Loss: 3.4786720275878906\n",
            "Training Iteration 4750, Loss: 4.081723690032959\n",
            "Training Iteration 4751, Loss: 6.274033069610596\n",
            "Training Iteration 4752, Loss: 2.0458734035491943\n",
            "Training Iteration 4753, Loss: 1.211344599723816\n",
            "Training Iteration 4754, Loss: 6.9461798667907715\n",
            "Training Iteration 4755, Loss: 3.9917399883270264\n",
            "Training Iteration 4756, Loss: 5.822481155395508\n",
            "Training Iteration 4757, Loss: 5.067225456237793\n",
            "Training Iteration 4758, Loss: 5.94014310836792\n",
            "Training Iteration 4759, Loss: 3.854933500289917\n",
            "Training Iteration 4760, Loss: 2.818969249725342\n",
            "Training Iteration 4761, Loss: 4.9725751876831055\n",
            "Training Iteration 4762, Loss: 6.61830997467041\n",
            "Training Iteration 4763, Loss: 4.143928527832031\n",
            "Training Iteration 4764, Loss: 2.562116861343384\n",
            "Training Iteration 4765, Loss: 8.540865898132324\n",
            "Training Iteration 4766, Loss: 7.550825595855713\n",
            "Training Iteration 4767, Loss: 6.65231466293335\n",
            "Training Iteration 4768, Loss: 5.548613548278809\n",
            "Training Iteration 4769, Loss: 6.293889045715332\n",
            "Training Iteration 4770, Loss: 4.791359901428223\n",
            "Training Iteration 4771, Loss: 5.273041248321533\n",
            "Training Iteration 4772, Loss: 4.161275863647461\n",
            "Training Iteration 4773, Loss: 4.587594509124756\n",
            "Training Iteration 4774, Loss: 6.895397663116455\n",
            "Training Iteration 4775, Loss: 3.3459722995758057\n",
            "Training Iteration 4776, Loss: 7.896821975708008\n",
            "Training Iteration 4777, Loss: 8.000852584838867\n",
            "Training Iteration 4778, Loss: 6.161303997039795\n",
            "Training Iteration 4779, Loss: 4.470703601837158\n",
            "Training Iteration 4780, Loss: 3.956890821456909\n",
            "Training Iteration 4781, Loss: 7.48051118850708\n",
            "Training Iteration 4782, Loss: 5.806044578552246\n",
            "Training Iteration 4783, Loss: 4.8849053382873535\n",
            "Training Iteration 4784, Loss: 9.097450256347656\n",
            "Training Iteration 4785, Loss: 4.738339424133301\n",
            "Training Iteration 4786, Loss: 1.1783297061920166\n",
            "Training Iteration 4787, Loss: 5.780386447906494\n",
            "Training Iteration 4788, Loss: 2.63033127784729\n",
            "Training Iteration 4789, Loss: 4.366922855377197\n",
            "Training Iteration 4790, Loss: 6.133272171020508\n",
            "Training Iteration 4791, Loss: 10.240954399108887\n",
            "Training Iteration 4792, Loss: 2.5937609672546387\n",
            "Training Iteration 4793, Loss: 6.079713821411133\n",
            "Training Iteration 4794, Loss: 4.979430198669434\n",
            "Training Iteration 4795, Loss: 6.50758171081543\n",
            "Training Iteration 4796, Loss: 5.332655906677246\n",
            "Training Iteration 4797, Loss: 3.7364604473114014\n",
            "Training Iteration 4798, Loss: 5.379575252532959\n",
            "Training Iteration 4799, Loss: 5.5925445556640625\n",
            "Training Iteration 4800, Loss: 7.352149486541748\n",
            "Training Iteration 4801, Loss: 5.938032150268555\n",
            "Training Iteration 4802, Loss: 5.055285453796387\n",
            "Training Iteration 4803, Loss: 7.383942127227783\n",
            "Training Iteration 4804, Loss: 8.572759628295898\n",
            "Training Iteration 4805, Loss: 4.998899459838867\n",
            "Training Iteration 4806, Loss: 9.140130996704102\n",
            "Training Iteration 4807, Loss: 12.053730010986328\n",
            "Training Iteration 4808, Loss: 3.343209743499756\n",
            "Training Iteration 4809, Loss: 10.374495506286621\n",
            "Training Iteration 4810, Loss: 4.509163856506348\n",
            "Training Iteration 4811, Loss: 7.9694437980651855\n",
            "Training Iteration 4812, Loss: 3.180691719055176\n",
            "Training Iteration 4813, Loss: 2.7329516410827637\n",
            "Training Iteration 4814, Loss: 5.273815155029297\n",
            "Training Iteration 4815, Loss: 6.401822090148926\n",
            "Training Iteration 4816, Loss: 4.68659782409668\n",
            "Training Iteration 4817, Loss: 1.9173887968063354\n",
            "Training Iteration 4818, Loss: 2.6425039768218994\n",
            "Training Iteration 4819, Loss: 6.4793572425842285\n",
            "Training Iteration 4820, Loss: 3.047119140625\n",
            "Training Iteration 4821, Loss: 3.474820613861084\n",
            "Training Iteration 4822, Loss: 3.3031086921691895\n",
            "Training Iteration 4823, Loss: 6.710865020751953\n",
            "Training Iteration 4824, Loss: 8.792275428771973\n",
            "Training Iteration 4825, Loss: 7.610948085784912\n",
            "Training Iteration 4826, Loss: 4.203533172607422\n",
            "Training Iteration 4827, Loss: 5.796072483062744\n",
            "Training Iteration 4828, Loss: 9.269811630249023\n",
            "Training Iteration 4829, Loss: 5.659077167510986\n",
            "Training Iteration 4830, Loss: 5.397215843200684\n",
            "Training Iteration 4831, Loss: 5.823494911193848\n",
            "Training Iteration 4832, Loss: 2.7068164348602295\n",
            "Training Iteration 4833, Loss: 4.739157199859619\n",
            "Training Iteration 4834, Loss: 4.481244087219238\n",
            "Training Iteration 4835, Loss: 6.982207775115967\n",
            "Training Iteration 4836, Loss: 5.223791122436523\n",
            "Training Iteration 4837, Loss: 3.2089295387268066\n",
            "Training Iteration 4838, Loss: 5.662576675415039\n",
            "Training Iteration 4839, Loss: 2.0868210792541504\n",
            "Training Iteration 4840, Loss: 7.535268306732178\n",
            "Training Iteration 4841, Loss: 7.079413414001465\n",
            "Training Iteration 4842, Loss: 6.076362609863281\n",
            "Training Iteration 4843, Loss: 6.099451065063477\n",
            "Training Iteration 4844, Loss: 5.724729537963867\n",
            "Training Iteration 4845, Loss: 5.58791971206665\n",
            "Training Iteration 4846, Loss: 4.963657379150391\n",
            "Training Iteration 4847, Loss: 5.226812362670898\n",
            "Training Iteration 4848, Loss: 8.115006446838379\n",
            "Training Iteration 4849, Loss: 7.643418788909912\n",
            "Training Iteration 4850, Loss: 3.7636799812316895\n",
            "Training Iteration 4851, Loss: 5.522082805633545\n",
            "Training Iteration 4852, Loss: 4.4674248695373535\n",
            "Training Iteration 4853, Loss: 5.350594520568848\n",
            "Training Iteration 4854, Loss: 4.324722766876221\n",
            "Training Iteration 4855, Loss: 7.314788341522217\n",
            "Training Iteration 4856, Loss: 7.677295684814453\n",
            "Training Iteration 4857, Loss: 5.099714279174805\n",
            "Training Iteration 4858, Loss: 5.328303813934326\n",
            "Training Iteration 4859, Loss: 6.310643672943115\n",
            "Training Iteration 4860, Loss: 8.208314895629883\n",
            "Training Iteration 4861, Loss: 7.339376449584961\n",
            "Training Iteration 4862, Loss: 6.991927146911621\n",
            "Training Iteration 4863, Loss: 6.884469985961914\n",
            "Training Iteration 4864, Loss: 8.619882583618164\n",
            "Training Iteration 4865, Loss: 6.567447185516357\n",
            "Training Iteration 4866, Loss: 2.094266414642334\n",
            "Training Iteration 4867, Loss: 2.7605350017547607\n",
            "Training Iteration 4868, Loss: 4.317440032958984\n",
            "Training Iteration 4869, Loss: 3.953524351119995\n",
            "Training Iteration 4870, Loss: 4.8824381828308105\n",
            "Training Iteration 4871, Loss: 4.182737350463867\n",
            "Training Iteration 4872, Loss: 2.7130115032196045\n",
            "Training Iteration 4873, Loss: 5.361074447631836\n",
            "Training Iteration 4874, Loss: 5.3016581535339355\n",
            "Training Iteration 4875, Loss: 3.046600103378296\n",
            "Training Iteration 4876, Loss: 2.6061198711395264\n",
            "Training Iteration 4877, Loss: 2.316465377807617\n",
            "Training Iteration 4878, Loss: 2.0969948768615723\n",
            "Training Iteration 4879, Loss: 2.018484592437744\n",
            "Training Iteration 4880, Loss: 3.714742660522461\n",
            "Training Iteration 4881, Loss: 3.8607277870178223\n",
            "Training Iteration 4882, Loss: 8.209084510803223\n",
            "Training Iteration 4883, Loss: 7.865356922149658\n",
            "Training Iteration 4884, Loss: 7.783466339111328\n",
            "Training Iteration 4885, Loss: 11.222268104553223\n",
            "Training Iteration 4886, Loss: 10.62575912475586\n",
            "Training Iteration 4887, Loss: 7.644235610961914\n",
            "Training Iteration 4888, Loss: 6.107266902923584\n",
            "Training Iteration 4889, Loss: 4.425876140594482\n",
            "Training Iteration 4890, Loss: 2.7866814136505127\n",
            "Training Iteration 4891, Loss: 3.623180866241455\n",
            "Training Iteration 4892, Loss: 6.461310863494873\n",
            "Training Iteration 4893, Loss: 5.4181623458862305\n",
            "Training Iteration 4894, Loss: 2.447634220123291\n",
            "Training Iteration 4895, Loss: 4.100626468658447\n",
            "Training Iteration 4896, Loss: 4.859594345092773\n",
            "Training Iteration 4897, Loss: 3.6149096488952637\n",
            "Training Iteration 4898, Loss: 5.559367656707764\n",
            "Training Iteration 4899, Loss: 2.9952080249786377\n",
            "Training Iteration 4900, Loss: 3.4051156044006348\n",
            "Training Iteration 4901, Loss: 6.370554447174072\n",
            "Training Iteration 4902, Loss: 5.6061577796936035\n",
            "Training Iteration 4903, Loss: 8.678177833557129\n",
            "Training Iteration 4904, Loss: 6.930148124694824\n",
            "Training Iteration 4905, Loss: 3.933213233947754\n",
            "Training Iteration 4906, Loss: 10.876503944396973\n",
            "Training Iteration 4907, Loss: 3.3495125770568848\n",
            "Training Iteration 4908, Loss: 3.382265567779541\n",
            "Training Iteration 4909, Loss: 2.827498435974121\n",
            "Training Iteration 4910, Loss: 2.59741473197937\n",
            "Training Iteration 4911, Loss: 3.9113874435424805\n",
            "Training Iteration 4912, Loss: 6.769009113311768\n",
            "Training Iteration 4913, Loss: 4.782290458679199\n",
            "Training Iteration 4914, Loss: 3.1218135356903076\n",
            "Training Iteration 4915, Loss: 3.5421786308288574\n",
            "Training Iteration 4916, Loss: 3.4357709884643555\n",
            "Training Iteration 4917, Loss: 4.410405158996582\n",
            "Training Iteration 4918, Loss: 4.823820114135742\n",
            "Training Iteration 4919, Loss: 8.552687644958496\n",
            "Training Iteration 4920, Loss: 6.510207653045654\n",
            "Training Iteration 4921, Loss: 4.928102970123291\n",
            "Training Iteration 4922, Loss: 4.230198860168457\n",
            "Training Iteration 4923, Loss: 4.998579978942871\n",
            "Training Iteration 4924, Loss: 4.0996527671813965\n",
            "Training Iteration 4925, Loss: 5.24444055557251\n",
            "Training Iteration 4926, Loss: 5.930911064147949\n",
            "Training Iteration 4927, Loss: 4.188100814819336\n",
            "Training Iteration 4928, Loss: 4.30146598815918\n",
            "Training Iteration 4929, Loss: 7.173152923583984\n",
            "Training Iteration 4930, Loss: 2.2254676818847656\n",
            "Training Iteration 4931, Loss: 5.143637657165527\n",
            "Training Iteration 4932, Loss: 7.027587890625\n",
            "Training Iteration 4933, Loss: 5.10378885269165\n",
            "Training Iteration 4934, Loss: 2.6569838523864746\n",
            "Training Iteration 4935, Loss: 2.356940269470215\n",
            "Training Iteration 4936, Loss: 3.4762885570526123\n",
            "Training Iteration 4937, Loss: 5.169421195983887\n",
            "Training Iteration 4938, Loss: 5.404021739959717\n",
            "Training Iteration 4939, Loss: 6.014674186706543\n",
            "Training Iteration 4940, Loss: 3.435523509979248\n",
            "Training Iteration 4941, Loss: 4.179638862609863\n",
            "Training Iteration 4942, Loss: 3.249480962753296\n",
            "Training Iteration 4943, Loss: 1.886711597442627\n",
            "Training Iteration 4944, Loss: 6.17054557800293\n",
            "Training Iteration 4945, Loss: 9.571864128112793\n",
            "Training Iteration 4946, Loss: 3.1821110248565674\n",
            "Training Iteration 4947, Loss: 3.7777180671691895\n",
            "Training Iteration 4948, Loss: 4.393802642822266\n",
            "Training Iteration 4949, Loss: 3.459643840789795\n",
            "Training Iteration 4950, Loss: 3.8550729751586914\n",
            "Training Iteration 4951, Loss: 4.204767227172852\n",
            "Training Iteration 4952, Loss: 4.40371561050415\n",
            "Training Iteration 4953, Loss: 4.395206451416016\n",
            "Training Iteration 4954, Loss: 2.734602451324463\n",
            "Training Iteration 4955, Loss: 4.71358060836792\n",
            "Training Iteration 4956, Loss: 5.754570960998535\n",
            "Training Iteration 4957, Loss: 3.1196491718292236\n",
            "Training Iteration 4958, Loss: 2.2908663749694824\n",
            "Training Iteration 4959, Loss: 2.1353626251220703\n",
            "Training Iteration 4960, Loss: 2.9281656742095947\n",
            "Training Iteration 4961, Loss: 4.111458778381348\n",
            "Training Iteration 4962, Loss: 4.063407897949219\n",
            "Training Iteration 4963, Loss: 2.2559473514556885\n",
            "Training Iteration 4964, Loss: 3.6715588569641113\n",
            "Training Iteration 4965, Loss: 2.0172672271728516\n",
            "Training Iteration 4966, Loss: 2.224055051803589\n",
            "Training Iteration 4967, Loss: 4.571357727050781\n",
            "Training Iteration 4968, Loss: 2.816831350326538\n",
            "Training Iteration 4969, Loss: 4.6494951248168945\n",
            "Training Iteration 4970, Loss: 3.9415574073791504\n",
            "Training Iteration 4971, Loss: 5.27036714553833\n",
            "Training Iteration 4972, Loss: 3.6742019653320312\n",
            "Training Iteration 4973, Loss: 4.337971210479736\n",
            "Training Iteration 4974, Loss: 8.088823318481445\n",
            "Training Iteration 4975, Loss: 3.905046224594116\n",
            "Training Iteration 4976, Loss: 4.70267391204834\n",
            "Training Iteration 4977, Loss: 4.501965522766113\n",
            "Training Iteration 4978, Loss: 5.120994567871094\n",
            "Training Iteration 4979, Loss: 6.313063144683838\n",
            "Training Iteration 4980, Loss: 5.332787036895752\n",
            "Training Iteration 4981, Loss: 5.07660436630249\n",
            "Training Iteration 4982, Loss: 10.595975875854492\n",
            "Training Iteration 4983, Loss: 7.631850719451904\n",
            "Training Iteration 4984, Loss: 2.481342077255249\n",
            "Training Iteration 4985, Loss: 5.440751552581787\n",
            "Training Iteration 4986, Loss: 3.149474620819092\n",
            "Training Iteration 4987, Loss: 4.768272876739502\n",
            "Training Iteration 4988, Loss: 3.195211172103882\n",
            "Training Iteration 4989, Loss: 4.0056986808776855\n",
            "Training Iteration 4990, Loss: 4.946751594543457\n",
            "Training Iteration 4991, Loss: 3.5900537967681885\n",
            "Training Iteration 4992, Loss: 3.721216917037964\n",
            "Training Iteration 4993, Loss: 6.331602573394775\n",
            "Training Iteration 4994, Loss: 5.663370132446289\n",
            "Training Iteration 4995, Loss: 6.572384834289551\n",
            "Training Iteration 4996, Loss: 2.0756795406341553\n",
            "Training Iteration 4997, Loss: 3.1163344383239746\n",
            "Training Iteration 4998, Loss: 4.802358627319336\n",
            "Training Iteration 4999, Loss: 4.5507330894470215\n",
            "Training Iteration 5000, Loss: 5.155614376068115\n",
            "Training Iteration 5001, Loss: 5.00861120223999\n",
            "Training Iteration 5002, Loss: 4.226569175720215\n",
            "Training Iteration 5003, Loss: 0.8824639320373535\n",
            "Training Iteration 5004, Loss: 4.3757405281066895\n",
            "Training Iteration 5005, Loss: 2.926086187362671\n",
            "Training Iteration 5006, Loss: 1.9627139568328857\n",
            "Training Iteration 5007, Loss: 6.088762283325195\n",
            "Training Iteration 5008, Loss: 4.3293890953063965\n",
            "Training Iteration 5009, Loss: 2.6859958171844482\n",
            "Training Iteration 5010, Loss: 4.688424110412598\n",
            "Training Iteration 5011, Loss: 3.4081764221191406\n",
            "Training Iteration 5012, Loss: 2.9773123264312744\n",
            "Training Iteration 5013, Loss: 1.9330641031265259\n",
            "Training Iteration 5014, Loss: 3.362091541290283\n",
            "Training Iteration 5015, Loss: 2.8089048862457275\n",
            "Training Iteration 5016, Loss: 4.4371867179870605\n",
            "Training Iteration 5017, Loss: 4.845376491546631\n",
            "Training Iteration 5018, Loss: 3.8787384033203125\n",
            "Training Iteration 5019, Loss: 4.334162712097168\n",
            "Training Iteration 5020, Loss: 3.8069698810577393\n",
            "Training Iteration 5021, Loss: 2.895235538482666\n",
            "Training Iteration 5022, Loss: 4.065122604370117\n",
            "Training Iteration 5023, Loss: 5.5388712882995605\n",
            "Training Iteration 5024, Loss: 3.9284286499023438\n",
            "Training Iteration 5025, Loss: 5.44866943359375\n",
            "Training Iteration 5026, Loss: 2.7410528659820557\n",
            "Training Iteration 5027, Loss: 4.6143927574157715\n",
            "Training Iteration 5028, Loss: 4.646614074707031\n",
            "Training Iteration 5029, Loss: 3.1043107509613037\n",
            "Training Iteration 5030, Loss: 3.1741151809692383\n",
            "Training Iteration 5031, Loss: 3.307346820831299\n",
            "Training Iteration 5032, Loss: 7.485260486602783\n",
            "Training Iteration 5033, Loss: 4.200181007385254\n",
            "Training Iteration 5034, Loss: 8.354989051818848\n",
            "Training Iteration 5035, Loss: 4.596869468688965\n",
            "Training Iteration 5036, Loss: 3.179492235183716\n",
            "Training Iteration 5037, Loss: 1.8153297901153564\n",
            "Training Iteration 5038, Loss: 3.882751703262329\n",
            "Training Iteration 5039, Loss: 4.8167877197265625\n",
            "Training Iteration 5040, Loss: 4.515564918518066\n",
            "Training Iteration 5041, Loss: 3.8378829956054688\n",
            "Training Iteration 5042, Loss: 2.3908698558807373\n",
            "Training Iteration 5043, Loss: 3.6229381561279297\n",
            "Training Iteration 5044, Loss: 6.231311321258545\n",
            "Training Iteration 5045, Loss: 4.9765400886535645\n",
            "Training Iteration 5046, Loss: 7.487640380859375\n",
            "Training Iteration 5047, Loss: 3.2010793685913086\n",
            "Training Iteration 5048, Loss: 4.236860275268555\n",
            "Training Iteration 5049, Loss: 3.7099649906158447\n",
            "Training Iteration 5050, Loss: 3.0111329555511475\n",
            "Training Iteration 5051, Loss: 3.398343086242676\n",
            "Training Iteration 5052, Loss: 5.209649085998535\n",
            "Training Iteration 5053, Loss: 2.889373540878296\n",
            "Training Iteration 5054, Loss: 6.287267684936523\n",
            "Training Iteration 5055, Loss: 3.3481149673461914\n",
            "Training Iteration 5056, Loss: 5.630831241607666\n",
            "Training Iteration 5057, Loss: 2.5926856994628906\n",
            "Training Iteration 5058, Loss: 2.5550875663757324\n",
            "Training Iteration 5059, Loss: 2.742795467376709\n",
            "Training Iteration 5060, Loss: 3.3756375312805176\n",
            "Training Iteration 5061, Loss: 4.891152381896973\n",
            "Training Iteration 5062, Loss: 4.979015350341797\n",
            "Training Iteration 5063, Loss: 2.7891299724578857\n",
            "Training Iteration 5064, Loss: 5.432388782501221\n",
            "Training Iteration 5065, Loss: 4.43430233001709\n",
            "Training Iteration 5066, Loss: 2.336691379547119\n",
            "Training Iteration 5067, Loss: 4.831974506378174\n",
            "Training Iteration 5068, Loss: 4.230947017669678\n",
            "Training Iteration 5069, Loss: 3.5525035858154297\n",
            "Training Iteration 5070, Loss: 0.6226052045822144\n",
            "Training Iteration 5071, Loss: 4.093951225280762\n",
            "Training Iteration 5072, Loss: 3.966068744659424\n",
            "Training Iteration 5073, Loss: 3.531384229660034\n",
            "Training Iteration 5074, Loss: 5.172147750854492\n",
            "Training Iteration 5075, Loss: 5.170363426208496\n",
            "Training Iteration 5076, Loss: 2.8378729820251465\n",
            "Training Iteration 5077, Loss: 2.889913558959961\n",
            "Training Iteration 5078, Loss: 2.9530205726623535\n",
            "Training Iteration 5079, Loss: 4.675662040710449\n",
            "Training Iteration 5080, Loss: 4.642934322357178\n",
            "Training Iteration 5081, Loss: 3.6331465244293213\n",
            "Training Iteration 5082, Loss: 6.336702346801758\n",
            "Training Iteration 5083, Loss: 6.086653709411621\n",
            "Training Iteration 5084, Loss: 4.432321071624756\n",
            "Training Iteration 5085, Loss: 4.0428314208984375\n",
            "Training Iteration 5086, Loss: 4.795676231384277\n",
            "Training Iteration 5087, Loss: 4.142693996429443\n",
            "Training Iteration 5088, Loss: 4.769947528839111\n",
            "Training Iteration 5089, Loss: 3.7511911392211914\n",
            "Training Iteration 5090, Loss: 2.720216989517212\n",
            "Training Iteration 5091, Loss: 3.332780361175537\n",
            "Training Iteration 5092, Loss: 4.853511333465576\n",
            "Training Iteration 5093, Loss: 7.113412380218506\n",
            "Training Iteration 5094, Loss: 1.1825870275497437\n",
            "Training Iteration 5095, Loss: 5.1005635261535645\n",
            "Training Iteration 5096, Loss: 7.959779262542725\n",
            "Training Iteration 5097, Loss: 2.437605857849121\n",
            "Training Iteration 5098, Loss: 3.9523022174835205\n",
            "Training Iteration 5099, Loss: 3.217986822128296\n",
            "Training Iteration 5100, Loss: 3.0639734268188477\n",
            "Training Iteration 5101, Loss: 3.2996222972869873\n",
            "Training Iteration 5102, Loss: 4.25285005569458\n",
            "Training Iteration 5103, Loss: 3.2972054481506348\n",
            "Training Iteration 5104, Loss: 7.389695167541504\n",
            "Training Iteration 5105, Loss: 3.6409475803375244\n",
            "Training Iteration 5106, Loss: 3.1314191818237305\n",
            "Training Iteration 5107, Loss: 3.362668991088867\n",
            "Training Iteration 5108, Loss: 3.8360953330993652\n",
            "Training Iteration 5109, Loss: 4.6709418296813965\n",
            "Training Iteration 5110, Loss: 3.6350667476654053\n",
            "Training Iteration 5111, Loss: 5.635277271270752\n",
            "Training Iteration 5112, Loss: 3.8007562160491943\n",
            "Training Iteration 5113, Loss: 4.558392524719238\n",
            "Training Iteration 5114, Loss: 6.135438442230225\n",
            "Training Iteration 5115, Loss: 2.3745834827423096\n",
            "Training Iteration 5116, Loss: 3.9874041080474854\n",
            "Training Iteration 5117, Loss: 7.16912317276001\n",
            "Training Iteration 5118, Loss: 6.322242736816406\n",
            "Training Iteration 5119, Loss: 5.404298305511475\n",
            "Training Iteration 5120, Loss: 4.444993019104004\n",
            "Training Iteration 5121, Loss: 11.67748737335205\n",
            "Training Iteration 5122, Loss: 10.841595649719238\n",
            "Training Iteration 5123, Loss: 4.598034858703613\n",
            "Training Iteration 5124, Loss: 6.22898006439209\n",
            "Training Iteration 5125, Loss: 2.7949366569519043\n",
            "Training Iteration 5126, Loss: 3.400190591812134\n",
            "Training Iteration 5127, Loss: 2.580291986465454\n",
            "Training Iteration 5128, Loss: 7.431889057159424\n",
            "Training Iteration 5129, Loss: 7.227649688720703\n",
            "Training Iteration 5130, Loss: 3.077580451965332\n",
            "Training Iteration 5131, Loss: 4.235766410827637\n",
            "Training Iteration 5132, Loss: 6.78780460357666\n",
            "Training Iteration 5133, Loss: 5.853059768676758\n",
            "Training Iteration 5134, Loss: 4.807595252990723\n",
            "Training Iteration 5135, Loss: 4.68522310256958\n",
            "Training Iteration 5136, Loss: 3.23063325881958\n",
            "Training Iteration 5137, Loss: 6.195273399353027\n",
            "Training Iteration 5138, Loss: 3.9897751808166504\n",
            "Training Iteration 5139, Loss: 2.7972729206085205\n",
            "Training Iteration 5140, Loss: 4.508545875549316\n",
            "Training Iteration 5141, Loss: 4.672001361846924\n",
            "Training Iteration 5142, Loss: 4.181406021118164\n",
            "Training Iteration 5143, Loss: 2.327216148376465\n",
            "Training Iteration 5144, Loss: 4.201694011688232\n",
            "Training Iteration 5145, Loss: 6.448402404785156\n",
            "Training Iteration 5146, Loss: 8.45785903930664\n",
            "Training Iteration 5147, Loss: 5.1507158279418945\n",
            "Training Iteration 5148, Loss: 3.248373031616211\n",
            "Training Iteration 5149, Loss: 1.87759268283844\n",
            "Training Iteration 5150, Loss: 2.7784204483032227\n",
            "Training Iteration 5151, Loss: 4.73701810836792\n",
            "Training Iteration 5152, Loss: 2.4775476455688477\n",
            "Training Iteration 5153, Loss: 4.225193023681641\n",
            "Training Iteration 5154, Loss: 6.851587295532227\n",
            "Training Iteration 5155, Loss: 4.541094779968262\n",
            "Training Iteration 5156, Loss: 5.745218276977539\n",
            "Training Iteration 5157, Loss: 4.045515537261963\n",
            "Training Iteration 5158, Loss: 3.8949034214019775\n",
            "Training Iteration 5159, Loss: 2.671539306640625\n",
            "Training Iteration 5160, Loss: 4.691631317138672\n",
            "Training Iteration 5161, Loss: 5.008636474609375\n",
            "Training Iteration 5162, Loss: 4.985085964202881\n",
            "Training Iteration 5163, Loss: 3.279676675796509\n",
            "Training Iteration 5164, Loss: 2.038902759552002\n",
            "Training Iteration 5165, Loss: 3.7743799686431885\n",
            "Training Iteration 5166, Loss: 4.829164981842041\n",
            "Training Iteration 5167, Loss: 3.4205803871154785\n",
            "Training Iteration 5168, Loss: 3.348238229751587\n",
            "Training Iteration 5169, Loss: 2.7399256229400635\n",
            "Training Iteration 5170, Loss: 2.7426133155822754\n",
            "Training Iteration 5171, Loss: 4.906547546386719\n",
            "Training Iteration 5172, Loss: 5.182259559631348\n",
            "Training Iteration 5173, Loss: 5.029179096221924\n",
            "Training Iteration 5174, Loss: 3.840209484100342\n",
            "Training Iteration 5175, Loss: 8.122024536132812\n",
            "Training Iteration 5176, Loss: 2.5458922386169434\n",
            "Training Iteration 5177, Loss: 3.010374069213867\n",
            "Training Iteration 5178, Loss: 4.823440074920654\n",
            "Training Iteration 5179, Loss: 2.695622205734253\n",
            "Training Iteration 5180, Loss: 6.873174667358398\n",
            "Training Iteration 5181, Loss: 5.468926429748535\n",
            "Training Iteration 5182, Loss: 6.351596832275391\n",
            "Training Iteration 5183, Loss: 5.557673931121826\n",
            "Training Iteration 5184, Loss: 5.81300163269043\n",
            "Training Iteration 5185, Loss: 5.5619049072265625\n",
            "Training Iteration 5186, Loss: 3.041259288787842\n",
            "Training Iteration 5187, Loss: 4.674098014831543\n",
            "Training Iteration 5188, Loss: 4.933160781860352\n",
            "Training Iteration 5189, Loss: 1.9410630464553833\n",
            "Training Iteration 5190, Loss: 4.866741180419922\n",
            "Training Iteration 5191, Loss: 4.640571594238281\n",
            "Training Iteration 5192, Loss: 7.018665313720703\n",
            "Training Iteration 5193, Loss: 7.776793956756592\n",
            "Training Iteration 5194, Loss: 5.518270492553711\n",
            "Training Iteration 5195, Loss: 5.01938533782959\n",
            "Training Iteration 5196, Loss: 6.094073295593262\n",
            "Training Iteration 5197, Loss: 8.947874069213867\n",
            "Training Iteration 5198, Loss: 7.174229621887207\n",
            "Training Iteration 5199, Loss: 3.9817967414855957\n",
            "Training Iteration 5200, Loss: 7.288142204284668\n",
            "Training Iteration 5201, Loss: 2.809535503387451\n",
            "Training Iteration 5202, Loss: 4.649528980255127\n",
            "Training Iteration 5203, Loss: 4.335483074188232\n",
            "Training Iteration 5204, Loss: 2.302366018295288\n",
            "Training Iteration 5205, Loss: 3.320140838623047\n",
            "Training Iteration 5206, Loss: 1.6835315227508545\n",
            "Training Iteration 5207, Loss: 2.853285074234009\n",
            "Training Iteration 5208, Loss: 4.938573360443115\n",
            "Training Iteration 5209, Loss: 6.067073822021484\n",
            "Training Iteration 5210, Loss: 1.638828992843628\n",
            "Training Iteration 5211, Loss: 6.122265815734863\n",
            "Training Iteration 5212, Loss: 6.547387599945068\n",
            "Training Iteration 5213, Loss: 3.986375570297241\n",
            "Training Iteration 5214, Loss: 3.2935104370117188\n",
            "Training Iteration 5215, Loss: 1.9975194931030273\n",
            "Training Iteration 5216, Loss: 4.486870765686035\n",
            "Training Iteration 5217, Loss: 4.361337661743164\n",
            "Training Iteration 5218, Loss: 8.734649658203125\n",
            "Training Iteration 5219, Loss: 4.699384689331055\n",
            "Training Iteration 5220, Loss: 4.429637908935547\n",
            "Training Iteration 5221, Loss: 5.896082878112793\n",
            "Training Iteration 5222, Loss: 3.865792751312256\n",
            "Training Iteration 5223, Loss: 8.838834762573242\n",
            "Training Iteration 5224, Loss: 7.092568874359131\n",
            "Training Iteration 5225, Loss: 3.2180674076080322\n",
            "Training Iteration 5226, Loss: 4.034948825836182\n",
            "Training Iteration 5227, Loss: 4.351516246795654\n",
            "Training Iteration 5228, Loss: 2.871737480163574\n",
            "Training Iteration 5229, Loss: 1.4049978256225586\n",
            "Training Iteration 5230, Loss: 4.571142673492432\n",
            "Training Iteration 5231, Loss: 4.991604328155518\n",
            "Training Iteration 5232, Loss: 4.919973373413086\n",
            "Training Iteration 5233, Loss: 4.049285411834717\n",
            "Training Iteration 5234, Loss: 3.8493449687957764\n",
            "Training Iteration 5235, Loss: 4.778829574584961\n",
            "Training Iteration 5236, Loss: 3.6652188301086426\n",
            "Training Iteration 5237, Loss: 6.622043609619141\n",
            "Training Iteration 5238, Loss: 5.58183479309082\n",
            "Training Iteration 5239, Loss: 2.7788314819335938\n",
            "Training Iteration 5240, Loss: 2.620195150375366\n",
            "Training Iteration 5241, Loss: 3.888822078704834\n",
            "Training Iteration 5242, Loss: 2.0191869735717773\n",
            "Training Iteration 5243, Loss: 1.9705634117126465\n",
            "Training Iteration 5244, Loss: 2.220428705215454\n",
            "Training Iteration 5245, Loss: 6.614945411682129\n",
            "Training Iteration 5246, Loss: 3.101907968521118\n",
            "Training Iteration 5247, Loss: 3.657806396484375\n",
            "Training Iteration 5248, Loss: 4.2722859382629395\n",
            "Training Iteration 5249, Loss: 1.3765597343444824\n",
            "Training Iteration 5250, Loss: 3.950535297393799\n",
            "Training Iteration 5251, Loss: 4.413183212280273\n",
            "Training Iteration 5252, Loss: 3.2447304725646973\n",
            "Training Iteration 5253, Loss: 6.630272388458252\n",
            "Training Iteration 5254, Loss: 4.098150253295898\n",
            "Training Iteration 5255, Loss: 4.341606140136719\n",
            "Training Iteration 5256, Loss: 3.2575011253356934\n",
            "Training Iteration 5257, Loss: 2.5896754264831543\n",
            "Training Iteration 5258, Loss: 2.8502495288848877\n",
            "Training Iteration 5259, Loss: 4.597178936004639\n",
            "Training Iteration 5260, Loss: 4.247402191162109\n",
            "Training Iteration 5261, Loss: 6.05098295211792\n",
            "Training Iteration 5262, Loss: 4.290187835693359\n",
            "Training Iteration 5263, Loss: 2.3874285221099854\n",
            "Training Iteration 5264, Loss: 3.907317638397217\n",
            "Training Iteration 5265, Loss: 5.448988437652588\n",
            "Training Iteration 5266, Loss: 4.906543254852295\n",
            "Training Iteration 5267, Loss: 5.044107913970947\n",
            "Training Iteration 5268, Loss: 5.134707450866699\n",
            "Training Iteration 5269, Loss: 5.037325859069824\n",
            "Training Iteration 5270, Loss: 4.560308456420898\n",
            "Training Iteration 5271, Loss: 5.432941913604736\n",
            "Training Iteration 5272, Loss: 2.9270668029785156\n",
            "Training Iteration 5273, Loss: 1.9919946193695068\n",
            "Training Iteration 5274, Loss: 3.811858654022217\n",
            "Training Iteration 5275, Loss: 4.510559558868408\n",
            "Training Iteration 5276, Loss: 3.9352569580078125\n",
            "Training Iteration 5277, Loss: 4.028649806976318\n",
            "Training Iteration 5278, Loss: 3.6345081329345703\n",
            "Training Iteration 5279, Loss: 3.9950308799743652\n",
            "Training Iteration 5280, Loss: 4.8256049156188965\n",
            "Training Iteration 5281, Loss: 7.416966915130615\n",
            "Training Iteration 5282, Loss: 8.062196731567383\n",
            "Training Iteration 5283, Loss: 3.0981149673461914\n",
            "Training Iteration 5284, Loss: 5.198322772979736\n",
            "Training Iteration 5285, Loss: 6.165795803070068\n",
            "Training Iteration 5286, Loss: 3.8382017612457275\n",
            "Training Iteration 5287, Loss: 5.461050987243652\n",
            "Training Iteration 5288, Loss: 7.145657539367676\n",
            "Training Iteration 5289, Loss: 6.754757881164551\n",
            "Training Iteration 5290, Loss: 3.3257062435150146\n",
            "Training Iteration 5291, Loss: 4.404178142547607\n",
            "Training Iteration 5292, Loss: 1.2286878824234009\n",
            "Training Iteration 5293, Loss: 3.259047508239746\n",
            "Training Iteration 5294, Loss: 6.238803386688232\n",
            "Training Iteration 5295, Loss: 3.6078286170959473\n",
            "Training Iteration 5296, Loss: 3.2308766841888428\n",
            "Training Iteration 5297, Loss: 5.311734199523926\n",
            "Training Iteration 5298, Loss: 1.6435085535049438\n",
            "Training Iteration 5299, Loss: 6.472290515899658\n",
            "Training Iteration 5300, Loss: 1.8059132099151611\n",
            "Training Iteration 5301, Loss: 3.721896171569824\n",
            "Training Iteration 5302, Loss: 4.318610191345215\n",
            "Training Iteration 5303, Loss: 5.592041969299316\n",
            "Training Iteration 5304, Loss: 5.5902228355407715\n",
            "Training Iteration 5305, Loss: 2.578646421432495\n",
            "Training Iteration 5306, Loss: 5.8301520347595215\n",
            "Training Iteration 5307, Loss: 3.501101016998291\n",
            "Training Iteration 5308, Loss: 4.234933376312256\n",
            "Training Iteration 5309, Loss: 3.061147689819336\n",
            "Training Iteration 5310, Loss: 3.616297721862793\n",
            "Training Iteration 5311, Loss: 2.2982993125915527\n",
            "Training Iteration 5312, Loss: 5.84113883972168\n",
            "Training Iteration 5313, Loss: 3.0252299308776855\n",
            "Training Iteration 5314, Loss: 3.283867835998535\n",
            "Training Iteration 5315, Loss: 4.108030796051025\n",
            "Training Iteration 5316, Loss: 3.316988706588745\n",
            "Training Iteration 5317, Loss: 2.72428560256958\n",
            "Training Iteration 5318, Loss: 4.25712251663208\n",
            "Training Iteration 5319, Loss: 2.9948790073394775\n",
            "Training Iteration 5320, Loss: 3.3378078937530518\n",
            "Training Iteration 5321, Loss: 2.9248406887054443\n",
            "Training Iteration 5322, Loss: 5.27759313583374\n",
            "Training Iteration 5323, Loss: 6.461615562438965\n",
            "Training Iteration 5324, Loss: 7.056580066680908\n",
            "Training Iteration 5325, Loss: 4.065688610076904\n",
            "Training Iteration 5326, Loss: 3.5043182373046875\n",
            "Training Iteration 5327, Loss: 9.88409423828125\n",
            "Training Iteration 5328, Loss: 4.293634414672852\n",
            "Training Iteration 5329, Loss: 2.401228666305542\n",
            "Training Iteration 5330, Loss: 3.334407091140747\n",
            "Training Iteration 5331, Loss: 2.9292898178100586\n",
            "Training Iteration 5332, Loss: 3.315404176712036\n",
            "Training Iteration 5333, Loss: 8.914528846740723\n",
            "Training Iteration 5334, Loss: 3.386728525161743\n",
            "Training Iteration 5335, Loss: 3.1115734577178955\n",
            "Training Iteration 5336, Loss: 3.7023463249206543\n",
            "Training Iteration 5337, Loss: 5.272488594055176\n",
            "Training Iteration 5338, Loss: 3.938701629638672\n",
            "Training Iteration 5339, Loss: 3.398242712020874\n",
            "Training Iteration 5340, Loss: 7.165251731872559\n",
            "Training Iteration 5341, Loss: 5.911086559295654\n",
            "Training Iteration 5342, Loss: 4.487246990203857\n",
            "Training Iteration 5343, Loss: 3.565208911895752\n",
            "Training Iteration 5344, Loss: 5.586061954498291\n",
            "Training Iteration 5345, Loss: 6.484974384307861\n",
            "Training Iteration 5346, Loss: 5.817049026489258\n",
            "Training Iteration 5347, Loss: 6.2429327964782715\n",
            "Training Iteration 5348, Loss: 5.2484564781188965\n",
            "Training Iteration 5349, Loss: 3.9301252365112305\n",
            "Training Iteration 5350, Loss: 3.9772040843963623\n",
            "Training Iteration 5351, Loss: 5.530986785888672\n",
            "Training Iteration 5352, Loss: 4.756758689880371\n",
            "Training Iteration 5353, Loss: 5.31572961807251\n",
            "Training Iteration 5354, Loss: 5.2272210121154785\n",
            "Training Iteration 5355, Loss: 5.561792850494385\n",
            "Training Iteration 5356, Loss: 9.103843688964844\n",
            "Training Iteration 5357, Loss: 3.7355473041534424\n",
            "Training Iteration 5358, Loss: 6.908365726470947\n",
            "Training Iteration 5359, Loss: 4.9433207511901855\n",
            "Training Iteration 5360, Loss: 5.974424362182617\n",
            "Training Iteration 5361, Loss: 4.157190322875977\n",
            "Training Iteration 5362, Loss: 6.275575160980225\n",
            "Training Iteration 5363, Loss: 4.934298038482666\n",
            "Training Iteration 5364, Loss: 6.56027364730835\n",
            "Training Iteration 5365, Loss: 3.3086233139038086\n",
            "Training Iteration 5366, Loss: 5.979419708251953\n",
            "Training Iteration 5367, Loss: 5.037032127380371\n",
            "Training Iteration 5368, Loss: 3.0784244537353516\n",
            "Training Iteration 5369, Loss: 3.741413116455078\n",
            "Training Iteration 5370, Loss: 3.678056478500366\n",
            "Training Iteration 5371, Loss: 7.592645645141602\n",
            "Training Iteration 5372, Loss: 2.9290716648101807\n",
            "Training Iteration 5373, Loss: 4.831790447235107\n",
            "Training Iteration 5374, Loss: 3.6507160663604736\n",
            "Training Iteration 5375, Loss: 6.79061222076416\n",
            "Training Iteration 5376, Loss: 9.636228561401367\n",
            "Training Iteration 5377, Loss: 6.586620330810547\n",
            "Training Iteration 5378, Loss: 3.865994930267334\n",
            "Training Iteration 5379, Loss: 3.9199047088623047\n",
            "Training Iteration 5380, Loss: 5.723233222961426\n",
            "Training Iteration 5381, Loss: 4.309564590454102\n",
            "Training Iteration 5382, Loss: 4.485891342163086\n",
            "Training Iteration 5383, Loss: 2.2989091873168945\n",
            "Training Iteration 5384, Loss: 6.184412479400635\n",
            "Training Iteration 5385, Loss: 3.949610948562622\n",
            "Training Iteration 5386, Loss: 3.5524277687072754\n",
            "Training Iteration 5387, Loss: 3.0833489894866943\n",
            "Training Iteration 5388, Loss: 1.4667097330093384\n",
            "Training Iteration 5389, Loss: 5.150045394897461\n",
            "Training Iteration 5390, Loss: 4.800602912902832\n",
            "Training Iteration 5391, Loss: 5.543879508972168\n",
            "Training Iteration 5392, Loss: 3.406080722808838\n",
            "Training Iteration 5393, Loss: 2.808481454849243\n",
            "Training Iteration 5394, Loss: 2.840092182159424\n",
            "Training Iteration 5395, Loss: 3.080963134765625\n",
            "Training Iteration 5396, Loss: 1.7916067838668823\n",
            "Training Iteration 5397, Loss: 3.4537899494171143\n",
            "Training Iteration 5398, Loss: 3.7479963302612305\n",
            "Training Iteration 5399, Loss: 4.501892566680908\n",
            "Training Iteration 5400, Loss: 5.927243709564209\n",
            "Training Iteration 5401, Loss: 6.00024938583374\n",
            "Training Iteration 5402, Loss: 2.5447890758514404\n",
            "Training Iteration 5403, Loss: 3.374295234680176\n",
            "Training Iteration 5404, Loss: 5.275928497314453\n",
            "Training Iteration 5405, Loss: 1.9558860063552856\n",
            "Training Iteration 5406, Loss: 5.317622661590576\n",
            "Training Iteration 5407, Loss: 3.5657758712768555\n",
            "Training Iteration 5408, Loss: 3.0799052715301514\n",
            "Training Iteration 5409, Loss: 3.1011908054351807\n",
            "Training Iteration 5410, Loss: 4.041345596313477\n",
            "Training Iteration 5411, Loss: 5.048882961273193\n",
            "Training Iteration 5412, Loss: 4.547451972961426\n",
            "Training Iteration 5413, Loss: 4.299717903137207\n",
            "Training Iteration 5414, Loss: 4.365983009338379\n",
            "Training Iteration 5415, Loss: 3.6722660064697266\n",
            "Training Iteration 5416, Loss: 9.927217483520508\n",
            "Training Iteration 5417, Loss: 9.700887680053711\n",
            "Training Iteration 5418, Loss: 6.803122520446777\n",
            "Training Iteration 5419, Loss: 5.928338527679443\n",
            "Training Iteration 5420, Loss: 4.9742913246154785\n",
            "Training Iteration 5421, Loss: 2.688385248184204\n",
            "Training Iteration 5422, Loss: 4.009260654449463\n",
            "Training Iteration 5423, Loss: 5.191180229187012\n",
            "Training Iteration 5424, Loss: 5.424067497253418\n",
            "Training Iteration 5425, Loss: 3.797729730606079\n",
            "Training Iteration 5426, Loss: 3.990574836730957\n",
            "Training Iteration 5427, Loss: 2.9069156646728516\n",
            "Training Iteration 5428, Loss: 5.70792818069458\n",
            "Training Iteration 5429, Loss: 3.017503023147583\n",
            "Training Iteration 5430, Loss: 7.240628719329834\n",
            "Training Iteration 5431, Loss: 3.255444288253784\n",
            "Training Iteration 5432, Loss: 4.856133460998535\n",
            "Training Iteration 5433, Loss: 3.5130720138549805\n",
            "Training Iteration 5434, Loss: 5.797979354858398\n",
            "Training Iteration 5435, Loss: 4.025719165802002\n",
            "Training Iteration 5436, Loss: 6.356810569763184\n",
            "Training Iteration 5437, Loss: 3.5388965606689453\n",
            "Training Iteration 5438, Loss: 7.048047065734863\n",
            "Training Iteration 5439, Loss: 6.764130592346191\n",
            "Training Iteration 5440, Loss: 3.814959764480591\n",
            "Training Iteration 5441, Loss: 8.66848373413086\n",
            "Training Iteration 5442, Loss: 7.479947566986084\n",
            "Training Iteration 5443, Loss: 4.820455551147461\n",
            "Training Iteration 5444, Loss: 3.285215377807617\n",
            "Training Iteration 5445, Loss: 4.176826000213623\n",
            "Training Iteration 5446, Loss: 6.028508186340332\n",
            "Training Iteration 5447, Loss: 4.736306667327881\n",
            "Training Iteration 5448, Loss: 6.117147922515869\n",
            "Training Iteration 5449, Loss: 4.577898979187012\n",
            "Training Iteration 5450, Loss: 4.626889228820801\n",
            "Training Iteration 5451, Loss: 5.398736000061035\n",
            "Training Iteration 5452, Loss: 3.2398297786712646\n",
            "Training Iteration 5453, Loss: 5.223458290100098\n",
            "Training Iteration 5454, Loss: 3.485715627670288\n",
            "Training Iteration 5455, Loss: 2.7699551582336426\n",
            "Training Iteration 5456, Loss: 4.096739292144775\n",
            "Training Iteration 5457, Loss: 3.4757072925567627\n",
            "Training Iteration 5458, Loss: 2.7206180095672607\n",
            "Training Iteration 5459, Loss: 4.383605003356934\n",
            "Training Iteration 5460, Loss: 4.601040840148926\n",
            "Training Iteration 5461, Loss: 2.5056934356689453\n",
            "Training Iteration 5462, Loss: 4.355459213256836\n",
            "Training Iteration 5463, Loss: 4.078276634216309\n",
            "Training Iteration 5464, Loss: 3.2712185382843018\n",
            "Training Iteration 5465, Loss: 5.184806823730469\n",
            "Training Iteration 5466, Loss: 4.520186901092529\n",
            "Training Iteration 5467, Loss: 7.811211585998535\n",
            "Training Iteration 5468, Loss: 6.096240043640137\n",
            "Training Iteration 5469, Loss: 2.0587103366851807\n",
            "Training Iteration 5470, Loss: 4.446667671203613\n",
            "Training Iteration 5471, Loss: 4.126757621765137\n",
            "Training Iteration 5472, Loss: 3.0903890132904053\n",
            "Training Iteration 5473, Loss: 3.5963134765625\n",
            "Training Iteration 5474, Loss: 3.834266185760498\n",
            "Training Iteration 5475, Loss: 4.9606781005859375\n",
            "Training Iteration 5476, Loss: 2.774277687072754\n",
            "Training Iteration 5477, Loss: 5.016937255859375\n",
            "Training Iteration 5478, Loss: 1.1835700273513794\n",
            "Training Iteration 5479, Loss: 2.651531457901001\n",
            "Training Iteration 5480, Loss: 4.030790328979492\n",
            "Training Iteration 5481, Loss: 4.634243488311768\n",
            "Training Iteration 5482, Loss: 3.7307863235473633\n",
            "Training Iteration 5483, Loss: 4.9839324951171875\n",
            "Training Iteration 5484, Loss: 4.809327602386475\n",
            "Training Iteration 5485, Loss: 6.729926586151123\n",
            "Training Iteration 5486, Loss: 7.095715045928955\n",
            "Training Iteration 5487, Loss: 8.809337615966797\n",
            "Training Iteration 5488, Loss: 5.940232276916504\n",
            "Training Iteration 5489, Loss: 5.702092170715332\n",
            "Training Iteration 5490, Loss: 5.749640464782715\n",
            "Training Iteration 5491, Loss: 4.31149435043335\n",
            "Training Iteration 5492, Loss: 3.6715505123138428\n",
            "Training Iteration 5493, Loss: 4.901778221130371\n",
            "Training Iteration 5494, Loss: 2.9888601303100586\n",
            "Training Iteration 5495, Loss: 6.779574871063232\n",
            "Training Iteration 5496, Loss: 5.711604595184326\n",
            "Training Iteration 5497, Loss: 3.0836567878723145\n",
            "Training Iteration 5498, Loss: 3.360799551010132\n",
            "Training Iteration 5499, Loss: 1.7544418573379517\n",
            "Training Iteration 5500, Loss: 4.928320407867432\n",
            "Training Iteration 5501, Loss: 7.101738929748535\n",
            "Training Iteration 5502, Loss: 8.382564544677734\n",
            "Training Iteration 5503, Loss: 10.731071472167969\n",
            "Training Iteration 5504, Loss: 10.072113990783691\n",
            "Training Iteration 5505, Loss: 5.343728542327881\n",
            "Training Iteration 5506, Loss: 4.065936088562012\n",
            "Training Iteration 5507, Loss: 3.9664382934570312\n",
            "Training Iteration 5508, Loss: 4.130123138427734\n",
            "Training Iteration 5509, Loss: 5.426035404205322\n",
            "Training Iteration 5510, Loss: 9.557273864746094\n",
            "Training Iteration 5511, Loss: 3.499878406524658\n",
            "Training Iteration 5512, Loss: 3.6022002696990967\n",
            "Training Iteration 5513, Loss: 2.920743942260742\n",
            "Training Iteration 5514, Loss: 3.1673200130462646\n",
            "Training Iteration 5515, Loss: 5.877236843109131\n",
            "Training Iteration 5516, Loss: 3.4435620307922363\n",
            "Training Iteration 5517, Loss: 2.2611095905303955\n",
            "Training Iteration 5518, Loss: 5.01464319229126\n",
            "Training Iteration 5519, Loss: 4.671688079833984\n",
            "Training Iteration 5520, Loss: 6.170439720153809\n",
            "Training Iteration 5521, Loss: 4.718804359436035\n",
            "Training Iteration 5522, Loss: 4.145217418670654\n",
            "Training Iteration 5523, Loss: 1.324513554573059\n",
            "Training Iteration 5524, Loss: 3.6093032360076904\n",
            "Training Iteration 5525, Loss: 4.255640506744385\n",
            "Training Iteration 5526, Loss: 5.489834308624268\n",
            "Training Iteration 5527, Loss: 5.032095909118652\n",
            "Training Iteration 5528, Loss: 3.4914474487304688\n",
            "Training Iteration 5529, Loss: 4.310556411743164\n",
            "Training Iteration 5530, Loss: 11.363958358764648\n",
            "Training Iteration 5531, Loss: 3.676525354385376\n",
            "Training Iteration 5532, Loss: 5.008706569671631\n",
            "Training Iteration 5533, Loss: 6.748929977416992\n",
            "Training Iteration 5534, Loss: 3.9939117431640625\n",
            "Training Iteration 5535, Loss: 2.4259305000305176\n",
            "Training Iteration 5536, Loss: 3.6967391967773438\n",
            "Training Iteration 5537, Loss: 3.47627329826355\n",
            "Training Iteration 5538, Loss: 6.0766143798828125\n",
            "Training Iteration 5539, Loss: 2.3947594165802\n",
            "Training Iteration 5540, Loss: 4.520669460296631\n",
            "Training Iteration 5541, Loss: 4.013136386871338\n",
            "Training Iteration 5542, Loss: 3.844393253326416\n",
            "Training Iteration 5543, Loss: 7.317439556121826\n",
            "Training Iteration 5544, Loss: 8.719364166259766\n",
            "Training Iteration 5545, Loss: 5.431161403656006\n",
            "Training Iteration 5546, Loss: 3.5424511432647705\n",
            "Training Iteration 5547, Loss: 2.4362740516662598\n",
            "Training Iteration 5548, Loss: 5.17031717300415\n",
            "Training Iteration 5549, Loss: 3.44865083694458\n",
            "Training Iteration 5550, Loss: 1.3237868547439575\n",
            "Training Iteration 5551, Loss: 5.182104110717773\n",
            "Training Iteration 5552, Loss: 6.440619945526123\n",
            "Training Iteration 5553, Loss: 5.773202419281006\n",
            "Training Iteration 5554, Loss: 6.755852699279785\n",
            "Training Iteration 5555, Loss: 6.044449329376221\n",
            "Training Iteration 5556, Loss: 4.611664772033691\n",
            "Training Iteration 5557, Loss: 6.3330559730529785\n",
            "Training Iteration 5558, Loss: 4.538795471191406\n",
            "Training Iteration 5559, Loss: 6.044915676116943\n",
            "Training Iteration 5560, Loss: 4.011616230010986\n",
            "Training Iteration 5561, Loss: 1.5164165496826172\n",
            "Training Iteration 5562, Loss: 6.473420143127441\n",
            "Training Iteration 5563, Loss: 3.9845449924468994\n",
            "Training Iteration 5564, Loss: 6.33746337890625\n",
            "Training Iteration 5565, Loss: 4.999121189117432\n",
            "Training Iteration 5566, Loss: 5.672189712524414\n",
            "Training Iteration 5567, Loss: 3.451159954071045\n",
            "Training Iteration 5568, Loss: 7.051047325134277\n",
            "Training Iteration 5569, Loss: 2.6931731700897217\n",
            "Training Iteration 5570, Loss: 5.0505523681640625\n",
            "Training Iteration 5571, Loss: 2.8669328689575195\n",
            "Training Iteration 5572, Loss: 4.703934192657471\n",
            "Training Iteration 5573, Loss: 8.981904029846191\n",
            "Training Iteration 5574, Loss: 4.439272403717041\n",
            "Training Iteration 5575, Loss: 3.8028810024261475\n",
            "Training Iteration 5576, Loss: 6.29838228225708\n",
            "Training Iteration 5577, Loss: 5.204617023468018\n",
            "Training Iteration 5578, Loss: 3.755615472793579\n",
            "Training Iteration 5579, Loss: 3.984098196029663\n",
            "Training Iteration 5580, Loss: 3.8426473140716553\n",
            "Training Iteration 5581, Loss: 4.807554721832275\n",
            "Training Iteration 5582, Loss: 2.569516658782959\n",
            "Training Iteration 5583, Loss: 5.601102352142334\n",
            "Training Iteration 5584, Loss: 4.430970191955566\n",
            "Training Iteration 5585, Loss: 4.855526447296143\n",
            "Training Iteration 5586, Loss: 2.6555490493774414\n",
            "Training Iteration 5587, Loss: 6.0718207359313965\n",
            "Training Iteration 5588, Loss: 3.665644407272339\n",
            "Training Iteration 5589, Loss: 3.5196709632873535\n",
            "Training Iteration 5590, Loss: 3.577317476272583\n",
            "Training Iteration 5591, Loss: 4.7642107009887695\n",
            "Training Iteration 5592, Loss: 6.4613037109375\n",
            "Training Iteration 5593, Loss: 5.252338886260986\n",
            "Training Iteration 5594, Loss: 5.723611354827881\n",
            "Training Iteration 5595, Loss: 5.435515403747559\n",
            "Training Iteration 5596, Loss: 4.6840739250183105\n",
            "Training Iteration 5597, Loss: 3.639345169067383\n",
            "Training Iteration 5598, Loss: 5.554960250854492\n",
            "Training Iteration 5599, Loss: 8.563652992248535\n",
            "Training Iteration 5600, Loss: 5.95515251159668\n",
            "Training Iteration 5601, Loss: 6.411240577697754\n",
            "Training Iteration 5602, Loss: 4.405463218688965\n",
            "Training Iteration 5603, Loss: 5.484755516052246\n",
            "Training Iteration 5604, Loss: 4.168642520904541\n",
            "Training Iteration 5605, Loss: 2.8630990982055664\n",
            "Training Iteration 5606, Loss: 3.9425816535949707\n",
            "Training Iteration 5607, Loss: 7.001456260681152\n",
            "Training Iteration 5608, Loss: 8.166955947875977\n",
            "Training Iteration 5609, Loss: 2.529888153076172\n",
            "Training Iteration 5610, Loss: 3.902578830718994\n",
            "Training Iteration 5611, Loss: 6.929089069366455\n",
            "Training Iteration 5612, Loss: 3.243009328842163\n",
            "Training Iteration 5613, Loss: 4.098707675933838\n",
            "Training Iteration 5614, Loss: 4.2473907470703125\n",
            "Training Iteration 5615, Loss: 4.7476935386657715\n",
            "Training Iteration 5616, Loss: 8.597882270812988\n",
            "Training Iteration 5617, Loss: 2.775787591934204\n",
            "Training Iteration 5618, Loss: 4.1022210121154785\n",
            "Training Iteration 5619, Loss: 5.712126731872559\n",
            "Training Iteration 5620, Loss: 8.58229923248291\n",
            "Training Iteration 5621, Loss: 6.50385856628418\n",
            "Training Iteration 5622, Loss: 8.048261642456055\n",
            "Training Iteration 5623, Loss: 5.460094451904297\n",
            "Training Iteration 5624, Loss: 5.461977005004883\n",
            "Training Iteration 5625, Loss: 4.303857326507568\n",
            "Training Iteration 5626, Loss: 5.051922798156738\n",
            "Training Iteration 5627, Loss: 3.818746566772461\n",
            "Training Iteration 5628, Loss: 6.1848554611206055\n",
            "Training Iteration 5629, Loss: 5.315771102905273\n",
            "Training Iteration 5630, Loss: 5.8680830001831055\n",
            "Training Iteration 5631, Loss: 4.087924480438232\n",
            "Training Iteration 5632, Loss: 3.65058970451355\n",
            "Training Iteration 5633, Loss: 4.693495273590088\n",
            "Training Iteration 5634, Loss: 3.108558416366577\n",
            "Training Iteration 5635, Loss: 4.107876777648926\n",
            "Training Iteration 5636, Loss: 2.1034674644470215\n",
            "Training Iteration 5637, Loss: 8.184747695922852\n",
            "Training Iteration 5638, Loss: 6.445769309997559\n",
            "Training Iteration 5639, Loss: 3.681509256362915\n",
            "Training Iteration 5640, Loss: 3.146430730819702\n",
            "Training Iteration 5641, Loss: 4.894183158874512\n",
            "Training Iteration 5642, Loss: 5.792445182800293\n",
            "Training Iteration 5643, Loss: 3.2875168323516846\n",
            "Training Iteration 5644, Loss: 4.6958537101745605\n",
            "Training Iteration 5645, Loss: 2.3347678184509277\n",
            "Training Iteration 5646, Loss: 5.905954360961914\n",
            "Training Iteration 5647, Loss: 5.799002647399902\n",
            "Training Iteration 5648, Loss: 6.8471784591674805\n",
            "Training Iteration 5649, Loss: 4.634263038635254\n",
            "Training Iteration 5650, Loss: 2.4459118843078613\n",
            "Training Iteration 5651, Loss: 3.8396494388580322\n",
            "Training Iteration 5652, Loss: 4.683064937591553\n",
            "Training Iteration 5653, Loss: 6.770347595214844\n",
            "Training Iteration 5654, Loss: 3.8659799098968506\n",
            "Training Iteration 5655, Loss: 5.72047758102417\n",
            "Training Iteration 5656, Loss: 5.667774200439453\n",
            "Training Iteration 5657, Loss: 3.4296791553497314\n",
            "Training Iteration 5658, Loss: 4.399773120880127\n",
            "Training Iteration 5659, Loss: 2.4496688842773438\n",
            "Training Iteration 5660, Loss: 3.8339169025421143\n",
            "Training Iteration 5661, Loss: 7.393915176391602\n",
            "Training Iteration 5662, Loss: 3.4154908657073975\n",
            "Training Iteration 5663, Loss: 6.161449432373047\n",
            "Training Iteration 5664, Loss: 3.6574044227600098\n",
            "Training Iteration 5665, Loss: 4.860553741455078\n",
            "Training Iteration 5666, Loss: 4.112103462219238\n",
            "Training Iteration 5667, Loss: 3.9005048274993896\n",
            "Training Iteration 5668, Loss: 4.422211647033691\n",
            "Training Iteration 5669, Loss: 0.3407502770423889\n",
            "Training Iteration 5670, Loss: 5.318281173706055\n",
            "Training Iteration 5671, Loss: 12.042426109313965\n",
            "Training Iteration 5672, Loss: 3.916754722595215\n",
            "Training Iteration 5673, Loss: 7.099262714385986\n",
            "Training Iteration 5674, Loss: 6.187901496887207\n",
            "Training Iteration 5675, Loss: 6.855276584625244\n",
            "Training Iteration 5676, Loss: 5.32687520980835\n",
            "Training Iteration 5677, Loss: 5.906525611877441\n",
            "Training Iteration 5678, Loss: 6.266904354095459\n",
            "Training Iteration 5679, Loss: 5.27078914642334\n",
            "Training Iteration 5680, Loss: 2.5099904537200928\n",
            "Training Iteration 5681, Loss: 3.5016427040100098\n",
            "Training Iteration 5682, Loss: 5.801424503326416\n",
            "Training Iteration 5683, Loss: 5.5095930099487305\n",
            "Training Iteration 5684, Loss: 5.328234672546387\n",
            "Training Iteration 5685, Loss: 4.468932151794434\n",
            "Training Iteration 5686, Loss: 2.7288873195648193\n",
            "Training Iteration 5687, Loss: 5.250543117523193\n",
            "Training Iteration 5688, Loss: 3.5181732177734375\n",
            "Training Iteration 5689, Loss: 6.322232246398926\n",
            "Training Iteration 5690, Loss: 5.467795372009277\n",
            "Training Iteration 5691, Loss: 5.3057942390441895\n",
            "Training Iteration 5692, Loss: 4.288937568664551\n",
            "Training Iteration 5693, Loss: 5.555332183837891\n",
            "Training Iteration 5694, Loss: 3.092669725418091\n",
            "Training Iteration 5695, Loss: 7.257819175720215\n",
            "Training Iteration 5696, Loss: 6.957579135894775\n",
            "Training Iteration 5697, Loss: 2.4194281101226807\n",
            "Training Iteration 5698, Loss: 2.5081684589385986\n",
            "Training Iteration 5699, Loss: 6.196625709533691\n",
            "Training Iteration 5700, Loss: 5.751704216003418\n",
            "Training Iteration 5701, Loss: 4.593449592590332\n",
            "Training Iteration 5702, Loss: 2.3790414333343506\n",
            "Training Iteration 5703, Loss: 1.9298808574676514\n",
            "Training Iteration 5704, Loss: 5.433106899261475\n",
            "Training Iteration 5705, Loss: 6.039469242095947\n",
            "Training Iteration 5706, Loss: 2.9528090953826904\n",
            "Training Iteration 5707, Loss: 4.537251949310303\n",
            "Training Iteration 5708, Loss: 3.662135601043701\n",
            "Training Iteration 5709, Loss: 5.7255120277404785\n",
            "Training Iteration 5710, Loss: 5.263668060302734\n",
            "Training Iteration 5711, Loss: 1.7288498878479004\n",
            "Training Iteration 5712, Loss: 3.387681007385254\n",
            "Training Iteration 5713, Loss: 5.886965274810791\n",
            "Training Iteration 5714, Loss: 3.188044309616089\n",
            "Training Iteration 5715, Loss: 4.630602836608887\n",
            "Training Iteration 5716, Loss: 4.780975818634033\n",
            "Training Iteration 5717, Loss: 3.1150717735290527\n",
            "Training Iteration 5718, Loss: 3.8159236907958984\n",
            "Training Iteration 5719, Loss: 4.401951789855957\n",
            "Training Iteration 5720, Loss: 6.1062211990356445\n",
            "Training Iteration 5721, Loss: 6.718070030212402\n",
            "Training Iteration 5722, Loss: 4.692674160003662\n",
            "Training Iteration 5723, Loss: 2.7490031719207764\n",
            "Training Iteration 5724, Loss: 2.2258493900299072\n",
            "Training Iteration 5725, Loss: 4.944518089294434\n",
            "Training Iteration 5726, Loss: 3.4071097373962402\n",
            "Training Iteration 5727, Loss: 4.383992671966553\n",
            "Training Iteration 5728, Loss: 4.053448677062988\n",
            "Training Iteration 5729, Loss: 2.9182188510894775\n",
            "Training Iteration 5730, Loss: 6.541331768035889\n",
            "Training Iteration 5731, Loss: 4.342219352722168\n",
            "Training Iteration 5732, Loss: 3.4244983196258545\n",
            "Training Iteration 5733, Loss: 4.833011627197266\n",
            "Training Iteration 5734, Loss: 5.408068656921387\n",
            "Training Iteration 5735, Loss: 4.419501304626465\n",
            "Training Iteration 5736, Loss: 3.2782609462738037\n",
            "Training Iteration 5737, Loss: 6.561727046966553\n",
            "Training Iteration 5738, Loss: 2.013167142868042\n",
            "Training Iteration 5739, Loss: 4.630011081695557\n",
            "Training Iteration 5740, Loss: 7.022391319274902\n",
            "Training Iteration 5741, Loss: 5.800300598144531\n",
            "Training Iteration 5742, Loss: 5.440949440002441\n",
            "Training Iteration 5743, Loss: 4.155648708343506\n",
            "Training Iteration 5744, Loss: 5.2648162841796875\n",
            "Training Iteration 5745, Loss: 2.067523241043091\n",
            "Training Iteration 5746, Loss: 4.273209571838379\n",
            "Training Iteration 5747, Loss: 5.704168319702148\n",
            "Training Iteration 5748, Loss: 5.85639762878418\n",
            "Training Iteration 5749, Loss: 3.0976099967956543\n",
            "Training Iteration 5750, Loss: 4.534750938415527\n",
            "Training Iteration 5751, Loss: 1.909350037574768\n",
            "Training Iteration 5752, Loss: 5.730622291564941\n",
            "Training Iteration 5753, Loss: 3.4941649436950684\n",
            "Training Iteration 5754, Loss: 5.043383598327637\n",
            "Training Iteration 5755, Loss: 2.4000282287597656\n",
            "Training Iteration 5756, Loss: 6.81164026260376\n",
            "Training Iteration 5757, Loss: 0.4763981103897095\n",
            "Training Iteration 5758, Loss: 3.303874969482422\n",
            "Training Iteration 5759, Loss: 3.669395923614502\n",
            "Training Iteration 5760, Loss: 2.8790416717529297\n",
            "Training Iteration 5761, Loss: 6.0950398445129395\n",
            "Training Iteration 5762, Loss: 6.4434590339660645\n",
            "Training Iteration 5763, Loss: 3.949751615524292\n",
            "Training Iteration 5764, Loss: 5.3754167556762695\n",
            "Training Iteration 5765, Loss: 3.8040387630462646\n",
            "Training Iteration 5766, Loss: 5.059840202331543\n",
            "Training Iteration 5767, Loss: 7.486847877502441\n",
            "Training Iteration 5768, Loss: 5.461063861846924\n",
            "Training Iteration 5769, Loss: 3.4907684326171875\n",
            "Training Iteration 5770, Loss: 9.086857795715332\n",
            "Training Iteration 5771, Loss: 6.1706061363220215\n",
            "Training Iteration 5772, Loss: 4.6376190185546875\n",
            "Training Iteration 5773, Loss: 9.403875350952148\n",
            "Training Iteration 5774, Loss: 5.195573329925537\n",
            "Training Iteration 5775, Loss: 6.841670036315918\n",
            "Training Iteration 5776, Loss: 3.165055274963379\n",
            "Training Iteration 5777, Loss: 5.012340068817139\n",
            "Training Iteration 5778, Loss: 10.597437858581543\n",
            "Training Iteration 5779, Loss: 5.686952114105225\n",
            "Training Iteration 5780, Loss: 5.606435298919678\n",
            "Training Iteration 5781, Loss: 4.965306282043457\n",
            "Training Iteration 5782, Loss: 5.032536506652832\n",
            "Training Iteration 5783, Loss: 4.769654750823975\n",
            "Training Iteration 5784, Loss: 3.034281015396118\n",
            "Training Iteration 5785, Loss: 3.56070876121521\n",
            "Training Iteration 5786, Loss: 6.826447010040283\n",
            "Training Iteration 5787, Loss: 5.5215678215026855\n",
            "Training Iteration 5788, Loss: 3.361096143722534\n",
            "Training Iteration 5789, Loss: 2.7707362174987793\n",
            "Training Iteration 5790, Loss: 3.9822959899902344\n",
            "Training Iteration 5791, Loss: 3.1535327434539795\n",
            "Training Iteration 5792, Loss: 7.641557693481445\n",
            "Training Iteration 5793, Loss: 6.109104633331299\n",
            "Training Iteration 5794, Loss: 5.533412456512451\n",
            "Training Iteration 5795, Loss: 3.3556814193725586\n",
            "Training Iteration 5796, Loss: 6.916643142700195\n",
            "Training Iteration 5797, Loss: 2.1556851863861084\n",
            "Training Iteration 5798, Loss: 4.800633430480957\n",
            "Training Iteration 5799, Loss: 5.579759120941162\n",
            "Training Iteration 5800, Loss: 2.8376288414001465\n",
            "Training Iteration 5801, Loss: 1.8825998306274414\n",
            "Training Iteration 5802, Loss: 4.526188850402832\n",
            "Training Iteration 5803, Loss: 5.591466426849365\n",
            "Training Iteration 5804, Loss: 3.7911601066589355\n",
            "Training Iteration 5805, Loss: 4.828181266784668\n",
            "Training Iteration 5806, Loss: 4.348349094390869\n",
            "Training Iteration 5807, Loss: 3.64326810836792\n",
            "Training Iteration 5808, Loss: 3.6712779998779297\n",
            "Training Iteration 5809, Loss: 3.660344123840332\n",
            "Training Iteration 5810, Loss: 3.7356021404266357\n",
            "Training Iteration 5811, Loss: 3.612898349761963\n",
            "Training Iteration 5812, Loss: 4.450110912322998\n",
            "Training Iteration 5813, Loss: 3.837329864501953\n",
            "Training Iteration 5814, Loss: 2.4470531940460205\n",
            "Training Iteration 5815, Loss: 3.2365872859954834\n",
            "Training Iteration 5816, Loss: 6.060070991516113\n",
            "Training Iteration 5817, Loss: 4.382486820220947\n",
            "Training Iteration 5818, Loss: 2.9844796657562256\n",
            "Training Iteration 5819, Loss: 4.799282073974609\n",
            "Training Iteration 5820, Loss: 2.5342888832092285\n",
            "Training Iteration 5821, Loss: 3.235290050506592\n",
            "Training Iteration 5822, Loss: 5.705730438232422\n",
            "Training Iteration 5823, Loss: 2.668365955352783\n",
            "Training Iteration 5824, Loss: 4.021670818328857\n",
            "Training Iteration 5825, Loss: 4.097840785980225\n",
            "Training Iteration 5826, Loss: 3.8367364406585693\n",
            "Training Iteration 5827, Loss: 2.065603733062744\n",
            "Training Iteration 5828, Loss: 3.0070738792419434\n",
            "Training Iteration 5829, Loss: 2.9456536769866943\n",
            "Training Iteration 5830, Loss: 6.816238880157471\n",
            "Training Iteration 5831, Loss: 8.646142959594727\n",
            "Training Iteration 5832, Loss: 4.9079365730285645\n",
            "Training Iteration 5833, Loss: 3.017643690109253\n",
            "Training Iteration 5834, Loss: 4.032417297363281\n",
            "Training Iteration 5835, Loss: 3.343580484390259\n",
            "Training Iteration 5836, Loss: 4.066878318786621\n",
            "Training Iteration 5837, Loss: 2.4602150917053223\n",
            "Training Iteration 5838, Loss: 6.160691738128662\n",
            "Training Iteration 5839, Loss: 10.145610809326172\n",
            "Training Iteration 5840, Loss: 4.482516288757324\n",
            "Training Iteration 5841, Loss: 4.325596809387207\n",
            "Training Iteration 5842, Loss: 3.2679355144500732\n",
            "Training Iteration 5843, Loss: 2.097874879837036\n",
            "Training Iteration 5844, Loss: 4.098108291625977\n",
            "Training Iteration 5845, Loss: 4.629199504852295\n",
            "Training Iteration 5846, Loss: 7.663436412811279\n",
            "Training Iteration 5847, Loss: 6.414360523223877\n",
            "Training Iteration 5848, Loss: 4.885584831237793\n",
            "Training Iteration 5849, Loss: 0.9834532141685486\n",
            "Training Iteration 5850, Loss: 6.599001884460449\n",
            "Training Iteration 5851, Loss: 7.707540988922119\n",
            "Training Iteration 5852, Loss: 6.275041103363037\n",
            "Training Iteration 5853, Loss: 7.0074310302734375\n",
            "Training Iteration 5854, Loss: 3.304060935974121\n",
            "Training Iteration 5855, Loss: 1.3840850591659546\n",
            "Training Iteration 5856, Loss: 4.209561347961426\n",
            "Training Iteration 5857, Loss: 4.971526622772217\n",
            "Training Iteration 5858, Loss: 2.946871280670166\n",
            "Training Iteration 5859, Loss: 1.709670066833496\n",
            "Training Iteration 5860, Loss: 2.366117477416992\n",
            "Training Iteration 5861, Loss: 6.333846569061279\n",
            "Training Iteration 5862, Loss: 8.64958667755127\n",
            "Training Iteration 5863, Loss: 2.974203109741211\n",
            "Training Iteration 5864, Loss: 8.81210994720459\n",
            "Training Iteration 5865, Loss: 2.765864610671997\n",
            "Training Iteration 5866, Loss: 6.520622253417969\n",
            "Training Iteration 5867, Loss: 5.739234447479248\n",
            "Training Iteration 5868, Loss: 6.18858528137207\n",
            "Training Iteration 5869, Loss: 5.12293815612793\n",
            "Training Iteration 5870, Loss: 3.8105897903442383\n",
            "Training Iteration 5871, Loss: 3.8934874534606934\n",
            "Training Iteration 5872, Loss: 7.893220901489258\n",
            "Training Iteration 5873, Loss: 3.6230547428131104\n",
            "Training Iteration 5874, Loss: 2.821965217590332\n",
            "Training Iteration 5875, Loss: 7.11318826675415\n",
            "Training Iteration 5876, Loss: 3.8826117515563965\n",
            "Training Iteration 5877, Loss: 5.645787715911865\n",
            "Training Iteration 5878, Loss: 6.401932716369629\n",
            "Training Iteration 5879, Loss: 5.781317234039307\n",
            "Training Iteration 5880, Loss: 3.8082571029663086\n",
            "Training Iteration 5881, Loss: 4.7520012855529785\n",
            "Training Iteration 5882, Loss: 4.564018726348877\n",
            "Training Iteration 5883, Loss: 4.433730602264404\n",
            "Training Iteration 5884, Loss: 4.593814849853516\n",
            "Training Iteration 5885, Loss: 6.47212028503418\n",
            "Training Iteration 5886, Loss: 4.12202262878418\n",
            "Training Iteration 5887, Loss: 9.024852752685547\n",
            "Training Iteration 5888, Loss: 8.419778823852539\n",
            "Training Iteration 5889, Loss: 2.7704756259918213\n",
            "Training Iteration 5890, Loss: 4.642938613891602\n",
            "Training Iteration 5891, Loss: 4.917052268981934\n",
            "Training Iteration 5892, Loss: 2.7540640830993652\n",
            "Training Iteration 5893, Loss: 2.3040878772735596\n",
            "Training Iteration 5894, Loss: 3.008310079574585\n",
            "Training Iteration 5895, Loss: 3.609072685241699\n",
            "Training Iteration 5896, Loss: 4.178677558898926\n",
            "Training Iteration 5897, Loss: 1.5320378541946411\n",
            "Training Iteration 5898, Loss: 3.238095760345459\n",
            "Training Iteration 5899, Loss: 4.487757682800293\n",
            "Training Iteration 5900, Loss: 1.932976245880127\n",
            "Training Iteration 5901, Loss: 3.099118232727051\n",
            "Training Iteration 5902, Loss: 4.687161922454834\n",
            "Training Iteration 5903, Loss: 5.958382606506348\n",
            "Training Iteration 5904, Loss: 6.018752574920654\n",
            "Training Iteration 5905, Loss: 3.74613094329834\n",
            "Training Iteration 5906, Loss: 5.8054986000061035\n",
            "Training Iteration 5907, Loss: 5.438548564910889\n",
            "Training Iteration 5908, Loss: 6.522922039031982\n",
            "Training Iteration 5909, Loss: 6.7546586990356445\n",
            "Training Iteration 5910, Loss: 4.58739709854126\n",
            "Training Iteration 5911, Loss: 4.091207027435303\n",
            "Training Iteration 5912, Loss: 3.588564872741699\n",
            "Training Iteration 5913, Loss: 3.244288206100464\n",
            "Training Iteration 5914, Loss: 5.17637300491333\n",
            "Training Iteration 5915, Loss: 3.0595905780792236\n",
            "Training Iteration 5916, Loss: 3.5891175270080566\n",
            "Training Iteration 5917, Loss: 5.112557411193848\n",
            "Training Iteration 5918, Loss: 4.031157493591309\n",
            "Training Iteration 5919, Loss: 2.3674817085266113\n",
            "Training Iteration 5920, Loss: 4.150125980377197\n",
            "Training Iteration 5921, Loss: 3.0758538246154785\n",
            "Training Iteration 5922, Loss: 2.908902406692505\n",
            "Training Iteration 5923, Loss: 2.737457752227783\n",
            "Training Iteration 5924, Loss: 4.621092319488525\n",
            "Training Iteration 5925, Loss: 3.603034019470215\n",
            "Training Iteration 5926, Loss: 2.90606427192688\n",
            "Training Iteration 5927, Loss: 4.829023838043213\n",
            "Training Iteration 5928, Loss: 4.976004600524902\n",
            "Training Iteration 5929, Loss: 3.767892837524414\n",
            "Training Iteration 5930, Loss: 2.8435511589050293\n",
            "Training Iteration 5931, Loss: 2.1369681358337402\n",
            "Training Iteration 5932, Loss: 3.7086410522460938\n",
            "Training Iteration 5933, Loss: 2.271653652191162\n",
            "Training Iteration 5934, Loss: 3.6545236110687256\n",
            "Training Iteration 5935, Loss: 4.122534275054932\n",
            "Training Iteration 5936, Loss: 2.325849771499634\n",
            "Training Iteration 5937, Loss: 4.169092655181885\n",
            "Training Iteration 5938, Loss: 5.336157321929932\n",
            "Training Iteration 5939, Loss: 6.055784702301025\n",
            "Training Iteration 5940, Loss: 3.4618208408355713\n",
            "Training Iteration 5941, Loss: 4.459799289703369\n",
            "Training Iteration 5942, Loss: 4.4812235832214355\n",
            "Training Iteration 5943, Loss: 5.214715957641602\n",
            "Training Iteration 5944, Loss: 6.442694664001465\n",
            "Training Iteration 5945, Loss: 3.7211549282073975\n",
            "Training Iteration 5946, Loss: 6.4291181564331055\n",
            "Training Iteration 5947, Loss: 2.2733640670776367\n",
            "Training Iteration 5948, Loss: 5.37100887298584\n",
            "Training Iteration 5949, Loss: 4.523463726043701\n",
            "Training Iteration 5950, Loss: 4.214712619781494\n",
            "Training Iteration 5951, Loss: 5.295651435852051\n",
            "Training Iteration 5952, Loss: 6.099881172180176\n",
            "Training Iteration 5953, Loss: 5.415249824523926\n",
            "Training Iteration 5954, Loss: 2.926800012588501\n",
            "Training Iteration 5955, Loss: 1.9256514310836792\n",
            "Training Iteration 5956, Loss: 4.268680572509766\n",
            "Training Iteration 5957, Loss: 5.7470622062683105\n",
            "Training Iteration 5958, Loss: 9.78316593170166\n",
            "Training Iteration 5959, Loss: 2.2777063846588135\n",
            "Training Iteration 5960, Loss: 4.515920162200928\n",
            "Training Iteration 5961, Loss: 4.005318641662598\n",
            "Training Iteration 5962, Loss: 8.109886169433594\n",
            "Training Iteration 5963, Loss: 5.0243306159973145\n",
            "Training Iteration 5964, Loss: 3.8385262489318848\n",
            "Training Iteration 5965, Loss: 6.5986480712890625\n",
            "Training Iteration 5966, Loss: 6.405193328857422\n",
            "Training Iteration 5967, Loss: 4.056256294250488\n",
            "Training Iteration 5968, Loss: 1.82498300075531\n",
            "Training Iteration 5969, Loss: 4.1394782066345215\n",
            "Training Iteration 5970, Loss: 3.7450978755950928\n",
            "Training Iteration 5971, Loss: 7.687982559204102\n",
            "Training Iteration 5972, Loss: 3.49483585357666\n",
            "Training Iteration 5973, Loss: 5.872420310974121\n",
            "Training Iteration 5974, Loss: 2.985036849975586\n",
            "Training Iteration 5975, Loss: 5.009091854095459\n",
            "Training Iteration 5976, Loss: 2.871290922164917\n",
            "Training Iteration 5977, Loss: 4.798905372619629\n",
            "Training Iteration 5978, Loss: 1.8015222549438477\n",
            "Training Iteration 5979, Loss: 3.637472629547119\n",
            "Training Iteration 5980, Loss: 4.505941390991211\n",
            "Training Iteration 5981, Loss: 5.683291435241699\n",
            "Training Iteration 5982, Loss: 4.634037017822266\n",
            "Training Iteration 5983, Loss: 5.263834476470947\n",
            "Training Iteration 5984, Loss: 4.378679275512695\n",
            "Training Iteration 5985, Loss: 1.6698224544525146\n",
            "Training Iteration 5986, Loss: 5.106818199157715\n",
            "Training Iteration 5987, Loss: 7.722522735595703\n",
            "Training Iteration 5988, Loss: 5.122708320617676\n",
            "Training Iteration 5989, Loss: 6.510689735412598\n",
            "Training Iteration 5990, Loss: 7.621410369873047\n",
            "Training Iteration 5991, Loss: 5.299428939819336\n",
            "Training Iteration 5992, Loss: 6.20583438873291\n",
            "Training Iteration 5993, Loss: 5.254716873168945\n",
            "Training Iteration 5994, Loss: 3.4738821983337402\n",
            "Training Iteration 5995, Loss: 4.121697425842285\n",
            "Training Iteration 5996, Loss: 4.381913661956787\n",
            "Training Iteration 5997, Loss: 4.752661228179932\n",
            "Training Iteration 5998, Loss: 2.9998433589935303\n",
            "Training Iteration 5999, Loss: 5.053633689880371\n",
            "Training Iteration 6000, Loss: 3.112732410430908\n",
            "Training Iteration 6001, Loss: 8.501282691955566\n",
            "Training Iteration 6002, Loss: 5.612836837768555\n",
            "Training Iteration 6003, Loss: 6.447368621826172\n",
            "Training Iteration 6004, Loss: 2.7629148960113525\n",
            "Training Iteration 6005, Loss: 3.956665515899658\n",
            "Training Iteration 6006, Loss: 2.080799102783203\n",
            "Training Iteration 6007, Loss: 3.382136344909668\n",
            "Training Iteration 6008, Loss: 7.512020587921143\n",
            "Training Iteration 6009, Loss: 3.030289888381958\n",
            "Training Iteration 6010, Loss: 2.674649715423584\n",
            "Training Iteration 6011, Loss: 4.7335100173950195\n",
            "Training Iteration 6012, Loss: 4.64461088180542\n",
            "Training Iteration 6013, Loss: 4.320187091827393\n",
            "Training Iteration 6014, Loss: 4.426016330718994\n",
            "Training Iteration 6015, Loss: 5.210186958312988\n",
            "Training Iteration 6016, Loss: 1.6099841594696045\n",
            "Training Iteration 6017, Loss: 4.272000789642334\n",
            "Training Iteration 6018, Loss: 3.6684694290161133\n",
            "Training Iteration 6019, Loss: 3.498741626739502\n",
            "Training Iteration 6020, Loss: 3.7469124794006348\n",
            "Training Iteration 6021, Loss: 3.0012452602386475\n",
            "Training Iteration 6022, Loss: 2.1543657779693604\n",
            "Training Iteration 6023, Loss: 9.943175315856934\n",
            "Training Iteration 6024, Loss: 5.331709861755371\n",
            "Training Iteration 6025, Loss: 4.359622955322266\n",
            "Training Iteration 6026, Loss: 6.71108865737915\n",
            "Training Iteration 6027, Loss: 2.4845616817474365\n",
            "Training Iteration 6028, Loss: 3.37768816947937\n",
            "Training Iteration 6029, Loss: 5.1836466789245605\n",
            "Training Iteration 6030, Loss: 2.3079326152801514\n",
            "Training Iteration 6031, Loss: 4.595902442932129\n",
            "Training Iteration 6032, Loss: 5.182426929473877\n",
            "Training Iteration 6033, Loss: 4.878032684326172\n",
            "Training Iteration 6034, Loss: 5.08992338180542\n",
            "Training Iteration 6035, Loss: 3.0264291763305664\n",
            "Training Iteration 6036, Loss: 2.9505457878112793\n",
            "Training Iteration 6037, Loss: 5.771107196807861\n",
            "Training Iteration 6038, Loss: 1.4510552883148193\n",
            "Training Iteration 6039, Loss: 2.957005262374878\n",
            "Training Iteration 6040, Loss: 3.060499429702759\n",
            "Training Iteration 6041, Loss: 5.304009437561035\n",
            "Training Iteration 6042, Loss: 3.5124197006225586\n",
            "Training Iteration 6043, Loss: 2.485652208328247\n",
            "Training Iteration 6044, Loss: 6.68365478515625\n",
            "Training Iteration 6045, Loss: 6.36178731918335\n",
            "Training Iteration 6046, Loss: 5.269343852996826\n",
            "Training Iteration 6047, Loss: 4.77065372467041\n",
            "Training Iteration 6048, Loss: 3.987034320831299\n",
            "Training Iteration 6049, Loss: 8.712458610534668\n",
            "Training Iteration 6050, Loss: 2.3785669803619385\n",
            "Training Iteration 6051, Loss: 3.0267441272735596\n",
            "Training Iteration 6052, Loss: 3.3368992805480957\n",
            "Training Iteration 6053, Loss: 5.853006362915039\n",
            "Training Iteration 6054, Loss: 2.229219913482666\n",
            "Training Iteration 6055, Loss: 4.953995227813721\n",
            "Training Iteration 6056, Loss: 4.529483318328857\n",
            "Training Iteration 6057, Loss: 2.396761417388916\n",
            "Training Iteration 6058, Loss: 4.493720531463623\n",
            "Training Iteration 6059, Loss: 5.256557941436768\n",
            "Training Iteration 6060, Loss: 4.186235427856445\n",
            "Training Iteration 6061, Loss: 4.428146839141846\n",
            "Training Iteration 6062, Loss: 6.046982765197754\n",
            "Training Iteration 6063, Loss: 2.799642562866211\n",
            "Training Iteration 6064, Loss: 5.222487926483154\n",
            "Training Iteration 6065, Loss: 6.497848987579346\n",
            "Training Iteration 6066, Loss: 4.6533074378967285\n",
            "Training Iteration 6067, Loss: 3.3974127769470215\n",
            "Training Iteration 6068, Loss: 2.4297289848327637\n",
            "Training Iteration 6069, Loss: 2.8739242553710938\n",
            "Training Iteration 6070, Loss: 3.8194098472595215\n",
            "Training Iteration 6071, Loss: 4.430574893951416\n",
            "Training Iteration 6072, Loss: 4.45717716217041\n",
            "Training Iteration 6073, Loss: 5.31821346282959\n",
            "Training Iteration 6074, Loss: 3.518564462661743\n",
            "Training Iteration 6075, Loss: 4.439479827880859\n",
            "Training Iteration 6076, Loss: 4.882270336151123\n",
            "Training Iteration 6077, Loss: 3.1131434440612793\n",
            "Training Iteration 6078, Loss: 4.330588340759277\n",
            "Training Iteration 6079, Loss: 4.248273849487305\n",
            "Training Iteration 6080, Loss: 4.256444931030273\n",
            "Training Iteration 6081, Loss: 4.829708576202393\n",
            "Training Iteration 6082, Loss: 4.664196968078613\n",
            "Training Iteration 6083, Loss: 4.816974639892578\n",
            "Training Iteration 6084, Loss: 1.9366202354431152\n",
            "Training Iteration 6085, Loss: 4.356662273406982\n",
            "Training Iteration 6086, Loss: 3.8765709400177\n",
            "Training Iteration 6087, Loss: 4.738238334655762\n",
            "Training Iteration 6088, Loss: 4.501755714416504\n",
            "Training Iteration 6089, Loss: 5.987994194030762\n",
            "Training Iteration 6090, Loss: 4.181511402130127\n",
            "Training Iteration 6091, Loss: 3.1648595333099365\n",
            "Training Iteration 6092, Loss: 7.0883307456970215\n",
            "Training Iteration 6093, Loss: 6.487430572509766\n",
            "Training Iteration 6094, Loss: 2.7432265281677246\n",
            "Training Iteration 6095, Loss: 5.247726917266846\n",
            "Training Iteration 6096, Loss: 4.000380039215088\n",
            "Training Iteration 6097, Loss: 6.903493881225586\n",
            "Training Iteration 6098, Loss: 4.455949306488037\n",
            "Training Iteration 6099, Loss: 2.527987241744995\n",
            "Training Iteration 6100, Loss: 3.54386568069458\n",
            "Training Iteration 6101, Loss: 3.3586483001708984\n",
            "Training Iteration 6102, Loss: 6.221338272094727\n",
            "Training Iteration 6103, Loss: 6.591131687164307\n",
            "Training Iteration 6104, Loss: 6.674333572387695\n",
            "Training Iteration 6105, Loss: 1.8117172718048096\n",
            "Training Iteration 6106, Loss: 4.631669521331787\n",
            "Training Iteration 6107, Loss: 2.917088031768799\n",
            "Training Iteration 6108, Loss: 3.510091781616211\n",
            "Training Iteration 6109, Loss: 5.050243854522705\n",
            "Training Iteration 6110, Loss: 12.347466468811035\n",
            "Training Iteration 6111, Loss: 3.14023756980896\n",
            "Training Iteration 6112, Loss: 2.9784493446350098\n",
            "Training Iteration 6113, Loss: 5.118594646453857\n",
            "Training Iteration 6114, Loss: 8.260604858398438\n",
            "Training Iteration 6115, Loss: 3.893001079559326\n",
            "Training Iteration 6116, Loss: 3.665456533432007\n",
            "Training Iteration 6117, Loss: 4.096240520477295\n",
            "Training Iteration 6118, Loss: 4.070464134216309\n",
            "Training Iteration 6119, Loss: 2.7296810150146484\n",
            "Training Iteration 6120, Loss: 3.4587481021881104\n",
            "Training Iteration 6121, Loss: 7.984545707702637\n",
            "Training Iteration 6122, Loss: 5.083171844482422\n",
            "Training Iteration 6123, Loss: 4.415544033050537\n",
            "Training Iteration 6124, Loss: 3.4741933345794678\n",
            "Training Iteration 6125, Loss: 3.767214059829712\n",
            "Training Iteration 6126, Loss: 2.08615779876709\n",
            "Training Iteration 6127, Loss: 4.600050926208496\n",
            "Training Iteration 6128, Loss: 2.4138660430908203\n",
            "Training Iteration 6129, Loss: 3.3678364753723145\n",
            "Training Iteration 6130, Loss: 4.5034308433532715\n",
            "Training Iteration 6131, Loss: 6.4606852531433105\n",
            "Training Iteration 6132, Loss: 3.0722250938415527\n",
            "Training Iteration 6133, Loss: 6.601678371429443\n",
            "Training Iteration 6134, Loss: 2.7952115535736084\n",
            "Training Iteration 6135, Loss: 2.7319161891937256\n",
            "Training Iteration 6136, Loss: 4.736489295959473\n",
            "Training Iteration 6137, Loss: 4.020520210266113\n",
            "Training Iteration 6138, Loss: 5.463291168212891\n",
            "Training Iteration 6139, Loss: 4.275130271911621\n",
            "Training Iteration 6140, Loss: 8.252850532531738\n",
            "Training Iteration 6141, Loss: 9.030027389526367\n",
            "Training Iteration 6142, Loss: 6.366339206695557\n",
            "Training Iteration 6143, Loss: 3.2633702754974365\n",
            "Training Iteration 6144, Loss: 3.186584711074829\n",
            "Training Iteration 6145, Loss: 4.634719371795654\n",
            "Training Iteration 6146, Loss: 1.922003984451294\n",
            "Training Iteration 6147, Loss: 2.056882619857788\n",
            "Training Iteration 6148, Loss: 3.045905828475952\n",
            "Training Iteration 6149, Loss: 7.0735697746276855\n",
            "Training Iteration 6150, Loss: 5.760926723480225\n",
            "Training Iteration 6151, Loss: 6.309050559997559\n",
            "Training Iteration 6152, Loss: 6.635677337646484\n",
            "Training Iteration 6153, Loss: 1.9098868370056152\n",
            "Training Iteration 6154, Loss: 1.835251808166504\n",
            "Training Iteration 6155, Loss: 2.8036248683929443\n",
            "Training Iteration 6156, Loss: 5.12346887588501\n",
            "Training Iteration 6157, Loss: 8.501585960388184\n",
            "Training Iteration 6158, Loss: 6.232610702514648\n",
            "Training Iteration 6159, Loss: 9.265710830688477\n",
            "Training Iteration 6160, Loss: 8.015596389770508\n",
            "Training Iteration 6161, Loss: 5.439157962799072\n",
            "Training Iteration 6162, Loss: 13.295194625854492\n",
            "Training Iteration 6163, Loss: 7.993023872375488\n",
            "Training Iteration 6164, Loss: 2.686199188232422\n",
            "Training Iteration 6165, Loss: 7.646244049072266\n",
            "Training Iteration 6166, Loss: 4.31265926361084\n",
            "Training Iteration 6167, Loss: 9.63317584991455\n",
            "Training Iteration 6168, Loss: 3.5400476455688477\n",
            "Training Iteration 6169, Loss: 3.5894150733947754\n",
            "Training Iteration 6170, Loss: 3.9372315406799316\n",
            "Training Iteration 6171, Loss: 2.7181663513183594\n",
            "Training Iteration 6172, Loss: 5.550611972808838\n",
            "Training Iteration 6173, Loss: 3.9838643074035645\n",
            "Training Iteration 6174, Loss: 4.021039962768555\n",
            "Training Iteration 6175, Loss: 2.866706132888794\n",
            "Training Iteration 6176, Loss: 4.076478004455566\n",
            "Training Iteration 6177, Loss: 3.388519287109375\n",
            "Training Iteration 6178, Loss: 3.8716516494750977\n",
            "Training Iteration 6179, Loss: 5.380849361419678\n",
            "Training Iteration 6180, Loss: 2.647526979446411\n",
            "Training Iteration 6181, Loss: 4.693118572235107\n",
            "Training Iteration 6182, Loss: 4.624683856964111\n",
            "Training Iteration 6183, Loss: 3.7893455028533936\n",
            "Training Iteration 6184, Loss: 4.7201247215271\n",
            "Training Iteration 6185, Loss: 3.584906816482544\n",
            "Training Iteration 6186, Loss: 3.116518259048462\n",
            "Training Iteration 6187, Loss: 4.091289043426514\n",
            "Training Iteration 6188, Loss: 1.9393428564071655\n",
            "Training Iteration 6189, Loss: 6.369829177856445\n",
            "Training Iteration 6190, Loss: 5.553225994110107\n",
            "Training Iteration 6191, Loss: 1.7623646259307861\n",
            "Training Iteration 6192, Loss: 2.8758745193481445\n",
            "Training Iteration 6193, Loss: 3.714308738708496\n",
            "Training Iteration 6194, Loss: 7.402902126312256\n",
            "Training Iteration 6195, Loss: 3.0886077880859375\n",
            "Training Iteration 6196, Loss: 4.123871326446533\n",
            "Training Iteration 6197, Loss: 2.518892288208008\n",
            "Training Iteration 6198, Loss: 4.174743175506592\n",
            "Training Iteration 6199, Loss: 9.605835914611816\n",
            "Training Iteration 6200, Loss: 5.247103691101074\n",
            "Training Iteration 6201, Loss: 5.145522117614746\n",
            "Training Iteration 6202, Loss: 1.5467828512191772\n",
            "Training Iteration 6203, Loss: 2.048196315765381\n",
            "Training Iteration 6204, Loss: 5.009327411651611\n",
            "Training Iteration 6205, Loss: 4.934627056121826\n",
            "Training Iteration 6206, Loss: 5.253605365753174\n",
            "Training Iteration 6207, Loss: 7.126791000366211\n",
            "Training Iteration 6208, Loss: 6.023247241973877\n",
            "Training Iteration 6209, Loss: 5.396396636962891\n",
            "Training Iteration 6210, Loss: 4.2189741134643555\n",
            "Training Iteration 6211, Loss: 2.9142892360687256\n",
            "Training Iteration 6212, Loss: 5.241061687469482\n",
            "Training Iteration 6213, Loss: 8.381119728088379\n",
            "Training Iteration 6214, Loss: 6.52956485748291\n",
            "Training Iteration 6215, Loss: 2.9071409702301025\n",
            "Training Iteration 6216, Loss: 4.178261756896973\n",
            "Training Iteration 6217, Loss: 6.449143409729004\n",
            "Training Iteration 6218, Loss: 3.241297721862793\n",
            "Training Iteration 6219, Loss: 1.4721661806106567\n",
            "Training Iteration 6220, Loss: 3.247798442840576\n",
            "Training Iteration 6221, Loss: 3.6068942546844482\n",
            "Training Iteration 6222, Loss: 2.7193191051483154\n",
            "Training Iteration 6223, Loss: 2.9767541885375977\n",
            "Training Iteration 6224, Loss: 6.3841423988342285\n",
            "Training Iteration 6225, Loss: 7.265742301940918\n",
            "Training Iteration 6226, Loss: 9.851602554321289\n",
            "Training Iteration 6227, Loss: 4.503707408905029\n",
            "Training Iteration 6228, Loss: 6.9600911140441895\n",
            "Training Iteration 6229, Loss: 7.182870864868164\n",
            "Training Iteration 6230, Loss: 4.295454978942871\n",
            "Training Iteration 6231, Loss: 4.85418176651001\n",
            "Training Iteration 6232, Loss: 1.9887243509292603\n",
            "Training Iteration 6233, Loss: 8.35295581817627\n",
            "Training Iteration 6234, Loss: 7.240022659301758\n",
            "Training Iteration 6235, Loss: 3.5765905380249023\n",
            "Training Iteration 6236, Loss: 4.2445597648620605\n",
            "Training Iteration 6237, Loss: 5.462326526641846\n",
            "Training Iteration 6238, Loss: 3.3714680671691895\n",
            "Training Iteration 6239, Loss: 2.5797994136810303\n",
            "Training Iteration 6240, Loss: 5.461215019226074\n",
            "Training Iteration 6241, Loss: 5.112969875335693\n",
            "Training Iteration 6242, Loss: 3.63397479057312\n",
            "Training Iteration 6243, Loss: 3.696340560913086\n",
            "Training Iteration 6244, Loss: 3.633089303970337\n",
            "Training Iteration 6245, Loss: 4.28662633895874\n",
            "Training Iteration 6246, Loss: 2.6967804431915283\n",
            "Training Iteration 6247, Loss: 2.85734224319458\n",
            "Training Iteration 6248, Loss: 2.466782569885254\n",
            "Training Iteration 6249, Loss: 5.2247724533081055\n",
            "Training Iteration 6250, Loss: 7.58823299407959\n",
            "Training Iteration 6251, Loss: 5.395484924316406\n",
            "Training Iteration 6252, Loss: 3.229799270629883\n",
            "Training Iteration 6253, Loss: 2.6571688652038574\n",
            "Training Iteration 6254, Loss: 7.138075351715088\n",
            "Training Iteration 6255, Loss: 1.6497300863265991\n",
            "Training Iteration 6256, Loss: 7.157910346984863\n",
            "Training Iteration 6257, Loss: 2.8365681171417236\n",
            "Training Iteration 6258, Loss: 3.41190242767334\n",
            "Training Iteration 6259, Loss: 2.971116542816162\n",
            "Training Iteration 6260, Loss: 2.7265868186950684\n",
            "Training Iteration 6261, Loss: 4.66062068939209\n",
            "Training Iteration 6262, Loss: 4.585318088531494\n",
            "Training Iteration 6263, Loss: 5.157700538635254\n",
            "Training Iteration 6264, Loss: 4.057132244110107\n",
            "Training Iteration 6265, Loss: 2.512305736541748\n",
            "Training Iteration 6266, Loss: 3.8097593784332275\n",
            "Training Iteration 6267, Loss: 4.581874370574951\n",
            "Training Iteration 6268, Loss: 4.354049205780029\n",
            "Training Iteration 6269, Loss: 5.921087741851807\n",
            "Training Iteration 6270, Loss: 6.555735111236572\n",
            "Training Iteration 6271, Loss: 4.682529926300049\n",
            "Training Iteration 6272, Loss: 4.913934230804443\n",
            "Training Iteration 6273, Loss: 5.347832679748535\n",
            "Training Iteration 6274, Loss: 3.9462149143218994\n",
            "Training Iteration 6275, Loss: 4.554017543792725\n",
            "Training Iteration 6276, Loss: 4.666704177856445\n",
            "Training Iteration 6277, Loss: 4.000570774078369\n",
            "Training Iteration 6278, Loss: 5.398870944976807\n",
            "Training Iteration 6279, Loss: 3.57653546333313\n",
            "Training Iteration 6280, Loss: 4.007315158843994\n",
            "Training Iteration 6281, Loss: 5.232821464538574\n",
            "Training Iteration 6282, Loss: 3.1169772148132324\n",
            "Training Iteration 6283, Loss: 4.882148742675781\n",
            "Training Iteration 6284, Loss: 4.556082725524902\n",
            "Training Iteration 6285, Loss: 7.198431968688965\n",
            "Training Iteration 6286, Loss: 3.821359157562256\n",
            "Training Iteration 6287, Loss: 4.413949966430664\n",
            "Training Iteration 6288, Loss: 2.275557518005371\n",
            "Training Iteration 6289, Loss: 3.1570966243743896\n",
            "Training Iteration 6290, Loss: 3.104841709136963\n",
            "Training Iteration 6291, Loss: 4.895583629608154\n",
            "Training Iteration 6292, Loss: 3.198208808898926\n",
            "Training Iteration 6293, Loss: 7.0695061683654785\n",
            "Training Iteration 6294, Loss: 3.0375750064849854\n",
            "Training Iteration 6295, Loss: 3.373063087463379\n",
            "Training Iteration 6296, Loss: 3.7542526721954346\n",
            "Training Iteration 6297, Loss: 5.027479648590088\n",
            "Training Iteration 6298, Loss: 7.865533828735352\n",
            "Training Iteration 6299, Loss: 3.025570869445801\n",
            "Training Iteration 6300, Loss: 3.9750120639801025\n",
            "Training Iteration 6301, Loss: 4.78070592880249\n",
            "Training Iteration 6302, Loss: 6.891526699066162\n",
            "Training Iteration 6303, Loss: 1.317246675491333\n",
            "Training Iteration 6304, Loss: 4.157434463500977\n",
            "Training Iteration 6305, Loss: 4.826840400695801\n",
            "Training Iteration 6306, Loss: 4.179103851318359\n",
            "Training Iteration 6307, Loss: 2.6817569732666016\n",
            "Training Iteration 6308, Loss: 3.1551811695098877\n",
            "Training Iteration 6309, Loss: 4.12799072265625\n",
            "Training Iteration 6310, Loss: 6.209964275360107\n",
            "Training Iteration 6311, Loss: 7.644130229949951\n",
            "Training Iteration 6312, Loss: 6.170309066772461\n",
            "Training Iteration 6313, Loss: 6.079493999481201\n",
            "Training Iteration 6314, Loss: 4.4611382484436035\n",
            "Training Iteration 6315, Loss: 3.7432358264923096\n",
            "Training Iteration 6316, Loss: 5.336791038513184\n",
            "Training Iteration 6317, Loss: 9.087224006652832\n",
            "Training Iteration 6318, Loss: 3.3594911098480225\n",
            "Training Iteration 6319, Loss: 6.915501594543457\n",
            "Training Iteration 6320, Loss: 2.397588014602661\n",
            "Training Iteration 6321, Loss: 9.584940910339355\n",
            "Training Iteration 6322, Loss: 2.76127552986145\n",
            "Training Iteration 6323, Loss: 8.08957576751709\n",
            "Training Iteration 6324, Loss: 2.5839197635650635\n",
            "Training Iteration 6325, Loss: 4.889814376831055\n",
            "Training Iteration 6326, Loss: 7.294710159301758\n",
            "Training Iteration 6327, Loss: 7.76861572265625\n",
            "Training Iteration 6328, Loss: 3.8282856941223145\n",
            "Training Iteration 6329, Loss: 7.610058307647705\n",
            "Training Iteration 6330, Loss: 5.898411750793457\n",
            "Training Iteration 6331, Loss: 3.349916934967041\n",
            "Training Iteration 6332, Loss: 8.368142127990723\n",
            "Training Iteration 6333, Loss: 2.5616142749786377\n",
            "Training Iteration 6334, Loss: 5.358193874359131\n",
            "Training Iteration 6335, Loss: 2.522815704345703\n",
            "Training Iteration 6336, Loss: 3.781975746154785\n",
            "Training Iteration 6337, Loss: 4.663781642913818\n",
            "Training Iteration 6338, Loss: 4.965026378631592\n",
            "Training Iteration 6339, Loss: 6.057827949523926\n",
            "Training Iteration 6340, Loss: 2.5584142208099365\n",
            "Training Iteration 6341, Loss: 4.608105659484863\n",
            "Training Iteration 6342, Loss: 3.7801008224487305\n",
            "Training Iteration 6343, Loss: 2.6798882484436035\n",
            "Training Iteration 6344, Loss: 3.854463815689087\n",
            "Training Iteration 6345, Loss: 1.607806921005249\n",
            "Training Iteration 6346, Loss: 2.9047622680664062\n",
            "Training Iteration 6347, Loss: 4.024361610412598\n",
            "Training Iteration 6348, Loss: 2.6986019611358643\n",
            "Training Iteration 6349, Loss: 7.4289140701293945\n",
            "Training Iteration 6350, Loss: 4.197141170501709\n",
            "Training Iteration 6351, Loss: 3.470989227294922\n",
            "Training Iteration 6352, Loss: 1.8696327209472656\n",
            "Training Iteration 6353, Loss: 1.5602357387542725\n",
            "Training Iteration 6354, Loss: 3.4555182456970215\n",
            "Training Iteration 6355, Loss: 6.311830997467041\n",
            "Training Iteration 6356, Loss: 2.538205623626709\n",
            "Training Iteration 6357, Loss: 7.491015434265137\n",
            "Training Iteration 6358, Loss: 3.472393035888672\n",
            "Training Iteration 6359, Loss: 4.9159955978393555\n",
            "Training Iteration 6360, Loss: 2.5782971382141113\n",
            "Training Iteration 6361, Loss: 2.3352839946746826\n",
            "Training Iteration 6362, Loss: 2.7071328163146973\n",
            "Training Iteration 6363, Loss: 3.281855344772339\n",
            "Training Iteration 6364, Loss: 2.7845444679260254\n",
            "Training Iteration 6365, Loss: 3.1070098876953125\n",
            "Training Iteration 6366, Loss: 2.5616531372070312\n",
            "Training Iteration 6367, Loss: 5.371840000152588\n",
            "Training Iteration 6368, Loss: 5.3227314949035645\n",
            "Training Iteration 6369, Loss: 3.795294761657715\n",
            "Training Iteration 6370, Loss: 6.082446098327637\n",
            "Training Iteration 6371, Loss: 3.7945244312286377\n",
            "Training Iteration 6372, Loss: 4.030088901519775\n",
            "Training Iteration 6373, Loss: 7.11605167388916\n",
            "Training Iteration 6374, Loss: 3.8204245567321777\n",
            "Training Iteration 6375, Loss: 1.6596999168395996\n",
            "Training Iteration 6376, Loss: 6.3644304275512695\n",
            "Training Iteration 6377, Loss: 7.1912665367126465\n",
            "Training Iteration 6378, Loss: 5.862034797668457\n",
            "Training Iteration 6379, Loss: 5.573798179626465\n",
            "Training Iteration 6380, Loss: 4.823081970214844\n",
            "Training Iteration 6381, Loss: 4.065835952758789\n",
            "Training Iteration 6382, Loss: 6.77923583984375\n",
            "Training Iteration 6383, Loss: 6.441540241241455\n",
            "Training Iteration 6384, Loss: 4.797488212585449\n",
            "Training Iteration 6385, Loss: 3.54785418510437\n",
            "Training Iteration 6386, Loss: 2.8413949012756348\n",
            "Training Iteration 6387, Loss: 4.360387325286865\n",
            "Training Iteration 6388, Loss: 6.061124801635742\n",
            "Training Iteration 6389, Loss: 9.659758567810059\n",
            "Training Iteration 6390, Loss: 2.4724411964416504\n",
            "Training Iteration 6391, Loss: 5.432779788970947\n",
            "Training Iteration 6392, Loss: 3.8868391513824463\n",
            "Training Iteration 6393, Loss: 2.5382473468780518\n",
            "Training Iteration 6394, Loss: 5.051386833190918\n",
            "Training Iteration 6395, Loss: 6.479907989501953\n",
            "Training Iteration 6396, Loss: 3.3053910732269287\n",
            "Training Iteration 6397, Loss: 3.7226154804229736\n",
            "Training Iteration 6398, Loss: 5.77229118347168\n",
            "Training Iteration 6399, Loss: 4.136819839477539\n",
            "Training Iteration 6400, Loss: 4.603713512420654\n",
            "Training Iteration 6401, Loss: 3.657933473587036\n",
            "Training Iteration 6402, Loss: 4.893363952636719\n",
            "Training Iteration 6403, Loss: 7.336950778961182\n",
            "Training Iteration 6404, Loss: 8.497415542602539\n",
            "Training Iteration 6405, Loss: 6.965036392211914\n",
            "Training Iteration 6406, Loss: 6.332950592041016\n",
            "Training Iteration 6407, Loss: 3.8740477561950684\n",
            "Training Iteration 6408, Loss: 7.592910289764404\n",
            "Training Iteration 6409, Loss: 7.71265172958374\n",
            "Training Iteration 6410, Loss: 7.01412296295166\n",
            "Training Iteration 6411, Loss: 8.577170372009277\n",
            "Training Iteration 6412, Loss: 2.764643669128418\n",
            "Training Iteration 6413, Loss: 3.1355109214782715\n",
            "Training Iteration 6414, Loss: 8.391950607299805\n",
            "Training Iteration 6415, Loss: 2.632824182510376\n",
            "Training Iteration 6416, Loss: 7.1487579345703125\n",
            "Training Iteration 6417, Loss: 4.798229694366455\n",
            "Training Iteration 6418, Loss: 4.134460926055908\n",
            "Training Iteration 6419, Loss: 3.5576720237731934\n",
            "Training Iteration 6420, Loss: 6.566831111907959\n",
            "Training Iteration 6421, Loss: 5.471555709838867\n",
            "Training Iteration 6422, Loss: 3.85567307472229\n",
            "Training Iteration 6423, Loss: 5.554343223571777\n",
            "Training Iteration 6424, Loss: 7.532161712646484\n",
            "Training Iteration 6425, Loss: 5.69319486618042\n",
            "Training Iteration 6426, Loss: 3.758727550506592\n",
            "Training Iteration 6427, Loss: 3.3726253509521484\n",
            "Training Iteration 6428, Loss: 5.220835208892822\n",
            "Training Iteration 6429, Loss: 3.642486095428467\n",
            "Training Iteration 6430, Loss: 3.898271083831787\n",
            "Training Iteration 6431, Loss: 4.417941093444824\n",
            "Training Iteration 6432, Loss: 4.665032386779785\n",
            "Training Iteration 6433, Loss: 7.113158226013184\n",
            "Training Iteration 6434, Loss: 5.195272445678711\n",
            "Training Iteration 6435, Loss: 5.807592868804932\n",
            "Training Iteration 6436, Loss: 5.855735778808594\n",
            "Training Iteration 6437, Loss: 3.498636245727539\n",
            "Training Iteration 6438, Loss: 5.758900165557861\n",
            "Training Iteration 6439, Loss: 3.965949058532715\n",
            "Training Iteration 6440, Loss: 1.0318150520324707\n",
            "Training Iteration 6441, Loss: 3.5706920623779297\n",
            "Training Iteration 6442, Loss: 2.714118480682373\n",
            "Training Iteration 6443, Loss: 3.231311798095703\n",
            "Training Iteration 6444, Loss: 6.238080024719238\n",
            "Training Iteration 6445, Loss: 2.879822254180908\n",
            "Training Iteration 6446, Loss: 4.576054096221924\n",
            "Training Iteration 6447, Loss: 5.710442066192627\n",
            "Training Iteration 6448, Loss: 5.740562915802002\n",
            "Training Iteration 6449, Loss: 4.187093257904053\n",
            "Training Iteration 6450, Loss: 4.969631671905518\n",
            "Training Iteration 6451, Loss: 6.647739887237549\n",
            "Training Iteration 6452, Loss: 4.250807285308838\n",
            "Training Iteration 6453, Loss: 7.316737174987793\n",
            "Training Iteration 6454, Loss: 4.651461124420166\n",
            "Training Iteration 6455, Loss: 3.8795483112335205\n",
            "Training Iteration 6456, Loss: 3.66939640045166\n",
            "Training Iteration 6457, Loss: 7.686666965484619\n",
            "Training Iteration 6458, Loss: 3.9515268802642822\n",
            "Training Iteration 6459, Loss: 7.1595458984375\n",
            "Training Iteration 6460, Loss: 5.065598964691162\n",
            "Training Iteration 6461, Loss: 8.125011444091797\n",
            "Training Iteration 6462, Loss: 2.9588773250579834\n",
            "Training Iteration 6463, Loss: 3.2639994621276855\n",
            "Training Iteration 6464, Loss: 3.606736660003662\n",
            "Training Iteration 6465, Loss: 3.165341854095459\n",
            "Training Iteration 6466, Loss: 3.629861831665039\n",
            "Training Iteration 6467, Loss: 2.817234992980957\n",
            "Training Iteration 6468, Loss: 8.184550285339355\n",
            "Training Iteration 6469, Loss: 7.55549430847168\n",
            "Training Iteration 6470, Loss: 4.837963581085205\n",
            "Training Iteration 6471, Loss: 4.303115367889404\n",
            "Training Iteration 6472, Loss: 3.718395948410034\n",
            "Training Iteration 6473, Loss: 2.3466639518737793\n",
            "Training Iteration 6474, Loss: 5.946235179901123\n",
            "Training Iteration 6475, Loss: 3.369574546813965\n",
            "Training Iteration 6476, Loss: 6.813586235046387\n",
            "Training Iteration 6477, Loss: 5.426283836364746\n",
            "Training Iteration 6478, Loss: 3.8800556659698486\n",
            "Training Iteration 6479, Loss: 4.327152252197266\n",
            "Training Iteration 6480, Loss: 3.734022378921509\n",
            "Training Iteration 6481, Loss: 3.9778809547424316\n",
            "Training Iteration 6482, Loss: 3.8132989406585693\n",
            "Training Iteration 6483, Loss: 4.105149745941162\n",
            "Training Iteration 6484, Loss: 1.7391213178634644\n",
            "Training Iteration 6485, Loss: 8.969111442565918\n",
            "Training Iteration 6486, Loss: 6.86458158493042\n",
            "Training Iteration 6487, Loss: 5.274367809295654\n",
            "Training Iteration 6488, Loss: 5.0758442878723145\n",
            "Training Iteration 6489, Loss: 3.866985321044922\n",
            "Training Iteration 6490, Loss: 4.687005043029785\n",
            "Training Iteration 6491, Loss: 3.8822569847106934\n",
            "Training Iteration 6492, Loss: 2.7584266662597656\n",
            "Training Iteration 6493, Loss: 3.111424207687378\n",
            "Training Iteration 6494, Loss: 4.406002998352051\n",
            "Training Iteration 6495, Loss: 4.508357524871826\n",
            "Training Iteration 6496, Loss: 1.5008468627929688\n",
            "Training Iteration 6497, Loss: 2.6439709663391113\n",
            "Training Iteration 6498, Loss: 5.646482467651367\n",
            "Training Iteration 6499, Loss: 3.408186435699463\n",
            "Training Iteration 6500, Loss: 4.73493766784668\n",
            "Training Iteration 6501, Loss: 3.256281614303589\n",
            "Training Iteration 6502, Loss: 3.1877613067626953\n",
            "Training Iteration 6503, Loss: 2.740591526031494\n",
            "Training Iteration 6504, Loss: 3.159562110900879\n",
            "Training Iteration 6505, Loss: 1.2659575939178467\n",
            "Training Iteration 6506, Loss: 2.1786069869995117\n",
            "Training Iteration 6507, Loss: 4.02653694152832\n",
            "Training Iteration 6508, Loss: 3.292980194091797\n",
            "Training Iteration 6509, Loss: 4.592473983764648\n",
            "Training Iteration 6510, Loss: 4.336655616760254\n",
            "Training Iteration 6511, Loss: 5.655902862548828\n",
            "Training Iteration 6512, Loss: 1.6519383192062378\n",
            "Training Iteration 6513, Loss: 3.6061723232269287\n",
            "Training Iteration 6514, Loss: 3.0566582679748535\n",
            "Training Iteration 6515, Loss: 4.185227394104004\n",
            "Training Iteration 6516, Loss: 2.6092114448547363\n",
            "Training Iteration 6517, Loss: 6.258124828338623\n",
            "Training Iteration 6518, Loss: 6.920608043670654\n",
            "Training Iteration 6519, Loss: 3.4730703830718994\n",
            "Training Iteration 6520, Loss: 1.3662182092666626\n",
            "Training Iteration 6521, Loss: 2.5576391220092773\n",
            "Training Iteration 6522, Loss: 4.053768157958984\n",
            "Training Iteration 6523, Loss: 4.9759345054626465\n",
            "Training Iteration 6524, Loss: 6.1537652015686035\n",
            "Training Iteration 6525, Loss: 4.700322151184082\n",
            "Training Iteration 6526, Loss: 6.6774396896362305\n",
            "Training Iteration 6527, Loss: 5.897490978240967\n",
            "Training Iteration 6528, Loss: 3.580617904663086\n",
            "Training Iteration 6529, Loss: 3.0506556034088135\n",
            "Training Iteration 6530, Loss: 3.689025402069092\n",
            "Training Iteration 6531, Loss: 2.029712200164795\n",
            "Training Iteration 6532, Loss: 2.9050240516662598\n",
            "Training Iteration 6533, Loss: 5.4462480545043945\n",
            "Training Iteration 6534, Loss: 3.5423858165740967\n",
            "Training Iteration 6535, Loss: 6.438033103942871\n",
            "Training Iteration 6536, Loss: 4.816371440887451\n",
            "Training Iteration 6537, Loss: 4.775611400604248\n",
            "Training Iteration 6538, Loss: 6.04832124710083\n",
            "Training Iteration 6539, Loss: 2.4117448329925537\n",
            "Training Iteration 6540, Loss: 3.8955554962158203\n",
            "Training Iteration 6541, Loss: 2.716496229171753\n",
            "Training Iteration 6542, Loss: 3.757514476776123\n",
            "Training Iteration 6543, Loss: 5.125804901123047\n",
            "Training Iteration 6544, Loss: 4.89277982711792\n",
            "Training Iteration 6545, Loss: 2.2923457622528076\n",
            "Training Iteration 6546, Loss: 6.342986106872559\n",
            "Training Iteration 6547, Loss: 5.720347881317139\n",
            "Training Iteration 6548, Loss: 2.2778830528259277\n",
            "Training Iteration 6549, Loss: 4.805202484130859\n",
            "Training Iteration 6550, Loss: 3.2752840518951416\n",
            "Training Iteration 6551, Loss: 3.1659162044525146\n",
            "Training Iteration 6552, Loss: 4.8141770362854\n",
            "Training Iteration 6553, Loss: 5.063234806060791\n",
            "Training Iteration 6554, Loss: 7.156152725219727\n",
            "tensor([[2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        ...,\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06]])\n",
            "Training loss for epcoh 6: 4.21531091972408\n",
            "Training accuracy for epoch 6: 0.2823789722656697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        ...,\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06],\n",
            "        [2.8719e-02, 9.4393e-01, 2.4238e-02, 3.1146e-03, 2.6758e-08, 2.5591e-06]])\n",
            "Validation loss for epcoh 6: 4.262442031599095\n",
            "Test accuracy for epoch 6: 0.2780956740672923\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch No: 7:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61e40208a5c04f5a971a69ce1606347e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Iteration 1, Loss: 7.641659736633301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 4.699281215667725\n",
            "Training Iteration 1565, Loss: 5.223748683929443\n",
            "Training Iteration 1566, Loss: 3.5904479026794434\n",
            "Training Iteration 1567, Loss: 4.5503973960876465\n",
            "Training Iteration 1568, Loss: 3.158687114715576\n",
            "Training Iteration 1569, Loss: 4.512761116027832\n",
            "Training Iteration 1570, Loss: 4.567755222320557\n",
            "Training Iteration 1571, Loss: 3.6679131984710693\n",
            "Training Iteration 1572, Loss: 5.622552394866943\n",
            "Training Iteration 1573, Loss: 4.033694267272949\n",
            "Training Iteration 1574, Loss: 5.170106410980225\n",
            "Training Iteration 1575, Loss: 5.346173286437988\n",
            "Training Iteration 1576, Loss: 2.3116207122802734\n",
            "Training Iteration 1577, Loss: 3.5026817321777344\n",
            "Training Iteration 1578, Loss: 3.7856438159942627\n",
            "Training Iteration 1579, Loss: 6.107600212097168\n",
            "Training Iteration 1580, Loss: 5.451439380645752\n",
            "Training Iteration 1581, Loss: 5.024309158325195\n",
            "Training Iteration 1582, Loss: 5.368967056274414\n",
            "Training Iteration 1583, Loss: 4.847869396209717\n",
            "Training Iteration 1584, Loss: 4.994048595428467\n",
            "Training Iteration 1585, Loss: 2.9502365589141846\n",
            "Training Iteration 1586, Loss: 1.703688621520996\n",
            "Training Iteration 1587, Loss: 3.906002998352051\n",
            "Training Iteration 1588, Loss: 5.286534786224365\n",
            "Training Iteration 1589, Loss: 4.816998481750488\n",
            "Training Iteration 1590, Loss: 3.110980749130249\n",
            "Training Iteration 1591, Loss: 3.9259870052337646\n",
            "Training Iteration 1592, Loss: 4.92219352722168\n",
            "Training Iteration 1593, Loss: 3.017939567565918\n",
            "Training Iteration 1594, Loss: 4.1493964195251465\n",
            "Training Iteration 1595, Loss: 2.1318109035491943\n",
            "Training Iteration 1596, Loss: 3.852457046508789\n",
            "Training Iteration 1597, Loss: 6.788114070892334\n",
            "Training Iteration 1598, Loss: 2.175487518310547\n",
            "Training Iteration 1599, Loss: 5.169859886169434\n",
            "Training Iteration 1600, Loss: 8.180926322937012\n",
            "Training Iteration 1601, Loss: 5.189058303833008\n",
            "Training Iteration 1602, Loss: 3.9969005584716797\n",
            "Training Iteration 1603, Loss: 5.464402675628662\n",
            "Training Iteration 1604, Loss: 5.396711826324463\n",
            "Training Iteration 1605, Loss: 2.628997564315796\n",
            "Training Iteration 1606, Loss: 5.133636951446533\n",
            "Training Iteration 1607, Loss: 5.626599311828613\n",
            "Training Iteration 1608, Loss: 7.005074501037598\n",
            "Training Iteration 1609, Loss: 4.897310256958008\n",
            "Training Iteration 1610, Loss: 5.019237518310547\n",
            "Training Iteration 1611, Loss: 2.0066871643066406\n",
            "Training Iteration 1612, Loss: 8.30244255065918\n",
            "Training Iteration 1613, Loss: 2.15073823928833\n",
            "Training Iteration 1614, Loss: 4.193978309631348\n",
            "Training Iteration 1615, Loss: 5.560302257537842\n",
            "Training Iteration 1616, Loss: 5.3891730308532715\n",
            "Training Iteration 1617, Loss: 1.7114198207855225\n",
            "Training Iteration 1618, Loss: 4.316195964813232\n",
            "Training Iteration 1619, Loss: 3.927008628845215\n",
            "Training Iteration 1620, Loss: 4.860393524169922\n",
            "Training Iteration 1621, Loss: 4.807981967926025\n",
            "Training Iteration 1622, Loss: 3.819282054901123\n",
            "Training Iteration 1623, Loss: 3.5084524154663086\n",
            "Training Iteration 1624, Loss: 4.166999340057373\n",
            "Training Iteration 1625, Loss: 1.477029800415039\n",
            "Training Iteration 1626, Loss: 5.3642964363098145\n",
            "Training Iteration 1627, Loss: 3.900465250015259\n",
            "Training Iteration 1628, Loss: 5.0679216384887695\n",
            "Training Iteration 1629, Loss: 5.4592437744140625\n",
            "Training Iteration 1630, Loss: 4.394411563873291\n",
            "Training Iteration 1631, Loss: 7.875791549682617\n",
            "Training Iteration 1632, Loss: 4.177755832672119\n",
            "Training Iteration 1633, Loss: 4.1532883644104\n",
            "Training Iteration 1634, Loss: 5.7731451988220215\n",
            "Training Iteration 1635, Loss: 3.8936667442321777\n",
            "Training Iteration 1636, Loss: 3.9993717670440674\n",
            "Training Iteration 1637, Loss: 8.754608154296875\n",
            "Training Iteration 1638, Loss: 11.065018653869629\n",
            "Training Iteration 1639, Loss: 3.10856294631958\n",
            "Training Iteration 1640, Loss: 4.9076457023620605\n",
            "Training Iteration 1641, Loss: 2.2646071910858154\n",
            "Training Iteration 1642, Loss: 2.6960935592651367\n",
            "Training Iteration 1643, Loss: 6.3849334716796875\n",
            "Training Iteration 1644, Loss: 4.239915370941162\n",
            "Training Iteration 1645, Loss: 4.553741455078125\n",
            "Training Iteration 1646, Loss: 6.6396918296813965\n",
            "Training Iteration 1647, Loss: 7.234165191650391\n",
            "Training Iteration 1648, Loss: 7.308719158172607\n",
            "Training Iteration 1649, Loss: 6.767750263214111\n",
            "Training Iteration 1650, Loss: 4.778965473175049\n",
            "Training Iteration 1651, Loss: 2.0561718940734863\n",
            "Training Iteration 1652, Loss: 3.294940948486328\n",
            "Training Iteration 1653, Loss: 5.7710347175598145\n",
            "Training Iteration 1654, Loss: 3.700836420059204\n",
            "Training Iteration 1655, Loss: 6.440330982208252\n",
            "Training Iteration 1656, Loss: 5.234796524047852\n",
            "Training Iteration 1657, Loss: 4.181201457977295\n",
            "Training Iteration 1658, Loss: 4.884043216705322\n",
            "Training Iteration 1659, Loss: 4.406732559204102\n",
            "Training Iteration 1660, Loss: 4.404087066650391\n",
            "Training Iteration 1661, Loss: 3.0791234970092773\n",
            "Training Iteration 1662, Loss: 3.3404974937438965\n",
            "Training Iteration 1663, Loss: 4.948237419128418\n",
            "Training Iteration 1664, Loss: 6.2666521072387695\n",
            "Training Iteration 1665, Loss: 9.629610061645508\n",
            "Training Iteration 1666, Loss: 6.147708415985107\n",
            "Training Iteration 1667, Loss: 4.007287502288818\n",
            "Training Iteration 1668, Loss: 5.405272006988525\n",
            "Training Iteration 1669, Loss: 3.3217854499816895\n",
            "Training Iteration 1670, Loss: 3.824082374572754\n",
            "Training Iteration 1671, Loss: 3.0633926391601562\n",
            "Training Iteration 1672, Loss: 3.0262959003448486\n",
            "Training Iteration 1673, Loss: 3.427868366241455\n",
            "Training Iteration 1674, Loss: 3.6415441036224365\n",
            "Training Iteration 1675, Loss: 7.529928207397461\n",
            "Training Iteration 1676, Loss: 9.124608993530273\n",
            "Training Iteration 1677, Loss: 6.260922908782959\n",
            "Training Iteration 1678, Loss: 10.27776050567627\n",
            "Training Iteration 1679, Loss: 2.7626028060913086\n",
            "Training Iteration 1680, Loss: 2.2261152267456055\n",
            "Training Iteration 1681, Loss: 3.8508527278900146\n",
            "Training Iteration 1682, Loss: 3.724133014678955\n",
            "Training Iteration 1683, Loss: 4.1723551750183105\n",
            "Training Iteration 1684, Loss: 2.109231472015381\n",
            "Training Iteration 1685, Loss: 5.457140922546387\n",
            "Training Iteration 1686, Loss: 6.953935146331787\n",
            "Training Iteration 1687, Loss: 3.7494149208068848\n",
            "Training Iteration 1688, Loss: 6.377378463745117\n",
            "Training Iteration 1689, Loss: 3.2857816219329834\n",
            "Training Iteration 1690, Loss: 5.110223770141602\n",
            "Training Iteration 1691, Loss: 8.492023468017578\n",
            "Training Iteration 1692, Loss: 6.688121318817139\n",
            "Training Iteration 1693, Loss: 12.00330924987793\n",
            "Training Iteration 1694, Loss: 9.50294017791748\n",
            "Training Iteration 1695, Loss: 8.048089981079102\n",
            "Training Iteration 1696, Loss: 9.071808815002441\n",
            "Training Iteration 1697, Loss: 2.847074031829834\n",
            "Training Iteration 1698, Loss: 3.7189741134643555\n",
            "Training Iteration 1699, Loss: 3.474271535873413\n",
            "Training Iteration 1700, Loss: 3.818605422973633\n",
            "Training Iteration 1701, Loss: 5.878736972808838\n",
            "Training Iteration 1702, Loss: 3.596181869506836\n",
            "Training Iteration 1703, Loss: 7.134721279144287\n",
            "Training Iteration 1704, Loss: 9.734332084655762\n",
            "Training Iteration 1705, Loss: 8.712035179138184\n",
            "Training Iteration 1706, Loss: 3.123183250427246\n",
            "Training Iteration 1707, Loss: 4.613969802856445\n",
            "Training Iteration 1708, Loss: 6.405114650726318\n",
            "Training Iteration 1709, Loss: 3.2338199615478516\n",
            "Training Iteration 1710, Loss: 6.365076065063477\n",
            "Training Iteration 1711, Loss: 5.188904762268066\n",
            "Training Iteration 1712, Loss: 2.777036666870117\n",
            "Training Iteration 1713, Loss: 2.7829291820526123\n",
            "Training Iteration 1714, Loss: 4.4226837158203125\n",
            "Training Iteration 1715, Loss: 3.8967957496643066\n",
            "Training Iteration 1716, Loss: 4.37649393081665\n",
            "Training Iteration 1717, Loss: 3.8436450958251953\n",
            "Training Iteration 1718, Loss: 6.973238945007324\n",
            "Training Iteration 1719, Loss: 4.010458469390869\n",
            "Training Iteration 1720, Loss: 6.776413440704346\n",
            "Training Iteration 1721, Loss: 5.267550945281982\n",
            "Training Iteration 1722, Loss: 6.265342712402344\n",
            "Training Iteration 1723, Loss: 4.05053186416626\n",
            "Training Iteration 1724, Loss: 7.242134094238281\n",
            "Training Iteration 1725, Loss: 3.479936122894287\n",
            "Training Iteration 1726, Loss: 5.251323699951172\n",
            "Training Iteration 1727, Loss: 2.5217814445495605\n",
            "Training Iteration 1728, Loss: 5.472511291503906\n",
            "Training Iteration 1729, Loss: 3.361442804336548\n",
            "Training Iteration 1730, Loss: 2.890467405319214\n",
            "Training Iteration 1731, Loss: 5.575796127319336\n",
            "Training Iteration 1732, Loss: 6.9482598304748535\n",
            "Training Iteration 1733, Loss: 4.771766662597656\n",
            "Training Iteration 1734, Loss: 6.264239311218262\n",
            "Training Iteration 1735, Loss: 4.446203708648682\n",
            "Training Iteration 1736, Loss: 5.250485897064209\n",
            "Training Iteration 1737, Loss: 5.86798620223999\n",
            "Training Iteration 1738, Loss: 6.256589889526367\n",
            "Training Iteration 1739, Loss: 3.9715564250946045\n",
            "Training Iteration 1740, Loss: 8.513699531555176\n",
            "Training Iteration 1741, Loss: 4.352107524871826\n",
            "Training Iteration 1742, Loss: 6.3707075119018555\n",
            "Training Iteration 1743, Loss: 3.24326229095459\n",
            "Training Iteration 1744, Loss: 3.0915122032165527\n",
            "Training Iteration 1745, Loss: 4.904596328735352\n",
            "Training Iteration 1746, Loss: 5.736592769622803\n",
            "Training Iteration 1747, Loss: 5.565279960632324\n",
            "Training Iteration 1748, Loss: 2.4443564414978027\n",
            "Training Iteration 1749, Loss: 4.648701190948486\n",
            "Training Iteration 1750, Loss: 5.772261619567871\n",
            "Training Iteration 1751, Loss: 2.0764594078063965\n",
            "Training Iteration 1752, Loss: 2.701608657836914\n",
            "Training Iteration 1753, Loss: 2.8875977993011475\n",
            "Training Iteration 1754, Loss: 4.1075639724731445\n",
            "Training Iteration 1755, Loss: 4.803980350494385\n",
            "Training Iteration 1756, Loss: 5.408770561218262\n",
            "Training Iteration 1757, Loss: 3.8830015659332275\n",
            "Training Iteration 1758, Loss: 5.526316165924072\n",
            "Training Iteration 1759, Loss: 3.7596018314361572\n",
            "Training Iteration 1760, Loss: 1.9515496492385864\n",
            "Training Iteration 1761, Loss: 1.4591902494430542\n",
            "Training Iteration 1762, Loss: 6.823453903198242\n",
            "Training Iteration 1763, Loss: 5.767564296722412\n",
            "Training Iteration 1764, Loss: 3.532254219055176\n",
            "Training Iteration 1765, Loss: 4.9721455574035645\n",
            "Training Iteration 1766, Loss: 3.330003261566162\n",
            "Training Iteration 1767, Loss: 4.983713150024414\n",
            "Training Iteration 1768, Loss: 0.7402064800262451\n",
            "Training Iteration 1769, Loss: 5.039040565490723\n",
            "Training Iteration 1770, Loss: 5.71051025390625\n",
            "Training Iteration 1771, Loss: 5.1345672607421875\n",
            "Training Iteration 1772, Loss: 7.4696431159973145\n",
            "Training Iteration 1773, Loss: 2.681401252746582\n",
            "Training Iteration 1774, Loss: 10.934449195861816\n",
            "Training Iteration 1775, Loss: 2.8645992279052734\n",
            "Training Iteration 1776, Loss: 3.9519848823547363\n",
            "Training Iteration 1777, Loss: 3.7821571826934814\n",
            "Training Iteration 1778, Loss: 3.7084951400756836\n",
            "Training Iteration 1779, Loss: 4.298748016357422\n",
            "Training Iteration 1780, Loss: 2.1799068450927734\n",
            "Training Iteration 1781, Loss: 4.413400173187256\n",
            "Training Iteration 1782, Loss: 4.942086696624756\n",
            "Training Iteration 1783, Loss: 3.8092029094696045\n",
            "Training Iteration 1784, Loss: 3.173968553543091\n",
            "Training Iteration 1785, Loss: 4.383365154266357\n",
            "Training Iteration 1786, Loss: 2.8996171951293945\n",
            "Training Iteration 1787, Loss: 2.34346342086792\n",
            "Training Iteration 1788, Loss: 3.1680831909179688\n",
            "Training Iteration 1789, Loss: 5.020814418792725\n",
            "Training Iteration 1790, Loss: 3.4049313068389893\n",
            "Training Iteration 1791, Loss: 7.247518539428711\n",
            "Training Iteration 1792, Loss: 4.506410121917725\n",
            "Training Iteration 1793, Loss: 1.7063754796981812\n",
            "Training Iteration 1794, Loss: 3.4900336265563965\n",
            "Training Iteration 1795, Loss: 3.5302844047546387\n",
            "Training Iteration 1796, Loss: 3.3745548725128174\n",
            "Training Iteration 1797, Loss: 9.975105285644531\n",
            "Training Iteration 1798, Loss: 7.0702643394470215\n",
            "Training Iteration 1799, Loss: 4.411032199859619\n",
            "Training Iteration 1800, Loss: 4.968410015106201\n",
            "Training Iteration 1801, Loss: 6.186689376831055\n",
            "Training Iteration 1802, Loss: 5.085423946380615\n",
            "Training Iteration 1803, Loss: 3.917898178100586\n",
            "Training Iteration 1804, Loss: 3.7524971961975098\n",
            "Training Iteration 1805, Loss: 6.2628936767578125\n",
            "Training Iteration 1806, Loss: 3.919527530670166\n",
            "Training Iteration 1807, Loss: 4.215407371520996\n",
            "Training Iteration 1808, Loss: 5.26628303527832\n",
            "Training Iteration 1809, Loss: 5.609536647796631\n",
            "Training Iteration 1810, Loss: 2.618229389190674\n",
            "Training Iteration 1811, Loss: 2.9864084720611572\n",
            "Training Iteration 1812, Loss: 3.985508918762207\n",
            "Training Iteration 1813, Loss: 3.3115572929382324\n",
            "Training Iteration 1814, Loss: 2.18390154838562\n",
            "Training Iteration 1815, Loss: 6.723998069763184\n",
            "Training Iteration 1816, Loss: 4.046828269958496\n",
            "Training Iteration 1817, Loss: 2.130286693572998\n",
            "Training Iteration 1818, Loss: 2.293120861053467\n",
            "Training Iteration 1819, Loss: 4.066279411315918\n",
            "Training Iteration 1820, Loss: 2.876993417739868\n",
            "Training Iteration 1821, Loss: 4.930623531341553\n",
            "Training Iteration 1822, Loss: 3.823396921157837\n",
            "Training Iteration 1823, Loss: 2.0169968605041504\n",
            "Training Iteration 1824, Loss: 4.839823246002197\n",
            "Training Iteration 1825, Loss: 6.312747955322266\n",
            "Training Iteration 1826, Loss: 4.556257724761963\n",
            "Training Iteration 1827, Loss: 2.6428420543670654\n",
            "Training Iteration 1828, Loss: 4.713334083557129\n",
            "Training Iteration 1829, Loss: 3.437847852706909\n",
            "Training Iteration 1830, Loss: 1.1591604948043823\n",
            "Training Iteration 1831, Loss: 4.868575572967529\n",
            "Training Iteration 1832, Loss: 4.532713890075684\n",
            "Training Iteration 1833, Loss: 5.624783039093018\n",
            "Training Iteration 1834, Loss: 4.971431255340576\n",
            "Training Iteration 1835, Loss: 4.3414201736450195\n",
            "Training Iteration 1836, Loss: 1.828630805015564\n",
            "Training Iteration 1837, Loss: 5.45340633392334\n",
            "Training Iteration 1838, Loss: 7.800609111785889\n",
            "Training Iteration 1839, Loss: 4.489495277404785\n",
            "Training Iteration 1840, Loss: 4.073692321777344\n",
            "Training Iteration 1841, Loss: 3.258747100830078\n",
            "Training Iteration 1842, Loss: 4.991288185119629\n",
            "Training Iteration 1843, Loss: 4.922814846038818\n",
            "Training Iteration 1844, Loss: 4.174739360809326\n",
            "Training Iteration 1845, Loss: 6.222413063049316\n",
            "Training Iteration 1846, Loss: 2.5179004669189453\n",
            "Training Iteration 1847, Loss: 2.283816337585449\n",
            "Training Iteration 1848, Loss: 2.155496597290039\n",
            "Training Iteration 1849, Loss: 4.534298419952393\n",
            "Training Iteration 1850, Loss: 3.9332127571105957\n",
            "Training Iteration 1851, Loss: 5.3615851402282715\n",
            "Training Iteration 1852, Loss: 6.57740592956543\n",
            "Training Iteration 1853, Loss: 6.130440711975098\n",
            "Training Iteration 1854, Loss: 4.5882110595703125\n",
            "Training Iteration 1855, Loss: 4.767821311950684\n",
            "Training Iteration 1856, Loss: 3.0994396209716797\n",
            "Training Iteration 1857, Loss: 3.1332290172576904\n",
            "Training Iteration 1858, Loss: 2.668419361114502\n",
            "Training Iteration 1859, Loss: 2.62680721282959\n",
            "Training Iteration 1860, Loss: 5.5623931884765625\n",
            "Training Iteration 1861, Loss: 1.512281894683838\n",
            "Training Iteration 1862, Loss: 3.072967290878296\n",
            "Training Iteration 1863, Loss: 6.611501693725586\n",
            "Training Iteration 1864, Loss: 4.6820173263549805\n",
            "Training Iteration 1865, Loss: 3.59513783454895\n",
            "Training Iteration 1866, Loss: 2.4461781978607178\n",
            "Training Iteration 1867, Loss: 2.8208518028259277\n",
            "Training Iteration 1868, Loss: 3.7735915184020996\n",
            "Training Iteration 1869, Loss: 3.207378625869751\n",
            "Training Iteration 1870, Loss: 2.753399133682251\n",
            "Training Iteration 1871, Loss: 2.8528778553009033\n",
            "Training Iteration 1872, Loss: 2.6534266471862793\n",
            "Training Iteration 1873, Loss: 2.7753636837005615\n",
            "Training Iteration 1874, Loss: 3.208653211593628\n",
            "Training Iteration 1875, Loss: 8.851102828979492\n",
            "Training Iteration 1876, Loss: 3.2626454830169678\n",
            "Training Iteration 1877, Loss: 4.9493231773376465\n",
            "Training Iteration 1878, Loss: 2.2822375297546387\n",
            "Training Iteration 1879, Loss: 6.58311128616333\n",
            "Training Iteration 1880, Loss: 5.150343894958496\n",
            "Training Iteration 1881, Loss: 6.777282238006592\n",
            "Training Iteration 1882, Loss: 5.6732258796691895\n",
            "Training Iteration 1883, Loss: 5.51779842376709\n",
            "Training Iteration 1884, Loss: 6.315288543701172\n",
            "Training Iteration 1885, Loss: 6.080041408538818\n",
            "Training Iteration 1886, Loss: 5.139827728271484\n",
            "Training Iteration 1887, Loss: 6.268259525299072\n",
            "Training Iteration 1888, Loss: 2.9166359901428223\n",
            "Training Iteration 1889, Loss: 3.5693390369415283\n",
            "Training Iteration 1890, Loss: 4.991694927215576\n",
            "Training Iteration 1891, Loss: 3.9450843334198\n",
            "Training Iteration 1892, Loss: 6.07837438583374\n",
            "Training Iteration 1893, Loss: 9.82178783416748\n",
            "Training Iteration 1894, Loss: 4.899751663208008\n",
            "Training Iteration 1895, Loss: 5.05126953125\n",
            "Training Iteration 1896, Loss: 5.265293598175049\n",
            "Training Iteration 1897, Loss: 7.876047611236572\n",
            "Training Iteration 1898, Loss: 5.684525489807129\n",
            "Training Iteration 1899, Loss: 7.500089168548584\n",
            "Training Iteration 1900, Loss: 3.77502703666687\n",
            "Training Iteration 1901, Loss: 5.927094459533691\n",
            "Training Iteration 1902, Loss: 6.427276611328125\n",
            "Training Iteration 1903, Loss: 4.183306694030762\n",
            "Training Iteration 1904, Loss: 2.515026330947876\n",
            "Training Iteration 1905, Loss: 5.599026679992676\n",
            "Training Iteration 1906, Loss: 2.5326685905456543\n",
            "Training Iteration 1907, Loss: 4.319324493408203\n",
            "Training Iteration 1908, Loss: 3.976161241531372\n",
            "Training Iteration 1909, Loss: 7.573302745819092\n",
            "Training Iteration 1910, Loss: 2.4980814456939697\n",
            "Training Iteration 1911, Loss: 6.537069320678711\n",
            "Training Iteration 1912, Loss: 4.4091620445251465\n",
            "Training Iteration 1913, Loss: 5.790090084075928\n",
            "Training Iteration 1914, Loss: 7.674969673156738\n",
            "Training Iteration 1915, Loss: 7.082192420959473\n",
            "Training Iteration 1916, Loss: 6.163394927978516\n",
            "Training Iteration 1917, Loss: 4.692653179168701\n",
            "Training Iteration 1918, Loss: 5.002854824066162\n",
            "Training Iteration 1919, Loss: 14.320902824401855\n",
            "Training Iteration 1920, Loss: 6.824198246002197\n",
            "Training Iteration 1921, Loss: 5.1493940353393555\n",
            "Training Iteration 1922, Loss: 7.553253173828125\n",
            "Training Iteration 1923, Loss: 4.312321662902832\n",
            "Training Iteration 1924, Loss: 5.952274322509766\n",
            "Training Iteration 1925, Loss: 4.803727149963379\n",
            "Training Iteration 1926, Loss: 2.781904458999634\n",
            "Training Iteration 1927, Loss: 5.48193359375\n",
            "Training Iteration 1928, Loss: 5.71510648727417\n",
            "Training Iteration 1929, Loss: 2.2168078422546387\n",
            "Training Iteration 1930, Loss: 4.208157062530518\n",
            "Training Iteration 1931, Loss: 2.591322422027588\n",
            "Training Iteration 1932, Loss: 3.4175896644592285\n",
            "Training Iteration 1933, Loss: 3.759087085723877\n",
            "Training Iteration 1934, Loss: 2.7167062759399414\n",
            "Training Iteration 1935, Loss: 2.8448073863983154\n",
            "Training Iteration 1936, Loss: 9.202930450439453\n",
            "Training Iteration 1937, Loss: 3.802387237548828\n",
            "Training Iteration 1938, Loss: 2.8625667095184326\n",
            "Training Iteration 1939, Loss: 9.543371200561523\n",
            "Training Iteration 1940, Loss: 3.9398462772369385\n",
            "Training Iteration 1941, Loss: 2.8591628074645996\n",
            "Training Iteration 1942, Loss: 4.782278060913086\n",
            "Training Iteration 1943, Loss: 3.660454750061035\n",
            "Training Iteration 1944, Loss: 5.1810479164123535\n",
            "Training Iteration 1945, Loss: 5.248987674713135\n",
            "Training Iteration 1946, Loss: 5.111793041229248\n",
            "Training Iteration 1947, Loss: 3.4857161045074463\n",
            "Training Iteration 1948, Loss: 4.193596363067627\n",
            "Training Iteration 1949, Loss: 2.828026533126831\n",
            "Training Iteration 1950, Loss: 2.056013822555542\n",
            "Training Iteration 1951, Loss: 5.075464725494385\n",
            "Training Iteration 1952, Loss: 6.2050299644470215\n",
            "Training Iteration 1953, Loss: 4.157116413116455\n",
            "Training Iteration 1954, Loss: 2.5303497314453125\n",
            "Training Iteration 1955, Loss: 3.2917654514312744\n",
            "Training Iteration 1956, Loss: 3.2210240364074707\n",
            "Training Iteration 1957, Loss: 7.369666576385498\n",
            "Training Iteration 1958, Loss: 3.532538890838623\n",
            "Training Iteration 1959, Loss: 2.8179500102996826\n",
            "Training Iteration 1960, Loss: 4.448923110961914\n",
            "Training Iteration 1961, Loss: 1.7500360012054443\n",
            "Training Iteration 1962, Loss: 8.309282302856445\n",
            "Training Iteration 1963, Loss: 3.2361743450164795\n",
            "Training Iteration 1964, Loss: 2.97795033454895\n",
            "Training Iteration 1965, Loss: 4.471774101257324\n",
            "Training Iteration 1966, Loss: 4.920266151428223\n",
            "Training Iteration 1967, Loss: 6.883688926696777\n",
            "Training Iteration 1968, Loss: 2.78365421295166\n",
            "Training Iteration 1969, Loss: 3.656420946121216\n",
            "Training Iteration 1970, Loss: 3.579525947570801\n",
            "Training Iteration 1971, Loss: 5.135480880737305\n",
            "Training Iteration 1972, Loss: 1.4063715934753418\n",
            "Training Iteration 1973, Loss: 3.082655429840088\n",
            "Training Iteration 1974, Loss: 2.520507335662842\n",
            "Training Iteration 1975, Loss: 3.337153673171997\n",
            "Training Iteration 1976, Loss: 2.757948637008667\n",
            "Training Iteration 1977, Loss: 3.0816726684570312\n",
            "Training Iteration 1978, Loss: 3.4252572059631348\n",
            "Training Iteration 1979, Loss: 5.7756476402282715\n",
            "Training Iteration 1980, Loss: 2.530748128890991\n",
            "Training Iteration 1981, Loss: 2.9410946369171143\n",
            "Training Iteration 1982, Loss: 5.0773024559021\n",
            "Training Iteration 1983, Loss: 5.046718597412109\n",
            "Training Iteration 1984, Loss: 2.4148366451263428\n",
            "Training Iteration 1985, Loss: 2.7712934017181396\n",
            "Training Iteration 1986, Loss: 6.454166412353516\n",
            "Training Iteration 1987, Loss: 4.054305076599121\n",
            "Training Iteration 1988, Loss: 3.28226375579834\n",
            "Training Iteration 1989, Loss: 2.7308456897735596\n",
            "Training Iteration 1990, Loss: 3.4851977825164795\n",
            "Training Iteration 1991, Loss: 2.43788480758667\n",
            "Training Iteration 1992, Loss: 5.064628601074219\n",
            "Training Iteration 1993, Loss: 4.480273246765137\n",
            "Training Iteration 1994, Loss: 3.271796703338623\n",
            "Training Iteration 1995, Loss: 5.901646137237549\n",
            "Training Iteration 1996, Loss: 2.7476770877838135\n",
            "Training Iteration 1997, Loss: 3.2954180240631104\n",
            "Training Iteration 1998, Loss: 5.11933708190918\n",
            "Training Iteration 1999, Loss: 4.406600475311279\n",
            "Training Iteration 2000, Loss: 3.98378324508667\n",
            "Training Iteration 2001, Loss: 2.414945602416992\n",
            "Training Iteration 2002, Loss: 5.4236884117126465\n",
            "Training Iteration 2003, Loss: 4.846698760986328\n",
            "Training Iteration 2004, Loss: 3.3513598442077637\n",
            "Training Iteration 2005, Loss: 5.097440719604492\n",
            "Training Iteration 2006, Loss: 2.637246608734131\n",
            "Training Iteration 2007, Loss: 3.5540900230407715\n",
            "Training Iteration 2008, Loss: 2.7980871200561523\n",
            "Training Iteration 2009, Loss: 5.159444808959961\n",
            "Training Iteration 2010, Loss: 6.909162998199463\n",
            "Training Iteration 2011, Loss: 2.49106764793396\n",
            "Training Iteration 2012, Loss: 5.864629745483398\n",
            "Training Iteration 2013, Loss: 5.559135913848877\n",
            "Training Iteration 2014, Loss: 3.9362356662750244\n",
            "Training Iteration 2015, Loss: 3.6907200813293457\n",
            "Training Iteration 2016, Loss: 2.4353339672088623\n",
            "Training Iteration 2017, Loss: 5.240999221801758\n",
            "Training Iteration 2018, Loss: 2.593369245529175\n",
            "Training Iteration 2019, Loss: 3.693108320236206\n",
            "Training Iteration 2020, Loss: 5.528515338897705\n",
            "Training Iteration 2021, Loss: 3.5380444526672363\n",
            "Training Iteration 2022, Loss: 3.9313299655914307\n",
            "Training Iteration 2023, Loss: 3.2874035835266113\n",
            "Training Iteration 2024, Loss: 6.826716423034668\n",
            "Training Iteration 2025, Loss: 6.654067039489746\n",
            "Training Iteration 2026, Loss: 8.895647048950195\n",
            "Training Iteration 2027, Loss: 3.8873071670532227\n",
            "Training Iteration 2028, Loss: 6.241026878356934\n",
            "Training Iteration 2029, Loss: 3.272035598754883\n",
            "Training Iteration 2030, Loss: 3.3157894611358643\n",
            "Training Iteration 2031, Loss: 7.022507190704346\n",
            "Training Iteration 2032, Loss: 2.824625015258789\n",
            "Training Iteration 2033, Loss: 8.820917129516602\n",
            "Training Iteration 2034, Loss: 6.328702926635742\n",
            "Training Iteration 2035, Loss: 4.154901504516602\n",
            "Training Iteration 2036, Loss: 4.810614109039307\n",
            "Training Iteration 2037, Loss: 3.361720561981201\n",
            "Training Iteration 2038, Loss: 7.03810977935791\n",
            "Training Iteration 2039, Loss: 4.197848320007324\n",
            "Training Iteration 2040, Loss: 6.279833793640137\n",
            "Training Iteration 2041, Loss: 6.379522323608398\n",
            "Training Iteration 2042, Loss: 4.238548755645752\n",
            "Training Iteration 2043, Loss: 5.272265434265137\n",
            "Training Iteration 2044, Loss: 1.2403723001480103\n",
            "Training Iteration 2045, Loss: 9.982909202575684\n",
            "Training Iteration 2046, Loss: 4.837953567504883\n",
            "Training Iteration 2047, Loss: 1.640915870666504\n",
            "Training Iteration 2048, Loss: 4.892417907714844\n",
            "Training Iteration 2049, Loss: 6.188620090484619\n",
            "Training Iteration 2050, Loss: 2.5376200675964355\n",
            "Training Iteration 2051, Loss: 2.690814733505249\n",
            "Training Iteration 2052, Loss: 9.715764045715332\n",
            "Training Iteration 2053, Loss: 3.794685125350952\n",
            "Training Iteration 2054, Loss: 5.072225093841553\n",
            "Training Iteration 2055, Loss: 3.87630558013916\n",
            "Training Iteration 2056, Loss: 3.2040557861328125\n",
            "Training Iteration 2057, Loss: 5.95186185836792\n",
            "Training Iteration 2058, Loss: 3.8605539798736572\n",
            "Training Iteration 2059, Loss: 4.496697425842285\n",
            "Training Iteration 2060, Loss: 5.317693710327148\n",
            "Training Iteration 2061, Loss: 7.2228546142578125\n",
            "Training Iteration 2062, Loss: 5.768091201782227\n",
            "Training Iteration 2063, Loss: 4.61862325668335\n",
            "Training Iteration 2064, Loss: 6.18791389465332\n",
            "Training Iteration 2065, Loss: 5.9238433837890625\n",
            "Training Iteration 2066, Loss: 4.88901948928833\n",
            "Training Iteration 2067, Loss: 4.946148872375488\n",
            "Training Iteration 2068, Loss: 5.10186767578125\n",
            "Training Iteration 2069, Loss: 4.619235992431641\n",
            "Training Iteration 2070, Loss: 5.0877909660339355\n",
            "Training Iteration 2071, Loss: 4.890082359313965\n",
            "Training Iteration 2072, Loss: 3.370619773864746\n",
            "Training Iteration 2073, Loss: 4.66172456741333\n",
            "Training Iteration 2074, Loss: 6.542996406555176\n",
            "Training Iteration 2075, Loss: 3.0118675231933594\n",
            "Training Iteration 2076, Loss: 6.205459117889404\n",
            "Training Iteration 2077, Loss: 3.2553415298461914\n",
            "Training Iteration 2078, Loss: 3.310459613800049\n",
            "Training Iteration 2079, Loss: 4.545279502868652\n",
            "Training Iteration 2080, Loss: 3.568899154663086\n",
            "Training Iteration 2081, Loss: 6.254580497741699\n",
            "Training Iteration 2082, Loss: 6.076576232910156\n",
            "Training Iteration 2083, Loss: 2.271212100982666\n",
            "Training Iteration 2084, Loss: 3.7294819355010986\n",
            "Training Iteration 2085, Loss: 4.4249372482299805\n",
            "Training Iteration 2086, Loss: 4.505207061767578\n",
            "Training Iteration 2087, Loss: 5.181813716888428\n",
            "Training Iteration 2088, Loss: 6.757586479187012\n",
            "Training Iteration 2089, Loss: 4.141970634460449\n",
            "Training Iteration 2090, Loss: 3.7985293865203857\n",
            "Training Iteration 2091, Loss: 3.0416746139526367\n",
            "Training Iteration 2092, Loss: 4.22441291809082\n",
            "Training Iteration 2093, Loss: 3.081214666366577\n",
            "Training Iteration 2094, Loss: 3.577333927154541\n",
            "Training Iteration 2095, Loss: 4.661021709442139\n",
            "Training Iteration 2096, Loss: 4.913718223571777\n",
            "Training Iteration 2097, Loss: 1.9980478286743164\n",
            "Training Iteration 2098, Loss: 4.555090427398682\n",
            "Training Iteration 2099, Loss: 4.346309185028076\n",
            "Training Iteration 2100, Loss: 2.6792635917663574\n",
            "Training Iteration 2101, Loss: 4.275262355804443\n",
            "Training Iteration 2102, Loss: 3.0404181480407715\n",
            "Training Iteration 2103, Loss: 1.9443590641021729\n",
            "Training Iteration 2104, Loss: 2.1787500381469727\n",
            "Training Iteration 2105, Loss: 3.558175563812256\n",
            "Training Iteration 2106, Loss: 6.487301826477051\n",
            "Training Iteration 2107, Loss: 5.8360395431518555\n",
            "Training Iteration 2108, Loss: 4.941614627838135\n",
            "Training Iteration 2109, Loss: 6.636203289031982\n",
            "Training Iteration 2110, Loss: 1.7773996591567993\n",
            "Training Iteration 2111, Loss: 5.123653888702393\n",
            "Training Iteration 2112, Loss: 1.633062481880188\n",
            "Training Iteration 2113, Loss: 5.002436637878418\n",
            "Training Iteration 2114, Loss: 3.9978816509246826\n",
            "Training Iteration 2115, Loss: 5.885138034820557\n",
            "Training Iteration 2116, Loss: 4.390903949737549\n",
            "Training Iteration 2117, Loss: 2.6542491912841797\n",
            "Training Iteration 2118, Loss: 2.7954304218292236\n",
            "Training Iteration 2119, Loss: 4.787154197692871\n",
            "Training Iteration 2120, Loss: 5.439955711364746\n",
            "Training Iteration 2121, Loss: 6.758533000946045\n",
            "Training Iteration 2122, Loss: 5.1581950187683105\n",
            "Training Iteration 2123, Loss: 2.4946863651275635\n",
            "Training Iteration 2124, Loss: 2.1638858318328857\n",
            "Training Iteration 2125, Loss: 4.194287300109863\n",
            "Training Iteration 2126, Loss: 4.809595108032227\n",
            "Training Iteration 2127, Loss: 1.8915183544158936\n",
            "Training Iteration 2128, Loss: 6.650226593017578\n",
            "Training Iteration 2129, Loss: 8.250994682312012\n",
            "Training Iteration 2130, Loss: 4.911862850189209\n",
            "Training Iteration 2131, Loss: 3.8007068634033203\n",
            "Training Iteration 2132, Loss: 1.5321674346923828\n",
            "Training Iteration 2133, Loss: 3.3668808937072754\n",
            "Training Iteration 2134, Loss: 3.364548921585083\n",
            "Training Iteration 2135, Loss: 2.79128098487854\n",
            "Training Iteration 2136, Loss: 8.66787052154541\n",
            "Training Iteration 2137, Loss: 3.0395267009735107\n",
            "Training Iteration 2138, Loss: 7.064344882965088\n",
            "Training Iteration 2139, Loss: 4.682826519012451\n",
            "Training Iteration 2140, Loss: 2.963547706604004\n",
            "Training Iteration 2141, Loss: 3.9242377281188965\n",
            "Training Iteration 2142, Loss: 2.812852382659912\n",
            "Training Iteration 2143, Loss: 5.3690314292907715\n",
            "Training Iteration 2144, Loss: 3.7239928245544434\n",
            "Training Iteration 2145, Loss: 2.9317355155944824\n",
            "Training Iteration 2146, Loss: 3.6726932525634766\n",
            "Training Iteration 2147, Loss: 6.967929363250732\n",
            "Training Iteration 2148, Loss: 10.65013313293457\n",
            "Training Iteration 2149, Loss: 4.04657506942749\n",
            "Training Iteration 2150, Loss: 4.827848434448242\n",
            "Training Iteration 2151, Loss: 2.1443593502044678\n",
            "Training Iteration 2152, Loss: 4.788081169128418\n",
            "Training Iteration 2153, Loss: 5.875049114227295\n",
            "Training Iteration 2154, Loss: 3.615596294403076\n",
            "Training Iteration 2155, Loss: 2.38623046875\n",
            "Training Iteration 2156, Loss: 4.622119903564453\n",
            "Training Iteration 2157, Loss: 5.527923583984375\n",
            "Training Iteration 2158, Loss: 3.3415379524230957\n",
            "Training Iteration 2159, Loss: 4.126276016235352\n",
            "Training Iteration 2160, Loss: 3.5303118228912354\n",
            "Training Iteration 2161, Loss: 3.2471563816070557\n",
            "Training Iteration 2162, Loss: 5.815486907958984\n",
            "Training Iteration 2163, Loss: 4.612859725952148\n",
            "Training Iteration 2164, Loss: 6.171539783477783\n",
            "Training Iteration 2165, Loss: 6.487115859985352\n",
            "Training Iteration 2166, Loss: 2.0217223167419434\n",
            "Training Iteration 2167, Loss: 3.974414825439453\n",
            "Training Iteration 2168, Loss: 3.574092388153076\n",
            "Training Iteration 2169, Loss: 3.324068546295166\n",
            "Training Iteration 2170, Loss: 1.8007129430770874\n",
            "Training Iteration 2171, Loss: 5.6821136474609375\n",
            "Training Iteration 2172, Loss: 2.3059756755828857\n",
            "Training Iteration 2173, Loss: 1.7304706573486328\n",
            "Training Iteration 2174, Loss: 3.1782572269439697\n",
            "Training Iteration 2175, Loss: 3.0824837684631348\n",
            "Training Iteration 2176, Loss: 4.64622688293457\n",
            "Training Iteration 2177, Loss: 6.896993637084961\n",
            "Training Iteration 2178, Loss: 2.54647159576416\n",
            "Training Iteration 2179, Loss: 3.314954996109009\n",
            "Training Iteration 2180, Loss: 2.830031156539917\n",
            "Training Iteration 2181, Loss: 2.1808462142944336\n",
            "Training Iteration 2182, Loss: 4.91848611831665\n",
            "Training Iteration 2183, Loss: 2.573216438293457\n",
            "Training Iteration 2184, Loss: 7.42177152633667\n",
            "Training Iteration 2185, Loss: 2.532881021499634\n",
            "Training Iteration 2186, Loss: 4.499204635620117\n",
            "Training Iteration 2187, Loss: 3.0720407962799072\n",
            "Training Iteration 2188, Loss: 4.601922988891602\n",
            "Training Iteration 2189, Loss: 2.7551076412200928\n",
            "Training Iteration 2190, Loss: 2.392415761947632\n",
            "Training Iteration 2191, Loss: 4.524391174316406\n",
            "Training Iteration 2192, Loss: 4.805249214172363\n",
            "Training Iteration 2193, Loss: 2.7547945976257324\n",
            "Training Iteration 2194, Loss: 4.8429341316223145\n",
            "Training Iteration 2195, Loss: 4.729146957397461\n",
            "Training Iteration 2196, Loss: 6.343339443206787\n",
            "Training Iteration 2197, Loss: 3.834590196609497\n",
            "Training Iteration 2198, Loss: 4.445982933044434\n",
            "Training Iteration 2199, Loss: 7.014676094055176\n",
            "Training Iteration 2200, Loss: 6.722092628479004\n",
            "Training Iteration 2201, Loss: 4.047176361083984\n",
            "Training Iteration 2202, Loss: 5.005417346954346\n",
            "Training Iteration 2203, Loss: 5.512250900268555\n",
            "Training Iteration 2204, Loss: 1.3340978622436523\n",
            "Training Iteration 2205, Loss: 2.1756603717803955\n",
            "Training Iteration 2206, Loss: 7.433472156524658\n",
            "Training Iteration 2207, Loss: 7.1685967445373535\n",
            "Training Iteration 2208, Loss: 6.57187032699585\n",
            "Training Iteration 2209, Loss: 4.934415340423584\n",
            "Training Iteration 2210, Loss: 4.232291221618652\n",
            "Training Iteration 2211, Loss: 3.7221994400024414\n",
            "Training Iteration 2212, Loss: 7.624568939208984\n",
            "Training Iteration 2213, Loss: 3.129356861114502\n",
            "Training Iteration 2214, Loss: 4.056994438171387\n",
            "Training Iteration 2215, Loss: 5.097623348236084\n",
            "Training Iteration 2216, Loss: 2.513355016708374\n",
            "Training Iteration 2217, Loss: 3.231822967529297\n",
            "Training Iteration 2218, Loss: 2.8940649032592773\n",
            "Training Iteration 2219, Loss: 1.248199701309204\n",
            "Training Iteration 2220, Loss: 3.2217531204223633\n",
            "Training Iteration 2221, Loss: 4.567454814910889\n",
            "Training Iteration 2222, Loss: 5.821719646453857\n",
            "Training Iteration 2223, Loss: 2.8097615242004395\n",
            "Training Iteration 2224, Loss: 5.085224151611328\n",
            "Training Iteration 2225, Loss: 1.9260857105255127\n",
            "Training Iteration 2226, Loss: 4.453306198120117\n",
            "Training Iteration 2227, Loss: 4.9000725746154785\n",
            "Training Iteration 2228, Loss: 3.4833409786224365\n",
            "Training Iteration 2229, Loss: 5.546025276184082\n",
            "Training Iteration 2230, Loss: 2.465602397918701\n",
            "Training Iteration 2231, Loss: 3.899623394012451\n",
            "Training Iteration 2232, Loss: 1.7379425764083862\n",
            "Training Iteration 2233, Loss: 2.3585095405578613\n",
            "Training Iteration 2234, Loss: 5.977047920227051\n",
            "Training Iteration 2235, Loss: 3.129727840423584\n",
            "Training Iteration 2236, Loss: 4.267677307128906\n",
            "Training Iteration 2237, Loss: 5.190623760223389\n",
            "Training Iteration 2238, Loss: 4.864560604095459\n",
            "Training Iteration 2239, Loss: 4.5655670166015625\n",
            "Training Iteration 2240, Loss: 4.260206699371338\n",
            "Training Iteration 2241, Loss: 5.727319240570068\n",
            "Training Iteration 2242, Loss: 5.423604488372803\n",
            "Training Iteration 2243, Loss: 5.416653156280518\n",
            "Training Iteration 2244, Loss: 4.18145751953125\n",
            "Training Iteration 2245, Loss: 3.6032094955444336\n",
            "Training Iteration 2246, Loss: 4.703736782073975\n",
            "Training Iteration 2247, Loss: 1.810122013092041\n",
            "Training Iteration 2248, Loss: 4.738951206207275\n",
            "Training Iteration 2249, Loss: 4.437897682189941\n",
            "Training Iteration 2250, Loss: 5.0764899253845215\n",
            "Training Iteration 2251, Loss: 2.7025022506713867\n",
            "Training Iteration 2252, Loss: 4.314422130584717\n",
            "Training Iteration 2253, Loss: 5.012141227722168\n",
            "Training Iteration 2254, Loss: 2.5220189094543457\n",
            "Training Iteration 2255, Loss: 2.639505386352539\n",
            "Training Iteration 2256, Loss: 4.3545708656311035\n",
            "Training Iteration 2257, Loss: 4.3238444328308105\n",
            "Training Iteration 2258, Loss: 4.274656295776367\n",
            "Training Iteration 2259, Loss: 7.234370231628418\n",
            "Training Iteration 2260, Loss: 2.0595383644104004\n",
            "Training Iteration 2261, Loss: 2.7669196128845215\n",
            "Training Iteration 2262, Loss: 1.297509789466858\n",
            "Training Iteration 2263, Loss: 3.1863439083099365\n",
            "Training Iteration 2264, Loss: 6.146017074584961\n",
            "Training Iteration 2265, Loss: 6.5908308029174805\n",
            "Training Iteration 2266, Loss: 5.011016845703125\n",
            "Training Iteration 2267, Loss: 5.839581489562988\n",
            "Training Iteration 2268, Loss: 5.94121789932251\n",
            "Training Iteration 2269, Loss: 3.8048782348632812\n",
            "Training Iteration 2270, Loss: 3.8902926445007324\n",
            "Training Iteration 2271, Loss: 4.663992881774902\n",
            "Training Iteration 2272, Loss: 3.6128766536712646\n",
            "Training Iteration 2273, Loss: 5.548443794250488\n",
            "Training Iteration 2274, Loss: 2.8032548427581787\n",
            "Training Iteration 2275, Loss: 4.257802486419678\n",
            "Training Iteration 2276, Loss: 3.813201427459717\n",
            "Training Iteration 2277, Loss: 3.3507730960845947\n",
            "Training Iteration 2278, Loss: 3.1358494758605957\n",
            "Training Iteration 2279, Loss: 7.813344955444336\n",
            "Training Iteration 2280, Loss: 6.2837018966674805\n",
            "Training Iteration 2281, Loss: 5.643030643463135\n",
            "Training Iteration 2282, Loss: 2.359281063079834\n",
            "Training Iteration 2283, Loss: 4.025320053100586\n",
            "Training Iteration 2284, Loss: 5.9503912925720215\n",
            "Training Iteration 2285, Loss: 4.728736877441406\n",
            "Training Iteration 2286, Loss: 4.441707611083984\n",
            "Training Iteration 2287, Loss: 5.247353553771973\n",
            "Training Iteration 2288, Loss: 3.3276031017303467\n",
            "Training Iteration 2289, Loss: 5.672451019287109\n",
            "Training Iteration 2290, Loss: 4.382161617279053\n",
            "Training Iteration 2291, Loss: 4.471734046936035\n",
            "Training Iteration 2292, Loss: 3.8082213401794434\n",
            "Training Iteration 2293, Loss: 3.832585573196411\n",
            "Training Iteration 2294, Loss: 3.3139002323150635\n",
            "Training Iteration 2295, Loss: 3.094235897064209\n",
            "Training Iteration 2296, Loss: 5.511150360107422\n",
            "Training Iteration 2297, Loss: 7.031448841094971\n",
            "Training Iteration 2298, Loss: 5.970773220062256\n",
            "Training Iteration 2299, Loss: 3.4866271018981934\n",
            "Training Iteration 2300, Loss: 3.422520399093628\n",
            "Training Iteration 2301, Loss: 2.7466259002685547\n",
            "Training Iteration 2302, Loss: 3.569990396499634\n",
            "Training Iteration 2303, Loss: 6.109572410583496\n",
            "Training Iteration 2304, Loss: 5.476522922515869\n",
            "Training Iteration 2305, Loss: 5.635408401489258\n",
            "Training Iteration 2306, Loss: 5.8587188720703125\n",
            "Training Iteration 2307, Loss: 7.379800796508789\n",
            "Training Iteration 2308, Loss: 4.311758995056152\n",
            "Training Iteration 2309, Loss: 8.315476417541504\n",
            "Training Iteration 2310, Loss: 7.888216495513916\n",
            "Training Iteration 2311, Loss: 4.2076592445373535\n",
            "Training Iteration 2312, Loss: 5.17493200302124\n",
            "Training Iteration 2313, Loss: 6.653266906738281\n",
            "Training Iteration 2314, Loss: 8.641998291015625\n",
            "Training Iteration 2315, Loss: 9.604219436645508\n",
            "Training Iteration 2316, Loss: 3.58132266998291\n",
            "Training Iteration 2317, Loss: 3.7731165885925293\n",
            "Training Iteration 2318, Loss: 5.684079170227051\n",
            "Training Iteration 2319, Loss: 2.1557674407958984\n",
            "Training Iteration 2320, Loss: 6.632355213165283\n",
            "Training Iteration 2321, Loss: 5.807318210601807\n",
            "Training Iteration 2322, Loss: 7.580469608306885\n",
            "Training Iteration 2323, Loss: 3.3186473846435547\n",
            "Training Iteration 2324, Loss: 5.2896881103515625\n",
            "Training Iteration 2325, Loss: 4.235899925231934\n",
            "Training Iteration 2326, Loss: 6.607354164123535\n",
            "Training Iteration 2327, Loss: 5.833308219909668\n",
            "Training Iteration 2328, Loss: 6.995431900024414\n",
            "Training Iteration 2329, Loss: 7.656460285186768\n",
            "Training Iteration 2330, Loss: 4.643953323364258\n",
            "Training Iteration 2331, Loss: 5.29738187789917\n",
            "Training Iteration 2332, Loss: 2.546140193939209\n",
            "Training Iteration 2333, Loss: 2.640104293823242\n",
            "Training Iteration 2334, Loss: 2.8405191898345947\n",
            "Training Iteration 2335, Loss: 2.9836902618408203\n",
            "Training Iteration 2336, Loss: 4.723830699920654\n",
            "Training Iteration 2337, Loss: 4.546224117279053\n",
            "Training Iteration 2338, Loss: 3.9893200397491455\n",
            "Training Iteration 2339, Loss: 4.962381839752197\n",
            "Training Iteration 2340, Loss: 3.6848182678222656\n",
            "Training Iteration 2341, Loss: 4.349349021911621\n",
            "Training Iteration 2342, Loss: 5.168505668640137\n",
            "Training Iteration 2343, Loss: 3.6520962715148926\n",
            "Training Iteration 2344, Loss: 3.7462902069091797\n",
            "Training Iteration 2345, Loss: 5.003589153289795\n",
            "Training Iteration 2346, Loss: 4.0308990478515625\n",
            "Training Iteration 2347, Loss: 3.907109498977661\n",
            "Training Iteration 2348, Loss: 7.91001033782959\n",
            "Training Iteration 2349, Loss: 2.8611035346984863\n",
            "Training Iteration 2350, Loss: 9.221941947937012\n",
            "Training Iteration 2351, Loss: 3.9775588512420654\n",
            "Training Iteration 2352, Loss: 8.515432357788086\n",
            "Training Iteration 2353, Loss: 5.788644790649414\n",
            "Training Iteration 2354, Loss: 8.756150245666504\n",
            "Training Iteration 2355, Loss: 3.936603546142578\n",
            "Training Iteration 2356, Loss: 3.8843069076538086\n",
            "Training Iteration 2357, Loss: 3.5531415939331055\n",
            "Training Iteration 2358, Loss: 9.155043601989746\n",
            "Training Iteration 2359, Loss: 6.137615203857422\n",
            "Training Iteration 2360, Loss: 2.7519688606262207\n",
            "Training Iteration 2361, Loss: 4.164732933044434\n",
            "Training Iteration 2362, Loss: 3.636035203933716\n",
            "Training Iteration 2363, Loss: 4.818042278289795\n",
            "Training Iteration 2364, Loss: 5.86829948425293\n",
            "Training Iteration 2365, Loss: 2.774609088897705\n",
            "Training Iteration 2366, Loss: 4.5896453857421875\n",
            "Training Iteration 2367, Loss: 4.446338653564453\n",
            "Training Iteration 2368, Loss: 4.7689971923828125\n",
            "Training Iteration 2369, Loss: 4.188652515411377\n",
            "Training Iteration 2370, Loss: 5.155610084533691\n",
            "Training Iteration 2371, Loss: 3.447601079940796\n",
            "Training Iteration 2372, Loss: 8.168314933776855\n",
            "Training Iteration 2373, Loss: 2.796250820159912\n",
            "Training Iteration 2374, Loss: 4.606927871704102\n",
            "Training Iteration 2375, Loss: 6.459285736083984\n",
            "Training Iteration 2376, Loss: 8.607744216918945\n",
            "Training Iteration 2377, Loss: 2.623084783554077\n",
            "Training Iteration 2378, Loss: 4.280585765838623\n",
            "Training Iteration 2379, Loss: 5.992162227630615\n",
            "Training Iteration 2380, Loss: 3.6971383094787598\n",
            "Training Iteration 2381, Loss: 2.241039752960205\n",
            "Training Iteration 2382, Loss: 6.2215576171875\n",
            "Training Iteration 2383, Loss: 3.057748794555664\n",
            "Training Iteration 2384, Loss: 3.715099573135376\n",
            "Training Iteration 2385, Loss: 7.018423080444336\n",
            "Training Iteration 2386, Loss: 7.667028427124023\n",
            "Training Iteration 2387, Loss: 5.958308696746826\n",
            "Training Iteration 2388, Loss: 3.062743663787842\n",
            "Training Iteration 2389, Loss: 2.6336517333984375\n",
            "Training Iteration 2390, Loss: 4.810738563537598\n",
            "Training Iteration 2391, Loss: 6.222897529602051\n",
            "Training Iteration 2392, Loss: 4.207901954650879\n",
            "Training Iteration 2393, Loss: 3.7545671463012695\n",
            "Training Iteration 2394, Loss: 3.928401470184326\n",
            "Training Iteration 2395, Loss: 4.191271781921387\n",
            "Training Iteration 2396, Loss: 7.96604585647583\n",
            "Training Iteration 2397, Loss: 4.057825565338135\n",
            "Training Iteration 2398, Loss: 2.98749041557312\n",
            "Training Iteration 2399, Loss: 2.64689040184021\n",
            "Training Iteration 2400, Loss: 3.961635112762451\n",
            "Training Iteration 2401, Loss: 5.142168998718262\n",
            "Training Iteration 2402, Loss: 3.3640942573547363\n",
            "Training Iteration 2403, Loss: 4.154403209686279\n",
            "Training Iteration 2404, Loss: 1.8553012609481812\n",
            "Training Iteration 2405, Loss: 2.9401073455810547\n",
            "Training Iteration 2406, Loss: 3.1558218002319336\n",
            "Training Iteration 2407, Loss: 2.2076847553253174\n",
            "Training Iteration 2408, Loss: 4.872705936431885\n",
            "Training Iteration 2409, Loss: 4.708939552307129\n",
            "Training Iteration 2410, Loss: 4.366980075836182\n",
            "Training Iteration 2411, Loss: 2.069373846054077\n",
            "Training Iteration 2412, Loss: 2.567166328430176\n",
            "Training Iteration 2413, Loss: 5.220404624938965\n",
            "Training Iteration 2414, Loss: 3.124582529067993\n",
            "Training Iteration 2415, Loss: 4.041827201843262\n",
            "Training Iteration 2416, Loss: 5.869678974151611\n",
            "Training Iteration 2417, Loss: 3.9323368072509766\n",
            "Training Iteration 2418, Loss: 4.425806999206543\n",
            "Training Iteration 2419, Loss: 5.621812343597412\n",
            "Training Iteration 2420, Loss: 5.559493064880371\n",
            "Training Iteration 2421, Loss: 5.827890396118164\n",
            "Training Iteration 2422, Loss: 6.812203407287598\n",
            "Training Iteration 2423, Loss: 5.10383415222168\n",
            "Training Iteration 2424, Loss: 4.105127334594727\n",
            "Training Iteration 2425, Loss: 4.617521286010742\n",
            "Training Iteration 2426, Loss: 6.027347087860107\n",
            "Training Iteration 2427, Loss: 3.615492343902588\n",
            "Training Iteration 2428, Loss: 5.373734474182129\n",
            "Training Iteration 2429, Loss: 5.856054782867432\n",
            "Training Iteration 2430, Loss: 3.525465965270996\n",
            "Training Iteration 2431, Loss: 3.1649329662323\n",
            "Training Iteration 2432, Loss: 6.644473075866699\n",
            "Training Iteration 2433, Loss: 2.0825648307800293\n",
            "Training Iteration 2434, Loss: 5.14737606048584\n",
            "Training Iteration 2435, Loss: 4.011363983154297\n",
            "Training Iteration 2436, Loss: 2.4210870265960693\n",
            "Training Iteration 2437, Loss: 5.877458095550537\n",
            "Training Iteration 2438, Loss: 7.76609468460083\n",
            "Training Iteration 2439, Loss: 2.9364545345306396\n",
            "Training Iteration 2440, Loss: 8.199016571044922\n",
            "Training Iteration 2441, Loss: 5.322706699371338\n",
            "Training Iteration 2442, Loss: 3.9149327278137207\n",
            "Training Iteration 2443, Loss: 11.201475143432617\n",
            "Training Iteration 2444, Loss: 3.4896535873413086\n",
            "Training Iteration 2445, Loss: 4.487277984619141\n",
            "Training Iteration 2446, Loss: 5.755606174468994\n",
            "Training Iteration 2447, Loss: 3.4483556747436523\n",
            "Training Iteration 2448, Loss: 9.939630508422852\n",
            "Training Iteration 2449, Loss: 4.37911319732666\n",
            "Training Iteration 2450, Loss: 5.695474147796631\n",
            "Training Iteration 2451, Loss: 3.391024589538574\n",
            "Training Iteration 2452, Loss: 3.6736888885498047\n",
            "Training Iteration 2453, Loss: 1.985384225845337\n",
            "Training Iteration 2454, Loss: 3.6689367294311523\n",
            "Training Iteration 2455, Loss: 2.8639209270477295\n",
            "Training Iteration 2456, Loss: 4.513474464416504\n",
            "Training Iteration 2457, Loss: 5.726593017578125\n",
            "Training Iteration 2458, Loss: 4.648982524871826\n",
            "Training Iteration 2459, Loss: 5.905256271362305\n",
            "Training Iteration 2460, Loss: 5.009038925170898\n",
            "Training Iteration 2461, Loss: 2.703674554824829\n",
            "Training Iteration 2462, Loss: 3.6763195991516113\n",
            "Training Iteration 2463, Loss: 5.881413459777832\n",
            "Training Iteration 2464, Loss: 6.477590560913086\n",
            "Training Iteration 2465, Loss: 2.454572916030884\n",
            "Training Iteration 2466, Loss: 3.776620864868164\n",
            "Training Iteration 2467, Loss: 5.4395551681518555\n",
            "Training Iteration 2468, Loss: 5.479833602905273\n",
            "Training Iteration 2469, Loss: 5.089140892028809\n",
            "Training Iteration 2470, Loss: 6.403280735015869\n",
            "Training Iteration 2471, Loss: 5.053749084472656\n",
            "Training Iteration 2472, Loss: 1.9428913593292236\n",
            "Training Iteration 2473, Loss: 3.903735876083374\n",
            "Training Iteration 2474, Loss: 3.825267791748047\n",
            "Training Iteration 2475, Loss: 3.7455291748046875\n",
            "Training Iteration 2476, Loss: 3.81313157081604\n",
            "Training Iteration 2477, Loss: 4.857726573944092\n",
            "Training Iteration 2478, Loss: 1.757049322128296\n",
            "Training Iteration 2479, Loss: 4.473289489746094\n",
            "Training Iteration 2480, Loss: 1.9586546421051025\n",
            "Training Iteration 2481, Loss: 6.108518600463867\n",
            "Training Iteration 2482, Loss: 5.152953624725342\n",
            "Training Iteration 2483, Loss: 9.539600372314453\n",
            "Training Iteration 2484, Loss: 3.749612331390381\n",
            "Training Iteration 2485, Loss: 7.184276103973389\n",
            "Training Iteration 2486, Loss: 2.6077613830566406\n",
            "Training Iteration 2487, Loss: 3.770463228225708\n",
            "Training Iteration 2488, Loss: 5.867934703826904\n",
            "Training Iteration 2489, Loss: 5.242383003234863\n",
            "Training Iteration 2490, Loss: 3.157471179962158\n",
            "Training Iteration 2491, Loss: 4.288182258605957\n",
            "Training Iteration 2492, Loss: 2.1827352046966553\n",
            "Training Iteration 2493, Loss: 3.4304440021514893\n",
            "Training Iteration 2494, Loss: 5.353315353393555\n",
            "Training Iteration 2495, Loss: 5.20303201675415\n",
            "Training Iteration 2496, Loss: 5.846031188964844\n",
            "Training Iteration 2497, Loss: 6.571163177490234\n",
            "Training Iteration 2498, Loss: 2.8614025115966797\n",
            "Training Iteration 2499, Loss: 5.510325908660889\n",
            "Training Iteration 2500, Loss: 5.127651214599609\n",
            "Training Iteration 2501, Loss: 3.7142038345336914\n",
            "Training Iteration 2502, Loss: 3.288588762283325\n",
            "Training Iteration 2503, Loss: 2.7246012687683105\n",
            "Training Iteration 2504, Loss: 4.994094371795654\n",
            "Training Iteration 2505, Loss: 5.019917964935303\n",
            "Training Iteration 2506, Loss: 4.136353015899658\n",
            "Training Iteration 2507, Loss: 4.663287162780762\n",
            "Training Iteration 2508, Loss: 5.32290506362915\n",
            "Training Iteration 2509, Loss: 5.291207790374756\n",
            "Training Iteration 2510, Loss: 5.801877975463867\n",
            "Training Iteration 2511, Loss: 3.206545829772949\n",
            "Training Iteration 2512, Loss: 2.690558910369873\n",
            "Training Iteration 2513, Loss: 6.026352405548096\n",
            "Training Iteration 2514, Loss: 5.149056434631348\n",
            "Training Iteration 2515, Loss: 3.6172709465026855\n",
            "Training Iteration 2516, Loss: 5.074098110198975\n",
            "Training Iteration 2517, Loss: 6.422446250915527\n",
            "Training Iteration 2518, Loss: 5.912659645080566\n",
            "Training Iteration 2519, Loss: 3.7624669075012207\n",
            "Training Iteration 2520, Loss: 5.179228782653809\n",
            "Training Iteration 2521, Loss: 2.1050171852111816\n",
            "Training Iteration 2522, Loss: 6.333755016326904\n",
            "Training Iteration 2523, Loss: 4.20728063583374\n",
            "Training Iteration 2524, Loss: 2.844167709350586\n",
            "Training Iteration 2525, Loss: 3.3173537254333496\n",
            "Training Iteration 2526, Loss: 5.073190212249756\n",
            "Training Iteration 2527, Loss: 4.419384002685547\n",
            "Training Iteration 2528, Loss: 2.1449637413024902\n",
            "Training Iteration 2529, Loss: 3.236453056335449\n",
            "Training Iteration 2530, Loss: 4.497726917266846\n",
            "Training Iteration 2531, Loss: 2.5689096450805664\n",
            "Training Iteration 2532, Loss: 3.1450679302215576\n",
            "Training Iteration 2533, Loss: 3.966203451156616\n",
            "Training Iteration 2534, Loss: 2.6174826622009277\n",
            "Training Iteration 2535, Loss: 3.3353028297424316\n",
            "Training Iteration 2536, Loss: 3.6380786895751953\n",
            "Training Iteration 2537, Loss: 5.947537422180176\n",
            "Training Iteration 2538, Loss: 3.210293769836426\n",
            "Training Iteration 2539, Loss: 4.422588348388672\n",
            "Training Iteration 2540, Loss: 4.477756500244141\n",
            "Training Iteration 2541, Loss: 4.109744548797607\n",
            "Training Iteration 2542, Loss: 3.223567008972168\n",
            "Training Iteration 2543, Loss: 3.666005849838257\n",
            "Training Iteration 2544, Loss: 4.156444072723389\n",
            "Training Iteration 2545, Loss: 2.534895658493042\n",
            "Training Iteration 2546, Loss: 3.664557695388794\n",
            "Training Iteration 2547, Loss: 6.13674783706665\n",
            "Training Iteration 2548, Loss: 5.5441765785217285\n",
            "Training Iteration 2549, Loss: 3.79068660736084\n",
            "Training Iteration 2550, Loss: 3.670701742172241\n",
            "Training Iteration 2551, Loss: 2.6558966636657715\n",
            "Training Iteration 2552, Loss: 5.075318336486816\n",
            "Training Iteration 2553, Loss: 3.3813343048095703\n",
            "Training Iteration 2554, Loss: 7.510835647583008\n",
            "Training Iteration 2555, Loss: 6.012877464294434\n",
            "Training Iteration 2556, Loss: 3.362842559814453\n",
            "Training Iteration 2557, Loss: 7.417287826538086\n",
            "Training Iteration 2558, Loss: 5.70220422744751\n",
            "Training Iteration 2559, Loss: 4.826968669891357\n",
            "Training Iteration 2560, Loss: 4.415828704833984\n",
            "Training Iteration 2561, Loss: 4.523618698120117\n",
            "Training Iteration 2562, Loss: 3.470923900604248\n",
            "Training Iteration 2563, Loss: 4.595037460327148\n",
            "Training Iteration 2564, Loss: 7.400845527648926\n",
            "Training Iteration 2565, Loss: 4.732001304626465\n",
            "Training Iteration 2566, Loss: 4.031778335571289\n",
            "Training Iteration 2567, Loss: 5.831486701965332\n",
            "Training Iteration 2568, Loss: 3.9425389766693115\n",
            "Training Iteration 2569, Loss: 4.060522556304932\n",
            "Training Iteration 2570, Loss: 3.300804615020752\n",
            "Training Iteration 2571, Loss: 4.5898895263671875\n",
            "Training Iteration 2572, Loss: 4.362510681152344\n",
            "Training Iteration 2573, Loss: 3.6744871139526367\n",
            "Training Iteration 2574, Loss: 6.009039878845215\n",
            "Training Iteration 2575, Loss: 3.378211498260498\n",
            "Training Iteration 2576, Loss: 3.9290695190429688\n",
            "Training Iteration 2577, Loss: 4.452002048492432\n",
            "Training Iteration 2578, Loss: 4.290968418121338\n",
            "Training Iteration 2579, Loss: 3.7536680698394775\n",
            "Training Iteration 2580, Loss: 2.4528918266296387\n",
            "Training Iteration 2581, Loss: 1.6938393115997314\n",
            "Training Iteration 2582, Loss: 6.233757019042969\n",
            "Training Iteration 2583, Loss: 5.664201259613037\n",
            "Training Iteration 2584, Loss: 5.917653560638428\n",
            "Training Iteration 2585, Loss: 4.169210433959961\n",
            "Training Iteration 2586, Loss: 3.251770257949829\n",
            "Training Iteration 2587, Loss: 6.388654708862305\n",
            "Training Iteration 2588, Loss: 6.6699981689453125\n",
            "Training Iteration 2589, Loss: 4.617255210876465\n",
            "Training Iteration 2590, Loss: 5.181973457336426\n",
            "Training Iteration 2591, Loss: 5.872814655303955\n",
            "Training Iteration 2592, Loss: 5.542825698852539\n",
            "Training Iteration 2593, Loss: 4.580202579498291\n",
            "Training Iteration 2594, Loss: 3.577936887741089\n",
            "Training Iteration 2595, Loss: 8.499080657958984\n",
            "Training Iteration 2596, Loss: 4.933671951293945\n",
            "Training Iteration 2597, Loss: 7.639739036560059\n",
            "Training Iteration 2598, Loss: 5.734956741333008\n",
            "Training Iteration 2599, Loss: 4.478893756866455\n",
            "Training Iteration 2600, Loss: 3.4750919342041016\n",
            "Training Iteration 2601, Loss: 2.894212245941162\n",
            "Training Iteration 2602, Loss: 8.321656227111816\n",
            "Training Iteration 2603, Loss: 4.076230525970459\n",
            "Training Iteration 2604, Loss: 9.65927791595459\n",
            "Training Iteration 2605, Loss: 6.227978229522705\n",
            "Training Iteration 2606, Loss: 3.7582955360412598\n",
            "Training Iteration 2607, Loss: 5.253055095672607\n",
            "Training Iteration 2608, Loss: 4.0593719482421875\n",
            "Training Iteration 2609, Loss: 6.078982353210449\n",
            "Training Iteration 2610, Loss: 3.7923390865325928\n",
            "Training Iteration 2611, Loss: 4.745654106140137\n",
            "Training Iteration 2612, Loss: 5.29790735244751\n",
            "Training Iteration 2613, Loss: 4.673304557800293\n",
            "Training Iteration 2614, Loss: 6.831033229827881\n",
            "Training Iteration 2615, Loss: 2.265594720840454\n",
            "Training Iteration 2616, Loss: 4.239975929260254\n",
            "Training Iteration 2617, Loss: 4.901268005371094\n",
            "Training Iteration 2618, Loss: 6.124769687652588\n",
            "Training Iteration 2619, Loss: 7.835878372192383\n",
            "Training Iteration 2620, Loss: 2.8120193481445312\n",
            "Training Iteration 2621, Loss: 5.089118003845215\n",
            "Training Iteration 2622, Loss: 4.880999565124512\n",
            "Training Iteration 2623, Loss: 5.795861721038818\n",
            "Training Iteration 2624, Loss: 3.102130174636841\n",
            "Training Iteration 2625, Loss: 4.946102619171143\n",
            "Training Iteration 2626, Loss: 4.9656982421875\n",
            "Training Iteration 2627, Loss: 5.904210567474365\n",
            "Training Iteration 2628, Loss: 5.5823211669921875\n",
            "Training Iteration 2629, Loss: 4.664279460906982\n",
            "Training Iteration 2630, Loss: 3.096472978591919\n",
            "Training Iteration 2631, Loss: 5.8627519607543945\n",
            "Training Iteration 2632, Loss: 4.517588138580322\n",
            "Training Iteration 2633, Loss: 4.2010626792907715\n",
            "Training Iteration 2634, Loss: 4.234432697296143\n",
            "Training Iteration 2635, Loss: 3.2740321159362793\n",
            "Training Iteration 2636, Loss: 2.2231907844543457\n",
            "Training Iteration 2637, Loss: 4.487494945526123\n",
            "Training Iteration 2638, Loss: 4.236312389373779\n",
            "Training Iteration 2639, Loss: 5.67333984375\n",
            "Training Iteration 2640, Loss: 8.581993103027344\n",
            "Training Iteration 2641, Loss: 2.6170568466186523\n",
            "Training Iteration 2642, Loss: 2.27701735496521\n",
            "Training Iteration 2643, Loss: 4.160700798034668\n",
            "Training Iteration 2644, Loss: 4.5104451179504395\n",
            "Training Iteration 2645, Loss: 3.5319693088531494\n",
            "Training Iteration 2646, Loss: 2.3250205516815186\n",
            "Training Iteration 2647, Loss: 1.3036093711853027\n",
            "Training Iteration 2648, Loss: 4.123885154724121\n",
            "Training Iteration 2649, Loss: 3.6057469844818115\n",
            "Training Iteration 2650, Loss: 4.990570545196533\n",
            "Training Iteration 2651, Loss: 2.6254918575286865\n",
            "Training Iteration 2652, Loss: 5.589022636413574\n",
            "Training Iteration 2653, Loss: 5.976920127868652\n",
            "Training Iteration 2654, Loss: 3.765725612640381\n",
            "Training Iteration 2655, Loss: 4.795483112335205\n",
            "Training Iteration 2656, Loss: 6.665537357330322\n",
            "Training Iteration 2657, Loss: 3.2614636421203613\n",
            "Training Iteration 2658, Loss: 2.03610897064209\n",
            "Training Iteration 2659, Loss: 2.286395788192749\n",
            "Training Iteration 2660, Loss: 5.412493705749512\n",
            "Training Iteration 2661, Loss: 2.6082377433776855\n",
            "Training Iteration 2662, Loss: 3.5897116661071777\n",
            "Training Iteration 2663, Loss: 3.9834933280944824\n",
            "Training Iteration 2664, Loss: 5.731060028076172\n",
            "Training Iteration 2665, Loss: 10.325297355651855\n",
            "Training Iteration 2666, Loss: 2.8237740993499756\n",
            "Training Iteration 2667, Loss: 2.3314707279205322\n",
            "Training Iteration 2668, Loss: 3.1432793140411377\n",
            "Training Iteration 2669, Loss: 2.9930689334869385\n",
            "Training Iteration 2670, Loss: 3.8899970054626465\n",
            "Training Iteration 2671, Loss: 4.3147196769714355\n",
            "Training Iteration 2672, Loss: 5.089151382446289\n",
            "Training Iteration 2673, Loss: 4.004184722900391\n",
            "Training Iteration 2674, Loss: 4.615301609039307\n",
            "Training Iteration 2675, Loss: 6.703344345092773\n",
            "Training Iteration 2676, Loss: 4.794601917266846\n",
            "Training Iteration 2677, Loss: 1.802935242652893\n",
            "Training Iteration 2678, Loss: 6.165578842163086\n",
            "Training Iteration 2679, Loss: 4.696990489959717\n",
            "Training Iteration 2680, Loss: 5.619229316711426\n",
            "Training Iteration 2681, Loss: 2.561328411102295\n",
            "Training Iteration 2682, Loss: 4.306126594543457\n",
            "Training Iteration 2683, Loss: 3.512080192565918\n",
            "Training Iteration 2684, Loss: 3.5399038791656494\n",
            "Training Iteration 2685, Loss: 1.388651728630066\n",
            "Training Iteration 2686, Loss: 2.31103253364563\n",
            "Training Iteration 2687, Loss: 6.480259418487549\n",
            "Training Iteration 2688, Loss: 2.815432548522949\n",
            "Training Iteration 2689, Loss: 5.162388324737549\n",
            "Training Iteration 2690, Loss: 5.336562156677246\n",
            "Training Iteration 2691, Loss: 3.7442078590393066\n",
            "Training Iteration 2692, Loss: 7.363879680633545\n",
            "Training Iteration 2693, Loss: 5.37515115737915\n",
            "Training Iteration 2694, Loss: 3.2970736026763916\n",
            "Training Iteration 2695, Loss: 3.8644542694091797\n",
            "Training Iteration 2696, Loss: 5.645662784576416\n",
            "Training Iteration 2697, Loss: 5.264240264892578\n",
            "Training Iteration 2698, Loss: 6.460285186767578\n",
            "Training Iteration 2699, Loss: 8.230621337890625\n",
            "Training Iteration 2700, Loss: 3.109994888305664\n",
            "Training Iteration 2701, Loss: 4.149167537689209\n",
            "Training Iteration 2702, Loss: 3.689387798309326\n",
            "Training Iteration 2703, Loss: 5.386096000671387\n",
            "Training Iteration 2704, Loss: 5.818955421447754\n",
            "Training Iteration 2705, Loss: 3.6638107299804688\n",
            "Training Iteration 2706, Loss: 9.740601539611816\n",
            "Training Iteration 2707, Loss: 5.609953880310059\n",
            "Training Iteration 2708, Loss: 8.183623313903809\n",
            "Training Iteration 2709, Loss: 3.1396470069885254\n",
            "Training Iteration 2710, Loss: 10.002169609069824\n",
            "Training Iteration 2711, Loss: 4.198302745819092\n",
            "Training Iteration 2712, Loss: 6.557641983032227\n",
            "Training Iteration 2713, Loss: 4.830887317657471\n",
            "Training Iteration 2714, Loss: 7.918881893157959\n",
            "Training Iteration 2715, Loss: 3.2009024620056152\n",
            "Training Iteration 2716, Loss: 3.6643118858337402\n",
            "Training Iteration 2717, Loss: 4.208102703094482\n",
            "Training Iteration 2718, Loss: 5.903186321258545\n",
            "Training Iteration 2719, Loss: 3.7713730335235596\n",
            "Training Iteration 2720, Loss: 3.2792840003967285\n",
            "Training Iteration 2721, Loss: 3.060640573501587\n",
            "Training Iteration 2722, Loss: 2.1064863204956055\n",
            "Training Iteration 2723, Loss: 5.633929252624512\n",
            "Training Iteration 2724, Loss: 3.8633928298950195\n",
            "Training Iteration 2725, Loss: 4.25314474105835\n",
            "Training Iteration 2726, Loss: 4.806077003479004\n",
            "Training Iteration 2727, Loss: 4.297612190246582\n",
            "Training Iteration 2728, Loss: 3.4045684337615967\n",
            "Training Iteration 2729, Loss: 2.216015100479126\n",
            "Training Iteration 2730, Loss: 9.309134483337402\n",
            "Training Iteration 2731, Loss: 4.599880218505859\n",
            "Training Iteration 2732, Loss: 3.7113852500915527\n",
            "Training Iteration 2733, Loss: 4.912404537200928\n",
            "Training Iteration 2734, Loss: 4.100464344024658\n",
            "Training Iteration 2735, Loss: 4.887713432312012\n",
            "Training Iteration 2736, Loss: 4.565673351287842\n",
            "Training Iteration 2737, Loss: 4.57392692565918\n",
            "Training Iteration 2738, Loss: 6.459907531738281\n",
            "Training Iteration 2739, Loss: 6.187695503234863\n",
            "Training Iteration 2740, Loss: 3.8644297122955322\n",
            "Training Iteration 2741, Loss: 5.515204906463623\n",
            "Training Iteration 2742, Loss: 5.148423194885254\n",
            "Training Iteration 2743, Loss: 1.833387017250061\n",
            "Training Iteration 2744, Loss: 8.053023338317871\n",
            "Training Iteration 2745, Loss: 1.579201340675354\n",
            "Training Iteration 2746, Loss: 5.827240943908691\n",
            "Training Iteration 2747, Loss: 2.995123863220215\n",
            "Training Iteration 2748, Loss: 4.1012163162231445\n",
            "Training Iteration 2749, Loss: 6.599314212799072\n",
            "Training Iteration 2750, Loss: 4.520998001098633\n",
            "Training Iteration 2751, Loss: 4.031204700469971\n",
            "Training Iteration 2752, Loss: 2.712019205093384\n",
            "Training Iteration 2753, Loss: 4.791778564453125\n",
            "Training Iteration 2754, Loss: 6.016362190246582\n",
            "Training Iteration 2755, Loss: 3.093187093734741\n",
            "Training Iteration 2756, Loss: 5.140347480773926\n",
            "Training Iteration 2757, Loss: 4.809009075164795\n",
            "Training Iteration 2758, Loss: 3.3859827518463135\n",
            "Training Iteration 2759, Loss: 4.061899185180664\n",
            "Training Iteration 2760, Loss: 3.4403696060180664\n",
            "Training Iteration 2761, Loss: 4.677940845489502\n",
            "Training Iteration 2762, Loss: 3.8661155700683594\n",
            "Training Iteration 2763, Loss: 3.773900032043457\n",
            "Training Iteration 2764, Loss: 4.527355670928955\n",
            "Training Iteration 2765, Loss: 5.231510639190674\n",
            "Training Iteration 2766, Loss: 4.3174872398376465\n",
            "Training Iteration 2767, Loss: 2.236337661743164\n",
            "Training Iteration 2768, Loss: 4.273506164550781\n",
            "Training Iteration 2769, Loss: 4.337596416473389\n",
            "Training Iteration 2770, Loss: 3.146892547607422\n",
            "Training Iteration 2771, Loss: 5.756182670593262\n",
            "Training Iteration 2772, Loss: 7.757822513580322\n",
            "Training Iteration 2773, Loss: 3.6625235080718994\n",
            "Training Iteration 2774, Loss: 5.228582859039307\n",
            "Training Iteration 2775, Loss: 4.044230937957764\n",
            "Training Iteration 2776, Loss: 2.1136386394500732\n",
            "Training Iteration 2777, Loss: 2.726166248321533\n",
            "Training Iteration 2778, Loss: 2.6317241191864014\n",
            "Training Iteration 2779, Loss: 1.0677084922790527\n",
            "Training Iteration 2780, Loss: 8.928041458129883\n",
            "Training Iteration 2781, Loss: 2.715121030807495\n",
            "Training Iteration 2782, Loss: 4.560643672943115\n",
            "Training Iteration 2783, Loss: 3.8149025440216064\n",
            "Training Iteration 2784, Loss: 3.2203257083892822\n",
            "Training Iteration 2785, Loss: 2.229067325592041\n",
            "Training Iteration 2786, Loss: 5.395118713378906\n",
            "Training Iteration 2787, Loss: 4.697926998138428\n",
            "Training Iteration 2788, Loss: 1.2807673215866089\n",
            "Training Iteration 2789, Loss: 6.127625942230225\n",
            "Training Iteration 2790, Loss: 3.7181873321533203\n",
            "Training Iteration 2791, Loss: 3.33601975440979\n",
            "Training Iteration 2792, Loss: 5.477739334106445\n",
            "Training Iteration 2793, Loss: 6.912165641784668\n",
            "Training Iteration 2794, Loss: 3.4479146003723145\n",
            "Training Iteration 2795, Loss: 4.466610908508301\n",
            "Training Iteration 2796, Loss: 2.66245174407959\n",
            "Training Iteration 2797, Loss: 4.8023905754089355\n",
            "Training Iteration 2798, Loss: 3.523268461227417\n",
            "Training Iteration 2799, Loss: 3.8161673545837402\n",
            "Training Iteration 2800, Loss: 5.122780799865723\n",
            "Training Iteration 2801, Loss: 5.461006164550781\n",
            "Training Iteration 2802, Loss: 4.770340919494629\n",
            "Training Iteration 2803, Loss: 2.636054754257202\n",
            "Training Iteration 2804, Loss: 6.067432880401611\n",
            "Training Iteration 2805, Loss: 1.5342392921447754\n",
            "Training Iteration 2806, Loss: 4.564042091369629\n",
            "Training Iteration 2807, Loss: 2.774711847305298\n",
            "Training Iteration 2808, Loss: 2.22287654876709\n",
            "Training Iteration 2809, Loss: 4.549411296844482\n",
            "Training Iteration 2810, Loss: 6.140172004699707\n",
            "Training Iteration 2811, Loss: 3.584320068359375\n",
            "Training Iteration 2812, Loss: 5.147567272186279\n",
            "Training Iteration 2813, Loss: 4.467001438140869\n",
            "Training Iteration 2814, Loss: 3.9239845275878906\n",
            "Training Iteration 2815, Loss: 6.313302516937256\n",
            "Training Iteration 2816, Loss: 3.9014205932617188\n",
            "Training Iteration 2817, Loss: 3.9183952808380127\n",
            "Training Iteration 2818, Loss: 5.815713882446289\n",
            "Training Iteration 2819, Loss: 6.6717448234558105\n",
            "Training Iteration 2820, Loss: 6.16574764251709\n",
            "Training Iteration 2821, Loss: 5.204482078552246\n",
            "Training Iteration 2822, Loss: 5.494279861450195\n",
            "Training Iteration 2823, Loss: 3.9208638668060303\n",
            "Training Iteration 2824, Loss: 6.436448097229004\n",
            "Training Iteration 2825, Loss: 7.636733531951904\n",
            "Training Iteration 2826, Loss: 4.094122886657715\n",
            "Training Iteration 2827, Loss: 4.495733737945557\n",
            "Training Iteration 2828, Loss: 6.05332088470459\n",
            "Training Iteration 2829, Loss: 5.492756366729736\n",
            "Training Iteration 2830, Loss: 3.676344633102417\n",
            "Training Iteration 2831, Loss: 3.7092554569244385\n",
            "Training Iteration 2832, Loss: 4.292498588562012\n",
            "Training Iteration 2833, Loss: 3.137382745742798\n",
            "Training Iteration 2834, Loss: 4.375522613525391\n",
            "Training Iteration 2835, Loss: 7.190792083740234\n",
            "Training Iteration 2836, Loss: 4.2055277824401855\n",
            "Training Iteration 2837, Loss: 4.36995792388916\n",
            "Training Iteration 2838, Loss: 2.081165313720703\n",
            "Training Iteration 2839, Loss: 2.7673065662384033\n",
            "Training Iteration 2840, Loss: 7.602350234985352\n",
            "Training Iteration 2841, Loss: 6.088192939758301\n",
            "Training Iteration 2842, Loss: 3.6948013305664062\n",
            "Training Iteration 2843, Loss: 2.6340296268463135\n",
            "Training Iteration 2844, Loss: 4.443829536437988\n",
            "Training Iteration 2845, Loss: 5.07861852645874\n",
            "Training Iteration 2846, Loss: 5.600635528564453\n",
            "Training Iteration 2847, Loss: 5.545715808868408\n",
            "Training Iteration 2848, Loss: 3.222429037094116\n",
            "Training Iteration 2849, Loss: 3.6153037548065186\n",
            "Training Iteration 2850, Loss: 4.647459506988525\n",
            "Training Iteration 2851, Loss: 3.1828746795654297\n",
            "Training Iteration 2852, Loss: 2.5900020599365234\n",
            "Training Iteration 2853, Loss: 4.561419486999512\n",
            "Training Iteration 2854, Loss: 5.6182332038879395\n",
            "Training Iteration 2855, Loss: 3.3937149047851562\n",
            "Training Iteration 2856, Loss: 2.307044506072998\n",
            "Training Iteration 2857, Loss: 3.173809051513672\n",
            "Training Iteration 2858, Loss: 3.7439076900482178\n",
            "Training Iteration 2859, Loss: 6.122827529907227\n",
            "Training Iteration 2860, Loss: 3.130763292312622\n",
            "Training Iteration 2861, Loss: 6.203284740447998\n",
            "Training Iteration 2862, Loss: 6.908642768859863\n",
            "Training Iteration 2863, Loss: 2.512714147567749\n",
            "Training Iteration 2864, Loss: 3.071324348449707\n",
            "Training Iteration 2865, Loss: 3.6494131088256836\n",
            "Training Iteration 2866, Loss: 3.337186336517334\n",
            "Training Iteration 2867, Loss: 4.764044284820557\n",
            "Training Iteration 2868, Loss: 4.283285140991211\n",
            "Training Iteration 2869, Loss: 3.2140583992004395\n",
            "Training Iteration 2870, Loss: 5.6877617835998535\n",
            "Training Iteration 2871, Loss: 4.92129373550415\n",
            "Training Iteration 2872, Loss: 2.0618643760681152\n",
            "Training Iteration 2873, Loss: 4.542688846588135\n",
            "Training Iteration 2874, Loss: 4.323272228240967\n",
            "Training Iteration 2875, Loss: 2.5276222229003906\n",
            "Training Iteration 2876, Loss: 2.658933401107788\n",
            "Training Iteration 2877, Loss: 3.7056479454040527\n",
            "Training Iteration 2878, Loss: 3.1660842895507812\n",
            "Training Iteration 2879, Loss: 3.2583839893341064\n",
            "Training Iteration 2880, Loss: 6.026342868804932\n",
            "Training Iteration 2881, Loss: 2.3504576683044434\n",
            "Training Iteration 2882, Loss: 1.965010166168213\n",
            "Training Iteration 2883, Loss: 2.9134292602539062\n",
            "Training Iteration 2884, Loss: 3.8510732650756836\n",
            "Training Iteration 2885, Loss: 5.417538166046143\n",
            "Training Iteration 2886, Loss: 1.9642021656036377\n",
            "Training Iteration 2887, Loss: 3.7628469467163086\n",
            "Training Iteration 2888, Loss: 6.158020496368408\n",
            "Training Iteration 2889, Loss: 4.699643135070801\n",
            "Training Iteration 2890, Loss: 2.665846824645996\n",
            "Training Iteration 2891, Loss: 6.483694076538086\n",
            "Training Iteration 2892, Loss: 4.10521936416626\n",
            "Training Iteration 2893, Loss: 8.576187133789062\n",
            "Training Iteration 2894, Loss: 5.2518310546875\n",
            "Training Iteration 2895, Loss: 3.939858913421631\n",
            "Training Iteration 2896, Loss: 5.409312725067139\n",
            "Training Iteration 2897, Loss: 3.4988839626312256\n",
            "Training Iteration 2898, Loss: 3.1122994422912598\n",
            "Training Iteration 2899, Loss: 5.512533664703369\n",
            "Training Iteration 2900, Loss: 4.324777126312256\n",
            "Training Iteration 2901, Loss: 3.4545891284942627\n",
            "Training Iteration 2902, Loss: 4.59969425201416\n",
            "Training Iteration 2903, Loss: 2.945425033569336\n",
            "Training Iteration 2904, Loss: 6.321510314941406\n",
            "Training Iteration 2905, Loss: 2.9653639793395996\n",
            "Training Iteration 2906, Loss: 3.4625139236450195\n",
            "Training Iteration 2907, Loss: 3.8274779319763184\n",
            "Training Iteration 2908, Loss: 3.884786605834961\n",
            "Training Iteration 2909, Loss: 3.0413646697998047\n",
            "Training Iteration 2910, Loss: 5.519048690795898\n",
            "Training Iteration 2911, Loss: 8.646210670471191\n",
            "Training Iteration 2912, Loss: 8.52536392211914\n",
            "Training Iteration 2913, Loss: 3.94824481010437\n",
            "Training Iteration 2914, Loss: 4.595430374145508\n",
            "Training Iteration 2915, Loss: 4.646188735961914\n",
            "Training Iteration 2916, Loss: 4.967576026916504\n",
            "Training Iteration 2917, Loss: 3.77683162689209\n",
            "Training Iteration 2918, Loss: 5.088449478149414\n",
            "Training Iteration 2919, Loss: 5.131651401519775\n",
            "Training Iteration 2920, Loss: 4.950902938842773\n",
            "Training Iteration 2921, Loss: 3.241258382797241\n",
            "Training Iteration 2922, Loss: 4.302986145019531\n",
            "Training Iteration 2923, Loss: 3.389679193496704\n",
            "Training Iteration 2924, Loss: 2.1533291339874268\n",
            "Training Iteration 2925, Loss: 3.705758571624756\n",
            "Training Iteration 2926, Loss: 4.790950775146484\n",
            "Training Iteration 2927, Loss: 7.080013275146484\n",
            "Training Iteration 2928, Loss: 2.284804105758667\n",
            "Training Iteration 2929, Loss: 3.8698832988739014\n",
            "Training Iteration 2930, Loss: 5.494760513305664\n",
            "Training Iteration 2931, Loss: 4.012543201446533\n",
            "Training Iteration 2932, Loss: 8.035529136657715\n",
            "Training Iteration 2933, Loss: 5.729564666748047\n",
            "Training Iteration 2934, Loss: 5.093523025512695\n",
            "Training Iteration 2935, Loss: 3.3822221755981445\n",
            "Training Iteration 2936, Loss: 1.5329961776733398\n",
            "Training Iteration 2937, Loss: 3.9938278198242188\n",
            "Training Iteration 2938, Loss: 0.5212494134902954\n",
            "Training Iteration 2939, Loss: 7.73585319519043\n",
            "Training Iteration 2940, Loss: 5.577349662780762\n",
            "Training Iteration 2941, Loss: 5.414926528930664\n",
            "Training Iteration 2942, Loss: 2.1703412532806396\n",
            "Training Iteration 2943, Loss: 4.123648166656494\n",
            "Training Iteration 2944, Loss: 3.880378007888794\n",
            "Training Iteration 2945, Loss: 6.573039531707764\n",
            "Training Iteration 2946, Loss: 4.786440372467041\n",
            "Training Iteration 2947, Loss: 5.598844528198242\n",
            "Training Iteration 2948, Loss: 5.705543041229248\n",
            "Training Iteration 2949, Loss: 3.3298499584198\n",
            "Training Iteration 2950, Loss: 0.5298896431922913\n",
            "Training Iteration 2951, Loss: 3.9107589721679688\n",
            "Training Iteration 2952, Loss: 4.598905563354492\n",
            "Training Iteration 2953, Loss: 5.147456645965576\n",
            "Training Iteration 2954, Loss: 4.295344352722168\n",
            "Training Iteration 2955, Loss: 7.17792272567749\n",
            "Training Iteration 2956, Loss: 2.192643880844116\n",
            "Training Iteration 2957, Loss: 5.875392436981201\n",
            "Training Iteration 2958, Loss: 4.29280948638916\n",
            "Training Iteration 2959, Loss: 3.905662775039673\n",
            "Training Iteration 2960, Loss: 8.193894386291504\n",
            "Training Iteration 2961, Loss: 3.707930326461792\n",
            "Training Iteration 2962, Loss: 8.702409744262695\n",
            "Training Iteration 2963, Loss: 7.475482940673828\n",
            "Training Iteration 2964, Loss: 7.788304805755615\n",
            "Training Iteration 2965, Loss: 1.848095417022705\n",
            "Training Iteration 2966, Loss: 5.825873374938965\n",
            "Training Iteration 2967, Loss: 4.087765693664551\n",
            "Training Iteration 2968, Loss: 4.52039098739624\n",
            "Training Iteration 2969, Loss: 2.448662757873535\n",
            "Training Iteration 2970, Loss: 2.4471821784973145\n",
            "Training Iteration 2971, Loss: 5.51019811630249\n",
            "Training Iteration 2972, Loss: 4.779544830322266\n",
            "Training Iteration 2973, Loss: 7.651289463043213\n",
            "Training Iteration 2974, Loss: 6.075985431671143\n",
            "Training Iteration 2975, Loss: 2.3987302780151367\n",
            "Training Iteration 2976, Loss: 7.582226276397705\n",
            "Training Iteration 2977, Loss: 6.83603048324585\n",
            "Training Iteration 2978, Loss: 5.378782272338867\n",
            "Training Iteration 2979, Loss: 4.812497615814209\n",
            "Training Iteration 2980, Loss: 6.042570114135742\n",
            "Training Iteration 2981, Loss: 4.727325439453125\n",
            "Training Iteration 2982, Loss: 3.4787139892578125\n",
            "Training Iteration 2983, Loss: 3.9639785289764404\n",
            "Training Iteration 2984, Loss: 4.1151580810546875\n",
            "Training Iteration 2985, Loss: 4.581882953643799\n",
            "Training Iteration 2986, Loss: 3.1310722827911377\n",
            "Training Iteration 2987, Loss: 3.0065836906433105\n",
            "Training Iteration 2988, Loss: 4.575267791748047\n",
            "Training Iteration 2989, Loss: 4.416572093963623\n",
            "Training Iteration 2990, Loss: 5.632139682769775\n",
            "Training Iteration 2991, Loss: 4.476795673370361\n",
            "Training Iteration 2992, Loss: 1.7952942848205566\n",
            "Training Iteration 2993, Loss: 3.4022960662841797\n",
            "Training Iteration 2994, Loss: 4.830778121948242\n",
            "Training Iteration 2995, Loss: 4.6942925453186035\n",
            "Training Iteration 2996, Loss: 3.517608165740967\n",
            "Training Iteration 2997, Loss: 11.997185707092285\n",
            "Training Iteration 2998, Loss: 4.196384906768799\n",
            "Training Iteration 2999, Loss: 5.544219017028809\n",
            "Training Iteration 3000, Loss: 3.492217540740967\n",
            "Training Iteration 3001, Loss: 5.091694355010986\n",
            "Training Iteration 3002, Loss: 2.847921371459961\n",
            "Training Iteration 3003, Loss: 6.320066452026367\n",
            "Training Iteration 3004, Loss: 4.3537397384643555\n",
            "Training Iteration 3005, Loss: 7.6649980545043945\n",
            "Training Iteration 3006, Loss: 3.8678956031799316\n",
            "Training Iteration 3007, Loss: 5.975276470184326\n",
            "Training Iteration 3008, Loss: 4.07588005065918\n",
            "Training Iteration 3009, Loss: 3.1644184589385986\n",
            "Training Iteration 3010, Loss: 3.5492329597473145\n",
            "Training Iteration 3011, Loss: 2.682476282119751\n",
            "Training Iteration 3012, Loss: 4.255296230316162\n",
            "Training Iteration 3013, Loss: 4.1850433349609375\n",
            "Training Iteration 3014, Loss: 4.003734111785889\n",
            "Training Iteration 3015, Loss: 5.571413040161133\n",
            "Training Iteration 3016, Loss: 3.4669008255004883\n",
            "Training Iteration 3017, Loss: 3.052692413330078\n",
            "Training Iteration 3018, Loss: 4.666626453399658\n",
            "Training Iteration 3019, Loss: 6.558907985687256\n",
            "Training Iteration 3020, Loss: 5.0553483963012695\n",
            "Training Iteration 3021, Loss: 3.760852813720703\n",
            "Training Iteration 3022, Loss: 3.286552667617798\n",
            "Training Iteration 3023, Loss: 3.8287370204925537\n",
            "Training Iteration 3024, Loss: 7.38866662979126\n",
            "Training Iteration 3025, Loss: 5.2315592765808105\n",
            "Training Iteration 3026, Loss: 6.957724571228027\n",
            "Training Iteration 3027, Loss: 8.39871883392334\n",
            "Training Iteration 3028, Loss: 3.913602113723755\n",
            "Training Iteration 3029, Loss: 4.841447353363037\n",
            "Training Iteration 3030, Loss: 4.473562717437744\n",
            "Training Iteration 3031, Loss: 5.163669109344482\n",
            "Training Iteration 3032, Loss: 3.5290935039520264\n",
            "Training Iteration 3033, Loss: 5.698463439941406\n",
            "Training Iteration 3034, Loss: 4.989521026611328\n",
            "Training Iteration 3035, Loss: 4.911033630371094\n",
            "Training Iteration 3036, Loss: 2.716911792755127\n",
            "Training Iteration 3037, Loss: 4.262194633483887\n",
            "Training Iteration 3038, Loss: 4.475922107696533\n",
            "Training Iteration 3039, Loss: 4.992832183837891\n",
            "Training Iteration 3040, Loss: 2.058107376098633\n",
            "Training Iteration 3041, Loss: 8.756695747375488\n",
            "Training Iteration 3042, Loss: 5.126161098480225\n",
            "Training Iteration 3043, Loss: 6.7909746170043945\n",
            "Training Iteration 3044, Loss: 3.6452834606170654\n",
            "Training Iteration 3045, Loss: 3.2357757091522217\n",
            "Training Iteration 3046, Loss: 6.743899822235107\n",
            "Training Iteration 3047, Loss: 3.789313316345215\n",
            "Training Iteration 3048, Loss: 7.380731582641602\n",
            "Training Iteration 3049, Loss: 3.847480058670044\n",
            "Training Iteration 3050, Loss: 3.13279390335083\n",
            "Training Iteration 3051, Loss: 3.161669969558716\n",
            "Training Iteration 3052, Loss: 3.8366892337799072\n",
            "Training Iteration 3053, Loss: 7.523687362670898\n",
            "Training Iteration 3054, Loss: 5.393414497375488\n",
            "Training Iteration 3055, Loss: 3.0707814693450928\n",
            "Training Iteration 3056, Loss: 3.8581342697143555\n",
            "Training Iteration 3057, Loss: 1.8207639455795288\n",
            "Training Iteration 3058, Loss: 2.899397611618042\n",
            "Training Iteration 3059, Loss: 6.341385364532471\n",
            "Training Iteration 3060, Loss: 4.264003753662109\n",
            "Training Iteration 3061, Loss: 2.684356689453125\n",
            "Training Iteration 3062, Loss: 3.5507123470306396\n",
            "Training Iteration 3063, Loss: 3.5949599742889404\n",
            "Training Iteration 3064, Loss: 2.765258312225342\n",
            "Training Iteration 3065, Loss: 3.544009208679199\n",
            "Training Iteration 3066, Loss: 5.14448881149292\n",
            "Training Iteration 3067, Loss: 1.4561203718185425\n",
            "Training Iteration 3068, Loss: 4.409771919250488\n",
            "Training Iteration 3069, Loss: 2.853398561477661\n",
            "Training Iteration 3070, Loss: 5.6402788162231445\n",
            "Training Iteration 3071, Loss: 3.777474880218506\n",
            "Training Iteration 3072, Loss: 4.0043206214904785\n",
            "Training Iteration 3073, Loss: 7.800374984741211\n",
            "Training Iteration 3074, Loss: 5.041226387023926\n",
            "Training Iteration 3075, Loss: 4.325565338134766\n",
            "Training Iteration 3076, Loss: 5.593676567077637\n",
            "Training Iteration 3077, Loss: 4.494814395904541\n",
            "Training Iteration 3078, Loss: 6.065434455871582\n",
            "Training Iteration 3079, Loss: 3.9723753929138184\n",
            "Training Iteration 3080, Loss: 3.5786240100860596\n",
            "Training Iteration 3081, Loss: 5.8493146896362305\n",
            "Training Iteration 3082, Loss: 4.112009525299072\n",
            "Training Iteration 3083, Loss: 4.335079669952393\n",
            "Training Iteration 3084, Loss: 2.8210670948028564\n",
            "Training Iteration 3085, Loss: 3.281888961791992\n",
            "Training Iteration 3086, Loss: 4.508594036102295\n",
            "Training Iteration 3087, Loss: 3.972015380859375\n",
            "Training Iteration 3088, Loss: 6.030434608459473\n",
            "Training Iteration 3089, Loss: 4.662718296051025\n",
            "Training Iteration 3090, Loss: 2.469346523284912\n",
            "Training Iteration 3091, Loss: 3.0444252490997314\n",
            "Training Iteration 3092, Loss: 3.5058727264404297\n",
            "Training Iteration 3093, Loss: 4.99204683303833\n",
            "Training Iteration 3094, Loss: 5.656331539154053\n",
            "Training Iteration 3095, Loss: 8.351754188537598\n",
            "Training Iteration 3096, Loss: 7.731503009796143\n",
            "Training Iteration 3097, Loss: 2.636852741241455\n",
            "Training Iteration 3098, Loss: 4.020554542541504\n",
            "Training Iteration 3099, Loss: 6.418931007385254\n",
            "Training Iteration 3100, Loss: 2.500119686126709\n",
            "Training Iteration 3101, Loss: 1.9859352111816406\n",
            "Training Iteration 3102, Loss: 5.5233306884765625\n",
            "Training Iteration 3103, Loss: 1.6543349027633667\n",
            "Training Iteration 3104, Loss: 8.618942260742188\n",
            "Training Iteration 3105, Loss: 2.1063039302825928\n",
            "Training Iteration 3106, Loss: 5.878173828125\n",
            "Training Iteration 3107, Loss: 3.5646278858184814\n",
            "Training Iteration 3108, Loss: 2.1675806045532227\n",
            "Training Iteration 3109, Loss: 3.840695381164551\n",
            "Training Iteration 3110, Loss: 3.190464735031128\n",
            "Training Iteration 3111, Loss: 4.873013973236084\n",
            "Training Iteration 3112, Loss: 2.8133764266967773\n",
            "Training Iteration 3113, Loss: 4.069201469421387\n",
            "Training Iteration 3114, Loss: 4.634883880615234\n",
            "Training Iteration 3115, Loss: 4.991735458374023\n",
            "Training Iteration 3116, Loss: 3.3621959686279297\n",
            "Training Iteration 3117, Loss: 3.362539768218994\n",
            "Training Iteration 3118, Loss: 5.517489433288574\n",
            "Training Iteration 3119, Loss: 2.322969913482666\n",
            "Training Iteration 3120, Loss: 8.918811798095703\n",
            "Training Iteration 3121, Loss: 5.434392929077148\n",
            "Training Iteration 3122, Loss: 12.801469802856445\n",
            "Training Iteration 3123, Loss: 4.378162860870361\n",
            "Training Iteration 3124, Loss: 3.8198232650756836\n",
            "Training Iteration 3125, Loss: 2.338083028793335\n",
            "Training Iteration 3126, Loss: 4.779577255249023\n",
            "Training Iteration 3127, Loss: 6.071749210357666\n",
            "Training Iteration 3128, Loss: 4.563244819641113\n",
            "Training Iteration 3129, Loss: 5.030861854553223\n",
            "Training Iteration 3130, Loss: 6.82952356338501\n",
            "Training Iteration 3131, Loss: 2.6776630878448486\n",
            "Training Iteration 3132, Loss: 3.0504560470581055\n",
            "Training Iteration 3133, Loss: 4.5354533195495605\n",
            "Training Iteration 3134, Loss: 7.567574977874756\n",
            "Training Iteration 3135, Loss: 4.59731388092041\n",
            "Training Iteration 3136, Loss: 6.090482711791992\n",
            "Training Iteration 3137, Loss: 4.500336647033691\n",
            "Training Iteration 3138, Loss: 4.009214401245117\n",
            "Training Iteration 3139, Loss: 3.1136951446533203\n",
            "Training Iteration 3140, Loss: 7.7778425216674805\n",
            "Training Iteration 3141, Loss: 6.439572334289551\n",
            "Training Iteration 3142, Loss: 5.247403144836426\n",
            "Training Iteration 3143, Loss: 9.823740005493164\n",
            "Training Iteration 3144, Loss: 2.599498987197876\n",
            "Training Iteration 3145, Loss: 3.429462432861328\n",
            "Training Iteration 3146, Loss: 6.4696173667907715\n",
            "Training Iteration 3147, Loss: 7.081130027770996\n",
            "Training Iteration 3148, Loss: 2.6645638942718506\n",
            "Training Iteration 3149, Loss: 3.8974082469940186\n",
            "Training Iteration 3150, Loss: 7.884824275970459\n",
            "Training Iteration 3151, Loss: 3.3660736083984375\n",
            "Training Iteration 3152, Loss: 5.20916748046875\n",
            "Training Iteration 3153, Loss: 5.401466369628906\n",
            "Training Iteration 3154, Loss: 10.050593376159668\n",
            "Training Iteration 3155, Loss: 9.628076553344727\n",
            "Training Iteration 3156, Loss: 2.496788263320923\n",
            "Training Iteration 3157, Loss: 6.531698226928711\n",
            "Training Iteration 3158, Loss: 2.430079221725464\n",
            "Training Iteration 3159, Loss: 3.7275161743164062\n",
            "Training Iteration 3160, Loss: 4.355316162109375\n",
            "Training Iteration 3161, Loss: 6.669918060302734\n",
            "Training Iteration 3162, Loss: 12.321377754211426\n",
            "Training Iteration 3163, Loss: 3.8893210887908936\n",
            "Training Iteration 3164, Loss: 7.571227073669434\n",
            "Training Iteration 3165, Loss: 3.8272247314453125\n",
            "Training Iteration 3166, Loss: 2.214465856552124\n",
            "Training Iteration 3167, Loss: 8.486005783081055\n",
            "Training Iteration 3168, Loss: 4.860896110534668\n",
            "Training Iteration 3169, Loss: 3.6877260208129883\n",
            "Training Iteration 3170, Loss: 5.596874237060547\n",
            "Training Iteration 3171, Loss: 4.9693074226379395\n",
            "Training Iteration 3172, Loss: 2.5598387718200684\n",
            "Training Iteration 3173, Loss: 3.4518587589263916\n",
            "Training Iteration 3174, Loss: 2.998178005218506\n",
            "Training Iteration 3175, Loss: 6.48065185546875\n",
            "Training Iteration 3176, Loss: 2.619274616241455\n",
            "Training Iteration 3177, Loss: 4.712040424346924\n",
            "Training Iteration 3178, Loss: 4.383957862854004\n",
            "Training Iteration 3179, Loss: 3.587089776992798\n",
            "Training Iteration 3180, Loss: 6.091448783874512\n",
            "Training Iteration 3181, Loss: 6.161925315856934\n",
            "Training Iteration 3182, Loss: 5.103808403015137\n",
            "Training Iteration 3183, Loss: 6.192920684814453\n",
            "Training Iteration 3184, Loss: 6.772561550140381\n",
            "Training Iteration 3185, Loss: 5.284997940063477\n",
            "Training Iteration 3186, Loss: 6.283158302307129\n",
            "Training Iteration 3187, Loss: 3.5308990478515625\n",
            "Training Iteration 3188, Loss: 4.5544257164001465\n",
            "Training Iteration 3189, Loss: 3.0814857482910156\n",
            "Training Iteration 3190, Loss: 7.460670471191406\n",
            "Training Iteration 3191, Loss: 6.949265956878662\n",
            "Training Iteration 3192, Loss: 3.6782238483428955\n",
            "Training Iteration 3193, Loss: 3.128812551498413\n",
            "Training Iteration 3194, Loss: 3.4354629516601562\n",
            "Training Iteration 3195, Loss: 2.5354766845703125\n",
            "Training Iteration 3196, Loss: 3.219780445098877\n",
            "Training Iteration 3197, Loss: 3.002242088317871\n",
            "Training Iteration 3198, Loss: 4.867278575897217\n",
            "Training Iteration 3199, Loss: 5.24367094039917\n",
            "Training Iteration 3200, Loss: 6.017854690551758\n",
            "Training Iteration 3201, Loss: 1.6882661581039429\n",
            "Training Iteration 3202, Loss: 2.6434412002563477\n",
            "Training Iteration 3203, Loss: 2.308654308319092\n",
            "Training Iteration 3204, Loss: 1.9591485261917114\n",
            "Training Iteration 3205, Loss: 7.258342742919922\n",
            "Training Iteration 3206, Loss: 5.120077610015869\n",
            "Training Iteration 3207, Loss: 3.4870834350585938\n",
            "Training Iteration 3208, Loss: 2.913461685180664\n",
            "Training Iteration 3209, Loss: 4.216569423675537\n",
            "Training Iteration 3210, Loss: 2.753889560699463\n",
            "Training Iteration 3211, Loss: 3.887877941131592\n",
            "Training Iteration 3212, Loss: 6.428121566772461\n",
            "Training Iteration 3213, Loss: 4.137781143188477\n",
            "Training Iteration 3214, Loss: 4.298099994659424\n",
            "Training Iteration 3215, Loss: 3.727415084838867\n",
            "Training Iteration 3216, Loss: 3.8809800148010254\n",
            "Training Iteration 3217, Loss: 3.790268898010254\n",
            "Training Iteration 3218, Loss: 4.164538860321045\n",
            "Training Iteration 3219, Loss: 3.0069503784179688\n",
            "Training Iteration 3220, Loss: 4.85073184967041\n",
            "Training Iteration 3221, Loss: 7.795795917510986\n",
            "Training Iteration 3222, Loss: 6.686131477355957\n",
            "Training Iteration 3223, Loss: 4.007440090179443\n",
            "Training Iteration 3224, Loss: 7.067380905151367\n",
            "Training Iteration 3225, Loss: 4.290722846984863\n",
            "Training Iteration 3226, Loss: 8.523078918457031\n",
            "Training Iteration 3227, Loss: 10.098334312438965\n",
            "Training Iteration 3228, Loss: 4.553764343261719\n",
            "Training Iteration 3229, Loss: 4.727303981781006\n",
            "Training Iteration 3230, Loss: 2.785719394683838\n",
            "Training Iteration 3231, Loss: 4.10089635848999\n",
            "Training Iteration 3232, Loss: 7.0719428062438965\n",
            "Training Iteration 3233, Loss: 5.429736614227295\n",
            "Training Iteration 3234, Loss: 6.684290409088135\n",
            "Training Iteration 3235, Loss: 5.718156814575195\n",
            "Training Iteration 3236, Loss: 8.125043869018555\n",
            "Training Iteration 3237, Loss: 5.924931526184082\n",
            "Training Iteration 3238, Loss: 6.066963195800781\n",
            "Training Iteration 3239, Loss: 2.3789079189300537\n",
            "Training Iteration 3240, Loss: 4.554571628570557\n",
            "Training Iteration 3241, Loss: 4.931125164031982\n",
            "Training Iteration 3242, Loss: 6.482341766357422\n",
            "Training Iteration 3243, Loss: 6.703792095184326\n",
            "Training Iteration 3244, Loss: 8.588665962219238\n",
            "Training Iteration 3245, Loss: 3.1406571865081787\n",
            "Training Iteration 3246, Loss: 6.43253755569458\n",
            "Training Iteration 3247, Loss: 4.998734951019287\n",
            "Training Iteration 3248, Loss: 5.571023941040039\n",
            "Training Iteration 3249, Loss: 4.919106483459473\n",
            "Training Iteration 3250, Loss: 3.9536943435668945\n",
            "Training Iteration 3251, Loss: 4.815033435821533\n",
            "Training Iteration 3252, Loss: 3.60935640335083\n",
            "Training Iteration 3253, Loss: 7.9500932693481445\n",
            "Training Iteration 3254, Loss: 6.031431198120117\n",
            "Training Iteration 3255, Loss: 6.202404022216797\n",
            "Training Iteration 3256, Loss: 4.980731964111328\n",
            "Training Iteration 3257, Loss: 4.838213920593262\n",
            "Training Iteration 3258, Loss: 2.924042224884033\n",
            "Training Iteration 3259, Loss: 4.848325729370117\n",
            "Training Iteration 3260, Loss: 4.948482513427734\n",
            "Training Iteration 3261, Loss: 5.2364397048950195\n",
            "Training Iteration 3262, Loss: 3.017038345336914\n",
            "Training Iteration 3263, Loss: 4.846975326538086\n",
            "Training Iteration 3264, Loss: 3.967998504638672\n",
            "Training Iteration 3265, Loss: 5.900969505310059\n",
            "Training Iteration 3266, Loss: 5.241394519805908\n",
            "Training Iteration 3267, Loss: 5.090602874755859\n",
            "Training Iteration 3268, Loss: 9.417665481567383\n",
            "Training Iteration 3269, Loss: 6.547677516937256\n",
            "Training Iteration 3270, Loss: 3.827876329421997\n",
            "Training Iteration 3271, Loss: 3.164278268814087\n",
            "Training Iteration 3272, Loss: 4.479928493499756\n",
            "Training Iteration 3273, Loss: 4.587663173675537\n",
            "Training Iteration 3274, Loss: 4.983968257904053\n",
            "Training Iteration 3275, Loss: 8.72730827331543\n",
            "Training Iteration 3276, Loss: 4.539303779602051\n",
            "Training Iteration 3277, Loss: 5.094096660614014\n",
            "Training Iteration 3278, Loss: 5.571469306945801\n",
            "Training Iteration 3279, Loss: 6.728748321533203\n",
            "Training Iteration 3280, Loss: 6.78307580947876\n",
            "Training Iteration 3281, Loss: 5.650256156921387\n",
            "Training Iteration 3282, Loss: 4.656304836273193\n",
            "Training Iteration 3283, Loss: 9.226290702819824\n",
            "Training Iteration 3284, Loss: 5.607592582702637\n",
            "Training Iteration 3285, Loss: 5.544532775878906\n",
            "Training Iteration 3286, Loss: 6.994999408721924\n",
            "Training Iteration 3287, Loss: 9.918024063110352\n",
            "Training Iteration 3288, Loss: 4.9205427169799805\n",
            "Training Iteration 3289, Loss: 6.0977654457092285\n",
            "Training Iteration 3290, Loss: 2.510387659072876\n",
            "Training Iteration 3291, Loss: 6.413547515869141\n",
            "Training Iteration 3292, Loss: 7.666254997253418\n",
            "Training Iteration 3293, Loss: 7.743221282958984\n",
            "Training Iteration 3294, Loss: 7.23171329498291\n",
            "Training Iteration 3295, Loss: 2.4409823417663574\n",
            "Training Iteration 3296, Loss: 4.232975006103516\n",
            "Training Iteration 3297, Loss: 3.217372179031372\n",
            "Training Iteration 3298, Loss: 7.535100936889648\n",
            "Training Iteration 3299, Loss: 5.960719585418701\n",
            "Training Iteration 3300, Loss: 9.24397087097168\n",
            "Training Iteration 3301, Loss: 7.364974021911621\n",
            "Training Iteration 3302, Loss: 6.1711225509643555\n",
            "Training Iteration 3303, Loss: 5.164339065551758\n",
            "Training Iteration 3304, Loss: 4.700737953186035\n",
            "Training Iteration 3305, Loss: 3.4976677894592285\n",
            "Training Iteration 3306, Loss: 5.801616191864014\n",
            "Training Iteration 3307, Loss: 4.312247276306152\n",
            "Training Iteration 3308, Loss: 4.2980146408081055\n",
            "Training Iteration 3309, Loss: 3.0804929733276367\n",
            "Training Iteration 3310, Loss: 3.485799789428711\n",
            "Training Iteration 3311, Loss: 6.117302894592285\n",
            "Training Iteration 3312, Loss: 7.319963455200195\n",
            "Training Iteration 3313, Loss: 1.6846575736999512\n",
            "Training Iteration 3314, Loss: 1.850927472114563\n",
            "Training Iteration 3315, Loss: 3.2953526973724365\n",
            "Training Iteration 3316, Loss: 1.0508756637573242\n",
            "Training Iteration 3317, Loss: 5.948797702789307\n",
            "Training Iteration 3318, Loss: 7.146322250366211\n",
            "Training Iteration 3319, Loss: 5.985994815826416\n",
            "Training Iteration 3320, Loss: 5.562814712524414\n",
            "Training Iteration 3321, Loss: 4.4140625\n",
            "Training Iteration 3322, Loss: 4.639394760131836\n",
            "Training Iteration 3323, Loss: 6.0180182456970215\n",
            "Training Iteration 3324, Loss: 7.946704387664795\n",
            "Training Iteration 3325, Loss: 1.0250047445297241\n",
            "Training Iteration 3326, Loss: 10.244770050048828\n",
            "Training Iteration 3327, Loss: 9.653965950012207\n",
            "Training Iteration 3328, Loss: 6.919068813323975\n",
            "Training Iteration 3329, Loss: 9.17809009552002\n",
            "Training Iteration 3330, Loss: 7.304398536682129\n",
            "Training Iteration 3331, Loss: 10.024259567260742\n",
            "Training Iteration 3332, Loss: 6.380237102508545\n",
            "Training Iteration 3333, Loss: 6.313788414001465\n",
            "Training Iteration 3334, Loss: 3.099097728729248\n",
            "Training Iteration 3335, Loss: 3.2768805027008057\n",
            "Training Iteration 3336, Loss: 4.051237106323242\n",
            "Training Iteration 3337, Loss: 6.2256903648376465\n",
            "Training Iteration 3338, Loss: 4.176639080047607\n",
            "Training Iteration 3339, Loss: 8.869775772094727\n",
            "Training Iteration 3340, Loss: 8.663721084594727\n",
            "Training Iteration 3341, Loss: 2.9786181449890137\n",
            "Training Iteration 3342, Loss: 4.624039649963379\n",
            "Training Iteration 3343, Loss: 5.4481706619262695\n",
            "Training Iteration 3344, Loss: 8.899982452392578\n",
            "Training Iteration 3345, Loss: 6.708871364593506\n",
            "Training Iteration 3346, Loss: 4.435849189758301\n",
            "Training Iteration 3347, Loss: 5.743494033813477\n",
            "Training Iteration 3348, Loss: 7.98541259765625\n",
            "Training Iteration 3349, Loss: 3.6837122440338135\n",
            "Training Iteration 3350, Loss: 4.020288467407227\n",
            "Training Iteration 3351, Loss: 5.313652992248535\n",
            "Training Iteration 3352, Loss: 3.8906941413879395\n",
            "Training Iteration 3353, Loss: 4.455538749694824\n",
            "Training Iteration 3354, Loss: 6.158379077911377\n",
            "Training Iteration 3355, Loss: 3.0879948139190674\n",
            "Training Iteration 3356, Loss: 6.463161945343018\n",
            "Training Iteration 3357, Loss: 3.8090970516204834\n",
            "Training Iteration 3358, Loss: 2.101292610168457\n",
            "Training Iteration 3359, Loss: 7.623138904571533\n",
            "Training Iteration 3360, Loss: 8.263090133666992\n",
            "Training Iteration 3361, Loss: 3.583221673965454\n",
            "Training Iteration 3362, Loss: 4.484734058380127\n",
            "Training Iteration 3363, Loss: 7.306792259216309\n",
            "Training Iteration 3364, Loss: 7.283388137817383\n",
            "Training Iteration 3365, Loss: 4.315183162689209\n",
            "Training Iteration 3366, Loss: 4.328305721282959\n",
            "Training Iteration 3367, Loss: 3.835087537765503\n",
            "Training Iteration 3368, Loss: 6.242773056030273\n",
            "Training Iteration 3369, Loss: 5.505701541900635\n",
            "Training Iteration 3370, Loss: 6.361756801605225\n",
            "Training Iteration 3371, Loss: 5.690057277679443\n",
            "Training Iteration 3372, Loss: 5.272634029388428\n",
            "Training Iteration 3373, Loss: 1.8757004737854004\n",
            "Training Iteration 3374, Loss: 5.661224365234375\n",
            "Training Iteration 3375, Loss: 3.7283520698547363\n",
            "Training Iteration 3376, Loss: 4.6037068367004395\n",
            "Training Iteration 3377, Loss: 8.91950511932373\n",
            "Training Iteration 3378, Loss: 6.958685874938965\n",
            "Training Iteration 3379, Loss: 5.448531627655029\n",
            "Training Iteration 3380, Loss: 5.3156514167785645\n",
            "Training Iteration 3381, Loss: 7.84033203125\n",
            "Training Iteration 3382, Loss: 5.032074451446533\n",
            "Training Iteration 3383, Loss: 4.554252624511719\n",
            "Training Iteration 3384, Loss: 4.176630973815918\n",
            "Training Iteration 3385, Loss: 4.267195224761963\n",
            "Training Iteration 3386, Loss: 5.581595420837402\n",
            "Training Iteration 3387, Loss: 2.7681472301483154\n",
            "Training Iteration 3388, Loss: 2.9934215545654297\n",
            "Training Iteration 3389, Loss: 4.307883262634277\n",
            "Training Iteration 3390, Loss: 5.3012776374816895\n",
            "Training Iteration 3391, Loss: 3.060235023498535\n",
            "Training Iteration 3392, Loss: 5.9095892906188965\n",
            "Training Iteration 3393, Loss: 6.643503189086914\n",
            "Training Iteration 3394, Loss: 7.8972601890563965\n",
            "Training Iteration 3395, Loss: 11.145567893981934\n",
            "Training Iteration 3396, Loss: 2.1421098709106445\n",
            "Training Iteration 3397, Loss: 4.391717910766602\n",
            "Training Iteration 3398, Loss: 4.600990295410156\n",
            "Training Iteration 3399, Loss: 4.131538391113281\n",
            "Training Iteration 3400, Loss: 6.589754104614258\n",
            "Training Iteration 3401, Loss: 3.814862012863159\n",
            "Training Iteration 3402, Loss: 6.803276538848877\n",
            "Training Iteration 3403, Loss: 3.601813793182373\n",
            "Training Iteration 3404, Loss: 3.7185401916503906\n",
            "Training Iteration 3405, Loss: 5.7912211418151855\n",
            "Training Iteration 3406, Loss: 4.04508638381958\n",
            "Training Iteration 3407, Loss: 4.648479461669922\n",
            "Training Iteration 3408, Loss: 9.857748985290527\n",
            "Training Iteration 3409, Loss: 4.616602897644043\n",
            "Training Iteration 3410, Loss: 5.7217206954956055\n",
            "Training Iteration 3411, Loss: 3.855536937713623\n",
            "Training Iteration 3412, Loss: 4.921655654907227\n",
            "Training Iteration 3413, Loss: 5.783627986907959\n",
            "Training Iteration 3414, Loss: 3.060488700866699\n",
            "Training Iteration 3415, Loss: 2.968367576599121\n",
            "Training Iteration 3416, Loss: 4.452433109283447\n",
            "Training Iteration 3417, Loss: 1.7349928617477417\n",
            "Training Iteration 3418, Loss: 4.942346572875977\n",
            "Training Iteration 3419, Loss: 4.474249362945557\n",
            "Training Iteration 3420, Loss: 2.8975131511688232\n",
            "Training Iteration 3421, Loss: 7.466519832611084\n",
            "Training Iteration 3422, Loss: 6.064033508300781\n",
            "Training Iteration 3423, Loss: 6.921942710876465\n",
            "Training Iteration 3424, Loss: 4.003723621368408\n",
            "Training Iteration 3425, Loss: 4.85964822769165\n",
            "Training Iteration 3426, Loss: 6.025818824768066\n",
            "Training Iteration 3427, Loss: 6.8189697265625\n",
            "Training Iteration 3428, Loss: 4.99075984954834\n",
            "Training Iteration 3429, Loss: 5.235626220703125\n",
            "Training Iteration 3430, Loss: 5.362833023071289\n",
            "Training Iteration 3431, Loss: 2.6612372398376465\n",
            "Training Iteration 3432, Loss: 4.362541198730469\n",
            "Training Iteration 3433, Loss: 5.099738121032715\n",
            "Training Iteration 3434, Loss: 4.295620918273926\n",
            "Training Iteration 3435, Loss: 7.563982963562012\n",
            "Training Iteration 3436, Loss: 5.893107891082764\n",
            "Training Iteration 3437, Loss: 6.837701797485352\n",
            "Training Iteration 3438, Loss: 6.050594329833984\n",
            "Training Iteration 3439, Loss: 4.942080497741699\n",
            "Training Iteration 3440, Loss: 5.770759582519531\n",
            "Training Iteration 3441, Loss: 6.829768657684326\n",
            "Training Iteration 3442, Loss: 5.456452369689941\n",
            "Training Iteration 3443, Loss: 9.394515991210938\n",
            "Training Iteration 3444, Loss: 2.2617592811584473\n",
            "Training Iteration 3445, Loss: 2.550872564315796\n",
            "Training Iteration 3446, Loss: 3.3688392639160156\n",
            "Training Iteration 3447, Loss: 5.495662212371826\n",
            "Training Iteration 3448, Loss: 4.990276336669922\n",
            "Training Iteration 3449, Loss: 3.1127853393554688\n",
            "Training Iteration 3450, Loss: 3.823920965194702\n",
            "Training Iteration 3451, Loss: 3.9807231426239014\n",
            "Training Iteration 3452, Loss: 3.98581600189209\n",
            "Training Iteration 3453, Loss: 3.763730525970459\n",
            "Training Iteration 3454, Loss: 5.679682731628418\n",
            "Training Iteration 3455, Loss: 6.425616264343262\n",
            "Training Iteration 3456, Loss: 3.4097020626068115\n",
            "Training Iteration 3457, Loss: 1.4588878154754639\n",
            "Training Iteration 3458, Loss: 7.455482006072998\n",
            "Training Iteration 3459, Loss: 5.7943596839904785\n",
            "Training Iteration 3460, Loss: 3.56087327003479\n",
            "Training Iteration 3461, Loss: 4.104986190795898\n",
            "Training Iteration 3462, Loss: 5.136103630065918\n",
            "Training Iteration 3463, Loss: 5.29023551940918\n",
            "Training Iteration 3464, Loss: 1.9655354022979736\n",
            "Training Iteration 3465, Loss: 3.293276309967041\n",
            "Training Iteration 3466, Loss: 1.7792388200759888\n",
            "Training Iteration 3467, Loss: 4.659330368041992\n",
            "Training Iteration 3468, Loss: 4.020649433135986\n",
            "Training Iteration 3469, Loss: 6.288695812225342\n",
            "Training Iteration 3470, Loss: 5.109434127807617\n",
            "Training Iteration 3471, Loss: 5.165027141571045\n",
            "Training Iteration 3472, Loss: 5.210038661956787\n",
            "Training Iteration 3473, Loss: 2.6703906059265137\n",
            "Training Iteration 3474, Loss: 4.378957271575928\n",
            "Training Iteration 3475, Loss: 4.848481178283691\n",
            "Training Iteration 3476, Loss: 2.637291669845581\n",
            "Training Iteration 3477, Loss: 4.013274192810059\n",
            "Training Iteration 3478, Loss: 2.2319183349609375\n",
            "Training Iteration 3479, Loss: 1.9054503440856934\n",
            "Training Iteration 3480, Loss: 4.541012763977051\n",
            "Training Iteration 3481, Loss: 5.736020565032959\n",
            "Training Iteration 3482, Loss: 2.653461217880249\n",
            "Training Iteration 3483, Loss: 8.183996200561523\n",
            "Training Iteration 3484, Loss: 4.058463096618652\n",
            "Training Iteration 3485, Loss: 2.991514205932617\n",
            "Training Iteration 3486, Loss: 1.7403279542922974\n",
            "Training Iteration 3487, Loss: 4.538540363311768\n",
            "Training Iteration 3488, Loss: 8.559711456298828\n",
            "Training Iteration 3489, Loss: 3.63979434967041\n",
            "Training Iteration 3490, Loss: 2.94979190826416\n",
            "Training Iteration 3491, Loss: 6.789797782897949\n",
            "Training Iteration 3492, Loss: 2.8040316104888916\n",
            "Training Iteration 3493, Loss: 2.6532833576202393\n",
            "Training Iteration 3494, Loss: 5.452984809875488\n",
            "Training Iteration 3495, Loss: 4.729937553405762\n",
            "Training Iteration 3496, Loss: 4.033663272857666\n",
            "Training Iteration 3497, Loss: 4.951854228973389\n",
            "Training Iteration 3498, Loss: 5.914218902587891\n",
            "Training Iteration 3499, Loss: 4.139656066894531\n",
            "Training Iteration 3500, Loss: 7.3213419914245605\n",
            "Training Iteration 3501, Loss: 8.000948905944824\n",
            "Training Iteration 3502, Loss: 2.186016082763672\n",
            "Training Iteration 3503, Loss: 5.639556407928467\n",
            "Training Iteration 3504, Loss: 5.971851348876953\n",
            "Training Iteration 3505, Loss: 6.4263410568237305\n",
            "Training Iteration 3506, Loss: 6.061595439910889\n",
            "Training Iteration 3507, Loss: 3.7319400310516357\n",
            "Training Iteration 3508, Loss: 2.712034225463867\n",
            "Training Iteration 3509, Loss: 3.277045965194702\n",
            "Training Iteration 3510, Loss: 5.709481716156006\n",
            "Training Iteration 3511, Loss: 2.793041467666626\n",
            "Training Iteration 3512, Loss: 3.790404796600342\n",
            "Training Iteration 3513, Loss: 3.852640390396118\n",
            "Training Iteration 3514, Loss: 8.670639991760254\n",
            "Training Iteration 3515, Loss: 4.908109188079834\n",
            "Training Iteration 3516, Loss: 4.014435291290283\n",
            "Training Iteration 3517, Loss: 6.1478376388549805\n",
            "Training Iteration 3518, Loss: 2.937842845916748\n",
            "Training Iteration 3519, Loss: 3.284679889678955\n",
            "Training Iteration 3520, Loss: 3.7241649627685547\n",
            "Training Iteration 3521, Loss: 5.454096794128418\n",
            "Training Iteration 3522, Loss: 3.9759445190429688\n",
            "Training Iteration 3523, Loss: 8.22627067565918\n",
            "Training Iteration 3524, Loss: 3.655663251876831\n",
            "Training Iteration 3525, Loss: 3.842101573944092\n",
            "Training Iteration 3526, Loss: 3.366715908050537\n",
            "Training Iteration 3527, Loss: 6.253416061401367\n",
            "Training Iteration 3528, Loss: 6.702936172485352\n",
            "Training Iteration 3529, Loss: 4.530447959899902\n",
            "Training Iteration 3530, Loss: 6.760682106018066\n",
            "Training Iteration 3531, Loss: 6.715580463409424\n",
            "Training Iteration 3532, Loss: 5.270747661590576\n",
            "Training Iteration 3533, Loss: 6.35013484954834\n",
            "Training Iteration 3534, Loss: 2.0314764976501465\n",
            "Training Iteration 3535, Loss: 4.368236064910889\n",
            "Training Iteration 3536, Loss: 5.601079940795898\n",
            "Training Iteration 3537, Loss: 3.7980494499206543\n",
            "Training Iteration 3538, Loss: 4.741451740264893\n",
            "Training Iteration 3539, Loss: 3.018191337585449\n",
            "Training Iteration 3540, Loss: 3.527822971343994\n",
            "Training Iteration 3541, Loss: 4.282527923583984\n",
            "Training Iteration 3542, Loss: 3.0098836421966553\n",
            "Training Iteration 3543, Loss: 4.448080539703369\n",
            "Training Iteration 3544, Loss: 5.486998081207275\n",
            "Training Iteration 3545, Loss: 6.451010227203369\n",
            "Training Iteration 3546, Loss: 6.442164897918701\n",
            "Training Iteration 3547, Loss: 4.401604175567627\n",
            "Training Iteration 3548, Loss: 7.269652843475342\n",
            "Training Iteration 3549, Loss: 5.703113079071045\n",
            "Training Iteration 3550, Loss: 5.802509307861328\n",
            "Training Iteration 3551, Loss: 5.739268779754639\n",
            "Training Iteration 3552, Loss: 8.085348129272461\n",
            "Training Iteration 3553, Loss: 7.687615871429443\n",
            "Training Iteration 3554, Loss: 6.957036018371582\n",
            "Training Iteration 3555, Loss: 7.054245948791504\n",
            "Training Iteration 3556, Loss: 3.399284601211548\n",
            "Training Iteration 3557, Loss: 3.852585792541504\n",
            "Training Iteration 3558, Loss: 1.5601385831832886\n",
            "Training Iteration 3559, Loss: 2.414423704147339\n",
            "Training Iteration 3560, Loss: 3.9145448207855225\n",
            "Training Iteration 3561, Loss: 4.7603278160095215\n",
            "Training Iteration 3562, Loss: 5.381137371063232\n",
            "Training Iteration 3563, Loss: 5.899597644805908\n",
            "Training Iteration 3564, Loss: 4.624089241027832\n",
            "Training Iteration 3565, Loss: 2.178767204284668\n",
            "Training Iteration 3566, Loss: 4.7685112953186035\n",
            "Training Iteration 3567, Loss: 2.3073277473449707\n",
            "Training Iteration 3568, Loss: 1.540847659111023\n",
            "Training Iteration 3569, Loss: 4.58738899230957\n",
            "Training Iteration 3570, Loss: 2.0495541095733643\n",
            "Training Iteration 3571, Loss: 3.402916193008423\n",
            "Training Iteration 3572, Loss: 5.959373474121094\n",
            "Training Iteration 3573, Loss: 5.152546405792236\n",
            "Training Iteration 3574, Loss: 3.6599886417388916\n",
            "Training Iteration 3575, Loss: 3.9429116249084473\n",
            "Training Iteration 3576, Loss: 4.740370273590088\n",
            "Training Iteration 3577, Loss: 3.251129150390625\n",
            "Training Iteration 3578, Loss: 6.02348518371582\n",
            "Training Iteration 3579, Loss: 4.950746536254883\n",
            "Training Iteration 3580, Loss: 3.505126953125\n",
            "Training Iteration 3581, Loss: 3.507952928543091\n",
            "Training Iteration 3582, Loss: 4.809417724609375\n",
            "Training Iteration 3583, Loss: 3.3357088565826416\n",
            "Training Iteration 3584, Loss: 3.6457271575927734\n",
            "Training Iteration 3585, Loss: 2.9353065490722656\n",
            "Training Iteration 3586, Loss: 4.486877918243408\n",
            "Training Iteration 3587, Loss: 6.788121700286865\n",
            "Training Iteration 3588, Loss: 6.013738632202148\n",
            "Training Iteration 3589, Loss: 3.5200424194335938\n",
            "Training Iteration 3590, Loss: 1.3815371990203857\n",
            "Training Iteration 3591, Loss: 2.160780906677246\n",
            "Training Iteration 3592, Loss: 2.2167141437530518\n",
            "Training Iteration 3593, Loss: 2.446805953979492\n",
            "Training Iteration 3594, Loss: 4.193365097045898\n",
            "Training Iteration 3595, Loss: 5.557931900024414\n",
            "Training Iteration 3596, Loss: 6.1099042892456055\n",
            "Training Iteration 3597, Loss: 4.835144519805908\n",
            "Training Iteration 3598, Loss: 6.073342800140381\n",
            "Training Iteration 3599, Loss: 6.857491970062256\n",
            "Training Iteration 3600, Loss: 6.854314804077148\n",
            "Training Iteration 3601, Loss: 3.6836891174316406\n",
            "Training Iteration 3602, Loss: 4.659140586853027\n",
            "Training Iteration 3603, Loss: 5.609175682067871\n",
            "Training Iteration 3604, Loss: 2.747375965118408\n",
            "Training Iteration 3605, Loss: 5.9294843673706055\n",
            "Training Iteration 3606, Loss: 2.6022651195526123\n",
            "Training Iteration 3607, Loss: 2.489018201828003\n",
            "Training Iteration 3608, Loss: 10.627983093261719\n",
            "Training Iteration 3609, Loss: 4.031073093414307\n",
            "Training Iteration 3610, Loss: 8.621787071228027\n",
            "Training Iteration 3611, Loss: 5.517620086669922\n",
            "Training Iteration 3612, Loss: 3.4074087142944336\n",
            "Training Iteration 3613, Loss: 3.4352262020111084\n",
            "Training Iteration 3614, Loss: 4.997448921203613\n",
            "Training Iteration 3615, Loss: 4.145165920257568\n",
            "Training Iteration 3616, Loss: 7.585283279418945\n",
            "Training Iteration 3617, Loss: 6.719871997833252\n",
            "Training Iteration 3618, Loss: 6.041933536529541\n",
            "Training Iteration 3619, Loss: 5.089578628540039\n",
            "Training Iteration 3620, Loss: 4.842621326446533\n",
            "Training Iteration 3621, Loss: 6.979592800140381\n",
            "Training Iteration 3622, Loss: 3.014369249343872\n",
            "Training Iteration 3623, Loss: 4.444878101348877\n",
            "Training Iteration 3624, Loss: 6.136368274688721\n",
            "Training Iteration 3625, Loss: 4.7496256828308105\n",
            "Training Iteration 3626, Loss: 4.175336837768555\n",
            "Training Iteration 3627, Loss: 8.521842956542969\n",
            "Training Iteration 3628, Loss: 5.00376558303833\n",
            "Training Iteration 3629, Loss: 6.105992317199707\n",
            "Training Iteration 3630, Loss: 9.471306800842285\n",
            "Training Iteration 3631, Loss: 4.579852104187012\n",
            "Training Iteration 3632, Loss: 8.312768936157227\n",
            "Training Iteration 3633, Loss: 5.409126281738281\n",
            "Training Iteration 3634, Loss: 5.790831565856934\n",
            "Training Iteration 3635, Loss: 4.193113327026367\n",
            "Training Iteration 3636, Loss: 1.9643892049789429\n",
            "Training Iteration 3637, Loss: 3.9915459156036377\n",
            "Training Iteration 3638, Loss: 3.5713448524475098\n",
            "Training Iteration 3639, Loss: 4.507053852081299\n",
            "Training Iteration 3640, Loss: 9.337244033813477\n",
            "Training Iteration 3641, Loss: 7.188492774963379\n",
            "Training Iteration 3642, Loss: 4.095909118652344\n",
            "Training Iteration 3643, Loss: 3.8005611896514893\n",
            "Training Iteration 3644, Loss: 5.125486373901367\n",
            "Training Iteration 3645, Loss: 6.576973915100098\n",
            "Training Iteration 3646, Loss: 4.616546630859375\n",
            "Training Iteration 3647, Loss: 2.164012908935547\n",
            "Training Iteration 3648, Loss: 6.124112129211426\n",
            "Training Iteration 3649, Loss: 3.7648308277130127\n",
            "Training Iteration 3650, Loss: 4.424640655517578\n",
            "Training Iteration 3651, Loss: 3.2724266052246094\n",
            "Training Iteration 3652, Loss: 4.600691318511963\n",
            "Training Iteration 3653, Loss: 6.200135707855225\n",
            "Training Iteration 3654, Loss: 2.339540958404541\n",
            "Training Iteration 3655, Loss: 6.229743003845215\n",
            "Training Iteration 3656, Loss: 2.637134552001953\n",
            "Training Iteration 3657, Loss: 4.833933353424072\n",
            "Training Iteration 3658, Loss: 5.008069038391113\n",
            "Training Iteration 3659, Loss: 7.8991241455078125\n",
            "Training Iteration 3660, Loss: 4.165770053863525\n",
            "Training Iteration 3661, Loss: 5.213890552520752\n",
            "Training Iteration 3662, Loss: 2.9141335487365723\n",
            "Training Iteration 3663, Loss: 2.5310018062591553\n",
            "Training Iteration 3664, Loss: 5.340297698974609\n",
            "Training Iteration 3665, Loss: 8.887337684631348\n",
            "Training Iteration 3666, Loss: 7.084158897399902\n",
            "Training Iteration 3667, Loss: 5.374122619628906\n",
            "Training Iteration 3668, Loss: 4.375955581665039\n",
            "Training Iteration 3669, Loss: 1.486823320388794\n",
            "Training Iteration 3670, Loss: 2.690333127975464\n",
            "Training Iteration 3671, Loss: 3.716768980026245\n",
            "Training Iteration 3672, Loss: 4.286515712738037\n",
            "Training Iteration 3673, Loss: 5.804468631744385\n",
            "Training Iteration 3674, Loss: 8.339218139648438\n",
            "Training Iteration 3675, Loss: 2.5593836307525635\n",
            "Training Iteration 3676, Loss: 10.30685043334961\n",
            "Training Iteration 3677, Loss: 4.0323028564453125\n",
            "Training Iteration 3678, Loss: 7.266015529632568\n",
            "Training Iteration 3679, Loss: 12.849815368652344\n",
            "Training Iteration 3680, Loss: 6.857117652893066\n",
            "Training Iteration 3681, Loss: 8.123246192932129\n",
            "Training Iteration 3682, Loss: 9.374984741210938\n",
            "Training Iteration 3683, Loss: 5.268243789672852\n",
            "Training Iteration 3684, Loss: 3.5609898567199707\n",
            "Training Iteration 3685, Loss: 3.366934299468994\n",
            "Training Iteration 3686, Loss: 4.831531524658203\n",
            "Training Iteration 3687, Loss: 5.934537887573242\n",
            "Training Iteration 3688, Loss: 5.693854331970215\n",
            "Training Iteration 3689, Loss: 8.919273376464844\n",
            "Training Iteration 3690, Loss: 4.118431568145752\n",
            "Training Iteration 3691, Loss: 8.249385833740234\n",
            "Training Iteration 3692, Loss: 1.4455043077468872\n",
            "Training Iteration 3693, Loss: 7.261280059814453\n",
            "Training Iteration 3694, Loss: 10.021429061889648\n",
            "Training Iteration 3695, Loss: 5.736039638519287\n",
            "Training Iteration 3696, Loss: 10.278630256652832\n",
            "Training Iteration 3697, Loss: 5.211156845092773\n",
            "Training Iteration 3698, Loss: 8.099206924438477\n",
            "Training Iteration 3699, Loss: 4.7793779373168945\n",
            "Training Iteration 3700, Loss: 6.926514625549316\n",
            "Training Iteration 3701, Loss: 4.2501420974731445\n",
            "Training Iteration 3702, Loss: 2.4236929416656494\n",
            "Training Iteration 3703, Loss: 6.106354236602783\n",
            "Training Iteration 3704, Loss: 7.515401363372803\n",
            "Training Iteration 3705, Loss: 2.8746490478515625\n",
            "Training Iteration 3706, Loss: 6.464988708496094\n",
            "Training Iteration 3707, Loss: 5.143805027008057\n",
            "Training Iteration 3708, Loss: 5.583517074584961\n",
            "Training Iteration 3709, Loss: 8.01787281036377\n",
            "Training Iteration 3710, Loss: 2.0438480377197266\n",
            "Training Iteration 3711, Loss: 6.570228099822998\n",
            "Training Iteration 3712, Loss: 4.331380844116211\n",
            "Training Iteration 3713, Loss: 2.530172824859619\n",
            "Training Iteration 3714, Loss: 7.314967155456543\n",
            "Training Iteration 3715, Loss: 3.4070487022399902\n",
            "Training Iteration 3716, Loss: 5.3443074226379395\n",
            "Training Iteration 3717, Loss: 6.985312461853027\n",
            "Training Iteration 3718, Loss: 4.052453517913818\n",
            "Training Iteration 3719, Loss: 4.810527801513672\n",
            "Training Iteration 3720, Loss: 5.818439960479736\n",
            "Training Iteration 3721, Loss: 4.048398971557617\n",
            "Training Iteration 3722, Loss: 5.847794055938721\n",
            "Training Iteration 3723, Loss: 4.721445560455322\n",
            "Training Iteration 3724, Loss: 6.776309967041016\n",
            "Training Iteration 3725, Loss: 4.175319671630859\n",
            "Training Iteration 3726, Loss: 2.652890682220459\n",
            "Training Iteration 3727, Loss: 7.897907257080078\n",
            "Training Iteration 3728, Loss: 7.670392036437988\n",
            "Training Iteration 3729, Loss: 5.577056884765625\n",
            "Training Iteration 3730, Loss: 7.782077789306641\n",
            "Training Iteration 3731, Loss: 4.62001895904541\n",
            "Training Iteration 3732, Loss: 7.471515655517578\n",
            "Training Iteration 3733, Loss: 5.694102764129639\n",
            "Training Iteration 3734, Loss: 6.733573913574219\n",
            "Training Iteration 3735, Loss: 5.68950080871582\n",
            "Training Iteration 3736, Loss: 3.22171950340271\n",
            "Training Iteration 3737, Loss: 3.91110897064209\n",
            "Training Iteration 3738, Loss: 3.956519365310669\n",
            "Training Iteration 3739, Loss: 3.6601057052612305\n",
            "Training Iteration 3740, Loss: 6.5162672996521\n",
            "Training Iteration 3741, Loss: 5.045033931732178\n",
            "Training Iteration 3742, Loss: 4.359072208404541\n",
            "Training Iteration 3743, Loss: 7.596714019775391\n",
            "Training Iteration 3744, Loss: 7.334979057312012\n",
            "Training Iteration 3745, Loss: 5.396097183227539\n",
            "Training Iteration 3746, Loss: 4.961792469024658\n",
            "Training Iteration 3747, Loss: 5.705971717834473\n",
            "Training Iteration 3748, Loss: 3.623619556427002\n",
            "Training Iteration 3749, Loss: 6.163909912109375\n",
            "Training Iteration 3750, Loss: 3.937391519546509\n",
            "Training Iteration 3751, Loss: 4.213292121887207\n",
            "Training Iteration 3752, Loss: 4.5422868728637695\n",
            "Training Iteration 3753, Loss: 5.7078142166137695\n",
            "Training Iteration 3754, Loss: 7.245859622955322\n",
            "Training Iteration 3755, Loss: 4.927305698394775\n",
            "Training Iteration 3756, Loss: 2.5509300231933594\n",
            "Training Iteration 3757, Loss: 5.264225006103516\n",
            "Training Iteration 3758, Loss: 4.221128940582275\n",
            "Training Iteration 3759, Loss: 6.8809661865234375\n",
            "Training Iteration 3760, Loss: 6.30999755859375\n",
            "Training Iteration 3761, Loss: 5.1214985847473145\n",
            "Training Iteration 3762, Loss: 4.024057865142822\n",
            "Training Iteration 3763, Loss: 5.131309509277344\n",
            "Training Iteration 3764, Loss: 5.186957359313965\n",
            "Training Iteration 3765, Loss: 5.734381198883057\n",
            "Training Iteration 3766, Loss: 4.957714557647705\n",
            "Training Iteration 3767, Loss: 4.763037204742432\n",
            "Training Iteration 3768, Loss: 3.0765795707702637\n",
            "Training Iteration 3769, Loss: 3.6616806983947754\n",
            "Training Iteration 3770, Loss: 4.373402118682861\n",
            "Training Iteration 3771, Loss: 8.008526802062988\n",
            "Training Iteration 3772, Loss: 7.516684532165527\n",
            "Training Iteration 3773, Loss: 4.334410667419434\n",
            "Training Iteration 3774, Loss: 8.092341423034668\n",
            "Training Iteration 3775, Loss: 5.286104202270508\n",
            "Training Iteration 3776, Loss: 8.397109985351562\n",
            "Training Iteration 3777, Loss: 7.198688507080078\n",
            "Training Iteration 3778, Loss: 5.169389247894287\n",
            "Training Iteration 3779, Loss: 3.3641834259033203\n",
            "Training Iteration 3780, Loss: 3.965283155441284\n",
            "Training Iteration 3781, Loss: 9.538201332092285\n",
            "Training Iteration 3782, Loss: 8.729385375976562\n",
            "Training Iteration 3783, Loss: 3.757639169692993\n",
            "Training Iteration 3784, Loss: 2.5881505012512207\n",
            "Training Iteration 3785, Loss: 3.1842615604400635\n",
            "Training Iteration 3786, Loss: 3.6859853267669678\n",
            "Training Iteration 3787, Loss: 6.464507579803467\n",
            "Training Iteration 3788, Loss: 2.858997344970703\n",
            "Training Iteration 3789, Loss: 5.589138507843018\n",
            "Training Iteration 3790, Loss: 11.2734375\n",
            "Training Iteration 3791, Loss: 3.242210865020752\n",
            "Training Iteration 3792, Loss: 4.651455402374268\n",
            "Training Iteration 3793, Loss: 3.5897648334503174\n",
            "Training Iteration 3794, Loss: 3.235234022140503\n",
            "Training Iteration 3795, Loss: 4.226658821105957\n",
            "Training Iteration 3796, Loss: 3.783808469772339\n",
            "Training Iteration 3797, Loss: 6.511294364929199\n",
            "Training Iteration 3798, Loss: 3.542975664138794\n",
            "Training Iteration 3799, Loss: 1.7664920091629028\n",
            "Training Iteration 3800, Loss: 5.722271919250488\n",
            "Training Iteration 3801, Loss: 6.149398326873779\n",
            "Training Iteration 3802, Loss: 2.584467649459839\n",
            "Training Iteration 3803, Loss: 4.7881011962890625\n",
            "Training Iteration 3804, Loss: 6.9265265464782715\n",
            "Training Iteration 3805, Loss: 5.177544593811035\n",
            "Training Iteration 3806, Loss: 5.940019130706787\n",
            "Training Iteration 3807, Loss: 4.575667381286621\n",
            "Training Iteration 3808, Loss: 5.4977312088012695\n",
            "Training Iteration 3809, Loss: 7.299077987670898\n",
            "Training Iteration 3810, Loss: 5.493246555328369\n",
            "Training Iteration 3811, Loss: 3.40399169921875\n",
            "Training Iteration 3812, Loss: 9.9855318069458\n",
            "Training Iteration 3813, Loss: 6.815954685211182\n",
            "Training Iteration 3814, Loss: 5.1112565994262695\n",
            "Training Iteration 3815, Loss: 3.112600088119507\n",
            "Training Iteration 3816, Loss: 5.797644138336182\n",
            "Training Iteration 3817, Loss: 4.794638633728027\n",
            "Training Iteration 3818, Loss: 3.932666540145874\n",
            "Training Iteration 3819, Loss: 6.261098384857178\n",
            "Training Iteration 3820, Loss: 4.668175220489502\n",
            "Training Iteration 3821, Loss: 3.497629404067993\n",
            "Training Iteration 3822, Loss: 3.2053027153015137\n",
            "Training Iteration 3823, Loss: 9.169171333312988\n",
            "Training Iteration 3824, Loss: 7.457207202911377\n",
            "Training Iteration 3825, Loss: 7.3858795166015625\n",
            "Training Iteration 3826, Loss: 5.978610038757324\n",
            "Training Iteration 3827, Loss: 4.9293622970581055\n",
            "Training Iteration 3828, Loss: 4.082642078399658\n",
            "Training Iteration 3829, Loss: 7.84332275390625\n",
            "Training Iteration 3830, Loss: 5.672125339508057\n",
            "Training Iteration 3831, Loss: 5.5636396408081055\n",
            "Training Iteration 3832, Loss: 5.098617076873779\n",
            "Training Iteration 3833, Loss: 6.735678672790527\n",
            "Training Iteration 3834, Loss: 5.059079647064209\n",
            "Training Iteration 3835, Loss: 4.568150043487549\n",
            "Training Iteration 3836, Loss: 10.496292114257812\n",
            "Training Iteration 3837, Loss: 5.292394638061523\n",
            "Training Iteration 3838, Loss: 4.010714530944824\n",
            "Training Iteration 3839, Loss: 5.858218193054199\n",
            "Training Iteration 3840, Loss: 7.675365924835205\n",
            "Training Iteration 3841, Loss: 6.76387882232666\n",
            "Training Iteration 3842, Loss: 3.214804172515869\n",
            "Training Iteration 3843, Loss: 4.460831165313721\n",
            "Training Iteration 3844, Loss: 4.675343036651611\n",
            "Training Iteration 3845, Loss: 6.044953346252441\n",
            "Training Iteration 3846, Loss: 3.5280299186706543\n",
            "Training Iteration 3847, Loss: 3.5364105701446533\n",
            "Training Iteration 3848, Loss: 6.189226150512695\n",
            "Training Iteration 3849, Loss: 3.744657039642334\n",
            "Training Iteration 3850, Loss: 5.988574981689453\n",
            "Training Iteration 3851, Loss: 4.78986120223999\n",
            "Training Iteration 3852, Loss: 2.747642755508423\n",
            "Training Iteration 3853, Loss: 4.316867828369141\n",
            "Training Iteration 3854, Loss: 5.487624168395996\n",
            "Training Iteration 3855, Loss: 4.784497261047363\n",
            "Training Iteration 3856, Loss: 2.665865898132324\n",
            "Training Iteration 3857, Loss: 7.009970664978027\n",
            "Training Iteration 3858, Loss: 5.806198596954346\n",
            "Training Iteration 3859, Loss: 6.265000343322754\n",
            "Training Iteration 3860, Loss: 7.6311469078063965\n",
            "Training Iteration 3861, Loss: 3.1368045806884766\n",
            "Training Iteration 3862, Loss: 4.952093601226807\n",
            "Training Iteration 3863, Loss: 5.065896034240723\n",
            "Training Iteration 3864, Loss: 4.310953140258789\n",
            "Training Iteration 3865, Loss: 5.770776271820068\n",
            "Training Iteration 3866, Loss: 4.3357977867126465\n",
            "Training Iteration 3867, Loss: 3.8388946056365967\n",
            "Training Iteration 3868, Loss: 4.7063822746276855\n",
            "Training Iteration 3869, Loss: 3.3476150035858154\n",
            "Training Iteration 3870, Loss: 2.6381325721740723\n",
            "Training Iteration 3871, Loss: 5.0632195472717285\n",
            "Training Iteration 3872, Loss: 4.500392436981201\n",
            "Training Iteration 3873, Loss: 3.2026267051696777\n",
            "Training Iteration 3874, Loss: 6.683362007141113\n",
            "Training Iteration 3875, Loss: 1.5629563331604004\n",
            "Training Iteration 3876, Loss: 2.079444408416748\n",
            "Training Iteration 3877, Loss: 4.947536468505859\n",
            "Training Iteration 3878, Loss: 4.880990028381348\n",
            "Training Iteration 3879, Loss: 2.7920000553131104\n",
            "Training Iteration 3880, Loss: 5.060890197753906\n",
            "Training Iteration 3881, Loss: 3.2300195693969727\n",
            "Training Iteration 3882, Loss: 6.584135055541992\n",
            "Training Iteration 3883, Loss: 4.942738056182861\n",
            "Training Iteration 3884, Loss: 3.230139970779419\n",
            "Training Iteration 3885, Loss: 10.54682445526123\n",
            "Training Iteration 3886, Loss: 2.386598587036133\n",
            "Training Iteration 3887, Loss: 5.874353408813477\n",
            "Training Iteration 3888, Loss: 3.7381303310394287\n",
            "Training Iteration 3889, Loss: 3.073371648788452\n",
            "Training Iteration 3890, Loss: 2.0601847171783447\n",
            "Training Iteration 3891, Loss: 5.46665620803833\n",
            "Training Iteration 3892, Loss: 4.927363872528076\n",
            "Training Iteration 3893, Loss: 2.7501354217529297\n",
            "Training Iteration 3894, Loss: 3.2844784259796143\n",
            "Training Iteration 3895, Loss: 5.005570411682129\n",
            "Training Iteration 3896, Loss: 3.000082492828369\n",
            "Training Iteration 3897, Loss: 5.798730373382568\n",
            "Training Iteration 3898, Loss: 4.150229454040527\n",
            "Training Iteration 3899, Loss: 2.7114315032958984\n",
            "Training Iteration 3900, Loss: 4.845234394073486\n",
            "Training Iteration 3901, Loss: 3.8063485622406006\n",
            "Training Iteration 3902, Loss: 2.9521608352661133\n",
            "Training Iteration 3903, Loss: 4.853177070617676\n",
            "Training Iteration 3904, Loss: 4.671243667602539\n",
            "Training Iteration 3905, Loss: 2.2835893630981445\n",
            "Training Iteration 3906, Loss: 2.7173831462860107\n",
            "Training Iteration 3907, Loss: 5.216288089752197\n",
            "Training Iteration 3908, Loss: 4.599372386932373\n",
            "Training Iteration 3909, Loss: 6.1488189697265625\n",
            "Training Iteration 3910, Loss: 5.407208442687988\n",
            "Training Iteration 3911, Loss: 4.487240314483643\n",
            "Training Iteration 3912, Loss: 3.178619861602783\n",
            "Training Iteration 3913, Loss: 4.8137688636779785\n",
            "Training Iteration 3914, Loss: 4.798142433166504\n",
            "Training Iteration 3915, Loss: 4.758410453796387\n",
            "Training Iteration 3916, Loss: 4.924308776855469\n",
            "Training Iteration 3917, Loss: 5.5220465660095215\n",
            "Training Iteration 3918, Loss: 4.723550319671631\n",
            "Training Iteration 3919, Loss: 5.21520471572876\n",
            "Training Iteration 3920, Loss: 3.4429450035095215\n",
            "Training Iteration 3921, Loss: 6.373224258422852\n",
            "Training Iteration 3922, Loss: 2.297647476196289\n",
            "Training Iteration 3923, Loss: 2.3199119567871094\n",
            "Training Iteration 3924, Loss: 4.387632369995117\n",
            "Training Iteration 3925, Loss: 6.390748023986816\n",
            "Training Iteration 3926, Loss: 4.0933709144592285\n",
            "Training Iteration 3927, Loss: 4.994093894958496\n",
            "Training Iteration 3928, Loss: 8.700765609741211\n",
            "Training Iteration 3929, Loss: 5.25873327255249\n",
            "Training Iteration 3930, Loss: 4.949923038482666\n",
            "Training Iteration 3931, Loss: 3.8511428833007812\n",
            "Training Iteration 3932, Loss: 5.8843488693237305\n",
            "Training Iteration 3933, Loss: 5.173288822174072\n",
            "Training Iteration 3934, Loss: 4.966785430908203\n",
            "Training Iteration 3935, Loss: 6.497710227966309\n",
            "Training Iteration 3936, Loss: 7.461478233337402\n",
            "Training Iteration 3937, Loss: 6.3906145095825195\n",
            "Training Iteration 3938, Loss: 6.906156063079834\n",
            "Training Iteration 3939, Loss: 4.772583484649658\n",
            "Training Iteration 3940, Loss: 5.496833801269531\n",
            "Training Iteration 3941, Loss: 5.16844367980957\n",
            "Training Iteration 3942, Loss: 6.604343414306641\n",
            "Training Iteration 3943, Loss: 2.0373964309692383\n",
            "Training Iteration 3944, Loss: 5.100958347320557\n",
            "Training Iteration 3945, Loss: 3.9127914905548096\n",
            "Training Iteration 3946, Loss: 3.3604423999786377\n",
            "Training Iteration 3947, Loss: 2.5255863666534424\n",
            "Training Iteration 3948, Loss: 4.797476291656494\n",
            "Training Iteration 3949, Loss: 1.8273718357086182\n",
            "Training Iteration 3950, Loss: 3.3047616481781006\n",
            "Training Iteration 3951, Loss: 5.192051410675049\n",
            "Training Iteration 3952, Loss: 4.806351184844971\n",
            "Training Iteration 3953, Loss: 6.241055965423584\n",
            "Training Iteration 3954, Loss: 3.7588722705841064\n",
            "Training Iteration 3955, Loss: 4.084659099578857\n",
            "Training Iteration 3956, Loss: 5.318878173828125\n",
            "Training Iteration 3957, Loss: 3.0591273307800293\n",
            "Training Iteration 3958, Loss: 5.150056838989258\n",
            "Training Iteration 3959, Loss: 5.03574275970459\n",
            "Training Iteration 3960, Loss: 4.442076206207275\n",
            "Training Iteration 3961, Loss: 4.9408674240112305\n",
            "Training Iteration 3962, Loss: 3.750894546508789\n",
            "Training Iteration 3963, Loss: 3.4106740951538086\n",
            "Training Iteration 3964, Loss: 2.5667946338653564\n",
            "Training Iteration 3965, Loss: 2.0480403900146484\n",
            "Training Iteration 3966, Loss: 1.6283963918685913\n",
            "Training Iteration 3967, Loss: 4.442993640899658\n",
            "Training Iteration 3968, Loss: 5.920248985290527\n",
            "Training Iteration 3969, Loss: 6.0853495597839355\n",
            "Training Iteration 3970, Loss: 4.165160179138184\n",
            "Training Iteration 3971, Loss: 6.767592430114746\n",
            "Training Iteration 3972, Loss: 6.412142753601074\n",
            "Training Iteration 3973, Loss: 2.649980068206787\n",
            "Training Iteration 3974, Loss: 6.10745906829834\n",
            "Training Iteration 3975, Loss: 4.119142055511475\n",
            "Training Iteration 3976, Loss: 7.743031024932861\n",
            "Training Iteration 3977, Loss: 1.8848886489868164\n",
            "Training Iteration 3978, Loss: 3.638735771179199\n",
            "Training Iteration 3979, Loss: 3.2351317405700684\n",
            "Training Iteration 3980, Loss: 7.004067420959473\n",
            "Training Iteration 3981, Loss: 4.065107822418213\n",
            "Training Iteration 3982, Loss: 4.518063545227051\n",
            "Training Iteration 3983, Loss: 2.854789972305298\n",
            "Training Iteration 3984, Loss: 4.775765419006348\n",
            "Training Iteration 3985, Loss: 3.5704574584960938\n",
            "Training Iteration 3986, Loss: 2.2622478008270264\n",
            "Training Iteration 3987, Loss: 7.583328723907471\n",
            "Training Iteration 3988, Loss: 2.8231799602508545\n",
            "Training Iteration 3989, Loss: 5.083822727203369\n",
            "Training Iteration 3990, Loss: 6.081009387969971\n",
            "Training Iteration 3991, Loss: 4.885254859924316\n",
            "Training Iteration 3992, Loss: 4.527949810028076\n",
            "Training Iteration 3993, Loss: 3.563871383666992\n",
            "Training Iteration 3994, Loss: 4.6788458824157715\n",
            "Training Iteration 3995, Loss: 5.779191970825195\n",
            "Training Iteration 3996, Loss: 3.902280330657959\n",
            "Training Iteration 3997, Loss: 5.051323890686035\n",
            "Training Iteration 3998, Loss: 5.080399990081787\n",
            "Training Iteration 3999, Loss: 3.795928955078125\n",
            "Training Iteration 4000, Loss: 5.101940631866455\n",
            "Training Iteration 4001, Loss: 6.63754415512085\n",
            "Training Iteration 4002, Loss: 6.626593112945557\n",
            "Training Iteration 4003, Loss: 3.017012119293213\n",
            "Training Iteration 4004, Loss: 3.0434322357177734\n",
            "Training Iteration 4005, Loss: 3.212282180786133\n",
            "Training Iteration 4006, Loss: 5.271979331970215\n",
            "Training Iteration 4007, Loss: 4.274972915649414\n",
            "Training Iteration 4008, Loss: 2.5390355587005615\n",
            "Training Iteration 4009, Loss: 2.392385721206665\n",
            "Training Iteration 4010, Loss: 4.393921852111816\n",
            "Training Iteration 4011, Loss: 2.931579828262329\n",
            "Training Iteration 4012, Loss: 4.742252826690674\n",
            "Training Iteration 4013, Loss: 4.090702056884766\n",
            "Training Iteration 4014, Loss: 2.7123465538024902\n",
            "Training Iteration 4015, Loss: 2.5171189308166504\n",
            "Training Iteration 4016, Loss: 5.180989742279053\n",
            "Training Iteration 4017, Loss: 2.9757437705993652\n",
            "Training Iteration 4018, Loss: 4.24623966217041\n",
            "Training Iteration 4019, Loss: 3.810249090194702\n",
            "Training Iteration 4020, Loss: 2.7495431900024414\n",
            "Training Iteration 4021, Loss: 3.6063449382781982\n",
            "Training Iteration 4022, Loss: 5.607951641082764\n",
            "Training Iteration 4023, Loss: 3.0054850578308105\n",
            "Training Iteration 4024, Loss: 4.637128829956055\n",
            "Training Iteration 4025, Loss: 4.043755054473877\n",
            "Training Iteration 4026, Loss: 3.0480878353118896\n",
            "Training Iteration 4027, Loss: 5.132035255432129\n",
            "Training Iteration 4028, Loss: 3.47491455078125\n",
            "Training Iteration 4029, Loss: 2.57362699508667\n",
            "Training Iteration 4030, Loss: 4.825209617614746\n",
            "Training Iteration 4031, Loss: 3.7066190242767334\n",
            "Training Iteration 4032, Loss: 5.8105878829956055\n",
            "Training Iteration 4033, Loss: 4.500389099121094\n",
            "Training Iteration 4034, Loss: 5.146805286407471\n",
            "Training Iteration 4035, Loss: 3.4180874824523926\n",
            "Training Iteration 4036, Loss: 3.663672924041748\n",
            "Training Iteration 4037, Loss: 4.182043075561523\n",
            "Training Iteration 4038, Loss: 5.677500247955322\n",
            "Training Iteration 4039, Loss: 2.058030366897583\n",
            "Training Iteration 4040, Loss: 6.0695905685424805\n",
            "Training Iteration 4041, Loss: 2.659045457839966\n",
            "Training Iteration 4042, Loss: 5.53106164932251\n",
            "Training Iteration 4043, Loss: 5.778192043304443\n",
            "Training Iteration 4044, Loss: 3.332439422607422\n",
            "Training Iteration 4045, Loss: 2.0599472522735596\n",
            "Training Iteration 4046, Loss: 3.249701738357544\n",
            "Training Iteration 4047, Loss: 4.298724174499512\n",
            "Training Iteration 4048, Loss: 8.31112003326416\n",
            "Training Iteration 4049, Loss: 3.878643274307251\n",
            "Training Iteration 4050, Loss: 6.630978107452393\n",
            "Training Iteration 4051, Loss: 6.541652679443359\n",
            "Training Iteration 4052, Loss: 3.162377119064331\n",
            "Training Iteration 4053, Loss: 5.44910192489624\n",
            "Training Iteration 4054, Loss: 4.6874308586120605\n",
            "Training Iteration 4055, Loss: 3.7590396404266357\n",
            "Training Iteration 4056, Loss: 4.295591354370117\n",
            "Training Iteration 4057, Loss: 5.787688732147217\n",
            "Training Iteration 4058, Loss: 4.107265949249268\n",
            "Training Iteration 4059, Loss: 3.7585365772247314\n",
            "Training Iteration 4060, Loss: 2.7852251529693604\n",
            "Training Iteration 4061, Loss: 3.0915706157684326\n",
            "Training Iteration 4062, Loss: 3.0157322883605957\n",
            "Training Iteration 4063, Loss: 2.1823368072509766\n",
            "Training Iteration 4064, Loss: 3.3663759231567383\n",
            "Training Iteration 4065, Loss: 4.749574184417725\n",
            "Training Iteration 4066, Loss: 3.7059268951416016\n",
            "Training Iteration 4067, Loss: 9.187277793884277\n",
            "Training Iteration 4068, Loss: 4.315797805786133\n",
            "Training Iteration 4069, Loss: 5.5415263175964355\n",
            "Training Iteration 4070, Loss: 3.4463729858398438\n",
            "Training Iteration 4071, Loss: 3.9578161239624023\n",
            "Training Iteration 4072, Loss: 6.090648174285889\n",
            "Training Iteration 4073, Loss: 5.614599704742432\n",
            "Training Iteration 4074, Loss: 4.245032787322998\n",
            "Training Iteration 4075, Loss: 4.480581760406494\n",
            "Training Iteration 4076, Loss: 6.6039042472839355\n",
            "Training Iteration 4077, Loss: 4.21118688583374\n",
            "Training Iteration 4078, Loss: 2.06294322013855\n",
            "Training Iteration 4079, Loss: 6.258673667907715\n",
            "Training Iteration 4080, Loss: 5.923340797424316\n",
            "Training Iteration 4081, Loss: 4.584844589233398\n",
            "Training Iteration 4082, Loss: 4.247745037078857\n",
            "Training Iteration 4083, Loss: 4.825728893280029\n",
            "Training Iteration 4084, Loss: 1.7878154516220093\n",
            "Training Iteration 4085, Loss: 5.495676040649414\n",
            "Training Iteration 4086, Loss: 7.043752670288086\n",
            "Training Iteration 4087, Loss: 2.540391683578491\n",
            "Training Iteration 4088, Loss: 4.989349365234375\n",
            "Training Iteration 4089, Loss: 6.126984596252441\n",
            "Training Iteration 4090, Loss: 3.4246015548706055\n",
            "Training Iteration 4091, Loss: 7.430002689361572\n",
            "Training Iteration 4092, Loss: 4.55333948135376\n",
            "Training Iteration 4093, Loss: 8.131263732910156\n",
            "Training Iteration 4094, Loss: 6.862884521484375\n",
            "Training Iteration 4095, Loss: 8.373841285705566\n",
            "Training Iteration 4096, Loss: 9.48344612121582\n",
            "Training Iteration 4097, Loss: 4.893751621246338\n",
            "Training Iteration 4098, Loss: 2.714251756668091\n",
            "Training Iteration 4099, Loss: 2.8753936290740967\n",
            "Training Iteration 4100, Loss: 5.533873081207275\n",
            "Training Iteration 4101, Loss: 9.522778511047363\n",
            "Training Iteration 4102, Loss: 8.016059875488281\n",
            "Training Iteration 4103, Loss: 4.843352317810059\n",
            "Training Iteration 4104, Loss: 4.200473785400391\n",
            "Training Iteration 4105, Loss: 3.5287346839904785\n",
            "Training Iteration 4106, Loss: 8.047517776489258\n",
            "Training Iteration 4107, Loss: 4.709580898284912\n",
            "Training Iteration 4108, Loss: 7.700390815734863\n",
            "Training Iteration 4109, Loss: 5.438583850860596\n",
            "Training Iteration 4110, Loss: 4.786287307739258\n",
            "Training Iteration 4111, Loss: 10.27480697631836\n",
            "Training Iteration 4112, Loss: 3.6005330085754395\n",
            "Training Iteration 4113, Loss: 2.5235443115234375\n",
            "Training Iteration 4114, Loss: 11.734545707702637\n",
            "Training Iteration 4115, Loss: 5.392510414123535\n",
            "Training Iteration 4116, Loss: 6.0794758796691895\n",
            "Training Iteration 4117, Loss: 2.6222243309020996\n",
            "Training Iteration 4118, Loss: 4.40946102142334\n",
            "Training Iteration 4119, Loss: 4.408021450042725\n",
            "Training Iteration 4120, Loss: 5.486400604248047\n",
            "Training Iteration 4121, Loss: 3.109581232070923\n",
            "Training Iteration 4122, Loss: 2.2092971801757812\n",
            "Training Iteration 4123, Loss: 3.3280553817749023\n",
            "Training Iteration 4124, Loss: 2.52689528465271\n",
            "Training Iteration 4125, Loss: 8.593825340270996\n",
            "Training Iteration 4126, Loss: 6.19955587387085\n",
            "Training Iteration 4127, Loss: 3.9162607192993164\n",
            "Training Iteration 4128, Loss: 4.912785530090332\n",
            "Training Iteration 4129, Loss: 4.564861297607422\n",
            "Training Iteration 4130, Loss: 4.081085205078125\n",
            "Training Iteration 4131, Loss: 3.8373470306396484\n",
            "Training Iteration 4132, Loss: 2.016829013824463\n",
            "Training Iteration 4133, Loss: 4.632498741149902\n",
            "Training Iteration 4134, Loss: 2.588723659515381\n",
            "Training Iteration 4135, Loss: 9.924731254577637\n",
            "Training Iteration 4136, Loss: 11.735114097595215\n",
            "Training Iteration 4137, Loss: 5.286834716796875\n",
            "Training Iteration 4138, Loss: 5.823163032531738\n",
            "Training Iteration 4139, Loss: 4.799188613891602\n",
            "Training Iteration 4140, Loss: 4.42323112487793\n",
            "Training Iteration 4141, Loss: 5.052979946136475\n",
            "Training Iteration 4142, Loss: 5.797885894775391\n",
            "Training Iteration 4143, Loss: 5.242767333984375\n",
            "Training Iteration 4144, Loss: 7.615259647369385\n",
            "Training Iteration 4145, Loss: 8.600728988647461\n",
            "Training Iteration 4146, Loss: 4.026273727416992\n",
            "Training Iteration 4147, Loss: 3.344928026199341\n",
            "Training Iteration 4148, Loss: 3.168152093887329\n",
            "Training Iteration 4149, Loss: 2.219968318939209\n",
            "Training Iteration 4150, Loss: 5.589201927185059\n",
            "Training Iteration 4151, Loss: 3.185711145401001\n",
            "Training Iteration 4152, Loss: 5.017613410949707\n",
            "Training Iteration 4153, Loss: 3.788055419921875\n",
            "Training Iteration 4154, Loss: 6.623514652252197\n",
            "Training Iteration 4155, Loss: 6.934721946716309\n",
            "Training Iteration 4156, Loss: 6.319338321685791\n",
            "Training Iteration 4157, Loss: 6.042288303375244\n",
            "Training Iteration 4158, Loss: 3.227517604827881\n",
            "Training Iteration 4159, Loss: 5.204561233520508\n",
            "Training Iteration 4160, Loss: 4.659448623657227\n",
            "Training Iteration 4161, Loss: 3.7183775901794434\n",
            "Training Iteration 4162, Loss: 2.8582427501678467\n",
            "Training Iteration 4163, Loss: 4.589678764343262\n",
            "Training Iteration 4164, Loss: 8.557658195495605\n",
            "Training Iteration 4165, Loss: 6.301455020904541\n",
            "Training Iteration 4166, Loss: 5.3733320236206055\n",
            "Training Iteration 4167, Loss: 4.697061061859131\n",
            "Training Iteration 4168, Loss: 5.127204895019531\n",
            "Training Iteration 4169, Loss: 5.717557907104492\n",
            "Training Iteration 4170, Loss: 3.9235689640045166\n",
            "Training Iteration 4171, Loss: 4.6434855461120605\n",
            "Training Iteration 4172, Loss: 6.05714750289917\n",
            "Training Iteration 4173, Loss: 2.9428791999816895\n",
            "Training Iteration 4174, Loss: 4.682178020477295\n",
            "Training Iteration 4175, Loss: 3.4343838691711426\n",
            "Training Iteration 4176, Loss: 3.817464590072632\n",
            "Training Iteration 4177, Loss: 3.6725125312805176\n",
            "Training Iteration 4178, Loss: 3.3571550846099854\n",
            "Training Iteration 4179, Loss: 5.392030239105225\n",
            "Training Iteration 4180, Loss: 7.584388256072998\n",
            "Training Iteration 4181, Loss: 4.720714569091797\n",
            "Training Iteration 4182, Loss: 2.9719014167785645\n",
            "Training Iteration 4183, Loss: 2.769153118133545\n",
            "Training Iteration 4184, Loss: 2.1505374908447266\n",
            "Training Iteration 4185, Loss: 3.4261393547058105\n",
            "Training Iteration 4186, Loss: 3.2360382080078125\n",
            "Training Iteration 4187, Loss: 4.623525619506836\n",
            "Training Iteration 4188, Loss: 4.141879558563232\n",
            "Training Iteration 4189, Loss: 4.202255725860596\n",
            "Training Iteration 4190, Loss: 4.064131259918213\n",
            "Training Iteration 4191, Loss: 3.1326701641082764\n",
            "Training Iteration 4192, Loss: 3.647341012954712\n",
            "Training Iteration 4193, Loss: 7.555987358093262\n",
            "Training Iteration 4194, Loss: 3.6232194900512695\n",
            "Training Iteration 4195, Loss: 3.5208263397216797\n",
            "Training Iteration 4196, Loss: 3.494633436203003\n",
            "Training Iteration 4197, Loss: 4.274823188781738\n",
            "Training Iteration 4198, Loss: 4.424177646636963\n",
            "Training Iteration 4199, Loss: 4.166099548339844\n",
            "Training Iteration 4200, Loss: 4.1796064376831055\n",
            "Training Iteration 4201, Loss: 3.8023123741149902\n",
            "Training Iteration 4202, Loss: 2.9439125061035156\n",
            "Training Iteration 4203, Loss: 5.26704216003418\n",
            "Training Iteration 4204, Loss: 2.380009889602661\n",
            "Training Iteration 4205, Loss: 3.4822278022766113\n",
            "Training Iteration 4206, Loss: 3.558018684387207\n",
            "Training Iteration 4207, Loss: 2.46931529045105\n",
            "Training Iteration 4208, Loss: 5.767364501953125\n",
            "Training Iteration 4209, Loss: 3.017551898956299\n",
            "Training Iteration 4210, Loss: 3.8726770877838135\n",
            "Training Iteration 4211, Loss: 3.2115390300750732\n",
            "Training Iteration 4212, Loss: 3.8007400035858154\n",
            "Training Iteration 4213, Loss: 3.4759092330932617\n",
            "Training Iteration 4214, Loss: 6.594601154327393\n",
            "Training Iteration 4215, Loss: 3.3804869651794434\n",
            "Training Iteration 4216, Loss: 4.5513081550598145\n",
            "Training Iteration 4217, Loss: 3.457296371459961\n",
            "Training Iteration 4218, Loss: 5.949218273162842\n",
            "Training Iteration 4219, Loss: 4.467818737030029\n",
            "Training Iteration 4220, Loss: 2.619359254837036\n",
            "Training Iteration 4221, Loss: 3.383870840072632\n",
            "Training Iteration 4222, Loss: 6.919306755065918\n",
            "Training Iteration 4223, Loss: 4.88890266418457\n",
            "Training Iteration 4224, Loss: 2.5731325149536133\n",
            "Training Iteration 4225, Loss: 3.6082558631896973\n",
            "Training Iteration 4226, Loss: 4.398633003234863\n",
            "Training Iteration 4227, Loss: 7.116373062133789\n",
            "Training Iteration 4228, Loss: 5.661369800567627\n",
            "Training Iteration 4229, Loss: 6.692030906677246\n",
            "Training Iteration 4230, Loss: 7.83535099029541\n",
            "Training Iteration 4231, Loss: 4.237569808959961\n",
            "Training Iteration 4232, Loss: 2.5862877368927\n",
            "Training Iteration 4233, Loss: 4.159310817718506\n",
            "Training Iteration 4234, Loss: 5.652252674102783\n",
            "Training Iteration 4235, Loss: 2.9846763610839844\n",
            "Training Iteration 4236, Loss: 6.234646797180176\n",
            "Training Iteration 4237, Loss: 4.546838760375977\n",
            "Training Iteration 4238, Loss: 3.3974344730377197\n",
            "Training Iteration 4239, Loss: 4.310533046722412\n",
            "Training Iteration 4240, Loss: 5.157753944396973\n",
            "Training Iteration 4241, Loss: 9.13092041015625\n",
            "Training Iteration 4242, Loss: 2.727827787399292\n",
            "Training Iteration 4243, Loss: 4.3022236824035645\n",
            "Training Iteration 4244, Loss: 3.6443467140197754\n",
            "Training Iteration 4245, Loss: 4.594720363616943\n",
            "Training Iteration 4246, Loss: 3.8078973293304443\n",
            "Training Iteration 4247, Loss: 4.547375202178955\n",
            "Training Iteration 4248, Loss: 6.303532123565674\n",
            "Training Iteration 4249, Loss: 5.5139594078063965\n",
            "Training Iteration 4250, Loss: 3.4629857540130615\n",
            "Training Iteration 4251, Loss: 6.222120761871338\n",
            "Training Iteration 4252, Loss: 5.815654277801514\n",
            "Training Iteration 4253, Loss: 3.632291793823242\n",
            "Training Iteration 4254, Loss: 3.59635853767395\n",
            "Training Iteration 4255, Loss: 6.090261936187744\n",
            "Training Iteration 4256, Loss: 7.717239856719971\n",
            "Training Iteration 4257, Loss: 5.128753662109375\n",
            "Training Iteration 4258, Loss: 4.724345684051514\n",
            "Training Iteration 4259, Loss: 3.8876194953918457\n",
            "Training Iteration 4260, Loss: 3.8261184692382812\n",
            "Training Iteration 4261, Loss: 2.447087287902832\n",
            "Training Iteration 4262, Loss: 3.545304775238037\n",
            "Training Iteration 4263, Loss: 4.271310329437256\n",
            "Training Iteration 4264, Loss: 5.128873825073242\n",
            "Training Iteration 4265, Loss: 5.144106864929199\n",
            "Training Iteration 4266, Loss: 5.688108444213867\n",
            "Training Iteration 4267, Loss: 2.6664652824401855\n",
            "Training Iteration 4268, Loss: 4.5682053565979\n",
            "Training Iteration 4269, Loss: 5.575374603271484\n",
            "Training Iteration 4270, Loss: 5.373513698577881\n",
            "Training Iteration 4271, Loss: 7.376629829406738\n",
            "Training Iteration 4272, Loss: 1.7412363290786743\n",
            "Training Iteration 4273, Loss: 4.954157829284668\n",
            "Training Iteration 4274, Loss: 5.953794956207275\n",
            "Training Iteration 4275, Loss: 3.0863499641418457\n",
            "Training Iteration 4276, Loss: 3.6107594966888428\n",
            "Training Iteration 4277, Loss: 2.721238851547241\n",
            "Training Iteration 4278, Loss: 5.103757858276367\n",
            "Training Iteration 4279, Loss: 7.448797702789307\n",
            "Training Iteration 4280, Loss: 3.839151382446289\n",
            "Training Iteration 4281, Loss: 2.4599640369415283\n",
            "Training Iteration 4282, Loss: 2.8931803703308105\n",
            "Training Iteration 4283, Loss: 4.811824798583984\n",
            "Training Iteration 4284, Loss: 4.6274309158325195\n",
            "Training Iteration 4285, Loss: 4.9391255378723145\n",
            "Training Iteration 4286, Loss: 1.5543184280395508\n",
            "Training Iteration 4287, Loss: 2.9578850269317627\n",
            "Training Iteration 4288, Loss: 7.296605587005615\n",
            "Training Iteration 4289, Loss: 7.163164138793945\n",
            "Training Iteration 4290, Loss: 4.5194807052612305\n",
            "Training Iteration 4291, Loss: 6.733814716339111\n",
            "Training Iteration 4292, Loss: 6.261246204376221\n",
            "Training Iteration 4293, Loss: 3.7056198120117188\n",
            "Training Iteration 4294, Loss: 5.468033790588379\n",
            "Training Iteration 4295, Loss: 5.572853088378906\n",
            "Training Iteration 4296, Loss: 5.3261003494262695\n",
            "Training Iteration 4297, Loss: 6.141234874725342\n",
            "Training Iteration 4298, Loss: 6.694483757019043\n",
            "Training Iteration 4299, Loss: 6.161134719848633\n",
            "Training Iteration 4300, Loss: 5.024254322052002\n",
            "Training Iteration 4301, Loss: 4.567835807800293\n",
            "Training Iteration 4302, Loss: 3.7137980461120605\n",
            "Training Iteration 4303, Loss: 5.021478176116943\n",
            "Training Iteration 4304, Loss: 3.297926425933838\n",
            "Training Iteration 4305, Loss: 6.545679092407227\n",
            "Training Iteration 4306, Loss: 6.209784984588623\n",
            "Training Iteration 4307, Loss: 5.118448257446289\n",
            "Training Iteration 4308, Loss: 3.6068930625915527\n",
            "Training Iteration 4309, Loss: 3.5078139305114746\n",
            "Training Iteration 4310, Loss: 3.874701976776123\n",
            "Training Iteration 4311, Loss: 6.960912227630615\n",
            "Training Iteration 4312, Loss: 2.4554178714752197\n",
            "Training Iteration 4313, Loss: 8.19627571105957\n",
            "Training Iteration 4314, Loss: 3.914458751678467\n",
            "Training Iteration 4315, Loss: 3.813839912414551\n",
            "Training Iteration 4316, Loss: 4.25185489654541\n",
            "Training Iteration 4317, Loss: 5.967965126037598\n",
            "Training Iteration 4318, Loss: 4.766752243041992\n",
            "Training Iteration 4319, Loss: 8.518782615661621\n",
            "Training Iteration 4320, Loss: 4.048556327819824\n",
            "Training Iteration 4321, Loss: 1.9634242057800293\n",
            "Training Iteration 4322, Loss: 3.629072427749634\n",
            "Training Iteration 4323, Loss: 4.556085109710693\n",
            "Training Iteration 4324, Loss: 5.109366416931152\n",
            "Training Iteration 4325, Loss: 4.635474681854248\n",
            "Training Iteration 4326, Loss: 4.920172691345215\n",
            "Training Iteration 4327, Loss: 6.741015434265137\n",
            "Training Iteration 4328, Loss: 2.5802595615386963\n",
            "Training Iteration 4329, Loss: 4.498966217041016\n",
            "Training Iteration 4330, Loss: 3.7656853199005127\n",
            "Training Iteration 4331, Loss: 4.945146560668945\n",
            "Training Iteration 4332, Loss: 5.206282615661621\n",
            "Training Iteration 4333, Loss: 6.079522132873535\n",
            "Training Iteration 4334, Loss: 6.7772603034973145\n",
            "Training Iteration 4335, Loss: 3.6203250885009766\n",
            "Training Iteration 4336, Loss: 5.048219680786133\n",
            "Training Iteration 4337, Loss: 5.764317989349365\n",
            "Training Iteration 4338, Loss: 5.647665500640869\n",
            "Training Iteration 4339, Loss: 1.8549944162368774\n",
            "Training Iteration 4340, Loss: 6.049691200256348\n",
            "Training Iteration 4341, Loss: 5.964846611022949\n",
            "Training Iteration 4342, Loss: 3.7044332027435303\n",
            "Training Iteration 4343, Loss: 3.274122714996338\n",
            "Training Iteration 4344, Loss: 5.532520294189453\n",
            "Training Iteration 4345, Loss: 3.032238006591797\n",
            "Training Iteration 4346, Loss: 6.126198768615723\n",
            "Training Iteration 4347, Loss: 4.402414321899414\n",
            "Training Iteration 4348, Loss: 4.572558403015137\n",
            "Training Iteration 4349, Loss: 2.5243704319000244\n",
            "Training Iteration 4350, Loss: 4.237668991088867\n",
            "Training Iteration 4351, Loss: 4.4043869972229\n",
            "Training Iteration 4352, Loss: 3.4397988319396973\n",
            "Training Iteration 4353, Loss: 4.238180160522461\n",
            "Training Iteration 4354, Loss: 3.6760048866271973\n",
            "Training Iteration 4355, Loss: 4.278660297393799\n",
            "Training Iteration 4356, Loss: 3.753842830657959\n",
            "Training Iteration 4357, Loss: 8.386981964111328\n",
            "Training Iteration 4358, Loss: 5.089064598083496\n",
            "Training Iteration 4359, Loss: 3.337115526199341\n",
            "Training Iteration 4360, Loss: 3.023834466934204\n",
            "Training Iteration 4361, Loss: 3.9489715099334717\n",
            "Training Iteration 4362, Loss: 5.190629005432129\n",
            "Training Iteration 4363, Loss: 5.114443778991699\n",
            "Training Iteration 4364, Loss: 3.3603084087371826\n",
            "Training Iteration 4365, Loss: 4.972665786743164\n",
            "Training Iteration 4366, Loss: 6.346903324127197\n",
            "Training Iteration 4367, Loss: 4.482523441314697\n",
            "Training Iteration 4368, Loss: 6.897412300109863\n",
            "Training Iteration 4369, Loss: 5.167634010314941\n",
            "Training Iteration 4370, Loss: 4.434356689453125\n",
            "Training Iteration 4371, Loss: 4.60124397277832\n",
            "Training Iteration 4372, Loss: 4.625550746917725\n",
            "Training Iteration 4373, Loss: 4.238508224487305\n",
            "Training Iteration 4374, Loss: 4.020178318023682\n",
            "Training Iteration 4375, Loss: 6.197689533233643\n",
            "Training Iteration 4376, Loss: 1.785154104232788\n",
            "Training Iteration 4377, Loss: 5.969919204711914\n",
            "Training Iteration 4378, Loss: 4.386977672576904\n",
            "Training Iteration 4379, Loss: 3.727750539779663\n",
            "Training Iteration 4380, Loss: 4.279311180114746\n",
            "Training Iteration 4381, Loss: 8.082840919494629\n",
            "Training Iteration 4382, Loss: 3.281101703643799\n",
            "Training Iteration 4383, Loss: 4.555393695831299\n",
            "Training Iteration 4384, Loss: 6.490693092346191\n",
            "Training Iteration 4385, Loss: 3.7270925045013428\n",
            "Training Iteration 4386, Loss: 6.636165618896484\n",
            "Training Iteration 4387, Loss: 5.4759907722473145\n",
            "Training Iteration 4388, Loss: 5.331775188446045\n",
            "Training Iteration 4389, Loss: 2.6572813987731934\n",
            "Training Iteration 4390, Loss: 2.6648452281951904\n",
            "Training Iteration 4391, Loss: 5.606213569641113\n",
            "Training Iteration 4392, Loss: 8.17026424407959\n",
            "Training Iteration 4393, Loss: 1.4896873235702515\n",
            "Training Iteration 4394, Loss: 3.997507333755493\n",
            "Training Iteration 4395, Loss: 3.849050283432007\n",
            "Training Iteration 4396, Loss: 2.835432767868042\n",
            "Training Iteration 4397, Loss: 3.3045053482055664\n",
            "Training Iteration 4398, Loss: 4.420328140258789\n",
            "Training Iteration 4399, Loss: 7.81088924407959\n",
            "Training Iteration 4400, Loss: 5.505759239196777\n",
            "Training Iteration 4401, Loss: 6.0868120193481445\n",
            "Training Iteration 4402, Loss: 5.464324951171875\n",
            "Training Iteration 4403, Loss: 2.2079708576202393\n",
            "Training Iteration 4404, Loss: 6.409783840179443\n",
            "Training Iteration 4405, Loss: 3.0315566062927246\n",
            "Training Iteration 4406, Loss: 4.949847221374512\n",
            "Training Iteration 4407, Loss: 4.677855491638184\n",
            "Training Iteration 4408, Loss: 5.033785820007324\n",
            "Training Iteration 4409, Loss: 8.027677536010742\n",
            "Training Iteration 4410, Loss: 7.958907604217529\n",
            "Training Iteration 4411, Loss: 3.9010610580444336\n",
            "Training Iteration 4412, Loss: 3.0101752281188965\n",
            "Training Iteration 4413, Loss: 7.843454360961914\n",
            "Training Iteration 4414, Loss: 3.8706412315368652\n",
            "Training Iteration 4415, Loss: 4.697950839996338\n",
            "Training Iteration 4416, Loss: 7.267430782318115\n",
            "Training Iteration 4417, Loss: 6.204103946685791\n",
            "Training Iteration 4418, Loss: 7.671320915222168\n",
            "Training Iteration 4419, Loss: 3.832576036453247\n",
            "Training Iteration 4420, Loss: 6.211016654968262\n",
            "Training Iteration 4421, Loss: 7.746365547180176\n",
            "Training Iteration 4422, Loss: 3.7518320083618164\n",
            "Training Iteration 4423, Loss: 7.814405918121338\n",
            "Training Iteration 4424, Loss: 2.394043207168579\n",
            "Training Iteration 4425, Loss: 4.0752410888671875\n",
            "Training Iteration 4426, Loss: 3.8306469917297363\n",
            "Training Iteration 4427, Loss: 6.701773166656494\n",
            "Training Iteration 4428, Loss: 7.278182029724121\n",
            "Training Iteration 4429, Loss: 8.844082832336426\n",
            "Training Iteration 4430, Loss: 11.573620796203613\n",
            "Training Iteration 4431, Loss: 7.882032871246338\n",
            "Training Iteration 4432, Loss: 5.575562953948975\n",
            "Training Iteration 4433, Loss: 2.1078109741210938\n",
            "Training Iteration 4434, Loss: 6.882913589477539\n",
            "Training Iteration 4435, Loss: 6.482049942016602\n",
            "Training Iteration 4436, Loss: 9.401423454284668\n",
            "Training Iteration 4437, Loss: 6.99209451675415\n",
            "Training Iteration 4438, Loss: 4.460188865661621\n",
            "Training Iteration 4439, Loss: 6.4293107986450195\n",
            "Training Iteration 4440, Loss: 2.9885683059692383\n",
            "Training Iteration 4441, Loss: 4.656005859375\n",
            "Training Iteration 4442, Loss: 5.2924370765686035\n",
            "Training Iteration 4443, Loss: 4.897425651550293\n",
            "Training Iteration 4444, Loss: 9.24262523651123\n",
            "Training Iteration 4445, Loss: 4.458999156951904\n",
            "Training Iteration 4446, Loss: 1.8187036514282227\n",
            "Training Iteration 4447, Loss: 3.255575656890869\n",
            "Training Iteration 4448, Loss: 2.8368470668792725\n",
            "Training Iteration 4449, Loss: 5.774054527282715\n",
            "Training Iteration 4450, Loss: 2.556790590286255\n",
            "Training Iteration 4451, Loss: 6.43979549407959\n",
            "Training Iteration 4452, Loss: 6.824332237243652\n",
            "Training Iteration 4453, Loss: 3.0970077514648438\n",
            "Training Iteration 4454, Loss: 4.386050224304199\n",
            "Training Iteration 4455, Loss: 4.369261741638184\n",
            "Training Iteration 4456, Loss: 3.8460991382598877\n",
            "Training Iteration 4457, Loss: 4.060530185699463\n",
            "Training Iteration 4458, Loss: 4.054598808288574\n",
            "Training Iteration 4459, Loss: 8.794342041015625\n",
            "Training Iteration 4460, Loss: 3.6417531967163086\n",
            "Training Iteration 4461, Loss: 1.715444564819336\n",
            "Training Iteration 4462, Loss: 2.3963582515716553\n",
            "Training Iteration 4463, Loss: 3.44240665435791\n",
            "Training Iteration 4464, Loss: 1.6911791563034058\n",
            "Training Iteration 4465, Loss: 4.865494251251221\n",
            "Training Iteration 4466, Loss: 2.996063470840454\n",
            "Training Iteration 4467, Loss: 5.868130683898926\n",
            "Training Iteration 4468, Loss: 4.197729110717773\n",
            "Training Iteration 4469, Loss: 7.447659015655518\n",
            "Training Iteration 4470, Loss: 3.6140999794006348\n",
            "Training Iteration 4471, Loss: 3.547149896621704\n",
            "Training Iteration 4472, Loss: 4.909637451171875\n",
            "Training Iteration 4473, Loss: 2.8989083766937256\n",
            "Training Iteration 4474, Loss: 4.097691535949707\n",
            "Training Iteration 4475, Loss: 6.24957275390625\n",
            "Training Iteration 4476, Loss: 5.819706916809082\n",
            "Training Iteration 4477, Loss: 6.820590019226074\n",
            "Training Iteration 4478, Loss: 6.32056188583374\n",
            "Training Iteration 4479, Loss: 5.640979766845703\n",
            "Training Iteration 4480, Loss: 3.3808863162994385\n",
            "Training Iteration 4481, Loss: 4.990901947021484\n",
            "Training Iteration 4482, Loss: 5.825046539306641\n",
            "Training Iteration 4483, Loss: 5.356714725494385\n",
            "Training Iteration 4484, Loss: 4.325113296508789\n",
            "Training Iteration 4485, Loss: 6.01168155670166\n",
            "Training Iteration 4486, Loss: 2.6866507530212402\n",
            "Training Iteration 4487, Loss: 2.428670644760132\n",
            "Training Iteration 4488, Loss: 4.339522361755371\n",
            "Training Iteration 4489, Loss: 3.0166985988616943\n",
            "Training Iteration 4490, Loss: 3.3256819248199463\n",
            "Training Iteration 4491, Loss: 6.600261688232422\n",
            "Training Iteration 4492, Loss: 3.6877167224884033\n",
            "Training Iteration 4493, Loss: 5.070634841918945\n",
            "Training Iteration 4494, Loss: 2.554614305496216\n",
            "Training Iteration 4495, Loss: 5.869555473327637\n",
            "Training Iteration 4496, Loss: 3.0716071128845215\n",
            "Training Iteration 4497, Loss: 4.376607894897461\n",
            "Training Iteration 4498, Loss: 4.250458717346191\n",
            "Training Iteration 4499, Loss: 3.962470054626465\n",
            "Training Iteration 4500, Loss: 3.7462096214294434\n",
            "Training Iteration 4501, Loss: 4.767332077026367\n",
            "Training Iteration 4502, Loss: 6.518956661224365\n",
            "Training Iteration 4503, Loss: 3.0163841247558594\n",
            "Training Iteration 4504, Loss: 3.2221665382385254\n",
            "Training Iteration 4505, Loss: 5.180696487426758\n",
            "Training Iteration 4506, Loss: 3.454521894454956\n",
            "Training Iteration 4507, Loss: 3.5477230548858643\n",
            "Training Iteration 4508, Loss: 9.563104629516602\n",
            "Training Iteration 4509, Loss: 5.507070541381836\n",
            "Training Iteration 4510, Loss: 3.9563634395599365\n",
            "Training Iteration 4511, Loss: 8.22617244720459\n",
            "Training Iteration 4512, Loss: 3.7671122550964355\n",
            "Training Iteration 4513, Loss: 3.19832181930542\n",
            "Training Iteration 4514, Loss: 6.9585700035095215\n",
            "Training Iteration 4515, Loss: 3.073735475540161\n",
            "Training Iteration 4516, Loss: 6.288055419921875\n",
            "Training Iteration 4517, Loss: 7.640353202819824\n",
            "Training Iteration 4518, Loss: 5.44110631942749\n",
            "Training Iteration 4519, Loss: 4.1604132652282715\n",
            "Training Iteration 4520, Loss: 3.8558459281921387\n",
            "Training Iteration 4521, Loss: 5.577072620391846\n",
            "Training Iteration 4522, Loss: 5.76370096206665\n",
            "Training Iteration 4523, Loss: 5.009437561035156\n",
            "Training Iteration 4524, Loss: 4.214121341705322\n",
            "Training Iteration 4525, Loss: 7.874268054962158\n",
            "Training Iteration 4526, Loss: 5.023021221160889\n",
            "Training Iteration 4527, Loss: 6.529073715209961\n",
            "Training Iteration 4528, Loss: 3.599775791168213\n",
            "Training Iteration 4529, Loss: 4.805902004241943\n",
            "Training Iteration 4530, Loss: 8.10782527923584\n",
            "Training Iteration 4531, Loss: 3.221188545227051\n",
            "Training Iteration 4532, Loss: 9.641907691955566\n",
            "Training Iteration 4533, Loss: 7.26185941696167\n",
            "Training Iteration 4534, Loss: 5.354833602905273\n",
            "Training Iteration 4535, Loss: 9.766897201538086\n",
            "Training Iteration 4536, Loss: 2.889890193939209\n",
            "Training Iteration 4537, Loss: 6.736118316650391\n",
            "Training Iteration 4538, Loss: 6.193329334259033\n",
            "Training Iteration 4539, Loss: 0.799518346786499\n",
            "Training Iteration 4540, Loss: 4.382172584533691\n",
            "Training Iteration 4541, Loss: 5.889674186706543\n",
            "Training Iteration 4542, Loss: 3.249624729156494\n",
            "Training Iteration 4543, Loss: 3.711362361907959\n",
            "Training Iteration 4544, Loss: 3.27774715423584\n",
            "Training Iteration 4545, Loss: 4.796948432922363\n",
            "Training Iteration 4546, Loss: 4.827042579650879\n",
            "Training Iteration 4547, Loss: 4.132647514343262\n",
            "Training Iteration 4548, Loss: 6.122668266296387\n",
            "Training Iteration 4549, Loss: 4.992645263671875\n",
            "Training Iteration 4550, Loss: 3.2827370166778564\n",
            "Training Iteration 4551, Loss: 3.1611554622650146\n",
            "Training Iteration 4552, Loss: 2.9358677864074707\n",
            "Training Iteration 4553, Loss: 4.128759860992432\n",
            "Training Iteration 4554, Loss: 4.6117963790893555\n",
            "Training Iteration 4555, Loss: 3.178579330444336\n",
            "Training Iteration 4556, Loss: 4.4592509269714355\n",
            "Training Iteration 4557, Loss: 2.9968976974487305\n",
            "Training Iteration 4558, Loss: 5.089119911193848\n",
            "Training Iteration 4559, Loss: 2.41892671585083\n",
            "Training Iteration 4560, Loss: 3.550485134124756\n",
            "Training Iteration 4561, Loss: 4.611978530883789\n",
            "Training Iteration 4562, Loss: 3.7343504428863525\n",
            "Training Iteration 4563, Loss: 6.674373626708984\n",
            "Training Iteration 4564, Loss: 5.200104713439941\n",
            "Training Iteration 4565, Loss: 5.8188934326171875\n",
            "Training Iteration 4566, Loss: 6.2604780197143555\n",
            "Training Iteration 4567, Loss: 5.099555969238281\n",
            "Training Iteration 4568, Loss: 5.831048011779785\n",
            "Training Iteration 4569, Loss: 4.155421257019043\n",
            "Training Iteration 4570, Loss: 6.570797920227051\n",
            "Training Iteration 4571, Loss: 5.343796253204346\n",
            "Training Iteration 4572, Loss: 4.853245258331299\n",
            "Training Iteration 4573, Loss: 5.421149730682373\n",
            "Training Iteration 4574, Loss: 3.449383020401001\n",
            "Training Iteration 4575, Loss: 4.4855523109436035\n",
            "Training Iteration 4576, Loss: 4.578826427459717\n",
            "Training Iteration 4577, Loss: 6.588775634765625\n",
            "Training Iteration 4578, Loss: 6.111782073974609\n",
            "Training Iteration 4579, Loss: 4.241539001464844\n",
            "Training Iteration 4580, Loss: 8.040550231933594\n",
            "Training Iteration 4581, Loss: 8.439460754394531\n",
            "Training Iteration 4582, Loss: 4.976319789886475\n",
            "Training Iteration 4583, Loss: 4.5403571128845215\n",
            "Training Iteration 4584, Loss: 4.137113571166992\n",
            "Training Iteration 4585, Loss: 3.7956318855285645\n",
            "Training Iteration 4586, Loss: 2.178717851638794\n",
            "Training Iteration 4587, Loss: 4.0504560470581055\n",
            "Training Iteration 4588, Loss: 5.987621307373047\n",
            "Training Iteration 4589, Loss: 5.314919471740723\n",
            "Training Iteration 4590, Loss: 3.7461836338043213\n",
            "Training Iteration 4591, Loss: 3.7982332706451416\n",
            "Training Iteration 4592, Loss: 2.4157488346099854\n",
            "Training Iteration 4593, Loss: 7.9575958251953125\n",
            "Training Iteration 4594, Loss: 8.820561408996582\n",
            "Training Iteration 4595, Loss: 6.339594841003418\n",
            "Training Iteration 4596, Loss: 8.151606559753418\n",
            "Training Iteration 4597, Loss: 3.338240623474121\n",
            "Training Iteration 4598, Loss: 5.612147331237793\n",
            "Training Iteration 4599, Loss: 4.863335132598877\n",
            "Training Iteration 4600, Loss: 4.82679557800293\n",
            "Training Iteration 4601, Loss: 3.330491065979004\n",
            "Training Iteration 4602, Loss: 3.9761364459991455\n",
            "Training Iteration 4603, Loss: 6.0916876792907715\n",
            "Training Iteration 4604, Loss: 4.314056873321533\n",
            "Training Iteration 4605, Loss: 3.9724698066711426\n",
            "Training Iteration 4606, Loss: 3.3116676807403564\n",
            "Training Iteration 4607, Loss: 2.763389825820923\n",
            "Training Iteration 4608, Loss: 2.619560480117798\n",
            "Training Iteration 4609, Loss: 3.6418991088867188\n",
            "Training Iteration 4610, Loss: 3.7172067165374756\n",
            "Training Iteration 4611, Loss: 3.5153017044067383\n",
            "Training Iteration 4612, Loss: 2.8718276023864746\n",
            "Training Iteration 4613, Loss: 2.277409315109253\n",
            "Training Iteration 4614, Loss: 2.611423969268799\n",
            "Training Iteration 4615, Loss: 4.312267303466797\n",
            "Training Iteration 4616, Loss: 3.0145492553710938\n",
            "Training Iteration 4617, Loss: 4.580460071563721\n",
            "Training Iteration 4618, Loss: 3.70889949798584\n",
            "Training Iteration 4619, Loss: 5.860437870025635\n",
            "Training Iteration 4620, Loss: 5.336042404174805\n",
            "Training Iteration 4621, Loss: 2.6242659091949463\n",
            "Training Iteration 4622, Loss: 0.9537047743797302\n",
            "Training Iteration 4623, Loss: 3.260225296020508\n",
            "Training Iteration 4624, Loss: 5.245100021362305\n",
            "Training Iteration 4625, Loss: 3.51118803024292\n",
            "Training Iteration 4626, Loss: 3.2198424339294434\n",
            "Training Iteration 4627, Loss: 4.040726661682129\n",
            "Training Iteration 4628, Loss: 4.097746849060059\n",
            "Training Iteration 4629, Loss: 5.907519817352295\n",
            "Training Iteration 4630, Loss: 4.462466239929199\n",
            "Training Iteration 4631, Loss: 6.407742977142334\n",
            "Training Iteration 4632, Loss: 2.786919593811035\n",
            "Training Iteration 4633, Loss: 6.144168853759766\n",
            "Training Iteration 4634, Loss: 4.024001121520996\n",
            "Training Iteration 4635, Loss: 2.4212098121643066\n",
            "Training Iteration 4636, Loss: 4.909306049346924\n",
            "Training Iteration 4637, Loss: 4.5709943771362305\n",
            "Training Iteration 4638, Loss: 4.163507461547852\n",
            "Training Iteration 4639, Loss: 4.596137523651123\n",
            "Training Iteration 4640, Loss: 5.295567989349365\n",
            "Training Iteration 4641, Loss: 3.7465107440948486\n",
            "Training Iteration 4642, Loss: 4.740629196166992\n",
            "Training Iteration 4643, Loss: 4.2992939949035645\n",
            "Training Iteration 4644, Loss: 3.408210039138794\n",
            "Training Iteration 4645, Loss: 2.457716703414917\n",
            "Training Iteration 4646, Loss: 3.1251330375671387\n",
            "Training Iteration 4647, Loss: 4.837615966796875\n",
            "Training Iteration 4648, Loss: 2.1205039024353027\n",
            "Training Iteration 4649, Loss: 4.95831823348999\n",
            "Training Iteration 4650, Loss: 1.9707529544830322\n",
            "Training Iteration 4651, Loss: 5.4650654792785645\n",
            "Training Iteration 4652, Loss: 3.7071261405944824\n",
            "Training Iteration 4653, Loss: 2.632551431655884\n",
            "Training Iteration 4654, Loss: 3.0501303672790527\n",
            "Training Iteration 4655, Loss: 4.349992752075195\n",
            "Training Iteration 4656, Loss: 3.0995545387268066\n",
            "Training Iteration 4657, Loss: 6.238621711730957\n",
            "Training Iteration 4658, Loss: 5.607639312744141\n",
            "Training Iteration 4659, Loss: 6.460487365722656\n",
            "Training Iteration 4660, Loss: 3.3515074253082275\n",
            "Training Iteration 4661, Loss: 4.397422790527344\n",
            "Training Iteration 4662, Loss: 3.1628894805908203\n",
            "Training Iteration 4663, Loss: 4.769735813140869\n",
            "Training Iteration 4664, Loss: 7.51039457321167\n",
            "Training Iteration 4665, Loss: 8.587606430053711\n",
            "Training Iteration 4666, Loss: 4.139508247375488\n",
            "Training Iteration 4667, Loss: 4.3915228843688965\n",
            "Training Iteration 4668, Loss: 3.8523576259613037\n",
            "Training Iteration 4669, Loss: 5.459461212158203\n",
            "Training Iteration 4670, Loss: 4.197876453399658\n",
            "Training Iteration 4671, Loss: 4.9193010330200195\n",
            "Training Iteration 4672, Loss: 3.553330659866333\n",
            "Training Iteration 4673, Loss: 4.227325916290283\n",
            "Training Iteration 4674, Loss: 4.274880409240723\n",
            "Training Iteration 4675, Loss: 3.562401533126831\n",
            "Training Iteration 4676, Loss: 3.226720094680786\n",
            "Training Iteration 4677, Loss: 2.9487931728363037\n",
            "Training Iteration 4678, Loss: 4.11208438873291\n",
            "Training Iteration 4679, Loss: 3.992854595184326\n",
            "Training Iteration 4680, Loss: 2.845177412033081\n",
            "Training Iteration 4681, Loss: 4.106200695037842\n",
            "Training Iteration 4682, Loss: 3.681445837020874\n",
            "Training Iteration 4683, Loss: 5.943462371826172\n",
            "Training Iteration 4684, Loss: 4.659185409545898\n",
            "Training Iteration 4685, Loss: 4.205301284790039\n",
            "Training Iteration 4686, Loss: 4.047983646392822\n",
            "Training Iteration 4687, Loss: 3.6565775871276855\n",
            "Training Iteration 4688, Loss: 4.688155651092529\n",
            "Training Iteration 4689, Loss: 3.130492687225342\n",
            "Training Iteration 4690, Loss: 4.702171802520752\n",
            "Training Iteration 4691, Loss: 4.246060371398926\n",
            "Training Iteration 4692, Loss: 3.7483935356140137\n",
            "Training Iteration 4693, Loss: 3.957751989364624\n",
            "Training Iteration 4694, Loss: 4.401792049407959\n",
            "Training Iteration 4695, Loss: 3.3265416622161865\n",
            "Training Iteration 4696, Loss: 4.242577075958252\n",
            "Training Iteration 4697, Loss: 5.530794620513916\n",
            "Training Iteration 4698, Loss: 5.543951034545898\n",
            "Training Iteration 4699, Loss: 8.006028175354004\n",
            "Training Iteration 4700, Loss: 3.12923002243042\n",
            "Training Iteration 4701, Loss: 4.8505330085754395\n",
            "Training Iteration 4702, Loss: 3.4879815578460693\n",
            "Training Iteration 4703, Loss: 3.9879980087280273\n",
            "Training Iteration 4704, Loss: 1.8197762966156006\n",
            "Training Iteration 4705, Loss: 4.218482494354248\n",
            "Training Iteration 4706, Loss: 4.944860458374023\n",
            "Training Iteration 4707, Loss: 2.9274556636810303\n",
            "Training Iteration 4708, Loss: 4.2267746925354\n",
            "Training Iteration 4709, Loss: 5.856972694396973\n",
            "Training Iteration 4710, Loss: 2.465104579925537\n",
            "Training Iteration 4711, Loss: 4.392266750335693\n",
            "Training Iteration 4712, Loss: 4.622568130493164\n",
            "Training Iteration 4713, Loss: 6.073803901672363\n",
            "Training Iteration 4714, Loss: 2.3724470138549805\n",
            "Training Iteration 4715, Loss: 4.690165042877197\n",
            "Training Iteration 4716, Loss: 2.5368425846099854\n",
            "Training Iteration 4717, Loss: 3.067462921142578\n",
            "Training Iteration 4718, Loss: 2.9754419326782227\n",
            "Training Iteration 4719, Loss: 5.48696756362915\n",
            "Training Iteration 4720, Loss: 5.434582233428955\n",
            "Training Iteration 4721, Loss: 6.09624719619751\n",
            "Training Iteration 4722, Loss: 3.2303004264831543\n",
            "Training Iteration 4723, Loss: 5.742578506469727\n",
            "Training Iteration 4724, Loss: 3.2587571144104004\n",
            "Training Iteration 4725, Loss: 4.7310895919799805\n",
            "Training Iteration 4726, Loss: 6.019816875457764\n",
            "Training Iteration 4727, Loss: 5.844351768493652\n",
            "Training Iteration 4728, Loss: 3.557842493057251\n",
            "Training Iteration 4729, Loss: 4.472603797912598\n",
            "Training Iteration 4730, Loss: 6.161480903625488\n",
            "Training Iteration 4731, Loss: 5.188318252563477\n",
            "Training Iteration 4732, Loss: 4.721516132354736\n",
            "Training Iteration 4733, Loss: 4.2060346603393555\n",
            "Training Iteration 4734, Loss: 3.3776180744171143\n",
            "Training Iteration 4735, Loss: 3.635678291320801\n",
            "Training Iteration 4736, Loss: 3.6128013134002686\n",
            "Training Iteration 4737, Loss: 3.4765312671661377\n",
            "Training Iteration 4738, Loss: 8.189154624938965\n",
            "Training Iteration 4739, Loss: 6.201733589172363\n",
            "Training Iteration 4740, Loss: 3.702974796295166\n",
            "Training Iteration 4741, Loss: 3.021888256072998\n",
            "Training Iteration 4742, Loss: 2.786550521850586\n",
            "Training Iteration 4743, Loss: 3.186075210571289\n",
            "Training Iteration 4744, Loss: 3.7978405952453613\n",
            "Training Iteration 4745, Loss: 3.74355149269104\n",
            "Training Iteration 4746, Loss: 6.528919696807861\n",
            "Training Iteration 4747, Loss: 3.9844090938568115\n",
            "Training Iteration 4748, Loss: 2.632246494293213\n",
            "Training Iteration 4749, Loss: 5.552015781402588\n",
            "Training Iteration 4750, Loss: 3.7468044757843018\n",
            "Training Iteration 4751, Loss: 4.161642074584961\n",
            "Training Iteration 4752, Loss: 3.697807788848877\n",
            "Training Iteration 4753, Loss: 5.713429927825928\n",
            "Training Iteration 4754, Loss: 4.7806549072265625\n",
            "Training Iteration 4755, Loss: 4.26825475692749\n",
            "Training Iteration 4756, Loss: 5.473346710205078\n",
            "Training Iteration 4757, Loss: 2.83259654045105\n",
            "Training Iteration 4758, Loss: 3.7319328784942627\n",
            "Training Iteration 4759, Loss: 2.270559072494507\n",
            "Training Iteration 4760, Loss: 2.565884590148926\n",
            "Training Iteration 4761, Loss: 3.87166166305542\n",
            "Training Iteration 4762, Loss: 3.8321118354797363\n",
            "Training Iteration 4763, Loss: 2.710104465484619\n",
            "Training Iteration 4764, Loss: 5.813382148742676\n",
            "Training Iteration 4765, Loss: 6.906355381011963\n",
            "Training Iteration 4766, Loss: 3.465500831604004\n",
            "Training Iteration 4767, Loss: 3.778299570083618\n",
            "Training Iteration 4768, Loss: 3.082263946533203\n",
            "Training Iteration 4769, Loss: 1.7849901914596558\n",
            "Training Iteration 4770, Loss: 2.691096782684326\n",
            "Training Iteration 4771, Loss: 2.6797099113464355\n",
            "Training Iteration 4772, Loss: 6.503316879272461\n",
            "Training Iteration 4773, Loss: 7.194882869720459\n",
            "Training Iteration 4774, Loss: 9.305219650268555\n",
            "Training Iteration 4775, Loss: 4.251396179199219\n",
            "Training Iteration 4776, Loss: 6.035772800445557\n",
            "Training Iteration 4777, Loss: 3.214165210723877\n",
            "Training Iteration 4778, Loss: 4.841267108917236\n",
            "Training Iteration 4779, Loss: 4.364607810974121\n",
            "Training Iteration 4780, Loss: 2.6258127689361572\n",
            "Training Iteration 4781, Loss: 4.7107834815979\n",
            "Training Iteration 4782, Loss: 5.4224700927734375\n",
            "Training Iteration 4783, Loss: 2.1160926818847656\n",
            "Training Iteration 4784, Loss: 2.82129168510437\n",
            "Training Iteration 4785, Loss: 4.981797218322754\n",
            "Training Iteration 4786, Loss: 7.766667366027832\n",
            "Training Iteration 4787, Loss: 2.1625335216522217\n",
            "Training Iteration 4788, Loss: 2.215622663497925\n",
            "Training Iteration 4789, Loss: 5.110324382781982\n",
            "Training Iteration 4790, Loss: 8.737332344055176\n",
            "Training Iteration 4791, Loss: 2.2044503688812256\n",
            "Training Iteration 4792, Loss: 3.9977095127105713\n",
            "Training Iteration 4793, Loss: 6.756696701049805\n",
            "Training Iteration 4794, Loss: 4.695642471313477\n",
            "Training Iteration 4795, Loss: 4.259185314178467\n",
            "Training Iteration 4796, Loss: 5.964831352233887\n",
            "Training Iteration 4797, Loss: 2.980417251586914\n",
            "Training Iteration 4798, Loss: 6.138296127319336\n",
            "Training Iteration 4799, Loss: 3.2252683639526367\n",
            "Training Iteration 4800, Loss: 4.196132183074951\n",
            "Training Iteration 4801, Loss: 5.15408992767334\n",
            "Training Iteration 4802, Loss: 5.644349575042725\n",
            "Training Iteration 4803, Loss: 2.275817632675171\n",
            "Training Iteration 4804, Loss: 3.666776180267334\n",
            "Training Iteration 4805, Loss: 2.380753517150879\n",
            "Training Iteration 4806, Loss: 3.959224224090576\n",
            "Training Iteration 4807, Loss: 4.788393020629883\n",
            "Training Iteration 4808, Loss: 4.38460111618042\n",
            "Training Iteration 4809, Loss: 3.751493215560913\n",
            "Training Iteration 4810, Loss: 6.608473777770996\n",
            "Training Iteration 4811, Loss: 2.4717957973480225\n",
            "Training Iteration 4812, Loss: 3.6460390090942383\n",
            "Training Iteration 4813, Loss: 3.364596128463745\n",
            "Training Iteration 4814, Loss: 1.9525058269500732\n",
            "Training Iteration 4815, Loss: 4.840419769287109\n",
            "Training Iteration 4816, Loss: 2.413125514984131\n",
            "Training Iteration 4817, Loss: 2.538175344467163\n",
            "Training Iteration 4818, Loss: 4.139429092407227\n",
            "Training Iteration 4819, Loss: 4.951679229736328\n",
            "Training Iteration 4820, Loss: 4.843933582305908\n",
            "Training Iteration 4821, Loss: 1.9836235046386719\n",
            "Training Iteration 4822, Loss: 3.8250999450683594\n",
            "Training Iteration 4823, Loss: 3.727750539779663\n",
            "Training Iteration 4824, Loss: 4.975574970245361\n",
            "Training Iteration 4825, Loss: 1.816117525100708\n",
            "Training Iteration 4826, Loss: 3.369709014892578\n",
            "Training Iteration 4827, Loss: 4.940077781677246\n",
            "Training Iteration 4828, Loss: 1.426114559173584\n",
            "Training Iteration 4829, Loss: 4.401179790496826\n",
            "Training Iteration 4830, Loss: 4.886994361877441\n",
            "Training Iteration 4831, Loss: 2.741570472717285\n",
            "Training Iteration 4832, Loss: 5.757734775543213\n",
            "Training Iteration 4833, Loss: 4.302494525909424\n",
            "Training Iteration 4834, Loss: 7.108009338378906\n",
            "Training Iteration 4835, Loss: 4.762269020080566\n",
            "Training Iteration 4836, Loss: 2.130941390991211\n",
            "Training Iteration 4837, Loss: 1.5712031126022339\n",
            "Training Iteration 4838, Loss: 3.4420669078826904\n",
            "Training Iteration 4839, Loss: 6.477833271026611\n",
            "Training Iteration 4840, Loss: 2.7994706630706787\n",
            "Training Iteration 4841, Loss: 5.602614879608154\n",
            "Training Iteration 4842, Loss: 3.695089340209961\n",
            "Training Iteration 4843, Loss: 2.133439064025879\n",
            "Training Iteration 4844, Loss: 3.60969877243042\n",
            "Training Iteration 4845, Loss: 2.539546012878418\n",
            "Training Iteration 4846, Loss: 3.3120315074920654\n",
            "Training Iteration 4847, Loss: 4.812952041625977\n",
            "Training Iteration 4848, Loss: 3.9097187519073486\n",
            "Training Iteration 4849, Loss: 2.7601606845855713\n",
            "Training Iteration 4850, Loss: 3.606072425842285\n",
            "Training Iteration 4851, Loss: 4.036928176879883\n",
            "Training Iteration 4852, Loss: 3.7919178009033203\n",
            "Training Iteration 4853, Loss: 6.796787738800049\n",
            "Training Iteration 4854, Loss: 3.9888017177581787\n",
            "Training Iteration 4855, Loss: 5.566832542419434\n",
            "Training Iteration 4856, Loss: 3.317074775695801\n",
            "Training Iteration 4857, Loss: 4.504290580749512\n",
            "Training Iteration 4858, Loss: 1.5369603633880615\n",
            "Training Iteration 4859, Loss: 3.2747254371643066\n",
            "Training Iteration 4860, Loss: 5.276700496673584\n",
            "Training Iteration 4861, Loss: 4.577744960784912\n",
            "Training Iteration 4862, Loss: 4.207061767578125\n",
            "Training Iteration 4863, Loss: 2.991159200668335\n",
            "Training Iteration 4864, Loss: 5.670428276062012\n",
            "Training Iteration 4865, Loss: 5.035832405090332\n",
            "Training Iteration 4866, Loss: 4.513612747192383\n",
            "Training Iteration 4867, Loss: 3.7724804878234863\n",
            "Training Iteration 4868, Loss: 3.6665310859680176\n",
            "Training Iteration 4869, Loss: 2.4540369510650635\n",
            "Training Iteration 4870, Loss: 3.40415096282959\n",
            "Training Iteration 4871, Loss: 3.109483242034912\n",
            "Training Iteration 4872, Loss: 3.5719969272613525\n",
            "Training Iteration 4873, Loss: 3.081402540206909\n",
            "Training Iteration 4874, Loss: 1.658212423324585\n",
            "Training Iteration 4875, Loss: 1.993459701538086\n",
            "Training Iteration 4876, Loss: 7.141529083251953\n",
            "Training Iteration 4877, Loss: 2.7438693046569824\n",
            "Training Iteration 4878, Loss: 3.7283506393432617\n",
            "Training Iteration 4879, Loss: 2.70950984954834\n",
            "Training Iteration 4880, Loss: 3.484849452972412\n",
            "Training Iteration 4881, Loss: 8.470492362976074\n",
            "Training Iteration 4882, Loss: 5.439914226531982\n",
            "Training Iteration 4883, Loss: 5.745432376861572\n",
            "Training Iteration 4884, Loss: 8.827495574951172\n",
            "Training Iteration 4885, Loss: 10.178055763244629\n",
            "Training Iteration 4886, Loss: 3.745999813079834\n",
            "Training Iteration 4887, Loss: 4.741772174835205\n",
            "Training Iteration 4888, Loss: 5.693948745727539\n",
            "Training Iteration 4889, Loss: 2.825956344604492\n",
            "Training Iteration 4890, Loss: 2.4380297660827637\n",
            "Training Iteration 4891, Loss: 6.2066850662231445\n",
            "Training Iteration 4892, Loss: 3.084977626800537\n",
            "Training Iteration 4893, Loss: 3.774244785308838\n",
            "Training Iteration 4894, Loss: 3.619002342224121\n",
            "Training Iteration 4895, Loss: 8.462163925170898\n",
            "Training Iteration 4896, Loss: 5.193672180175781\n",
            "Training Iteration 4897, Loss: 4.950533390045166\n",
            "Training Iteration 4898, Loss: 2.8221569061279297\n",
            "Training Iteration 4899, Loss: 4.409892559051514\n",
            "Training Iteration 4900, Loss: 3.177347183227539\n",
            "Training Iteration 4901, Loss: 3.2345919609069824\n",
            "Training Iteration 4902, Loss: 5.310507297515869\n",
            "Training Iteration 4903, Loss: 1.6801729202270508\n",
            "Training Iteration 4904, Loss: 4.65825891494751\n",
            "Training Iteration 4905, Loss: 2.346021890640259\n",
            "Training Iteration 4906, Loss: 5.443248271942139\n",
            "Training Iteration 4907, Loss: 3.687753915786743\n",
            "Training Iteration 4908, Loss: 5.470308780670166\n",
            "Training Iteration 4909, Loss: 5.371700763702393\n",
            "Training Iteration 4910, Loss: 2.8233964443206787\n",
            "Training Iteration 4911, Loss: 3.475961685180664\n",
            "Training Iteration 4912, Loss: 3.5782852172851562\n",
            "Training Iteration 4913, Loss: 5.868576526641846\n",
            "Training Iteration 4914, Loss: 4.060009956359863\n",
            "Training Iteration 4915, Loss: 4.627269744873047\n",
            "Training Iteration 4916, Loss: 1.4677374362945557\n",
            "Training Iteration 4917, Loss: 3.5555198192596436\n",
            "Training Iteration 4918, Loss: 4.398887634277344\n",
            "Training Iteration 4919, Loss: 4.377902507781982\n",
            "Training Iteration 4920, Loss: 4.358536243438721\n",
            "Training Iteration 4921, Loss: 3.7994863986968994\n",
            "Training Iteration 4922, Loss: 4.754317283630371\n",
            "Training Iteration 4923, Loss: 3.2955336570739746\n",
            "Training Iteration 4924, Loss: 2.6087961196899414\n",
            "Training Iteration 4925, Loss: 5.902174949645996\n",
            "Training Iteration 4926, Loss: 3.134398937225342\n",
            "Training Iteration 4927, Loss: 6.174572944641113\n",
            "Training Iteration 4928, Loss: 8.156229972839355\n",
            "Training Iteration 4929, Loss: 4.982994079589844\n",
            "Training Iteration 4930, Loss: 7.519410133361816\n",
            "Training Iteration 4931, Loss: 2.7103683948516846\n",
            "Training Iteration 4932, Loss: 5.064668655395508\n",
            "Training Iteration 4933, Loss: 2.385974884033203\n",
            "Training Iteration 4934, Loss: 5.780461311340332\n",
            "Training Iteration 4935, Loss: 4.885615348815918\n",
            "Training Iteration 4936, Loss: 3.807003974914551\n",
            "Training Iteration 4937, Loss: 5.363396167755127\n",
            "Training Iteration 4938, Loss: 3.715909719467163\n",
            "Training Iteration 4939, Loss: 5.943118095397949\n",
            "Training Iteration 4940, Loss: 7.48991060256958\n",
            "Training Iteration 4941, Loss: 1.9181697368621826\n",
            "Training Iteration 4942, Loss: 3.772672653198242\n",
            "Training Iteration 4943, Loss: 7.835367679595947\n",
            "Training Iteration 4944, Loss: 1.6991629600524902\n",
            "Training Iteration 4945, Loss: 3.777709484100342\n",
            "Training Iteration 4946, Loss: 3.4051520824432373\n",
            "Training Iteration 4947, Loss: 3.41752028465271\n",
            "Training Iteration 4948, Loss: 4.572420597076416\n",
            "Training Iteration 4949, Loss: 4.007722854614258\n",
            "Training Iteration 4950, Loss: 4.832284927368164\n",
            "Training Iteration 4951, Loss: 8.502821922302246\n",
            "Training Iteration 4952, Loss: 5.51223087310791\n",
            "Training Iteration 4953, Loss: 4.12354040145874\n",
            "Training Iteration 4954, Loss: 6.644491672515869\n",
            "Training Iteration 4955, Loss: 2.762085199356079\n",
            "Training Iteration 4956, Loss: 4.965415000915527\n",
            "Training Iteration 4957, Loss: 8.6476411819458\n",
            "Training Iteration 4958, Loss: 4.550626277923584\n",
            "Training Iteration 4959, Loss: 2.984541654586792\n",
            "Training Iteration 4960, Loss: 3.760885715484619\n",
            "Training Iteration 4961, Loss: 3.088221311569214\n",
            "Training Iteration 4962, Loss: 5.248932361602783\n",
            "Training Iteration 4963, Loss: 3.941574811935425\n",
            "Training Iteration 4964, Loss: 3.7280433177948\n",
            "Training Iteration 4965, Loss: 5.7972564697265625\n",
            "Training Iteration 4966, Loss: 5.09236478805542\n",
            "Training Iteration 4967, Loss: 5.833120346069336\n",
            "Training Iteration 4968, Loss: 4.118495941162109\n",
            "Training Iteration 4969, Loss: 4.383473873138428\n",
            "Training Iteration 4970, Loss: 7.722172737121582\n",
            "Training Iteration 4971, Loss: 3.3280673027038574\n",
            "Training Iteration 4972, Loss: 3.663973569869995\n",
            "Training Iteration 4973, Loss: 4.961401462554932\n",
            "Training Iteration 4974, Loss: 3.0021471977233887\n",
            "Training Iteration 4975, Loss: 6.412228584289551\n",
            "Training Iteration 4976, Loss: 6.774268627166748\n",
            "Training Iteration 4977, Loss: 1.9801278114318848\n",
            "Training Iteration 4978, Loss: 3.2799737453460693\n",
            "Training Iteration 4979, Loss: 2.7899203300476074\n",
            "Training Iteration 4980, Loss: 3.573910713195801\n",
            "Training Iteration 4981, Loss: 4.423646926879883\n",
            "Training Iteration 4982, Loss: 2.4019742012023926\n",
            "Training Iteration 4983, Loss: 5.2720489501953125\n",
            "Training Iteration 4984, Loss: 4.277595520019531\n",
            "Training Iteration 4985, Loss: 4.1154465675354\n",
            "Training Iteration 4986, Loss: 3.7199721336364746\n",
            "Training Iteration 4987, Loss: 5.285098075866699\n",
            "Training Iteration 4988, Loss: 2.6265432834625244\n",
            "Training Iteration 4989, Loss: 3.4729011058807373\n",
            "Training Iteration 4990, Loss: 4.504364967346191\n",
            "Training Iteration 4991, Loss: 3.3152828216552734\n",
            "Training Iteration 4992, Loss: 5.893281936645508\n",
            "Training Iteration 4993, Loss: 5.306074142456055\n",
            "Training Iteration 4994, Loss: 4.954183578491211\n",
            "Training Iteration 4995, Loss: 4.557507514953613\n",
            "Training Iteration 4996, Loss: 3.2457656860351562\n",
            "Training Iteration 4997, Loss: 1.189695954322815\n",
            "Training Iteration 4998, Loss: 3.475379467010498\n",
            "Training Iteration 4999, Loss: 5.894322872161865\n",
            "Training Iteration 5000, Loss: 4.1329569816589355\n",
            "Training Iteration 5001, Loss: 2.7689902782440186\n",
            "Training Iteration 5002, Loss: 1.4338858127593994\n",
            "Training Iteration 5003, Loss: 4.904824733734131\n",
            "Training Iteration 5004, Loss: 3.3794972896575928\n",
            "Training Iteration 5005, Loss: 5.056606292724609\n",
            "Training Iteration 5006, Loss: 4.171504974365234\n",
            "Training Iteration 5007, Loss: 4.739028453826904\n",
            "Training Iteration 5008, Loss: 4.943479061126709\n",
            "Training Iteration 5009, Loss: 5.353224277496338\n",
            "Training Iteration 5010, Loss: 3.370488166809082\n",
            "Training Iteration 5011, Loss: 5.295485496520996\n",
            "Training Iteration 5012, Loss: 2.419135093688965\n",
            "Training Iteration 5013, Loss: 6.6345720291137695\n",
            "Training Iteration 5014, Loss: 5.118511199951172\n",
            "Training Iteration 5015, Loss: 4.18255615234375\n",
            "Training Iteration 5016, Loss: 2.7999954223632812\n",
            "Training Iteration 5017, Loss: 5.087493419647217\n",
            "Training Iteration 5018, Loss: 4.482475757598877\n",
            "Training Iteration 5019, Loss: 1.8421761989593506\n",
            "Training Iteration 5020, Loss: 6.875601768493652\n",
            "Training Iteration 5021, Loss: 3.374255657196045\n",
            "Training Iteration 5022, Loss: 5.128004550933838\n",
            "Training Iteration 5023, Loss: 6.15558385848999\n",
            "Training Iteration 5024, Loss: 2.7575840950012207\n",
            "Training Iteration 5025, Loss: 0.8053732514381409\n",
            "Training Iteration 5026, Loss: 3.7635395526885986\n",
            "Training Iteration 5027, Loss: 4.514359474182129\n",
            "Training Iteration 5028, Loss: 2.583955764770508\n",
            "Training Iteration 5029, Loss: 3.6519665718078613\n",
            "Training Iteration 5030, Loss: 4.929160118103027\n",
            "Training Iteration 5031, Loss: 3.232337236404419\n",
            "Training Iteration 5032, Loss: 2.017547845840454\n",
            "Training Iteration 5033, Loss: 4.831292629241943\n",
            "Training Iteration 5034, Loss: 2.9287285804748535\n",
            "Training Iteration 5035, Loss: 4.488926887512207\n",
            "Training Iteration 5036, Loss: 3.482186794281006\n",
            "Training Iteration 5037, Loss: 3.929353713989258\n",
            "Training Iteration 5038, Loss: 5.217605113983154\n",
            "Training Iteration 5039, Loss: 5.402709484100342\n",
            "Training Iteration 5040, Loss: 4.776095390319824\n",
            "Training Iteration 5041, Loss: 3.167677879333496\n",
            "Training Iteration 5042, Loss: 2.975099563598633\n",
            "Training Iteration 5043, Loss: 4.227855682373047\n",
            "Training Iteration 5044, Loss: 7.983644485473633\n",
            "Training Iteration 5045, Loss: 2.4094181060791016\n",
            "Training Iteration 5046, Loss: 2.247675895690918\n",
            "Training Iteration 5047, Loss: 6.839576721191406\n",
            "Training Iteration 5048, Loss: 4.033644199371338\n",
            "Training Iteration 5049, Loss: 5.287443161010742\n",
            "Training Iteration 5050, Loss: 4.436061859130859\n",
            "Training Iteration 5051, Loss: 4.339015483856201\n",
            "Training Iteration 5052, Loss: 6.179285526275635\n",
            "Training Iteration 5053, Loss: 5.633279800415039\n",
            "Training Iteration 5054, Loss: 6.386811256408691\n",
            "Training Iteration 5055, Loss: 3.9943959712982178\n",
            "Training Iteration 5056, Loss: 6.12598991394043\n",
            "Training Iteration 5057, Loss: 6.074053764343262\n",
            "Training Iteration 5058, Loss: 3.9052770137786865\n",
            "Training Iteration 5059, Loss: 2.063347578048706\n",
            "Training Iteration 5060, Loss: 4.0599541664123535\n",
            "Training Iteration 5061, Loss: 3.1019773483276367\n",
            "Training Iteration 5062, Loss: 11.914812088012695\n",
            "Training Iteration 5063, Loss: 3.7937521934509277\n",
            "Training Iteration 5064, Loss: 4.880247116088867\n",
            "Training Iteration 5065, Loss: 3.4056122303009033\n",
            "Training Iteration 5066, Loss: 3.4513230323791504\n",
            "Training Iteration 5067, Loss: 3.4011800289154053\n",
            "Training Iteration 5068, Loss: 3.41336989402771\n",
            "Training Iteration 5069, Loss: 6.218734264373779\n",
            "Training Iteration 5070, Loss: 5.651690483093262\n",
            "Training Iteration 5071, Loss: 5.210874557495117\n",
            "Training Iteration 5072, Loss: 7.993908882141113\n",
            "Training Iteration 5073, Loss: 4.30356502532959\n",
            "Training Iteration 5074, Loss: 5.611806869506836\n",
            "Training Iteration 5075, Loss: 4.477491855621338\n",
            "Training Iteration 5076, Loss: 6.712957859039307\n",
            "Training Iteration 5077, Loss: 4.276049613952637\n",
            "Training Iteration 5078, Loss: 5.014059066772461\n",
            "Training Iteration 5079, Loss: 1.9031822681427002\n",
            "Training Iteration 5080, Loss: 5.053467750549316\n",
            "Training Iteration 5081, Loss: 4.964559078216553\n",
            "Training Iteration 5082, Loss: 4.513265609741211\n",
            "Training Iteration 5083, Loss: 3.9684956073760986\n",
            "Training Iteration 5084, Loss: 4.083680152893066\n",
            "Training Iteration 5085, Loss: 2.151599407196045\n",
            "Training Iteration 5086, Loss: 3.080658435821533\n",
            "Training Iteration 5087, Loss: 4.672418594360352\n",
            "Training Iteration 5088, Loss: 5.048558235168457\n",
            "Training Iteration 5089, Loss: 5.773891925811768\n",
            "Training Iteration 5090, Loss: 4.652736663818359\n",
            "Training Iteration 5091, Loss: 4.444069862365723\n",
            "Training Iteration 5092, Loss: 4.569932460784912\n",
            "Training Iteration 5093, Loss: 5.41602087020874\n",
            "Training Iteration 5094, Loss: 6.909361362457275\n",
            "Training Iteration 5095, Loss: 6.949795722961426\n",
            "Training Iteration 5096, Loss: 7.738563537597656\n",
            "Training Iteration 5097, Loss: 8.400283813476562\n",
            "Training Iteration 5098, Loss: 5.8324689865112305\n",
            "Training Iteration 5099, Loss: 8.404558181762695\n",
            "Training Iteration 5100, Loss: 5.802830219268799\n",
            "Training Iteration 5101, Loss: 6.87474250793457\n",
            "Training Iteration 5102, Loss: 2.7408790588378906\n",
            "Training Iteration 5103, Loss: 2.1862189769744873\n",
            "Training Iteration 5104, Loss: 4.575588703155518\n",
            "Training Iteration 5105, Loss: 6.4079999923706055\n",
            "Training Iteration 5106, Loss: 8.386343955993652\n",
            "Training Iteration 5107, Loss: 5.917403221130371\n",
            "Training Iteration 5108, Loss: 5.538302898406982\n",
            "Training Iteration 5109, Loss: 3.369180917739868\n",
            "Training Iteration 5110, Loss: 4.0146565437316895\n",
            "Training Iteration 5111, Loss: 2.3092567920684814\n",
            "Training Iteration 5112, Loss: 3.768740653991699\n",
            "Training Iteration 5113, Loss: 4.776833534240723\n",
            "Training Iteration 5114, Loss: 9.718156814575195\n",
            "Training Iteration 5115, Loss: 2.899472236633301\n",
            "Training Iteration 5116, Loss: 8.132401466369629\n",
            "Training Iteration 5117, Loss: 5.134323596954346\n",
            "Training Iteration 5118, Loss: 8.727327346801758\n",
            "Training Iteration 5119, Loss: 5.475936412811279\n",
            "Training Iteration 5120, Loss: 6.885916233062744\n",
            "Training Iteration 5121, Loss: 1.6740691661834717\n",
            "Training Iteration 5122, Loss: 5.981392860412598\n",
            "Training Iteration 5123, Loss: 7.559965133666992\n",
            "Training Iteration 5124, Loss: 3.673402786254883\n",
            "Training Iteration 5125, Loss: 4.411219596862793\n",
            "Training Iteration 5126, Loss: 4.814006328582764\n",
            "Training Iteration 5127, Loss: 3.6718387603759766\n",
            "Training Iteration 5128, Loss: 6.548035621643066\n",
            "Training Iteration 5129, Loss: 2.9792678356170654\n",
            "Training Iteration 5130, Loss: 3.823441982269287\n",
            "Training Iteration 5131, Loss: 10.355708122253418\n",
            "Training Iteration 5132, Loss: 4.483891010284424\n",
            "Training Iteration 5133, Loss: 2.6278235912323\n",
            "Training Iteration 5134, Loss: 5.576283931732178\n",
            "Training Iteration 5135, Loss: 4.104527473449707\n",
            "Training Iteration 5136, Loss: 3.226564407348633\n",
            "Training Iteration 5137, Loss: 2.145479202270508\n",
            "Training Iteration 5138, Loss: 5.518986225128174\n",
            "Training Iteration 5139, Loss: 3.316814661026001\n",
            "Training Iteration 5140, Loss: 5.58298921585083\n",
            "Training Iteration 5141, Loss: 3.957657814025879\n",
            "Training Iteration 5142, Loss: 2.817899703979492\n",
            "Training Iteration 5143, Loss: 5.226285457611084\n",
            "Training Iteration 5144, Loss: 6.285417079925537\n",
            "Training Iteration 5145, Loss: 4.828578948974609\n",
            "Training Iteration 5146, Loss: 7.792179584503174\n",
            "Training Iteration 5147, Loss: 5.877262592315674\n",
            "Training Iteration 5148, Loss: 5.289116382598877\n",
            "Training Iteration 5149, Loss: 4.316873073577881\n",
            "Training Iteration 5150, Loss: 4.360296726226807\n",
            "Training Iteration 5151, Loss: 5.308123588562012\n",
            "Training Iteration 5152, Loss: 5.590677261352539\n",
            "Training Iteration 5153, Loss: 0.8763428926467896\n",
            "Training Iteration 5154, Loss: 2.2474894523620605\n",
            "Training Iteration 5155, Loss: 6.1357526779174805\n",
            "Training Iteration 5156, Loss: 2.7994210720062256\n",
            "Training Iteration 5157, Loss: 3.4014220237731934\n",
            "Training Iteration 5158, Loss: 4.755807876586914\n",
            "Training Iteration 5159, Loss: 4.9769110679626465\n",
            "Training Iteration 5160, Loss: 2.5705490112304688\n",
            "Training Iteration 5161, Loss: 3.336235523223877\n",
            "Training Iteration 5162, Loss: 4.30415678024292\n",
            "Training Iteration 5163, Loss: 3.512446641921997\n",
            "Training Iteration 5164, Loss: 3.3007452487945557\n",
            "Training Iteration 5165, Loss: 5.374974727630615\n",
            "Training Iteration 5166, Loss: 5.802027225494385\n",
            "Training Iteration 5167, Loss: 7.497978210449219\n",
            "Training Iteration 5168, Loss: 6.088427543640137\n",
            "Training Iteration 5169, Loss: 3.384873867034912\n",
            "Training Iteration 5170, Loss: 3.495229959487915\n",
            "Training Iteration 5171, Loss: 4.675830841064453\n",
            "Training Iteration 5172, Loss: 3.363004684448242\n",
            "Training Iteration 5173, Loss: 3.2615833282470703\n",
            "Training Iteration 5174, Loss: 5.268682956695557\n",
            "Training Iteration 5175, Loss: 2.9099578857421875\n",
            "Training Iteration 5176, Loss: 8.309033393859863\n",
            "Training Iteration 5177, Loss: 2.8772168159484863\n",
            "Training Iteration 5178, Loss: 5.301392555236816\n",
            "Training Iteration 5179, Loss: 3.4950342178344727\n",
            "Training Iteration 5180, Loss: 3.3752825260162354\n",
            "Training Iteration 5181, Loss: 7.06893253326416\n",
            "Training Iteration 5182, Loss: 4.9851179122924805\n",
            "Training Iteration 5183, Loss: 3.4160406589508057\n",
            "Training Iteration 5184, Loss: 4.526505947113037\n",
            "Training Iteration 5185, Loss: 3.7712464332580566\n",
            "Training Iteration 5186, Loss: 4.566805839538574\n",
            "Training Iteration 5187, Loss: 4.365780353546143\n",
            "Training Iteration 5188, Loss: 2.889897346496582\n",
            "Training Iteration 5189, Loss: 4.862715721130371\n",
            "Training Iteration 5190, Loss: 5.243042945861816\n",
            "Training Iteration 5191, Loss: 4.905147075653076\n",
            "Training Iteration 5192, Loss: 2.852738380432129\n",
            "Training Iteration 5193, Loss: 4.448487281799316\n",
            "Training Iteration 5194, Loss: 3.955361843109131\n",
            "Training Iteration 5195, Loss: 3.341332197189331\n",
            "Training Iteration 5196, Loss: 5.276904582977295\n",
            "Training Iteration 5197, Loss: 4.94051456451416\n",
            "Training Iteration 5198, Loss: 3.2404942512512207\n",
            "Training Iteration 5199, Loss: 7.524603843688965\n",
            "Training Iteration 5200, Loss: 2.134298801422119\n",
            "Training Iteration 5201, Loss: 6.708635330200195\n",
            "Training Iteration 5202, Loss: 4.742455005645752\n",
            "Training Iteration 5203, Loss: 3.2857067584991455\n",
            "Training Iteration 5204, Loss: 7.152461051940918\n",
            "Training Iteration 5205, Loss: 4.418789386749268\n",
            "Training Iteration 5206, Loss: 5.177325248718262\n",
            "Training Iteration 5207, Loss: 2.7189149856567383\n",
            "Training Iteration 5208, Loss: 4.199722766876221\n",
            "Training Iteration 5209, Loss: 3.9209139347076416\n",
            "Training Iteration 5210, Loss: 5.072417736053467\n",
            "Training Iteration 5211, Loss: 7.130093574523926\n",
            "Training Iteration 5212, Loss: 3.194648504257202\n",
            "Training Iteration 5213, Loss: 5.490471839904785\n",
            "Training Iteration 5214, Loss: 3.8883256912231445\n",
            "Training Iteration 5215, Loss: 2.154167890548706\n",
            "Training Iteration 5216, Loss: 3.655515193939209\n",
            "Training Iteration 5217, Loss: 2.0511515140533447\n",
            "Training Iteration 5218, Loss: 11.133637428283691\n",
            "Training Iteration 5219, Loss: 3.2852513790130615\n",
            "Training Iteration 5220, Loss: 3.2367584705352783\n",
            "Training Iteration 5221, Loss: 7.348453998565674\n",
            "Training Iteration 5222, Loss: 3.4438676834106445\n",
            "Training Iteration 5223, Loss: 5.950128078460693\n",
            "Training Iteration 5224, Loss: 2.676949977874756\n",
            "Training Iteration 5225, Loss: 2.8123221397399902\n",
            "Training Iteration 5226, Loss: 7.974701404571533\n",
            "Training Iteration 5227, Loss: 3.971442222595215\n",
            "Training Iteration 5228, Loss: 4.825765132904053\n",
            "Training Iteration 5229, Loss: 2.780518054962158\n",
            "Training Iteration 5230, Loss: 4.786779403686523\n",
            "Training Iteration 5231, Loss: 5.519155025482178\n",
            "Training Iteration 5232, Loss: 3.691100597381592\n",
            "Training Iteration 5233, Loss: 3.098029613494873\n",
            "Training Iteration 5234, Loss: 8.012837409973145\n",
            "Training Iteration 5235, Loss: 6.567111015319824\n",
            "Training Iteration 5236, Loss: 3.959606647491455\n",
            "Training Iteration 5237, Loss: 1.5785325765609741\n",
            "Training Iteration 5238, Loss: 3.4764366149902344\n",
            "Training Iteration 5239, Loss: 5.724532604217529\n",
            "Training Iteration 5240, Loss: 6.970145225524902\n",
            "Training Iteration 5241, Loss: 4.1718645095825195\n",
            "Training Iteration 5242, Loss: 3.5700230598449707\n",
            "Training Iteration 5243, Loss: 4.327066421508789\n",
            "Training Iteration 5244, Loss: 5.969600200653076\n",
            "Training Iteration 5245, Loss: 3.7752838134765625\n",
            "Training Iteration 5246, Loss: 3.727612257003784\n",
            "Training Iteration 5247, Loss: 3.3051695823669434\n",
            "Training Iteration 5248, Loss: 4.778564453125\n",
            "Training Iteration 5249, Loss: 5.3525261878967285\n",
            "Training Iteration 5250, Loss: 2.5400171279907227\n",
            "Training Iteration 5251, Loss: 3.9552290439605713\n",
            "Training Iteration 5252, Loss: 2.993076801300049\n",
            "Training Iteration 5253, Loss: 7.199236869812012\n",
            "Training Iteration 5254, Loss: 2.858696937561035\n",
            "Training Iteration 5255, Loss: 2.2282819747924805\n",
            "Training Iteration 5256, Loss: 3.727306604385376\n",
            "Training Iteration 5257, Loss: 1.6715447902679443\n",
            "Training Iteration 5258, Loss: 7.3785719871521\n",
            "Training Iteration 5259, Loss: 4.432693004608154\n",
            "Training Iteration 5260, Loss: 1.6474608182907104\n",
            "Training Iteration 5261, Loss: 2.008934736251831\n",
            "Training Iteration 5262, Loss: 4.767043113708496\n",
            "Training Iteration 5263, Loss: 5.391151428222656\n",
            "Training Iteration 5264, Loss: 2.8636882305145264\n",
            "Training Iteration 5265, Loss: 4.121061325073242\n",
            "Training Iteration 5266, Loss: 5.054818630218506\n",
            "Training Iteration 5267, Loss: 2.0461480617523193\n",
            "Training Iteration 5268, Loss: 2.5722081661224365\n",
            "Training Iteration 5269, Loss: 6.3943939208984375\n",
            "Training Iteration 5270, Loss: 4.176112174987793\n",
            "Training Iteration 5271, Loss: 4.464025497436523\n",
            "Training Iteration 5272, Loss: 1.8502328395843506\n",
            "Training Iteration 5273, Loss: 2.451658010482788\n",
            "Training Iteration 5274, Loss: 1.2191495895385742\n",
            "Training Iteration 5275, Loss: 3.1869351863861084\n",
            "Training Iteration 5276, Loss: 4.307199001312256\n",
            "Training Iteration 5277, Loss: 2.7103464603424072\n",
            "Training Iteration 5278, Loss: 4.818182945251465\n",
            "Training Iteration 5279, Loss: 4.3795166015625\n",
            "Training Iteration 5280, Loss: 4.662818431854248\n",
            "Training Iteration 5281, Loss: 3.047349214553833\n",
            "Training Iteration 5282, Loss: 2.42604398727417\n",
            "Training Iteration 5283, Loss: 6.004556655883789\n",
            "Training Iteration 5284, Loss: 4.9447340965271\n",
            "Training Iteration 5285, Loss: 3.223402976989746\n",
            "Training Iteration 5286, Loss: 5.01209020614624\n",
            "Training Iteration 5287, Loss: 4.805392742156982\n",
            "Training Iteration 5288, Loss: 3.5826332569122314\n",
            "Training Iteration 5289, Loss: 3.1320383548736572\n",
            "Training Iteration 5290, Loss: 3.3709967136383057\n",
            "Training Iteration 5291, Loss: 4.795511245727539\n",
            "Training Iteration 5292, Loss: 2.449596643447876\n",
            "Training Iteration 5293, Loss: 5.330790042877197\n",
            "Training Iteration 5294, Loss: 5.0849127769470215\n",
            "Training Iteration 5295, Loss: 4.594021320343018\n",
            "Training Iteration 5296, Loss: 5.170324802398682\n",
            "Training Iteration 5297, Loss: 7.949403285980225\n",
            "Training Iteration 5298, Loss: 4.003599643707275\n",
            "Training Iteration 5299, Loss: 3.177060127258301\n",
            "Training Iteration 5300, Loss: 3.997368335723877\n",
            "Training Iteration 5301, Loss: 2.508047342300415\n",
            "Training Iteration 5302, Loss: 2.3414599895477295\n",
            "Training Iteration 5303, Loss: 2.5086617469787598\n",
            "Training Iteration 5304, Loss: 3.5550858974456787\n",
            "Training Iteration 5305, Loss: 5.3134002685546875\n",
            "Training Iteration 5306, Loss: 7.042953014373779\n",
            "Training Iteration 5307, Loss: 4.449069976806641\n",
            "Training Iteration 5308, Loss: 4.642052173614502\n",
            "Training Iteration 5309, Loss: 4.558684349060059\n",
            "Training Iteration 5310, Loss: 4.242151260375977\n",
            "Training Iteration 5311, Loss: 4.960488796234131\n",
            "Training Iteration 5312, Loss: 4.8101677894592285\n",
            "Training Iteration 5313, Loss: 2.8119170665740967\n",
            "Training Iteration 5314, Loss: 4.7754082679748535\n",
            "Training Iteration 5315, Loss: 3.349137783050537\n",
            "Training Iteration 5316, Loss: 4.968912124633789\n",
            "Training Iteration 5317, Loss: 5.4774017333984375\n",
            "Training Iteration 5318, Loss: 4.824118137359619\n",
            "Training Iteration 5319, Loss: 2.5377681255340576\n",
            "Training Iteration 5320, Loss: 4.043834209442139\n",
            "Training Iteration 5321, Loss: 4.01469612121582\n",
            "Training Iteration 5322, Loss: 5.0253167152404785\n",
            "Training Iteration 5323, Loss: 5.946216106414795\n",
            "Training Iteration 5324, Loss: 3.3116681575775146\n",
            "Training Iteration 5325, Loss: 4.906972885131836\n",
            "Training Iteration 5326, Loss: 7.0670037269592285\n",
            "Training Iteration 5327, Loss: 5.687000751495361\n",
            "Training Iteration 5328, Loss: 6.661075115203857\n",
            "Training Iteration 5329, Loss: 7.096870422363281\n",
            "Training Iteration 5330, Loss: 6.5181660652160645\n",
            "Training Iteration 5331, Loss: 3.7602498531341553\n",
            "Training Iteration 5332, Loss: 4.915975093841553\n",
            "Training Iteration 5333, Loss: 3.334094524383545\n",
            "Training Iteration 5334, Loss: 3.666562557220459\n",
            "Training Iteration 5335, Loss: 4.787242412567139\n",
            "Training Iteration 5336, Loss: 6.350187301635742\n",
            "Training Iteration 5337, Loss: 8.82861042022705\n",
            "Training Iteration 5338, Loss: 7.0085930824279785\n",
            "Training Iteration 5339, Loss: 2.628749370574951\n",
            "Training Iteration 5340, Loss: 8.571375846862793\n",
            "Training Iteration 5341, Loss: 6.454899787902832\n",
            "Training Iteration 5342, Loss: 2.356905221939087\n",
            "Training Iteration 5343, Loss: 4.389224529266357\n",
            "Training Iteration 5344, Loss: 7.932173252105713\n",
            "Training Iteration 5345, Loss: 5.950742721557617\n",
            "Training Iteration 5346, Loss: 7.057516574859619\n",
            "Training Iteration 5347, Loss: 6.596055507659912\n",
            "Training Iteration 5348, Loss: 7.384836673736572\n",
            "Training Iteration 5349, Loss: 3.8540916442871094\n",
            "Training Iteration 5350, Loss: 5.02216911315918\n",
            "Training Iteration 5351, Loss: 5.383641719818115\n",
            "Training Iteration 5352, Loss: 4.499447822570801\n",
            "Training Iteration 5353, Loss: 6.0711493492126465\n",
            "Training Iteration 5354, Loss: 6.540623664855957\n",
            "Training Iteration 5355, Loss: 4.137083053588867\n",
            "Training Iteration 5356, Loss: 3.1371660232543945\n",
            "Training Iteration 5357, Loss: 3.0492820739746094\n",
            "Training Iteration 5358, Loss: 5.522712707519531\n",
            "Training Iteration 5359, Loss: 5.879965305328369\n",
            "Training Iteration 5360, Loss: 3.2269206047058105\n",
            "Training Iteration 5361, Loss: 4.331980228424072\n",
            "Training Iteration 5362, Loss: 4.152725696563721\n",
            "Training Iteration 5363, Loss: 5.657691478729248\n",
            "Training Iteration 5364, Loss: 4.540417671203613\n",
            "Training Iteration 5365, Loss: 1.2683302164077759\n",
            "Training Iteration 5366, Loss: 2.4753167629241943\n",
            "Training Iteration 5367, Loss: 1.1802542209625244\n",
            "Training Iteration 5368, Loss: 3.150334119796753\n",
            "Training Iteration 5369, Loss: 3.2052745819091797\n",
            "Training Iteration 5370, Loss: 5.235673904418945\n",
            "Training Iteration 5371, Loss: 3.7410778999328613\n",
            "Training Iteration 5372, Loss: 3.3561296463012695\n",
            "Training Iteration 5373, Loss: 2.9866504669189453\n",
            "Training Iteration 5374, Loss: 3.7225732803344727\n",
            "Training Iteration 5375, Loss: 7.136327743530273\n",
            "Training Iteration 5376, Loss: 4.502610206604004\n",
            "Training Iteration 5377, Loss: 3.4745073318481445\n",
            "Training Iteration 5378, Loss: 3.4424219131469727\n",
            "Training Iteration 5379, Loss: 4.898136138916016\n",
            "Training Iteration 5380, Loss: 6.435744285583496\n",
            "Training Iteration 5381, Loss: 3.4367146492004395\n",
            "Training Iteration 5382, Loss: 3.325183868408203\n",
            "Training Iteration 5383, Loss: 5.736069679260254\n",
            "Training Iteration 5384, Loss: 6.45818567276001\n",
            "Training Iteration 5385, Loss: 3.6832540035247803\n",
            "Training Iteration 5386, Loss: 3.4556188583374023\n",
            "Training Iteration 5387, Loss: 4.165441513061523\n",
            "Training Iteration 5388, Loss: 6.2892842292785645\n",
            "Training Iteration 5389, Loss: 3.0850577354431152\n",
            "Training Iteration 5390, Loss: 8.711273193359375\n",
            "Training Iteration 5391, Loss: 2.9651339054107666\n",
            "Training Iteration 5392, Loss: 3.1193273067474365\n",
            "Training Iteration 5393, Loss: 2.5983266830444336\n",
            "Training Iteration 5394, Loss: 2.6051766872406006\n",
            "Training Iteration 5395, Loss: 4.728529453277588\n",
            "Training Iteration 5396, Loss: 2.4443745613098145\n",
            "Training Iteration 5397, Loss: 2.842245101928711\n",
            "Training Iteration 5398, Loss: 4.53143310546875\n",
            "Training Iteration 5399, Loss: 5.6985673904418945\n",
            "Training Iteration 5400, Loss: 7.120812892913818\n",
            "Training Iteration 5401, Loss: 5.271283149719238\n",
            "Training Iteration 5402, Loss: 3.773900270462036\n",
            "Training Iteration 5403, Loss: 4.201703071594238\n",
            "Training Iteration 5404, Loss: 4.48039436340332\n",
            "Training Iteration 5405, Loss: 4.812561511993408\n",
            "Training Iteration 5406, Loss: 3.744947671890259\n",
            "Training Iteration 5407, Loss: 4.129796981811523\n",
            "Training Iteration 5408, Loss: 6.0164713859558105\n",
            "Training Iteration 5409, Loss: 2.696427822113037\n",
            "Training Iteration 5410, Loss: 2.252455711364746\n",
            "Training Iteration 5411, Loss: 2.3138365745544434\n",
            "Training Iteration 5412, Loss: 2.4550094604492188\n",
            "Training Iteration 5413, Loss: 3.6376585960388184\n",
            "Training Iteration 5414, Loss: 3.59738826751709\n",
            "Training Iteration 5415, Loss: 4.744340896606445\n",
            "Training Iteration 5416, Loss: 6.124360084533691\n",
            "Training Iteration 5417, Loss: 5.9485182762146\n",
            "Training Iteration 5418, Loss: 3.9105420112609863\n",
            "Training Iteration 5419, Loss: 5.529756546020508\n",
            "Training Iteration 5420, Loss: 4.128445148468018\n",
            "Training Iteration 5421, Loss: 7.522164821624756\n",
            "Training Iteration 5422, Loss: 7.539018154144287\n",
            "Training Iteration 5423, Loss: 5.118433952331543\n",
            "Training Iteration 5424, Loss: 2.67568039894104\n",
            "Training Iteration 5425, Loss: 3.4904637336730957\n",
            "Training Iteration 5426, Loss: 4.919569492340088\n",
            "Training Iteration 5427, Loss: 5.170810222625732\n",
            "Training Iteration 5428, Loss: 3.7016031742095947\n",
            "Training Iteration 5429, Loss: 3.2233188152313232\n",
            "Training Iteration 5430, Loss: 2.9063210487365723\n",
            "Training Iteration 5431, Loss: 2.478468894958496\n",
            "Training Iteration 5432, Loss: 3.89509654045105\n",
            "Training Iteration 5433, Loss: 3.5793838500976562\n",
            "Training Iteration 5434, Loss: 4.196603298187256\n",
            "Training Iteration 5435, Loss: 5.372849464416504\n",
            "Training Iteration 5436, Loss: 5.1580810546875\n",
            "Training Iteration 5437, Loss: 2.3394665718078613\n",
            "Training Iteration 5438, Loss: 4.041184902191162\n",
            "Training Iteration 5439, Loss: 6.199719429016113\n",
            "Training Iteration 5440, Loss: 5.799462795257568\n",
            "Training Iteration 5441, Loss: 4.782323360443115\n",
            "Training Iteration 5442, Loss: 4.033112049102783\n",
            "Training Iteration 5443, Loss: 2.6441173553466797\n",
            "Training Iteration 5444, Loss: 3.605968952178955\n",
            "Training Iteration 5445, Loss: 4.694940567016602\n",
            "Training Iteration 5446, Loss: 5.659443378448486\n",
            "Training Iteration 5447, Loss: 10.428519248962402\n",
            "Training Iteration 5448, Loss: 6.818408489227295\n",
            "Training Iteration 5449, Loss: 4.5134077072143555\n",
            "Training Iteration 5450, Loss: 3.1731042861938477\n",
            "Training Iteration 5451, Loss: 5.896394729614258\n",
            "Training Iteration 5452, Loss: 3.85294771194458\n",
            "Training Iteration 5453, Loss: 5.080279350280762\n",
            "Training Iteration 5454, Loss: 5.048117637634277\n",
            "Training Iteration 5455, Loss: 5.7778496742248535\n",
            "Training Iteration 5456, Loss: 3.00716233253479\n",
            "Training Iteration 5457, Loss: 2.4558920860290527\n",
            "Training Iteration 5458, Loss: 5.247718334197998\n",
            "Training Iteration 5459, Loss: 4.337503433227539\n",
            "Training Iteration 5460, Loss: 5.247906684875488\n",
            "Training Iteration 5461, Loss: 4.255619049072266\n",
            "Training Iteration 5462, Loss: 5.1598992347717285\n",
            "Training Iteration 5463, Loss: 3.871243953704834\n",
            "Training Iteration 5464, Loss: 3.7669477462768555\n",
            "Training Iteration 5465, Loss: 3.8177459239959717\n",
            "Training Iteration 5466, Loss: 1.8291409015655518\n",
            "Training Iteration 5467, Loss: 3.558309555053711\n",
            "Training Iteration 5468, Loss: 5.3694844245910645\n",
            "Training Iteration 5469, Loss: 2.8670949935913086\n",
            "Training Iteration 5470, Loss: 2.2591538429260254\n",
            "Training Iteration 5471, Loss: 4.293990612030029\n",
            "Training Iteration 5472, Loss: 4.145028114318848\n",
            "Training Iteration 5473, Loss: 2.743969678878784\n",
            "Training Iteration 5474, Loss: 7.939783096313477\n",
            "Training Iteration 5475, Loss: 1.5686047077178955\n",
            "Training Iteration 5476, Loss: 3.687349319458008\n",
            "Training Iteration 5477, Loss: 2.780514717102051\n",
            "Training Iteration 5478, Loss: 5.704636573791504\n",
            "Training Iteration 5479, Loss: 4.375823974609375\n",
            "Training Iteration 5480, Loss: 4.450481414794922\n",
            "Training Iteration 5481, Loss: 6.408905982971191\n",
            "Training Iteration 5482, Loss: 5.249939441680908\n",
            "Training Iteration 5483, Loss: 6.829751014709473\n",
            "Training Iteration 5484, Loss: 3.411438465118408\n",
            "Training Iteration 5485, Loss: 6.677443027496338\n",
            "Training Iteration 5486, Loss: 4.316420078277588\n",
            "Training Iteration 5487, Loss: 4.771970748901367\n",
            "Training Iteration 5488, Loss: 2.7598021030426025\n",
            "Training Iteration 5489, Loss: 5.5483551025390625\n",
            "Training Iteration 5490, Loss: 5.850026607513428\n",
            "Training Iteration 5491, Loss: 4.515012741088867\n",
            "Training Iteration 5492, Loss: 3.43200421333313\n",
            "Training Iteration 5493, Loss: 4.335059642791748\n",
            "Training Iteration 5494, Loss: 5.232290267944336\n",
            "Training Iteration 5495, Loss: 4.3064374923706055\n",
            "Training Iteration 5496, Loss: 6.153173446655273\n",
            "Training Iteration 5497, Loss: 6.087484836578369\n",
            "Training Iteration 5498, Loss: 2.668522596359253\n",
            "Training Iteration 5499, Loss: 5.515498638153076\n",
            "Training Iteration 5500, Loss: 3.3376777172088623\n",
            "Training Iteration 5501, Loss: 4.769700527191162\n",
            "Training Iteration 5502, Loss: 4.530906677246094\n",
            "Training Iteration 5503, Loss: 3.702085494995117\n",
            "Training Iteration 5504, Loss: 5.025104999542236\n",
            "Training Iteration 5505, Loss: 3.280283212661743\n",
            "Training Iteration 5506, Loss: 5.179998874664307\n",
            "Training Iteration 5507, Loss: 3.535335063934326\n",
            "Training Iteration 5508, Loss: 7.399057865142822\n",
            "Training Iteration 5509, Loss: 3.0005064010620117\n",
            "Training Iteration 5510, Loss: 2.9053478240966797\n",
            "Training Iteration 5511, Loss: 5.455049991607666\n",
            "Training Iteration 5512, Loss: 2.9881508350372314\n",
            "Training Iteration 5513, Loss: 3.3177192211151123\n",
            "Training Iteration 5514, Loss: 4.990738391876221\n",
            "Training Iteration 5515, Loss: 6.386512756347656\n",
            "Training Iteration 5516, Loss: 5.121942043304443\n",
            "Training Iteration 5517, Loss: 3.2566912174224854\n",
            "Training Iteration 5518, Loss: 3.378915309906006\n",
            "Training Iteration 5519, Loss: 6.167984962463379\n",
            "Training Iteration 5520, Loss: 3.3637912273406982\n",
            "Training Iteration 5521, Loss: 8.347691535949707\n",
            "Training Iteration 5522, Loss: 4.034057140350342\n",
            "Training Iteration 5523, Loss: 7.199413299560547\n",
            "Training Iteration 5524, Loss: 5.840315818786621\n",
            "Training Iteration 5525, Loss: 8.456741333007812\n",
            "Training Iteration 5526, Loss: 8.748344421386719\n",
            "Training Iteration 5527, Loss: 4.013514041900635\n",
            "Training Iteration 5528, Loss: 5.7156171798706055\n",
            "Training Iteration 5529, Loss: 4.177033424377441\n",
            "Training Iteration 5530, Loss: 3.8645384311676025\n",
            "Training Iteration 5531, Loss: 5.226643085479736\n",
            "Training Iteration 5532, Loss: 8.565767288208008\n",
            "Training Iteration 5533, Loss: 4.497264385223389\n",
            "Training Iteration 5534, Loss: 2.455690383911133\n",
            "Training Iteration 5535, Loss: 2.051060438156128\n",
            "Training Iteration 5536, Loss: 4.49845027923584\n",
            "Training Iteration 5537, Loss: 4.07298469543457\n",
            "Training Iteration 5538, Loss: 5.422875881195068\n",
            "Training Iteration 5539, Loss: 8.312857627868652\n",
            "Training Iteration 5540, Loss: 3.30667781829834\n",
            "Training Iteration 5541, Loss: 4.2579755783081055\n",
            "Training Iteration 5542, Loss: 3.4155380725860596\n",
            "Training Iteration 5543, Loss: 3.5893683433532715\n",
            "Training Iteration 5544, Loss: 5.185013771057129\n",
            "Training Iteration 5545, Loss: 6.218016624450684\n",
            "Training Iteration 5546, Loss: 8.144449234008789\n",
            "Training Iteration 5547, Loss: 1.8576164245605469\n",
            "Training Iteration 5548, Loss: 4.944820404052734\n",
            "Training Iteration 5549, Loss: 2.5098917484283447\n",
            "Training Iteration 5550, Loss: 5.990949630737305\n",
            "Training Iteration 5551, Loss: 2.5554251670837402\n",
            "Training Iteration 5552, Loss: 6.2113165855407715\n",
            "Training Iteration 5553, Loss: 3.6748950481414795\n",
            "Training Iteration 5554, Loss: 6.9937663078308105\n",
            "Training Iteration 5555, Loss: 1.9284567832946777\n",
            "Training Iteration 5556, Loss: 4.0196146965026855\n",
            "Training Iteration 5557, Loss: 2.055579662322998\n",
            "Training Iteration 5558, Loss: 4.058596134185791\n",
            "Training Iteration 5559, Loss: 4.393026351928711\n",
            "Training Iteration 5560, Loss: 4.083040237426758\n",
            "Training Iteration 5561, Loss: 5.207757949829102\n",
            "Training Iteration 5562, Loss: 4.6231184005737305\n",
            "Training Iteration 5563, Loss: 5.299804210662842\n",
            "Training Iteration 5564, Loss: 7.177372455596924\n",
            "Training Iteration 5565, Loss: 3.077378273010254\n",
            "Training Iteration 5566, Loss: 4.507897853851318\n",
            "Training Iteration 5567, Loss: 6.394769191741943\n",
            "Training Iteration 5568, Loss: 3.715332508087158\n",
            "Training Iteration 5569, Loss: 7.099650859832764\n",
            "Training Iteration 5570, Loss: 6.897688865661621\n",
            "Training Iteration 5571, Loss: 3.1773712635040283\n",
            "Training Iteration 5572, Loss: 5.564890384674072\n",
            "Training Iteration 5573, Loss: 6.24382209777832\n",
            "Training Iteration 5574, Loss: 4.598440647125244\n",
            "Training Iteration 5575, Loss: 4.87289571762085\n",
            "Training Iteration 5576, Loss: 5.134456634521484\n",
            "Training Iteration 5577, Loss: 5.081038475036621\n",
            "Training Iteration 5578, Loss: 5.026984691619873\n",
            "Training Iteration 5579, Loss: 5.569330215454102\n",
            "Training Iteration 5580, Loss: 3.8772788047790527\n",
            "Training Iteration 5581, Loss: 3.423414707183838\n",
            "Training Iteration 5582, Loss: 3.323091983795166\n",
            "Training Iteration 5583, Loss: 2.5981361865997314\n",
            "Training Iteration 5584, Loss: 3.7907767295837402\n",
            "Training Iteration 5585, Loss: 3.9806201457977295\n",
            "Training Iteration 5586, Loss: 7.492732524871826\n",
            "Training Iteration 5587, Loss: 3.7414374351501465\n",
            "Training Iteration 5588, Loss: 3.8611629009246826\n",
            "Training Iteration 5589, Loss: 4.960623264312744\n",
            "Training Iteration 5590, Loss: 3.853586435317993\n",
            "Training Iteration 5591, Loss: 4.532613277435303\n",
            "Training Iteration 5592, Loss: 3.3788950443267822\n",
            "Training Iteration 5593, Loss: 5.778796672821045\n",
            "Training Iteration 5594, Loss: 6.430992126464844\n",
            "Training Iteration 5595, Loss: 3.528507709503174\n",
            "Training Iteration 5596, Loss: 2.3598883152008057\n",
            "Training Iteration 5597, Loss: 6.34745979309082\n",
            "Training Iteration 5598, Loss: 3.45304536819458\n",
            "Training Iteration 5599, Loss: 5.453216552734375\n",
            "Training Iteration 5600, Loss: 3.178525686264038\n",
            "Training Iteration 5601, Loss: 3.931173086166382\n",
            "Training Iteration 5602, Loss: 6.91607141494751\n",
            "Training Iteration 5603, Loss: 3.744683265686035\n",
            "Training Iteration 5604, Loss: 4.688197135925293\n",
            "Training Iteration 5605, Loss: 4.539583683013916\n",
            "Training Iteration 5606, Loss: 4.779318809509277\n",
            "Training Iteration 5607, Loss: 5.36281681060791\n",
            "Training Iteration 5608, Loss: 3.8879001140594482\n",
            "Training Iteration 5609, Loss: 2.6442642211914062\n",
            "Training Iteration 5610, Loss: 5.592543125152588\n",
            "Training Iteration 5611, Loss: 5.094151973724365\n",
            "Training Iteration 5612, Loss: 7.668514251708984\n",
            "Training Iteration 5613, Loss: 2.2650680541992188\n",
            "Training Iteration 5614, Loss: 2.388542413711548\n",
            "Training Iteration 5615, Loss: 4.490651607513428\n",
            "Training Iteration 5616, Loss: 2.4001216888427734\n",
            "Training Iteration 5617, Loss: 6.977915287017822\n",
            "Training Iteration 5618, Loss: 2.170804262161255\n",
            "Training Iteration 5619, Loss: 4.676177978515625\n",
            "Training Iteration 5620, Loss: 5.691097736358643\n",
            "Training Iteration 5621, Loss: 2.194873571395874\n",
            "Training Iteration 5622, Loss: 3.3248181343078613\n",
            "Training Iteration 5623, Loss: 2.38356614112854\n",
            "Training Iteration 5624, Loss: 3.659907102584839\n",
            "Training Iteration 5625, Loss: 6.256672382354736\n",
            "Training Iteration 5626, Loss: 5.790712356567383\n",
            "Training Iteration 5627, Loss: 2.8855812549591064\n",
            "Training Iteration 5628, Loss: 5.216325283050537\n",
            "Training Iteration 5629, Loss: 4.117494106292725\n",
            "Training Iteration 5630, Loss: 7.78245735168457\n",
            "Training Iteration 5631, Loss: 8.126838684082031\n",
            "Training Iteration 5632, Loss: 3.765547037124634\n",
            "Training Iteration 5633, Loss: 10.61950969696045\n",
            "Training Iteration 5634, Loss: 1.8532166481018066\n",
            "Training Iteration 5635, Loss: 3.1057820320129395\n",
            "Training Iteration 5636, Loss: 8.468412399291992\n",
            "Training Iteration 5637, Loss: 4.439056396484375\n",
            "Training Iteration 5638, Loss: 3.2451674938201904\n",
            "Training Iteration 5639, Loss: 6.785952091217041\n",
            "Training Iteration 5640, Loss: 9.567351341247559\n",
            "Training Iteration 5641, Loss: 3.441807985305786\n",
            "Training Iteration 5642, Loss: 2.812901258468628\n",
            "Training Iteration 5643, Loss: 2.6116654872894287\n",
            "Training Iteration 5644, Loss: 3.8083343505859375\n",
            "Training Iteration 5645, Loss: 9.510509490966797\n",
            "Training Iteration 5646, Loss: 2.009441375732422\n",
            "Training Iteration 5647, Loss: 6.564249515533447\n",
            "Training Iteration 5648, Loss: 6.365157127380371\n",
            "Training Iteration 5649, Loss: 3.709383726119995\n",
            "Training Iteration 5650, Loss: 4.193180084228516\n",
            "Training Iteration 5651, Loss: 3.9994945526123047\n",
            "Training Iteration 5652, Loss: 4.02529764175415\n",
            "Training Iteration 5653, Loss: 4.726177215576172\n",
            "Training Iteration 5654, Loss: 7.113443374633789\n",
            "Training Iteration 5655, Loss: 5.4862589836120605\n",
            "Training Iteration 5656, Loss: 3.683908462524414\n",
            "Training Iteration 5657, Loss: 4.058948040008545\n",
            "Training Iteration 5658, Loss: 4.509988307952881\n",
            "Training Iteration 5659, Loss: 2.2936344146728516\n",
            "Training Iteration 5660, Loss: 6.458011150360107\n",
            "Training Iteration 5661, Loss: 6.549260139465332\n",
            "Training Iteration 5662, Loss: 6.214660167694092\n",
            "Training Iteration 5663, Loss: 4.564558029174805\n",
            "Training Iteration 5664, Loss: 3.4814906120300293\n",
            "Training Iteration 5665, Loss: 4.5864763259887695\n",
            "Training Iteration 5666, Loss: 5.365297794342041\n",
            "Training Iteration 5667, Loss: 4.308691024780273\n",
            "Training Iteration 5668, Loss: 8.206981658935547\n",
            "Training Iteration 5669, Loss: 2.482731342315674\n",
            "Training Iteration 5670, Loss: 5.867271900177002\n",
            "Training Iteration 5671, Loss: 1.3930861949920654\n",
            "Training Iteration 5672, Loss: 7.889742851257324\n",
            "Training Iteration 5673, Loss: 3.5977485179901123\n",
            "Training Iteration 5674, Loss: 8.778058052062988\n",
            "Training Iteration 5675, Loss: 9.539385795593262\n",
            "Training Iteration 5676, Loss: 6.287382125854492\n",
            "Training Iteration 5677, Loss: 5.835933685302734\n",
            "Training Iteration 5678, Loss: 5.2441534996032715\n",
            "Training Iteration 5679, Loss: 4.126857280731201\n",
            "Training Iteration 5680, Loss: 2.6172099113464355\n",
            "Training Iteration 5681, Loss: 3.8746180534362793\n",
            "Training Iteration 5682, Loss: 3.955641269683838\n",
            "Training Iteration 5683, Loss: 8.066120147705078\n",
            "Training Iteration 5684, Loss: 4.551522731781006\n",
            "Training Iteration 5685, Loss: 4.9451985359191895\n",
            "Training Iteration 5686, Loss: 3.868628978729248\n",
            "Training Iteration 5687, Loss: 3.352626323699951\n",
            "Training Iteration 5688, Loss: 5.350160121917725\n",
            "Training Iteration 5689, Loss: 4.369100570678711\n",
            "Training Iteration 5690, Loss: 4.54926872253418\n",
            "Training Iteration 5691, Loss: 8.007722854614258\n",
            "Training Iteration 5692, Loss: 3.347191333770752\n",
            "Training Iteration 5693, Loss: 4.275577068328857\n",
            "Training Iteration 5694, Loss: 7.861058235168457\n",
            "Training Iteration 5695, Loss: 5.3120293617248535\n",
            "Training Iteration 5696, Loss: 2.510283946990967\n",
            "Training Iteration 5697, Loss: 2.963224172592163\n",
            "Training Iteration 5698, Loss: 4.374381065368652\n",
            "Training Iteration 5699, Loss: 5.020223617553711\n",
            "Training Iteration 5700, Loss: 4.560753345489502\n",
            "Training Iteration 5701, Loss: 5.202252388000488\n",
            "Training Iteration 5702, Loss: 4.008420944213867\n",
            "Training Iteration 5703, Loss: 4.396152019500732\n",
            "Training Iteration 5704, Loss: 5.853153705596924\n",
            "Training Iteration 5705, Loss: 3.947807788848877\n",
            "Training Iteration 5706, Loss: 4.5554304122924805\n",
            "Training Iteration 5707, Loss: 3.385101795196533\n",
            "Training Iteration 5708, Loss: 7.565986633300781\n",
            "Training Iteration 5709, Loss: 3.4570155143737793\n",
            "Training Iteration 5710, Loss: 2.852968215942383\n",
            "Training Iteration 5711, Loss: 1.5677896738052368\n",
            "Training Iteration 5712, Loss: 5.253376483917236\n",
            "Training Iteration 5713, Loss: 2.7752418518066406\n",
            "Training Iteration 5714, Loss: 6.3683762550354\n",
            "Training Iteration 5715, Loss: 8.880512237548828\n",
            "Training Iteration 5716, Loss: 3.385390043258667\n",
            "Training Iteration 5717, Loss: 1.8600093126296997\n",
            "Training Iteration 5718, Loss: 4.070140361785889\n",
            "Training Iteration 5719, Loss: 5.3152313232421875\n",
            "Training Iteration 5720, Loss: 3.6011745929718018\n",
            "Training Iteration 5721, Loss: 6.007794380187988\n",
            "Training Iteration 5722, Loss: 2.6775994300842285\n",
            "Training Iteration 5723, Loss: 4.431906700134277\n",
            "Training Iteration 5724, Loss: 3.90616512298584\n",
            "Training Iteration 5725, Loss: 5.5313310623168945\n",
            "Training Iteration 5726, Loss: 7.050859451293945\n",
            "Training Iteration 5727, Loss: 5.363291263580322\n",
            "Training Iteration 5728, Loss: 7.786320209503174\n",
            "Training Iteration 5729, Loss: 3.666853427886963\n",
            "Training Iteration 5730, Loss: 3.973721504211426\n",
            "Training Iteration 5731, Loss: 3.6430020332336426\n",
            "Training Iteration 5732, Loss: 8.002901077270508\n",
            "Training Iteration 5733, Loss: 5.986265182495117\n",
            "Training Iteration 5734, Loss: 4.861922740936279\n",
            "Training Iteration 5735, Loss: 4.655811786651611\n",
            "Training Iteration 5736, Loss: 3.207468032836914\n",
            "Training Iteration 5737, Loss: 4.942659854888916\n",
            "Training Iteration 5738, Loss: 3.8666903972625732\n",
            "Training Iteration 5739, Loss: 3.218656063079834\n",
            "Training Iteration 5740, Loss: 5.081986904144287\n",
            "Training Iteration 5741, Loss: 4.63434362411499\n",
            "Training Iteration 5742, Loss: 5.204608917236328\n",
            "Training Iteration 5743, Loss: 0.8784682154655457\n",
            "Training Iteration 5744, Loss: 7.858224868774414\n",
            "Training Iteration 5745, Loss: 3.938412666320801\n",
            "Training Iteration 5746, Loss: 5.838881492614746\n",
            "Training Iteration 5747, Loss: 4.483363151550293\n",
            "Training Iteration 5748, Loss: 3.4517130851745605\n",
            "Training Iteration 5749, Loss: 2.354187250137329\n",
            "Training Iteration 5750, Loss: 2.9321718215942383\n",
            "Training Iteration 5751, Loss: 1.8317776918411255\n",
            "Training Iteration 5752, Loss: 1.733248233795166\n",
            "Training Iteration 5753, Loss: 5.5153727531433105\n",
            "Training Iteration 5754, Loss: 4.786787033081055\n",
            "Training Iteration 5755, Loss: 1.3366243839263916\n",
            "Training Iteration 5756, Loss: 8.801581382751465\n",
            "Training Iteration 5757, Loss: 5.7368011474609375\n",
            "Training Iteration 5758, Loss: 11.56568717956543\n",
            "Training Iteration 5759, Loss: 8.116780281066895\n",
            "Training Iteration 5760, Loss: 7.346292972564697\n",
            "Training Iteration 5761, Loss: 5.174245357513428\n",
            "Training Iteration 5762, Loss: 5.171239852905273\n",
            "Training Iteration 5763, Loss: 5.09710168838501\n",
            "Training Iteration 5764, Loss: 3.866875171661377\n",
            "Training Iteration 5765, Loss: 3.5746915340423584\n",
            "Training Iteration 5766, Loss: 4.325498580932617\n",
            "Training Iteration 5767, Loss: 5.2994890213012695\n",
            "Training Iteration 5768, Loss: 2.9818930625915527\n",
            "Training Iteration 5769, Loss: 2.8964240550994873\n",
            "Training Iteration 5770, Loss: 5.378948211669922\n",
            "Training Iteration 5771, Loss: 5.785078048706055\n",
            "Training Iteration 5772, Loss: 4.422619342803955\n",
            "Training Iteration 5773, Loss: 2.515225648880005\n",
            "Training Iteration 5774, Loss: 3.7392563819885254\n",
            "Training Iteration 5775, Loss: 9.038588523864746\n",
            "Training Iteration 5776, Loss: 1.874761939048767\n",
            "Training Iteration 5777, Loss: 5.366514205932617\n",
            "Training Iteration 5778, Loss: 7.1551833152771\n",
            "Training Iteration 5779, Loss: 6.222334384918213\n",
            "Training Iteration 5780, Loss: 1.7488065958023071\n",
            "Training Iteration 5781, Loss: 9.012762069702148\n",
            "Training Iteration 5782, Loss: 4.095413684844971\n",
            "Training Iteration 5783, Loss: 5.169478416442871\n",
            "Training Iteration 5784, Loss: 5.586181640625\n",
            "Training Iteration 5785, Loss: 2.2413110733032227\n",
            "Training Iteration 5786, Loss: 5.066769123077393\n",
            "Training Iteration 5787, Loss: 6.068756580352783\n",
            "Training Iteration 5788, Loss: 3.0170037746429443\n",
            "Training Iteration 5789, Loss: 3.2378957271575928\n",
            "Training Iteration 5790, Loss: 7.836399078369141\n",
            "Training Iteration 5791, Loss: 2.6672723293304443\n",
            "Training Iteration 5792, Loss: 8.26859188079834\n",
            "Training Iteration 5793, Loss: 4.373405456542969\n",
            "Training Iteration 5794, Loss: 9.203736305236816\n",
            "Training Iteration 5795, Loss: 2.724677801132202\n",
            "Training Iteration 5796, Loss: 6.975561618804932\n",
            "Training Iteration 5797, Loss: 3.809762954711914\n",
            "Training Iteration 5798, Loss: 3.330770492553711\n",
            "Training Iteration 5799, Loss: 10.87032413482666\n",
            "Training Iteration 5800, Loss: 4.347524642944336\n",
            "Training Iteration 5801, Loss: 6.3780131340026855\n",
            "Training Iteration 5802, Loss: 3.5539979934692383\n",
            "Training Iteration 5803, Loss: 8.046232223510742\n",
            "Training Iteration 5804, Loss: 4.7156476974487305\n",
            "Training Iteration 5805, Loss: 5.7941789627075195\n",
            "Training Iteration 5806, Loss: 2.8521909713745117\n",
            "Training Iteration 5807, Loss: 7.7379302978515625\n",
            "Training Iteration 5808, Loss: 5.019360065460205\n",
            "Training Iteration 5809, Loss: 6.924354553222656\n",
            "Training Iteration 5810, Loss: 5.140310287475586\n",
            "Training Iteration 5811, Loss: 3.9593966007232666\n",
            "Training Iteration 5812, Loss: 3.2099928855895996\n",
            "Training Iteration 5813, Loss: 4.1573076248168945\n",
            "Training Iteration 5814, Loss: 1.7336270809173584\n",
            "Training Iteration 5815, Loss: 6.273664474487305\n",
            "Training Iteration 5816, Loss: 5.764702796936035\n",
            "Training Iteration 5817, Loss: 4.599307537078857\n",
            "Training Iteration 5818, Loss: 5.108259201049805\n",
            "Training Iteration 5819, Loss: 6.869145393371582\n",
            "Training Iteration 5820, Loss: 5.262179374694824\n",
            "Training Iteration 5821, Loss: 5.354909896850586\n",
            "Training Iteration 5822, Loss: 3.8894762992858887\n",
            "Training Iteration 5823, Loss: 4.3987627029418945\n",
            "Training Iteration 5824, Loss: 3.4975502490997314\n",
            "Training Iteration 5825, Loss: 5.548227787017822\n",
            "Training Iteration 5826, Loss: 8.132136344909668\n",
            "Training Iteration 5827, Loss: 8.814004898071289\n",
            "Training Iteration 5828, Loss: 4.786103248596191\n",
            "Training Iteration 5829, Loss: 3.4570975303649902\n",
            "Training Iteration 5830, Loss: 1.6171172857284546\n",
            "Training Iteration 5831, Loss: 4.204958438873291\n",
            "Training Iteration 5832, Loss: 4.8847808837890625\n",
            "Training Iteration 5833, Loss: 5.27015495300293\n",
            "Training Iteration 5834, Loss: 4.6789679527282715\n",
            "Training Iteration 5835, Loss: 3.894845485687256\n",
            "Training Iteration 5836, Loss: 5.1013665199279785\n",
            "Training Iteration 5837, Loss: 2.856001138687134\n",
            "Training Iteration 5838, Loss: 4.757851600646973\n",
            "Training Iteration 5839, Loss: 4.279890060424805\n",
            "Training Iteration 5840, Loss: 4.317475318908691\n",
            "Training Iteration 5841, Loss: 3.203685760498047\n",
            "Training Iteration 5842, Loss: 2.060605764389038\n",
            "Training Iteration 5843, Loss: 2.864102363586426\n",
            "Training Iteration 5844, Loss: 3.247986078262329\n",
            "Training Iteration 5845, Loss: 4.68563985824585\n",
            "Training Iteration 5846, Loss: 5.302857398986816\n",
            "Training Iteration 5847, Loss: 4.196471214294434\n",
            "Training Iteration 5848, Loss: 7.4485392570495605\n",
            "Training Iteration 5849, Loss: 2.838958263397217\n",
            "Training Iteration 5850, Loss: 3.5657389163970947\n",
            "Training Iteration 5851, Loss: 2.8581721782684326\n",
            "Training Iteration 5852, Loss: 5.145316123962402\n",
            "Training Iteration 5853, Loss: 4.40456485748291\n",
            "Training Iteration 5854, Loss: 5.284217834472656\n",
            "Training Iteration 5855, Loss: 2.98146390914917\n",
            "Training Iteration 5856, Loss: 3.9296329021453857\n",
            "Training Iteration 5857, Loss: 4.038867473602295\n",
            "Training Iteration 5858, Loss: 3.3704261779785156\n",
            "Training Iteration 5859, Loss: 1.7409037351608276\n",
            "Training Iteration 5860, Loss: 5.323897838592529\n",
            "Training Iteration 5861, Loss: 4.3180060386657715\n",
            "Training Iteration 5862, Loss: 8.307435035705566\n",
            "Training Iteration 5863, Loss: 5.582022190093994\n",
            "Training Iteration 5864, Loss: 5.804042816162109\n",
            "Training Iteration 5865, Loss: 3.681445598602295\n",
            "Training Iteration 5866, Loss: 3.97525691986084\n",
            "Training Iteration 5867, Loss: 3.3461620807647705\n",
            "Training Iteration 5868, Loss: 3.930469274520874\n",
            "Training Iteration 5869, Loss: 3.678590774536133\n",
            "Training Iteration 5870, Loss: 5.135497570037842\n",
            "Training Iteration 5871, Loss: 6.854428291320801\n",
            "Training Iteration 5872, Loss: 7.392568588256836\n",
            "Training Iteration 5873, Loss: 7.804161548614502\n",
            "Training Iteration 5874, Loss: 4.042085647583008\n",
            "Training Iteration 5875, Loss: 3.959423065185547\n",
            "Training Iteration 5876, Loss: 3.8364927768707275\n",
            "Training Iteration 5877, Loss: 6.370944499969482\n",
            "Training Iteration 5878, Loss: 4.743362903594971\n",
            "Training Iteration 5879, Loss: 6.629628658294678\n",
            "Training Iteration 5880, Loss: 3.6449272632598877\n",
            "Training Iteration 5881, Loss: 2.4348514080047607\n",
            "Training Iteration 5882, Loss: 3.465872287750244\n",
            "Training Iteration 5883, Loss: 3.4812331199645996\n",
            "Training Iteration 5884, Loss: 3.0282094478607178\n",
            "Training Iteration 5885, Loss: 4.888468265533447\n",
            "Training Iteration 5886, Loss: 2.263254165649414\n",
            "Training Iteration 5887, Loss: 3.911179542541504\n",
            "Training Iteration 5888, Loss: 2.9636449813842773\n",
            "Training Iteration 5889, Loss: 4.648938179016113\n",
            "Training Iteration 5890, Loss: 3.256108522415161\n",
            "Training Iteration 5891, Loss: 3.5579776763916016\n",
            "Training Iteration 5892, Loss: 2.5755245685577393\n",
            "Training Iteration 5893, Loss: 2.5998663902282715\n",
            "Training Iteration 5894, Loss: 5.874392032623291\n",
            "Training Iteration 5895, Loss: 5.704697608947754\n",
            "Training Iteration 5896, Loss: 6.16200065612793\n",
            "Training Iteration 5897, Loss: 3.636190414428711\n",
            "Training Iteration 5898, Loss: 4.264512538909912\n",
            "Training Iteration 5899, Loss: 5.153225421905518\n",
            "Training Iteration 5900, Loss: 2.56892466545105\n",
            "Training Iteration 5901, Loss: 3.838860273361206\n",
            "Training Iteration 5902, Loss: 6.211971759796143\n",
            "Training Iteration 5903, Loss: 3.5433225631713867\n",
            "Training Iteration 5904, Loss: 5.553557395935059\n",
            "Training Iteration 5905, Loss: 3.7173876762390137\n",
            "Training Iteration 5906, Loss: 4.6820807456970215\n",
            "Training Iteration 5907, Loss: 5.3659186363220215\n",
            "Training Iteration 5908, Loss: 4.248845100402832\n",
            "Training Iteration 5909, Loss: 4.455447196960449\n",
            "Training Iteration 5910, Loss: 1.954064130783081\n",
            "Training Iteration 5911, Loss: 3.0936779975891113\n",
            "Training Iteration 5912, Loss: 2.9314870834350586\n",
            "Training Iteration 5913, Loss: 4.756941795349121\n",
            "Training Iteration 5914, Loss: 4.963314533233643\n",
            "Training Iteration 5915, Loss: 5.443772315979004\n",
            "Training Iteration 5916, Loss: 3.9258744716644287\n",
            "Training Iteration 5917, Loss: 4.518616199493408\n",
            "Training Iteration 5918, Loss: 5.090682506561279\n",
            "Training Iteration 5919, Loss: 4.095540523529053\n",
            "Training Iteration 5920, Loss: 2.9522976875305176\n",
            "Training Iteration 5921, Loss: 2.125720739364624\n",
            "Training Iteration 5922, Loss: 7.909738063812256\n",
            "Training Iteration 5923, Loss: 3.2675065994262695\n",
            "Training Iteration 5924, Loss: 1.978816270828247\n",
            "Training Iteration 5925, Loss: 5.605938911437988\n",
            "Training Iteration 5926, Loss: 3.9037885665893555\n",
            "Training Iteration 5927, Loss: 3.421178102493286\n",
            "Training Iteration 5928, Loss: 3.1343564987182617\n",
            "Training Iteration 5929, Loss: 5.049281120300293\n",
            "Training Iteration 5930, Loss: 4.122189521789551\n",
            "Training Iteration 5931, Loss: 3.5385870933532715\n",
            "Training Iteration 5932, Loss: 4.744058609008789\n",
            "Training Iteration 5933, Loss: 4.371682167053223\n",
            "Training Iteration 5934, Loss: 1.7883007526397705\n",
            "Training Iteration 5935, Loss: 3.7188796997070312\n",
            "Training Iteration 5936, Loss: 5.027910232543945\n",
            "Training Iteration 5937, Loss: 5.296975135803223\n",
            "Training Iteration 5938, Loss: 4.283320903778076\n",
            "Training Iteration 5939, Loss: 3.238009452819824\n",
            "Training Iteration 5940, Loss: 6.099872589111328\n",
            "Training Iteration 5941, Loss: 4.548144817352295\n",
            "Training Iteration 5942, Loss: 5.483996868133545\n",
            "Training Iteration 5943, Loss: 3.2029836177825928\n",
            "Training Iteration 5944, Loss: 8.640036582946777\n",
            "Training Iteration 5945, Loss: 4.705717086791992\n",
            "Training Iteration 5946, Loss: 2.9436724185943604\n",
            "Training Iteration 5947, Loss: 5.342317581176758\n",
            "Training Iteration 5948, Loss: 2.7410500049591064\n",
            "Training Iteration 5949, Loss: 3.3860626220703125\n",
            "Training Iteration 5950, Loss: 10.057254791259766\n",
            "Training Iteration 5951, Loss: 5.407290458679199\n",
            "Training Iteration 5952, Loss: 5.642560958862305\n",
            "Training Iteration 5953, Loss: 4.602958679199219\n",
            "Training Iteration 5954, Loss: 4.293681621551514\n",
            "Training Iteration 5955, Loss: 3.8626980781555176\n",
            "Training Iteration 5956, Loss: 5.382303237915039\n",
            "Training Iteration 5957, Loss: 3.509894847869873\n",
            "Training Iteration 5958, Loss: 5.680941104888916\n",
            "Training Iteration 5959, Loss: 4.2687482833862305\n",
            "Training Iteration 5960, Loss: 6.02731466293335\n",
            "Training Iteration 5961, Loss: 6.015476226806641\n",
            "Training Iteration 5962, Loss: 4.2375359535217285\n",
            "Training Iteration 5963, Loss: 2.7588179111480713\n",
            "Training Iteration 5964, Loss: 5.788586139678955\n",
            "Training Iteration 5965, Loss: 2.3585453033447266\n",
            "Training Iteration 5966, Loss: 2.909811496734619\n",
            "Training Iteration 5967, Loss: 4.278770923614502\n",
            "Training Iteration 5968, Loss: 5.927978515625\n",
            "Training Iteration 5969, Loss: 5.756080150604248\n",
            "Training Iteration 5970, Loss: 10.661588668823242\n",
            "Training Iteration 5971, Loss: 6.996979236602783\n",
            "Training Iteration 5972, Loss: 3.7820210456848145\n",
            "Training Iteration 5973, Loss: 6.6450724601745605\n",
            "Training Iteration 5974, Loss: 7.76887845993042\n",
            "Training Iteration 5975, Loss: 4.196305274963379\n",
            "Training Iteration 5976, Loss: 3.4853134155273438\n",
            "Training Iteration 5977, Loss: 4.0493011474609375\n",
            "Training Iteration 5978, Loss: 5.939844131469727\n",
            "Training Iteration 5979, Loss: 6.139469146728516\n",
            "Training Iteration 5980, Loss: 3.913179874420166\n",
            "Training Iteration 5981, Loss: 4.5670576095581055\n",
            "Training Iteration 5982, Loss: 2.802881956100464\n",
            "Training Iteration 5983, Loss: 4.224032402038574\n",
            "Training Iteration 5984, Loss: 7.147324562072754\n",
            "Training Iteration 5985, Loss: 6.047646522521973\n",
            "Training Iteration 5986, Loss: 3.493165969848633\n",
            "Training Iteration 5987, Loss: 5.404628276824951\n",
            "Training Iteration 5988, Loss: 4.29054069519043\n",
            "Training Iteration 5989, Loss: 7.027482986450195\n",
            "Training Iteration 5990, Loss: 2.790250062942505\n",
            "Training Iteration 5991, Loss: 4.233595371246338\n",
            "Training Iteration 5992, Loss: 1.8789410591125488\n",
            "Training Iteration 5993, Loss: 3.41872239112854\n",
            "Training Iteration 5994, Loss: 5.928596019744873\n",
            "Training Iteration 5995, Loss: 8.622169494628906\n",
            "Training Iteration 5996, Loss: 3.2528202533721924\n",
            "Training Iteration 5997, Loss: 5.476268291473389\n",
            "Training Iteration 5998, Loss: 6.890549182891846\n",
            "Training Iteration 5999, Loss: 3.8250410556793213\n",
            "Training Iteration 6000, Loss: 4.116730213165283\n",
            "Training Iteration 6001, Loss: 2.97165584564209\n",
            "Training Iteration 6002, Loss: 4.409611701965332\n",
            "Training Iteration 6003, Loss: 7.6304521560668945\n",
            "Training Iteration 6004, Loss: 3.9338877201080322\n",
            "Training Iteration 6005, Loss: 5.231916427612305\n",
            "Training Iteration 6006, Loss: 3.1959574222564697\n",
            "Training Iteration 6007, Loss: 4.558717250823975\n",
            "Training Iteration 6008, Loss: 3.6758205890655518\n",
            "Training Iteration 6009, Loss: 7.4967265129089355\n",
            "Training Iteration 6010, Loss: 3.16143798828125\n",
            "Training Iteration 6011, Loss: 3.630800724029541\n",
            "Training Iteration 6012, Loss: 3.774094343185425\n",
            "Training Iteration 6013, Loss: 3.091304302215576\n",
            "Training Iteration 6014, Loss: 2.8957653045654297\n",
            "Training Iteration 6015, Loss: 4.078876972198486\n",
            "Training Iteration 6016, Loss: 2.8612022399902344\n",
            "Training Iteration 6017, Loss: 5.202177047729492\n",
            "Training Iteration 6018, Loss: 8.262731552124023\n",
            "Training Iteration 6019, Loss: 3.3328895568847656\n",
            "Training Iteration 6020, Loss: 6.437360763549805\n",
            "Training Iteration 6021, Loss: 7.415041446685791\n",
            "Training Iteration 6022, Loss: 3.0783374309539795\n",
            "Training Iteration 6023, Loss: 4.777404308319092\n",
            "Training Iteration 6024, Loss: 3.9774680137634277\n",
            "Training Iteration 6025, Loss: 5.140760898590088\n",
            "Training Iteration 6026, Loss: 1.643672227859497\n",
            "Training Iteration 6027, Loss: 3.1789402961730957\n",
            "Training Iteration 6028, Loss: 4.017934799194336\n",
            "Training Iteration 6029, Loss: 2.8401334285736084\n",
            "Training Iteration 6030, Loss: 3.616349458694458\n",
            "Training Iteration 6031, Loss: 4.945969104766846\n",
            "Training Iteration 6032, Loss: 2.9556164741516113\n",
            "Training Iteration 6033, Loss: 4.089377403259277\n",
            "Training Iteration 6034, Loss: 5.70169734954834\n",
            "Training Iteration 6035, Loss: 1.5247132778167725\n",
            "Training Iteration 6036, Loss: 2.376943349838257\n",
            "Training Iteration 6037, Loss: 7.665542125701904\n",
            "Training Iteration 6038, Loss: 5.786441802978516\n",
            "Training Iteration 6039, Loss: 8.836959838867188\n",
            "Training Iteration 6040, Loss: 6.193273544311523\n",
            "Training Iteration 6041, Loss: 4.080671787261963\n",
            "Training Iteration 6042, Loss: 3.383838653564453\n",
            "Training Iteration 6043, Loss: 3.9127190113067627\n",
            "Training Iteration 6044, Loss: 5.040061950683594\n",
            "Training Iteration 6045, Loss: 4.171322345733643\n",
            "Training Iteration 6046, Loss: 2.4666597843170166\n",
            "Training Iteration 6047, Loss: 3.1235878467559814\n",
            "Training Iteration 6048, Loss: 4.57383918762207\n",
            "Training Iteration 6049, Loss: 3.8616600036621094\n",
            "Training Iteration 6050, Loss: 3.6211938858032227\n",
            "Training Iteration 6051, Loss: 5.997758865356445\n",
            "Training Iteration 6052, Loss: 4.921591758728027\n",
            "Training Iteration 6053, Loss: 1.4981565475463867\n",
            "Training Iteration 6054, Loss: 5.270689487457275\n",
            "Training Iteration 6055, Loss: 3.897176742553711\n",
            "Training Iteration 6056, Loss: 5.110016345977783\n",
            "Training Iteration 6057, Loss: 3.5753324031829834\n",
            "Training Iteration 6058, Loss: 6.017739295959473\n",
            "Training Iteration 6059, Loss: 4.310272216796875\n",
            "Training Iteration 6060, Loss: 3.249654531478882\n",
            "Training Iteration 6061, Loss: 3.261838912963867\n",
            "Training Iteration 6062, Loss: 2.5780134201049805\n",
            "Training Iteration 6063, Loss: 4.1758270263671875\n",
            "Training Iteration 6064, Loss: 3.8874001502990723\n",
            "Training Iteration 6065, Loss: 4.166513442993164\n",
            "Training Iteration 6066, Loss: 6.957602500915527\n",
            "Training Iteration 6067, Loss: 3.5626206398010254\n",
            "Training Iteration 6068, Loss: 0.5692055225372314\n",
            "Training Iteration 6069, Loss: 3.27020525932312\n",
            "Training Iteration 6070, Loss: 7.478006839752197\n",
            "Training Iteration 6071, Loss: 3.199108839035034\n",
            "Training Iteration 6072, Loss: 4.641968727111816\n",
            "Training Iteration 6073, Loss: 6.268125534057617\n",
            "Training Iteration 6074, Loss: 1.8492916822433472\n",
            "Training Iteration 6075, Loss: 3.403813600540161\n",
            "Training Iteration 6076, Loss: 2.9451889991760254\n",
            "Training Iteration 6077, Loss: 5.007054805755615\n",
            "Training Iteration 6078, Loss: 3.7814626693725586\n",
            "Training Iteration 6079, Loss: 4.326699256896973\n",
            "Training Iteration 6080, Loss: 6.5239362716674805\n",
            "Training Iteration 6081, Loss: 2.745502233505249\n",
            "Training Iteration 6082, Loss: 4.617423057556152\n",
            "Training Iteration 6083, Loss: 6.409067153930664\n",
            "Training Iteration 6084, Loss: 4.7167816162109375\n",
            "Training Iteration 6085, Loss: 2.985468626022339\n",
            "Training Iteration 6086, Loss: 4.468296051025391\n",
            "Training Iteration 6087, Loss: 7.296713829040527\n",
            "Training Iteration 6088, Loss: 6.809139728546143\n",
            "Training Iteration 6089, Loss: 6.764642715454102\n",
            "Training Iteration 6090, Loss: 5.4636030197143555\n",
            "Training Iteration 6091, Loss: 5.065770626068115\n",
            "Training Iteration 6092, Loss: 5.424030780792236\n",
            "Training Iteration 6093, Loss: 4.720948219299316\n",
            "Training Iteration 6094, Loss: 5.235032558441162\n",
            "Training Iteration 6095, Loss: 4.882559299468994\n",
            "Training Iteration 6096, Loss: 4.0882792472839355\n",
            "Training Iteration 6097, Loss: 4.7418212890625\n",
            "Training Iteration 6098, Loss: 2.337022304534912\n",
            "Training Iteration 6099, Loss: 4.961968898773193\n",
            "Training Iteration 6100, Loss: 2.0824122428894043\n",
            "Training Iteration 6101, Loss: 7.1835455894470215\n",
            "Training Iteration 6102, Loss: 3.290037155151367\n",
            "Training Iteration 6103, Loss: 3.1005311012268066\n",
            "Training Iteration 6104, Loss: 3.869255781173706\n",
            "Training Iteration 6105, Loss: 5.738734245300293\n",
            "Training Iteration 6106, Loss: 3.5003859996795654\n",
            "Training Iteration 6107, Loss: 6.1874494552612305\n",
            "Training Iteration 6108, Loss: 8.552888870239258\n",
            "Training Iteration 6109, Loss: 7.973153591156006\n",
            "Training Iteration 6110, Loss: 2.4309983253479004\n",
            "Training Iteration 6111, Loss: 6.162155628204346\n",
            "Training Iteration 6112, Loss: 5.07257604598999\n",
            "Training Iteration 6113, Loss: 4.664730072021484\n",
            "Training Iteration 6114, Loss: 4.777552604675293\n",
            "Training Iteration 6115, Loss: 7.909997940063477\n",
            "Training Iteration 6116, Loss: 5.529423236846924\n",
            "Training Iteration 6117, Loss: 2.901754379272461\n",
            "Training Iteration 6118, Loss: 2.5331027507781982\n",
            "Training Iteration 6119, Loss: 1.2793145179748535\n",
            "Training Iteration 6120, Loss: 2.6235504150390625\n",
            "Training Iteration 6121, Loss: 8.986764907836914\n",
            "Training Iteration 6122, Loss: 6.5768938064575195\n",
            "Training Iteration 6123, Loss: 6.533499717712402\n",
            "Training Iteration 6124, Loss: 2.2718687057495117\n",
            "Training Iteration 6125, Loss: 3.6217658519744873\n",
            "Training Iteration 6126, Loss: 3.976315975189209\n",
            "Training Iteration 6127, Loss: 5.188905715942383\n",
            "Training Iteration 6128, Loss: 6.513571739196777\n",
            "Training Iteration 6129, Loss: 4.858401298522949\n",
            "Training Iteration 6130, Loss: 3.1223230361938477\n",
            "Training Iteration 6131, Loss: 4.854729652404785\n",
            "Training Iteration 6132, Loss: 2.5888073444366455\n",
            "Training Iteration 6133, Loss: 2.9281506538391113\n",
            "Training Iteration 6134, Loss: 5.837794303894043\n",
            "Training Iteration 6135, Loss: 3.0667190551757812\n",
            "Training Iteration 6136, Loss: 3.935298204421997\n",
            "Training Iteration 6137, Loss: 5.100788593292236\n",
            "Training Iteration 6138, Loss: 4.463536739349365\n",
            "Training Iteration 6139, Loss: 6.11263370513916\n",
            "Training Iteration 6140, Loss: 5.627604007720947\n",
            "Training Iteration 6141, Loss: 4.307143688201904\n",
            "Training Iteration 6142, Loss: 3.710740327835083\n",
            "Training Iteration 6143, Loss: 1.9635275602340698\n",
            "Training Iteration 6144, Loss: 6.019822597503662\n",
            "Training Iteration 6145, Loss: 4.663884162902832\n",
            "Training Iteration 6146, Loss: 7.523776054382324\n",
            "Training Iteration 6147, Loss: 4.991799831390381\n",
            "Training Iteration 6148, Loss: 3.4229421615600586\n",
            "Training Iteration 6149, Loss: 2.450495481491089\n",
            "Training Iteration 6150, Loss: 2.4671781063079834\n",
            "Training Iteration 6151, Loss: 3.7562668323516846\n",
            "Training Iteration 6152, Loss: 4.726357460021973\n",
            "Training Iteration 6153, Loss: 2.0978353023529053\n",
            "Training Iteration 6154, Loss: 5.250760078430176\n",
            "Training Iteration 6155, Loss: 4.858633995056152\n",
            "Training Iteration 6156, Loss: 3.749906063079834\n",
            "Training Iteration 6157, Loss: 2.8419349193573\n",
            "Training Iteration 6158, Loss: 4.099299430847168\n",
            "Training Iteration 6159, Loss: 4.370662212371826\n",
            "Training Iteration 6160, Loss: 5.021474838256836\n",
            "Training Iteration 6161, Loss: 3.7572619915008545\n",
            "Training Iteration 6162, Loss: 3.2834174633026123\n",
            "Training Iteration 6163, Loss: 5.170403003692627\n",
            "Training Iteration 6164, Loss: 5.006143569946289\n",
            "Training Iteration 6165, Loss: 5.619760513305664\n",
            "Training Iteration 6166, Loss: 3.41335391998291\n",
            "Training Iteration 6167, Loss: 2.7251830101013184\n",
            "Training Iteration 6168, Loss: 3.408524990081787\n",
            "Training Iteration 6169, Loss: 2.5239572525024414\n",
            "Training Iteration 6170, Loss: 6.360857009887695\n",
            "Training Iteration 6171, Loss: 5.083574295043945\n",
            "Training Iteration 6172, Loss: 4.12527322769165\n",
            "Training Iteration 6173, Loss: 2.1732065677642822\n",
            "Training Iteration 6174, Loss: 2.274271249771118\n",
            "Training Iteration 6175, Loss: 3.280517578125\n",
            "Training Iteration 6176, Loss: 4.228259563446045\n",
            "Training Iteration 6177, Loss: 2.902207851409912\n",
            "Training Iteration 6178, Loss: 2.619384288787842\n",
            "Training Iteration 6179, Loss: 2.1954541206359863\n",
            "Training Iteration 6180, Loss: 2.245011806488037\n",
            "Training Iteration 6181, Loss: 5.842358112335205\n",
            "Training Iteration 6182, Loss: 11.935774803161621\n",
            "Training Iteration 6183, Loss: 5.971786022186279\n",
            "Training Iteration 6184, Loss: 3.4545817375183105\n",
            "Training Iteration 6185, Loss: 4.962204933166504\n",
            "Training Iteration 6186, Loss: 2.5313143730163574\n",
            "Training Iteration 6187, Loss: 4.273860931396484\n",
            "Training Iteration 6188, Loss: 2.1591176986694336\n",
            "Training Iteration 6189, Loss: 6.24952507019043\n",
            "Training Iteration 6190, Loss: 8.207504272460938\n",
            "Training Iteration 6191, Loss: 6.321284770965576\n",
            "Training Iteration 6192, Loss: 3.302520513534546\n",
            "Training Iteration 6193, Loss: 2.418644905090332\n",
            "Training Iteration 6194, Loss: 5.002784252166748\n",
            "Training Iteration 6195, Loss: 2.3238682746887207\n",
            "Training Iteration 6196, Loss: 4.094388008117676\n",
            "Training Iteration 6197, Loss: 3.9149909019470215\n",
            "Training Iteration 6198, Loss: 5.260666847229004\n",
            "Training Iteration 6199, Loss: 5.123848915100098\n",
            "Training Iteration 6200, Loss: 4.5667595863342285\n",
            "Training Iteration 6201, Loss: 4.850280284881592\n",
            "Training Iteration 6202, Loss: 4.548055648803711\n",
            "Training Iteration 6203, Loss: 4.197430610656738\n",
            "Training Iteration 6204, Loss: 6.160947799682617\n",
            "Training Iteration 6205, Loss: 2.1368579864501953\n",
            "Training Iteration 6206, Loss: 5.960062503814697\n",
            "Training Iteration 6207, Loss: 4.3765482902526855\n",
            "Training Iteration 6208, Loss: 7.632442951202393\n",
            "Training Iteration 6209, Loss: 2.8508152961730957\n",
            "Training Iteration 6210, Loss: 3.1508843898773193\n",
            "Training Iteration 6211, Loss: 5.08226203918457\n",
            "Training Iteration 6212, Loss: 5.009005546569824\n",
            "Training Iteration 6213, Loss: 6.305487155914307\n",
            "Training Iteration 6214, Loss: 4.085421562194824\n",
            "Training Iteration 6215, Loss: 2.326439380645752\n",
            "Training Iteration 6216, Loss: 3.338021993637085\n",
            "Training Iteration 6217, Loss: 6.002146244049072\n",
            "Training Iteration 6218, Loss: 7.575277328491211\n",
            "Training Iteration 6219, Loss: 4.326516628265381\n",
            "Training Iteration 6220, Loss: 5.375965118408203\n",
            "Training Iteration 6221, Loss: 4.868896961212158\n",
            "Training Iteration 6222, Loss: 5.85484504699707\n",
            "Training Iteration 6223, Loss: 5.211175441741943\n",
            "Training Iteration 6224, Loss: 3.4453341960906982\n",
            "Training Iteration 6225, Loss: 4.210597515106201\n",
            "Training Iteration 6226, Loss: 3.4121320247650146\n",
            "Training Iteration 6227, Loss: 3.5465009212493896\n",
            "Training Iteration 6228, Loss: 3.4303557872772217\n",
            "Training Iteration 6229, Loss: 7.406599044799805\n",
            "Training Iteration 6230, Loss: 3.7472660541534424\n",
            "Training Iteration 6231, Loss: 3.7923922538757324\n",
            "Training Iteration 6232, Loss: 4.168030261993408\n",
            "Training Iteration 6233, Loss: 4.796970367431641\n",
            "Training Iteration 6234, Loss: 4.222815036773682\n",
            "Training Iteration 6235, Loss: 5.699919700622559\n",
            "Training Iteration 6236, Loss: 5.138785362243652\n",
            "Training Iteration 6237, Loss: 6.617145538330078\n",
            "Training Iteration 6238, Loss: 2.0902810096740723\n",
            "Training Iteration 6239, Loss: 5.115121841430664\n",
            "Training Iteration 6240, Loss: 6.138615608215332\n",
            "Training Iteration 6241, Loss: 7.732285499572754\n",
            "Training Iteration 6242, Loss: 3.644651174545288\n",
            "Training Iteration 6243, Loss: 2.4713141918182373\n",
            "Training Iteration 6244, Loss: 3.6017370223999023\n",
            "Training Iteration 6245, Loss: 2.016383171081543\n",
            "Training Iteration 6246, Loss: 6.044462203979492\n",
            "Training Iteration 6247, Loss: 4.421777248382568\n",
            "Training Iteration 6248, Loss: 6.494146347045898\n",
            "Training Iteration 6249, Loss: 3.951165199279785\n",
            "Training Iteration 6250, Loss: 5.237784385681152\n",
            "Training Iteration 6251, Loss: 5.373899936676025\n",
            "Training Iteration 6252, Loss: 2.8113462924957275\n",
            "Training Iteration 6253, Loss: 2.9388058185577393\n",
            "Training Iteration 6254, Loss: 5.645453453063965\n",
            "Training Iteration 6255, Loss: 3.8755879402160645\n",
            "Training Iteration 6256, Loss: 6.077945709228516\n",
            "Training Iteration 6257, Loss: 7.715753555297852\n",
            "Training Iteration 6258, Loss: 3.5149474143981934\n",
            "Training Iteration 6259, Loss: 6.091210842132568\n",
            "Training Iteration 6260, Loss: 3.0371453762054443\n",
            "Training Iteration 6261, Loss: 1.9654786586761475\n",
            "Training Iteration 6262, Loss: 5.612233638763428\n",
            "Training Iteration 6263, Loss: 3.641923427581787\n",
            "Training Iteration 6264, Loss: 7.026327610015869\n",
            "Training Iteration 6265, Loss: 3.8262338638305664\n",
            "Training Iteration 6266, Loss: 3.421931505203247\n",
            "Training Iteration 6267, Loss: 4.656763076782227\n",
            "Training Iteration 6268, Loss: 3.3352372646331787\n",
            "Training Iteration 6269, Loss: 6.542023658752441\n",
            "Training Iteration 6270, Loss: 4.690488815307617\n",
            "Training Iteration 6271, Loss: 3.3235666751861572\n",
            "Training Iteration 6272, Loss: 3.5471601486206055\n",
            "Training Iteration 6273, Loss: 6.350964546203613\n",
            "Training Iteration 6274, Loss: 3.821763038635254\n",
            "Training Iteration 6275, Loss: 2.03240704536438\n",
            "Training Iteration 6276, Loss: 2.6687848567962646\n",
            "Training Iteration 6277, Loss: 5.7322306632995605\n",
            "Training Iteration 6278, Loss: 6.813036918640137\n",
            "Training Iteration 6279, Loss: 4.575758934020996\n",
            "Training Iteration 6280, Loss: 3.611863613128662\n",
            "Training Iteration 6281, Loss: 4.537725448608398\n",
            "Training Iteration 6282, Loss: 8.919666290283203\n",
            "Training Iteration 6283, Loss: 5.316125392913818\n",
            "Training Iteration 6284, Loss: 4.916632175445557\n",
            "Training Iteration 6285, Loss: 5.079032897949219\n",
            "Training Iteration 6286, Loss: 5.704414367675781\n",
            "Training Iteration 6287, Loss: 3.187329053878784\n",
            "Training Iteration 6288, Loss: 8.076746940612793\n",
            "Training Iteration 6289, Loss: 4.700950622558594\n",
            "Training Iteration 6290, Loss: 3.617844343185425\n",
            "Training Iteration 6291, Loss: 3.7717325687408447\n",
            "Training Iteration 6292, Loss: 7.261690139770508\n",
            "Training Iteration 6293, Loss: 6.179352760314941\n",
            "Training Iteration 6294, Loss: 3.1127898693084717\n",
            "Training Iteration 6295, Loss: 6.919558048248291\n",
            "Training Iteration 6296, Loss: 5.45065450668335\n",
            "Training Iteration 6297, Loss: 4.3651323318481445\n",
            "Training Iteration 6298, Loss: 7.075465679168701\n",
            "Training Iteration 6299, Loss: 6.907184600830078\n",
            "Training Iteration 6300, Loss: 2.8057539463043213\n",
            "Training Iteration 6301, Loss: 3.961540937423706\n",
            "Training Iteration 6302, Loss: 7.984758377075195\n",
            "Training Iteration 6303, Loss: 5.504238128662109\n",
            "Training Iteration 6304, Loss: 4.902280807495117\n",
            "Training Iteration 6305, Loss: 5.379697799682617\n",
            "Training Iteration 6306, Loss: 5.056976318359375\n",
            "Training Iteration 6307, Loss: 6.455136299133301\n",
            "Training Iteration 6308, Loss: 5.1036458015441895\n",
            "Training Iteration 6309, Loss: 4.696245193481445\n",
            "Training Iteration 6310, Loss: 3.853820562362671\n",
            "Training Iteration 6311, Loss: 4.570232391357422\n",
            "Training Iteration 6312, Loss: 4.59179162979126\n",
            "Training Iteration 6313, Loss: 5.082375526428223\n",
            "Training Iteration 6314, Loss: 1.7055206298828125\n",
            "Training Iteration 6315, Loss: 3.26399564743042\n",
            "Training Iteration 6316, Loss: 6.777477741241455\n",
            "Training Iteration 6317, Loss: 6.506733417510986\n",
            "Training Iteration 6318, Loss: 4.117319107055664\n",
            "Training Iteration 6319, Loss: 5.578755855560303\n",
            "Training Iteration 6320, Loss: 10.62645435333252\n",
            "Training Iteration 6321, Loss: 5.330239295959473\n",
            "Training Iteration 6322, Loss: 6.473435401916504\n",
            "Training Iteration 6323, Loss: 5.026971340179443\n",
            "Training Iteration 6324, Loss: 3.8955955505371094\n",
            "Training Iteration 6325, Loss: 5.0040082931518555\n",
            "Training Iteration 6326, Loss: 4.563131809234619\n",
            "Training Iteration 6327, Loss: 2.7286438941955566\n",
            "Training Iteration 6328, Loss: 5.204751491546631\n",
            "Training Iteration 6329, Loss: 1.3798655271530151\n",
            "Training Iteration 6330, Loss: 4.296207427978516\n",
            "Training Iteration 6331, Loss: 4.259576320648193\n",
            "Training Iteration 6332, Loss: 6.913483619689941\n",
            "Training Iteration 6333, Loss: 5.259084701538086\n",
            "Training Iteration 6334, Loss: 7.585648536682129\n",
            "Training Iteration 6335, Loss: 4.311737537384033\n",
            "Training Iteration 6336, Loss: 7.301614761352539\n",
            "Training Iteration 6337, Loss: 6.359654903411865\n",
            "Training Iteration 6338, Loss: 4.244830131530762\n",
            "Training Iteration 6339, Loss: 6.144002914428711\n",
            "Training Iteration 6340, Loss: 2.4245100021362305\n",
            "Training Iteration 6341, Loss: 3.1113696098327637\n",
            "Training Iteration 6342, Loss: 1.9780077934265137\n",
            "Training Iteration 6343, Loss: 5.1571478843688965\n",
            "Training Iteration 6344, Loss: 2.8956496715545654\n",
            "Training Iteration 6345, Loss: 5.347851276397705\n",
            "Training Iteration 6346, Loss: 3.697039842605591\n",
            "Training Iteration 6347, Loss: 4.492999076843262\n",
            "Training Iteration 6348, Loss: 3.3260085582733154\n",
            "Training Iteration 6349, Loss: 4.234925746917725\n",
            "Training Iteration 6350, Loss: 5.327502250671387\n",
            "Training Iteration 6351, Loss: 4.323254108428955\n",
            "Training Iteration 6352, Loss: 3.2370362281799316\n",
            "Training Iteration 6353, Loss: 2.4299275875091553\n",
            "Training Iteration 6354, Loss: 3.988312244415283\n",
            "Training Iteration 6355, Loss: 5.96439790725708\n",
            "Training Iteration 6356, Loss: 2.544663190841675\n",
            "Training Iteration 6357, Loss: 6.300598621368408\n",
            "Training Iteration 6358, Loss: 2.7486419677734375\n",
            "Training Iteration 6359, Loss: 4.27011251449585\n",
            "Training Iteration 6360, Loss: 4.253875732421875\n",
            "Training Iteration 6361, Loss: 4.396512031555176\n",
            "Training Iteration 6362, Loss: 5.815166473388672\n",
            "Training Iteration 6363, Loss: 3.6793763637542725\n",
            "Training Iteration 6364, Loss: 4.904297828674316\n",
            "Training Iteration 6365, Loss: 4.432465076446533\n",
            "Training Iteration 6366, Loss: 4.651610374450684\n",
            "Training Iteration 6367, Loss: 4.707273960113525\n",
            "Training Iteration 6368, Loss: 4.404641628265381\n",
            "Training Iteration 6369, Loss: 7.2527971267700195\n",
            "Training Iteration 6370, Loss: 4.884631633758545\n",
            "Training Iteration 6371, Loss: 6.549307823181152\n",
            "Training Iteration 6372, Loss: 2.4740312099456787\n",
            "Training Iteration 6373, Loss: 4.161075592041016\n",
            "Training Iteration 6374, Loss: 4.588674068450928\n",
            "Training Iteration 6375, Loss: 5.497979164123535\n",
            "Training Iteration 6376, Loss: 7.079100608825684\n",
            "Training Iteration 6377, Loss: 6.042366981506348\n",
            "Training Iteration 6378, Loss: 4.253518104553223\n",
            "Training Iteration 6379, Loss: 3.661119222640991\n",
            "Training Iteration 6380, Loss: 5.041888236999512\n",
            "Training Iteration 6381, Loss: 4.161303520202637\n",
            "Training Iteration 6382, Loss: 4.923453330993652\n",
            "Training Iteration 6383, Loss: 2.5599942207336426\n",
            "Training Iteration 6384, Loss: 4.947611331939697\n",
            "Training Iteration 6385, Loss: 3.819054126739502\n",
            "Training Iteration 6386, Loss: 7.9230241775512695\n",
            "Training Iteration 6387, Loss: 6.75142240524292\n",
            "Training Iteration 6388, Loss: 9.639094352722168\n",
            "Training Iteration 6389, Loss: 5.5822672843933105\n",
            "Training Iteration 6390, Loss: 4.895357608795166\n",
            "Training Iteration 6391, Loss: 4.065303325653076\n",
            "Training Iteration 6392, Loss: 5.098908424377441\n",
            "Training Iteration 6393, Loss: 6.302860736846924\n",
            "Training Iteration 6394, Loss: 6.861963748931885\n",
            "Training Iteration 6395, Loss: 3.983431339263916\n",
            "Training Iteration 6396, Loss: 7.207368850708008\n",
            "Training Iteration 6397, Loss: 5.934871196746826\n",
            "Training Iteration 6398, Loss: 7.759037017822266\n",
            "Training Iteration 6399, Loss: 6.166677951812744\n",
            "Training Iteration 6400, Loss: 3.8340773582458496\n",
            "Training Iteration 6401, Loss: 7.304067134857178\n",
            "Training Iteration 6402, Loss: 4.843050956726074\n",
            "Training Iteration 6403, Loss: 4.060878276824951\n",
            "Training Iteration 6404, Loss: 6.890078067779541\n",
            "Training Iteration 6405, Loss: 5.602505207061768\n",
            "Training Iteration 6406, Loss: 4.971389293670654\n",
            "Training Iteration 6407, Loss: 6.917359828948975\n",
            "Training Iteration 6408, Loss: 4.085122585296631\n",
            "Training Iteration 6409, Loss: 4.9291090965271\n",
            "Training Iteration 6410, Loss: 4.096017837524414\n",
            "Training Iteration 6411, Loss: 7.133264541625977\n",
            "Training Iteration 6412, Loss: 4.6043195724487305\n",
            "Training Iteration 6413, Loss: 4.282736301422119\n",
            "Training Iteration 6414, Loss: 6.4831695556640625\n",
            "Training Iteration 6415, Loss: 6.167160987854004\n",
            "Training Iteration 6416, Loss: 5.105198860168457\n",
            "Training Iteration 6417, Loss: 6.4650983810424805\n",
            "Training Iteration 6418, Loss: 2.8244314193725586\n",
            "Training Iteration 6419, Loss: 6.3928022384643555\n",
            "Training Iteration 6420, Loss: 2.99993634223938\n",
            "Training Iteration 6421, Loss: 4.523960590362549\n",
            "Training Iteration 6422, Loss: 4.856202125549316\n",
            "Training Iteration 6423, Loss: 3.807420492172241\n",
            "Training Iteration 6424, Loss: 2.4872171878814697\n",
            "Training Iteration 6425, Loss: 4.619241714477539\n",
            "Training Iteration 6426, Loss: 2.721179485321045\n",
            "Training Iteration 6427, Loss: 1.7304627895355225\n",
            "Training Iteration 6428, Loss: 4.344995498657227\n",
            "Training Iteration 6429, Loss: 4.540515899658203\n",
            "Training Iteration 6430, Loss: 1.5915428400039673\n",
            "Training Iteration 6431, Loss: 3.9285409450531006\n",
            "Training Iteration 6432, Loss: 6.388998508453369\n",
            "Training Iteration 6433, Loss: 4.710813999176025\n",
            "Training Iteration 6434, Loss: 3.1418144702911377\n",
            "Training Iteration 6435, Loss: 4.913405895233154\n",
            "Training Iteration 6436, Loss: 4.6949687004089355\n",
            "Training Iteration 6437, Loss: 6.18006706237793\n",
            "Training Iteration 6438, Loss: 3.710766553878784\n",
            "Training Iteration 6439, Loss: 2.2660441398620605\n",
            "Training Iteration 6440, Loss: 5.015003681182861\n",
            "Training Iteration 6441, Loss: 2.1051228046417236\n",
            "Training Iteration 6442, Loss: 9.025199890136719\n",
            "Training Iteration 6443, Loss: 4.34401798248291\n",
            "Training Iteration 6444, Loss: 8.783233642578125\n",
            "Training Iteration 6445, Loss: 4.173625469207764\n",
            "Training Iteration 6446, Loss: 1.614170789718628\n",
            "Training Iteration 6447, Loss: 0.9660180807113647\n",
            "Training Iteration 6448, Loss: 4.921387195587158\n",
            "Training Iteration 6449, Loss: 5.942073822021484\n",
            "Training Iteration 6450, Loss: 5.944430828094482\n",
            "Training Iteration 6451, Loss: 4.446479320526123\n",
            "Training Iteration 6452, Loss: 4.664241313934326\n",
            "Training Iteration 6453, Loss: 6.286897659301758\n",
            "Training Iteration 6454, Loss: 7.769888877868652\n",
            "Training Iteration 6455, Loss: 5.249776840209961\n",
            "Training Iteration 6456, Loss: 3.3525872230529785\n",
            "Training Iteration 6457, Loss: 9.520186424255371\n",
            "Training Iteration 6458, Loss: 2.0640487670898438\n",
            "Training Iteration 6459, Loss: 5.355766773223877\n",
            "Training Iteration 6460, Loss: 4.190633296966553\n",
            "Training Iteration 6461, Loss: 2.5272130966186523\n",
            "Training Iteration 6462, Loss: 7.913363456726074\n",
            "Training Iteration 6463, Loss: 6.320195198059082\n",
            "Training Iteration 6464, Loss: 3.7688958644866943\n",
            "Training Iteration 6465, Loss: 6.219198226928711\n",
            "Training Iteration 6466, Loss: 5.032063007354736\n",
            "Training Iteration 6467, Loss: 8.918343544006348\n",
            "Training Iteration 6468, Loss: 6.38457727432251\n",
            "Training Iteration 6469, Loss: 2.6599254608154297\n",
            "Training Iteration 6470, Loss: 3.2008512020111084\n",
            "Training Iteration 6471, Loss: 4.058577060699463\n",
            "Training Iteration 6472, Loss: 4.10938835144043\n",
            "Training Iteration 6473, Loss: 4.81342077255249\n",
            "Training Iteration 6474, Loss: 4.432994365692139\n",
            "Training Iteration 6475, Loss: 4.460169792175293\n",
            "Training Iteration 6476, Loss: 3.310577630996704\n",
            "Training Iteration 6477, Loss: 4.19970178604126\n",
            "Training Iteration 6478, Loss: 3.8035993576049805\n",
            "Training Iteration 6479, Loss: 3.2291815280914307\n",
            "Training Iteration 6480, Loss: 4.155052661895752\n",
            "Training Iteration 6481, Loss: 2.03963303565979\n",
            "Training Iteration 6482, Loss: 5.051722049713135\n",
            "Training Iteration 6483, Loss: 3.6341123580932617\n",
            "Training Iteration 6484, Loss: 4.0392913818359375\n",
            "Training Iteration 6485, Loss: 4.641816139221191\n",
            "Training Iteration 6486, Loss: 3.7402431964874268\n",
            "Training Iteration 6487, Loss: 3.4422221183776855\n",
            "Training Iteration 6488, Loss: 4.916548728942871\n",
            "Training Iteration 6489, Loss: 4.786536693572998\n",
            "Training Iteration 6490, Loss: 2.563600540161133\n",
            "Training Iteration 6491, Loss: 4.05668306350708\n",
            "Training Iteration 6492, Loss: 4.49714469909668\n",
            "Training Iteration 6493, Loss: 3.1862945556640625\n",
            "Training Iteration 6494, Loss: 1.7474523782730103\n",
            "Training Iteration 6495, Loss: 6.027479648590088\n",
            "Training Iteration 6496, Loss: 4.069825172424316\n",
            "Training Iteration 6497, Loss: 4.812755584716797\n",
            "Training Iteration 6498, Loss: 6.5478596687316895\n",
            "Training Iteration 6499, Loss: 3.943377733230591\n",
            "Training Iteration 6500, Loss: 4.1076555252075195\n",
            "Training Iteration 6501, Loss: 5.197751045227051\n",
            "Training Iteration 6502, Loss: 3.14260196685791\n",
            "Training Iteration 6503, Loss: 8.036209106445312\n",
            "Training Iteration 6504, Loss: 5.609101295471191\n",
            "Training Iteration 6505, Loss: 4.966741561889648\n",
            "Training Iteration 6506, Loss: 4.695901870727539\n",
            "Training Iteration 6507, Loss: 6.042116641998291\n",
            "Training Iteration 6508, Loss: 4.18351936340332\n",
            "Training Iteration 6509, Loss: 4.10865592956543\n",
            "Training Iteration 6510, Loss: 2.8200743198394775\n",
            "Training Iteration 6511, Loss: 3.788940668106079\n",
            "Training Iteration 6512, Loss: 5.009241104125977\n",
            "Training Iteration 6513, Loss: 4.561039447784424\n",
            "Training Iteration 6514, Loss: 7.271828651428223\n",
            "Training Iteration 6515, Loss: 2.5392651557922363\n",
            "Training Iteration 6516, Loss: 3.4901938438415527\n",
            "Training Iteration 6517, Loss: 2.59651780128479\n",
            "Training Iteration 6518, Loss: 7.1153459548950195\n",
            "Training Iteration 6519, Loss: 5.6482977867126465\n",
            "Training Iteration 6520, Loss: 8.211167335510254\n",
            "Training Iteration 6521, Loss: 5.771568775177002\n",
            "Training Iteration 6522, Loss: 7.681288719177246\n",
            "Training Iteration 6523, Loss: 7.326333999633789\n",
            "Training Iteration 6524, Loss: 4.077890396118164\n",
            "Training Iteration 6525, Loss: 5.73386812210083\n",
            "Training Iteration 6526, Loss: 4.419445037841797\n",
            "Training Iteration 6527, Loss: 3.9010696411132812\n",
            "Training Iteration 6528, Loss: 10.079570770263672\n",
            "Training Iteration 6529, Loss: 12.6694917678833\n",
            "Training Iteration 6530, Loss: 8.080371856689453\n",
            "Training Iteration 6531, Loss: 5.4712677001953125\n",
            "Training Iteration 6532, Loss: 5.858965873718262\n",
            "Training Iteration 6533, Loss: 5.178219318389893\n",
            "Training Iteration 6534, Loss: 3.812516212463379\n",
            "Training Iteration 6535, Loss: 5.429683685302734\n",
            "Training Iteration 6536, Loss: 10.399177551269531\n",
            "Training Iteration 6537, Loss: 6.866544246673584\n",
            "Training Iteration 6538, Loss: 5.125783443450928\n",
            "Training Iteration 6539, Loss: 3.8030784130096436\n",
            "Training Iteration 6540, Loss: 10.43536376953125\n",
            "Training Iteration 6541, Loss: 9.95853042602539\n",
            "Training Iteration 6542, Loss: 6.549901008605957\n",
            "Training Iteration 6543, Loss: 6.798317909240723\n",
            "Training Iteration 6544, Loss: 9.80621337890625\n",
            "Training Iteration 6545, Loss: 3.5315194129943848\n",
            "Training Iteration 6546, Loss: 4.6446146965026855\n",
            "Training Iteration 6547, Loss: 4.503183364868164\n",
            "Training Iteration 6548, Loss: 1.781203269958496\n",
            "Training Iteration 6549, Loss: 4.5177531242370605\n",
            "Training Iteration 6550, Loss: 4.618902683258057\n",
            "Training Iteration 6551, Loss: 6.659046173095703\n",
            "Training Iteration 6552, Loss: 3.848106861114502\n",
            "Training Iteration 6553, Loss: 4.26809024810791\n",
            "Training Iteration 6554, Loss: 2.883683919906616\n",
            "tensor([[1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        ...,\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04]])\n",
            "Training loss for epcoh 7: 3.234274737821107\n",
            "Training accuracy for epoch 7: 0.08770457406630298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        ...,\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04],\n",
            "        [1.3819e-01, 1.1361e-01, 1.1656e-03, 7.1553e-01, 3.1337e-02, 1.6853e-04]])\n",
            "Validation loss for epcoh 7: 3.22711774428637\n",
            "Test accuracy for epoch 7: 0.08926527809567407\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch No: 8:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f6cf0cf57d54d50a14a7abe44018bd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Iteration 1, Loss: 4.998380661010742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 4.4460225105285645\n",
            "Training Iteration 1565, Loss: 3.1758296489715576\n",
            "Training Iteration 1566, Loss: 3.1885476112365723\n",
            "Training Iteration 1567, Loss: 2.1888742446899414\n",
            "Training Iteration 1568, Loss: 3.4059739112854004\n",
            "Training Iteration 1569, Loss: 4.923968315124512\n",
            "Training Iteration 1570, Loss: 4.779064178466797\n",
            "Training Iteration 1571, Loss: 4.601027488708496\n",
            "Training Iteration 1572, Loss: 6.075123310089111\n",
            "Training Iteration 1573, Loss: 6.600707054138184\n",
            "Training Iteration 1574, Loss: 3.8128550052642822\n",
            "Training Iteration 1575, Loss: 5.33490514755249\n",
            "Training Iteration 1576, Loss: 1.8330273628234863\n",
            "Training Iteration 1577, Loss: 6.573373794555664\n",
            "Training Iteration 1578, Loss: 4.4501261711120605\n",
            "Training Iteration 1579, Loss: 4.333555221557617\n",
            "Training Iteration 1580, Loss: 2.3023526668548584\n",
            "Training Iteration 1581, Loss: 3.786206007003784\n",
            "Training Iteration 1582, Loss: 2.0949783325195312\n",
            "Training Iteration 1583, Loss: 7.428098201751709\n",
            "Training Iteration 1584, Loss: 7.271139621734619\n",
            "Training Iteration 1585, Loss: 3.3100435733795166\n",
            "Training Iteration 1586, Loss: 4.94140100479126\n",
            "Training Iteration 1587, Loss: 7.950216770172119\n",
            "Training Iteration 1588, Loss: 4.088537216186523\n",
            "Training Iteration 1589, Loss: 6.014914512634277\n",
            "Training Iteration 1590, Loss: 8.958343505859375\n",
            "Training Iteration 1591, Loss: 2.993360996246338\n",
            "Training Iteration 1592, Loss: 5.027575969696045\n",
            "Training Iteration 1593, Loss: 4.876524448394775\n",
            "Training Iteration 1594, Loss: 4.486746788024902\n",
            "Training Iteration 1595, Loss: 3.1455626487731934\n",
            "Training Iteration 1596, Loss: 4.33727502822876\n",
            "Training Iteration 1597, Loss: 4.4593186378479\n",
            "Training Iteration 1598, Loss: 3.6062660217285156\n",
            "Training Iteration 1599, Loss: 4.436909198760986\n",
            "Training Iteration 1600, Loss: 3.257866144180298\n",
            "Training Iteration 1601, Loss: 4.81902551651001\n",
            "Training Iteration 1602, Loss: 4.577317714691162\n",
            "Training Iteration 1603, Loss: 2.9402921199798584\n",
            "Training Iteration 1604, Loss: 3.0909664630889893\n",
            "Training Iteration 1605, Loss: 4.806944370269775\n",
            "Training Iteration 1606, Loss: 4.646905899047852\n",
            "Training Iteration 1607, Loss: 6.109909534454346\n",
            "Training Iteration 1608, Loss: 3.3655388355255127\n",
            "Training Iteration 1609, Loss: 5.303250312805176\n",
            "Training Iteration 1610, Loss: 3.6350836753845215\n",
            "Training Iteration 1611, Loss: 7.491938591003418\n",
            "Training Iteration 1612, Loss: 3.1034393310546875\n",
            "Training Iteration 1613, Loss: 4.193410396575928\n",
            "Training Iteration 1614, Loss: 4.2425031661987305\n",
            "Training Iteration 1615, Loss: 7.239228248596191\n",
            "Training Iteration 1616, Loss: 2.745354652404785\n",
            "Training Iteration 1617, Loss: 7.218557357788086\n",
            "Training Iteration 1618, Loss: 3.6999478340148926\n",
            "Training Iteration 1619, Loss: 4.649556636810303\n",
            "Training Iteration 1620, Loss: 4.3283162117004395\n",
            "Training Iteration 1621, Loss: 4.506077289581299\n",
            "Training Iteration 1622, Loss: 4.683816432952881\n",
            "Training Iteration 1623, Loss: 3.1273696422576904\n",
            "Training Iteration 1624, Loss: 3.239454507827759\n",
            "Training Iteration 1625, Loss: 5.506950378417969\n",
            "Training Iteration 1626, Loss: 4.151011943817139\n",
            "Training Iteration 1627, Loss: 4.00144100189209\n",
            "Training Iteration 1628, Loss: 2.6119296550750732\n",
            "Training Iteration 1629, Loss: 4.389806747436523\n",
            "Training Iteration 1630, Loss: 3.3743019104003906\n",
            "Training Iteration 1631, Loss: 3.8760809898376465\n",
            "Training Iteration 1632, Loss: 5.656582355499268\n",
            "Training Iteration 1633, Loss: 4.257734298706055\n",
            "Training Iteration 1634, Loss: 7.429140090942383\n",
            "Training Iteration 1635, Loss: 6.32541036605835\n",
            "Training Iteration 1636, Loss: 4.519504070281982\n",
            "Training Iteration 1637, Loss: 9.196148872375488\n",
            "Training Iteration 1638, Loss: 5.547613143920898\n",
            "Training Iteration 1639, Loss: 5.401617527008057\n",
            "Training Iteration 1640, Loss: 2.514023780822754\n",
            "Training Iteration 1641, Loss: 3.3808934688568115\n",
            "Training Iteration 1642, Loss: 4.363516330718994\n",
            "Training Iteration 1643, Loss: 3.929590940475464\n",
            "Training Iteration 1644, Loss: 6.9782938957214355\n",
            "Training Iteration 1645, Loss: 6.697996616363525\n",
            "Training Iteration 1646, Loss: 4.525537490844727\n",
            "Training Iteration 1647, Loss: 7.055124282836914\n",
            "Training Iteration 1648, Loss: 5.065337657928467\n",
            "Training Iteration 1649, Loss: 3.068751811981201\n",
            "Training Iteration 1650, Loss: 5.24641752243042\n",
            "Training Iteration 1651, Loss: 8.51203727722168\n",
            "Training Iteration 1652, Loss: 9.2440824508667\n",
            "Training Iteration 1653, Loss: 4.987618923187256\n",
            "Training Iteration 1654, Loss: 9.142399787902832\n",
            "Training Iteration 1655, Loss: 8.95213508605957\n",
            "Training Iteration 1656, Loss: 8.142932891845703\n",
            "Training Iteration 1657, Loss: 3.466218948364258\n",
            "Training Iteration 1658, Loss: 3.5795257091522217\n",
            "Training Iteration 1659, Loss: 5.012964725494385\n",
            "Training Iteration 1660, Loss: 3.6685633659362793\n",
            "Training Iteration 1661, Loss: 5.1220245361328125\n",
            "Training Iteration 1662, Loss: 4.451705455780029\n",
            "Training Iteration 1663, Loss: 6.479714393615723\n",
            "Training Iteration 1664, Loss: 5.151357650756836\n",
            "Training Iteration 1665, Loss: 5.2692108154296875\n",
            "Training Iteration 1666, Loss: 5.511849880218506\n",
            "Training Iteration 1667, Loss: 8.835904121398926\n",
            "Training Iteration 1668, Loss: 6.227565288543701\n",
            "Training Iteration 1669, Loss: 3.636448621749878\n",
            "Training Iteration 1670, Loss: 3.0250046253204346\n",
            "Training Iteration 1671, Loss: 3.5792789459228516\n",
            "Training Iteration 1672, Loss: 8.052046775817871\n",
            "Training Iteration 1673, Loss: 5.022966384887695\n",
            "Training Iteration 1674, Loss: 4.929423809051514\n",
            "Training Iteration 1675, Loss: 2.2003400325775146\n",
            "Training Iteration 1676, Loss: 6.280908107757568\n",
            "Training Iteration 1677, Loss: 4.048519611358643\n",
            "Training Iteration 1678, Loss: 5.096396446228027\n",
            "Training Iteration 1679, Loss: 5.6723246574401855\n",
            "Training Iteration 1680, Loss: 6.233266353607178\n",
            "Training Iteration 1681, Loss: 5.814054489135742\n",
            "Training Iteration 1682, Loss: 5.086757659912109\n",
            "Training Iteration 1683, Loss: 2.6458520889282227\n",
            "Training Iteration 1684, Loss: 3.8481605052948\n",
            "Training Iteration 1685, Loss: 4.3096232414245605\n",
            "Training Iteration 1686, Loss: 7.403078079223633\n",
            "Training Iteration 1687, Loss: 3.467500925064087\n",
            "Training Iteration 1688, Loss: 2.086240768432617\n",
            "Training Iteration 1689, Loss: 5.87526273727417\n",
            "Training Iteration 1690, Loss: 5.0867204666137695\n",
            "Training Iteration 1691, Loss: 2.4255714416503906\n",
            "Training Iteration 1692, Loss: 6.713982582092285\n",
            "Training Iteration 1693, Loss: 4.363765716552734\n",
            "Training Iteration 1694, Loss: 2.4894819259643555\n",
            "Training Iteration 1695, Loss: 4.453674793243408\n",
            "Training Iteration 1696, Loss: 5.574567794799805\n",
            "Training Iteration 1697, Loss: 2.051790952682495\n",
            "Training Iteration 1698, Loss: 4.166240692138672\n",
            "Training Iteration 1699, Loss: 5.572138786315918\n",
            "Training Iteration 1700, Loss: 5.176808834075928\n",
            "Training Iteration 1701, Loss: 5.54036283493042\n",
            "Training Iteration 1702, Loss: 5.580491065979004\n",
            "Training Iteration 1703, Loss: 3.504063129425049\n",
            "Training Iteration 1704, Loss: 2.8870797157287598\n",
            "Training Iteration 1705, Loss: 5.502452373504639\n",
            "Training Iteration 1706, Loss: 4.00193977355957\n",
            "Training Iteration 1707, Loss: 5.160871505737305\n",
            "Training Iteration 1708, Loss: 5.192168235778809\n",
            "Training Iteration 1709, Loss: 3.0393500328063965\n",
            "Training Iteration 1710, Loss: 3.336961507797241\n",
            "Training Iteration 1711, Loss: 4.137207508087158\n",
            "Training Iteration 1712, Loss: 3.402435541152954\n",
            "Training Iteration 1713, Loss: 3.267646312713623\n",
            "Training Iteration 1714, Loss: 3.754129648208618\n",
            "Training Iteration 1715, Loss: 4.189638137817383\n",
            "Training Iteration 1716, Loss: 4.345707893371582\n",
            "Training Iteration 1717, Loss: 4.089046001434326\n",
            "Training Iteration 1718, Loss: 3.459664821624756\n",
            "Training Iteration 1719, Loss: 2.862814426422119\n",
            "Training Iteration 1720, Loss: 6.224295139312744\n",
            "Training Iteration 1721, Loss: 4.511974334716797\n",
            "Training Iteration 1722, Loss: 4.483721733093262\n",
            "Training Iteration 1723, Loss: 4.657158851623535\n",
            "Training Iteration 1724, Loss: 5.193495273590088\n",
            "Training Iteration 1725, Loss: 5.104951858520508\n",
            "Training Iteration 1726, Loss: 1.73544442653656\n",
            "Training Iteration 1727, Loss: 3.747436285018921\n",
            "Training Iteration 1728, Loss: 3.1563501358032227\n",
            "Training Iteration 1729, Loss: 5.516299247741699\n",
            "Training Iteration 1730, Loss: 3.8469552993774414\n",
            "Training Iteration 1731, Loss: 4.102644920349121\n",
            "Training Iteration 1732, Loss: 8.132569313049316\n",
            "Training Iteration 1733, Loss: 3.7262229919433594\n",
            "Training Iteration 1734, Loss: 4.218566417694092\n",
            "Training Iteration 1735, Loss: 3.273685932159424\n",
            "Training Iteration 1736, Loss: 3.8031113147735596\n",
            "Training Iteration 1737, Loss: 4.455092906951904\n",
            "Training Iteration 1738, Loss: 4.375011444091797\n",
            "Training Iteration 1739, Loss: 6.140370845794678\n",
            "Training Iteration 1740, Loss: 11.538043975830078\n",
            "Training Iteration 1741, Loss: 2.291588306427002\n",
            "Training Iteration 1742, Loss: 4.362669467926025\n",
            "Training Iteration 1743, Loss: 4.691214561462402\n",
            "Training Iteration 1744, Loss: 6.634970664978027\n",
            "Training Iteration 1745, Loss: 3.0532689094543457\n",
            "Training Iteration 1746, Loss: 4.056931018829346\n",
            "Training Iteration 1747, Loss: 3.9415485858917236\n",
            "Training Iteration 1748, Loss: 3.1138367652893066\n",
            "Training Iteration 1749, Loss: 3.4607999324798584\n",
            "Training Iteration 1750, Loss: 4.905271530151367\n",
            "Training Iteration 1751, Loss: 5.9690775871276855\n",
            "Training Iteration 1752, Loss: 2.5770254135131836\n",
            "Training Iteration 1753, Loss: 6.325894355773926\n",
            "Training Iteration 1754, Loss: 0.6914951801300049\n",
            "Training Iteration 1755, Loss: 11.145363807678223\n",
            "Training Iteration 1756, Loss: 7.348015785217285\n",
            "Training Iteration 1757, Loss: 5.61740779876709\n",
            "Training Iteration 1758, Loss: 4.482405185699463\n",
            "Training Iteration 1759, Loss: 3.6917195320129395\n",
            "Training Iteration 1760, Loss: 7.619830131530762\n",
            "Training Iteration 1761, Loss: 6.21544075012207\n",
            "Training Iteration 1762, Loss: 1.9984385967254639\n",
            "Training Iteration 1763, Loss: 3.82365345954895\n",
            "Training Iteration 1764, Loss: 3.178377628326416\n",
            "Training Iteration 1765, Loss: 2.208141803741455\n",
            "Training Iteration 1766, Loss: 3.804943323135376\n",
            "Training Iteration 1767, Loss: 5.752268314361572\n",
            "Training Iteration 1768, Loss: 5.481170177459717\n",
            "Training Iteration 1769, Loss: 4.6834306716918945\n",
            "Training Iteration 1770, Loss: 4.676864147186279\n",
            "Training Iteration 1771, Loss: 7.407430171966553\n",
            "Training Iteration 1772, Loss: 3.0025088787078857\n",
            "Training Iteration 1773, Loss: 1.115027666091919\n",
            "Training Iteration 1774, Loss: 5.074092864990234\n",
            "Training Iteration 1775, Loss: 4.86342191696167\n",
            "Training Iteration 1776, Loss: 3.3950767517089844\n",
            "Training Iteration 1777, Loss: 5.125730514526367\n",
            "Training Iteration 1778, Loss: 3.4620964527130127\n",
            "Training Iteration 1779, Loss: 4.463989734649658\n",
            "Training Iteration 1780, Loss: 2.9357004165649414\n",
            "Training Iteration 1781, Loss: 3.414024591445923\n",
            "Training Iteration 1782, Loss: 4.827628135681152\n",
            "Training Iteration 1783, Loss: 4.709478855133057\n",
            "Training Iteration 1784, Loss: 3.414080858230591\n",
            "Training Iteration 1785, Loss: 4.260551452636719\n",
            "Training Iteration 1786, Loss: 6.191641807556152\n",
            "Training Iteration 1787, Loss: 4.9537224769592285\n",
            "Training Iteration 1788, Loss: 6.032345771789551\n",
            "Training Iteration 1789, Loss: 4.395689487457275\n",
            "Training Iteration 1790, Loss: 2.350440263748169\n",
            "Training Iteration 1791, Loss: 2.1502597332000732\n",
            "Training Iteration 1792, Loss: 3.3189797401428223\n",
            "Training Iteration 1793, Loss: 5.01032018661499\n",
            "Training Iteration 1794, Loss: 5.432391166687012\n",
            "Training Iteration 1795, Loss: 3.032383441925049\n",
            "Training Iteration 1796, Loss: 3.1542437076568604\n",
            "Training Iteration 1797, Loss: 3.5155539512634277\n",
            "Training Iteration 1798, Loss: 8.549678802490234\n",
            "Training Iteration 1799, Loss: 5.4151153564453125\n",
            "Training Iteration 1800, Loss: 2.200450897216797\n",
            "Training Iteration 1801, Loss: 4.357414245605469\n",
            "Training Iteration 1802, Loss: 8.60091495513916\n",
            "Training Iteration 1803, Loss: 2.947829008102417\n",
            "Training Iteration 1804, Loss: 4.170988082885742\n",
            "Training Iteration 1805, Loss: 2.922487258911133\n",
            "Training Iteration 1806, Loss: 6.740177631378174\n",
            "Training Iteration 1807, Loss: 4.517733573913574\n",
            "Training Iteration 1808, Loss: 5.390758514404297\n",
            "Training Iteration 1809, Loss: 9.723990440368652\n",
            "Training Iteration 1810, Loss: 7.451003551483154\n",
            "Training Iteration 1811, Loss: 3.5066633224487305\n",
            "Training Iteration 1812, Loss: 6.279524326324463\n",
            "Training Iteration 1813, Loss: 7.157342433929443\n",
            "Training Iteration 1814, Loss: 10.173117637634277\n",
            "Training Iteration 1815, Loss: 7.706109046936035\n",
            "Training Iteration 1816, Loss: 6.993794918060303\n",
            "Training Iteration 1817, Loss: 6.168471813201904\n",
            "Training Iteration 1818, Loss: 5.902741432189941\n",
            "Training Iteration 1819, Loss: 5.642230987548828\n",
            "Training Iteration 1820, Loss: 6.419953346252441\n",
            "Training Iteration 1821, Loss: 4.24829626083374\n",
            "Training Iteration 1822, Loss: 5.41666316986084\n",
            "Training Iteration 1823, Loss: 6.7085981369018555\n",
            "Training Iteration 1824, Loss: 5.718266487121582\n",
            "Training Iteration 1825, Loss: 4.883426189422607\n",
            "Training Iteration 1826, Loss: 11.512638092041016\n",
            "Training Iteration 1827, Loss: 8.566168785095215\n",
            "Training Iteration 1828, Loss: 7.345332622528076\n",
            "Training Iteration 1829, Loss: 5.592380046844482\n",
            "Training Iteration 1830, Loss: 4.588986873626709\n",
            "Training Iteration 1831, Loss: 4.172718524932861\n",
            "Training Iteration 1832, Loss: 7.472346305847168\n",
            "Training Iteration 1833, Loss: 6.721564769744873\n",
            "Training Iteration 1834, Loss: 3.498960494995117\n",
            "Training Iteration 1835, Loss: 4.30557918548584\n",
            "Training Iteration 1836, Loss: 6.3260016441345215\n",
            "Training Iteration 1837, Loss: 3.3345751762390137\n",
            "Training Iteration 1838, Loss: 6.213456153869629\n",
            "Training Iteration 1839, Loss: 8.077275276184082\n",
            "Training Iteration 1840, Loss: 10.825312614440918\n",
            "Training Iteration 1841, Loss: 6.866575717926025\n",
            "Training Iteration 1842, Loss: 5.323308944702148\n",
            "Training Iteration 1843, Loss: 1.7724437713623047\n",
            "Training Iteration 1844, Loss: 5.055817127227783\n",
            "Training Iteration 1845, Loss: 5.095067024230957\n",
            "Training Iteration 1846, Loss: 5.792049407958984\n",
            "Training Iteration 1847, Loss: 5.297009468078613\n",
            "Training Iteration 1848, Loss: 4.042226314544678\n",
            "Training Iteration 1849, Loss: 2.559457778930664\n",
            "Training Iteration 1850, Loss: 2.949946403503418\n",
            "Training Iteration 1851, Loss: 4.678066253662109\n",
            "Training Iteration 1852, Loss: 5.594712734222412\n",
            "Training Iteration 1853, Loss: 3.1423838138580322\n",
            "Training Iteration 1854, Loss: 6.056000709533691\n",
            "Training Iteration 1855, Loss: 4.052711486816406\n",
            "Training Iteration 1856, Loss: 2.937791109085083\n",
            "Training Iteration 1857, Loss: 3.683021068572998\n",
            "Training Iteration 1858, Loss: 3.7937283515930176\n",
            "Training Iteration 1859, Loss: 3.9111006259918213\n",
            "Training Iteration 1860, Loss: 4.830183029174805\n",
            "Training Iteration 1861, Loss: 6.210404396057129\n",
            "Training Iteration 1862, Loss: 3.74933123588562\n",
            "Training Iteration 1863, Loss: 5.190005779266357\n",
            "Training Iteration 1864, Loss: 6.881908893585205\n",
            "Training Iteration 1865, Loss: 4.1404805183410645\n",
            "Training Iteration 1866, Loss: 7.66033411026001\n",
            "Training Iteration 1867, Loss: 4.604772567749023\n",
            "Training Iteration 1868, Loss: 3.974648952484131\n",
            "Training Iteration 1869, Loss: 3.793745279312134\n",
            "Training Iteration 1870, Loss: 7.924304962158203\n",
            "Training Iteration 1871, Loss: 4.6299824714660645\n",
            "Training Iteration 1872, Loss: 2.79531192779541\n",
            "Training Iteration 1873, Loss: 3.695509910583496\n",
            "Training Iteration 1874, Loss: 2.07698392868042\n",
            "Training Iteration 1875, Loss: 6.696260452270508\n",
            "Training Iteration 1876, Loss: 4.022132396697998\n",
            "Training Iteration 1877, Loss: 6.181146621704102\n",
            "Training Iteration 1878, Loss: 2.892197608947754\n",
            "Training Iteration 1879, Loss: 3.6139392852783203\n",
            "Training Iteration 1880, Loss: 5.32515811920166\n",
            "Training Iteration 1881, Loss: 5.375615119934082\n",
            "Training Iteration 1882, Loss: 3.347158908843994\n",
            "Training Iteration 1883, Loss: 2.746060848236084\n",
            "Training Iteration 1884, Loss: 5.877380847930908\n",
            "Training Iteration 1885, Loss: 4.800929546356201\n",
            "Training Iteration 1886, Loss: 3.006584882736206\n",
            "Training Iteration 1887, Loss: 5.929339408874512\n",
            "Training Iteration 1888, Loss: 7.03926944732666\n",
            "Training Iteration 1889, Loss: 6.019790172576904\n",
            "Training Iteration 1890, Loss: 5.976541519165039\n",
            "Training Iteration 1891, Loss: 6.31773042678833\n",
            "Training Iteration 1892, Loss: 2.6613125801086426\n",
            "Training Iteration 1893, Loss: 3.4675116539001465\n",
            "Training Iteration 1894, Loss: 3.633566379547119\n",
            "Training Iteration 1895, Loss: 3.7200584411621094\n",
            "Training Iteration 1896, Loss: 3.1808700561523438\n",
            "Training Iteration 1897, Loss: 4.158355712890625\n",
            "Training Iteration 1898, Loss: 3.820300340652466\n",
            "Training Iteration 1899, Loss: 3.7764856815338135\n",
            "Training Iteration 1900, Loss: 4.735594749450684\n",
            "Training Iteration 1901, Loss: 6.128476142883301\n",
            "Training Iteration 1902, Loss: 3.679990768432617\n",
            "Training Iteration 1903, Loss: 4.200220108032227\n",
            "Training Iteration 1904, Loss: 3.852452278137207\n",
            "Training Iteration 1905, Loss: 3.7114293575286865\n",
            "Training Iteration 1906, Loss: 4.820985794067383\n",
            "Training Iteration 1907, Loss: 3.0844361782073975\n",
            "Training Iteration 1908, Loss: 2.081281900405884\n",
            "Training Iteration 1909, Loss: 5.866967678070068\n",
            "Training Iteration 1910, Loss: 5.496265888214111\n",
            "Training Iteration 1911, Loss: 3.9566969871520996\n",
            "Training Iteration 1912, Loss: 5.256620407104492\n",
            "Training Iteration 1913, Loss: 5.334832668304443\n",
            "Training Iteration 1914, Loss: 1.7382508516311646\n",
            "Training Iteration 1915, Loss: 5.432437896728516\n",
            "Training Iteration 1916, Loss: 3.5552923679351807\n",
            "Training Iteration 1917, Loss: 1.6953353881835938\n",
            "Training Iteration 1918, Loss: 1.5626678466796875\n",
            "Training Iteration 1919, Loss: 4.750317573547363\n",
            "Training Iteration 1920, Loss: 4.148417949676514\n",
            "Training Iteration 1921, Loss: 5.741296291351318\n",
            "Training Iteration 1922, Loss: 5.367823600769043\n",
            "Training Iteration 1923, Loss: 4.157960891723633\n",
            "Training Iteration 1924, Loss: 3.1437487602233887\n",
            "Training Iteration 1925, Loss: 5.24357271194458\n",
            "Training Iteration 1926, Loss: 5.2203779220581055\n",
            "Training Iteration 1927, Loss: 3.8905701637268066\n",
            "Training Iteration 1928, Loss: 3.3288705348968506\n",
            "Training Iteration 1929, Loss: 4.888729095458984\n",
            "Training Iteration 1930, Loss: 7.290318489074707\n",
            "Training Iteration 1931, Loss: 5.793169975280762\n",
            "Training Iteration 1932, Loss: 2.64013671875\n",
            "Training Iteration 1933, Loss: 8.703009605407715\n",
            "Training Iteration 1934, Loss: 1.8847259283065796\n",
            "Training Iteration 1935, Loss: 6.548986434936523\n",
            "Training Iteration 1936, Loss: 5.267442226409912\n",
            "Training Iteration 1937, Loss: 5.543420791625977\n",
            "Training Iteration 1938, Loss: 6.762944221496582\n",
            "Training Iteration 1939, Loss: 2.346700429916382\n",
            "Training Iteration 1940, Loss: 3.865403890609741\n",
            "Training Iteration 1941, Loss: 6.4088287353515625\n",
            "Training Iteration 1942, Loss: 3.928051233291626\n",
            "Training Iteration 1943, Loss: 3.195021152496338\n",
            "Training Iteration 1944, Loss: 4.92542028427124\n",
            "Training Iteration 1945, Loss: 6.478848457336426\n",
            "Training Iteration 1946, Loss: 4.712562084197998\n",
            "Training Iteration 1947, Loss: 4.5412187576293945\n",
            "Training Iteration 1948, Loss: 6.304844856262207\n",
            "Training Iteration 1949, Loss: 5.463850498199463\n",
            "Training Iteration 1950, Loss: 4.237176895141602\n",
            "Training Iteration 1951, Loss: 9.208932876586914\n",
            "Training Iteration 1952, Loss: 4.619286060333252\n",
            "Training Iteration 1953, Loss: 4.908885478973389\n",
            "Training Iteration 1954, Loss: 5.36484956741333\n",
            "Training Iteration 1955, Loss: 1.9749033451080322\n",
            "Training Iteration 1956, Loss: 5.403622150421143\n",
            "Training Iteration 1957, Loss: 8.210599899291992\n",
            "Training Iteration 1958, Loss: 7.530275344848633\n",
            "Training Iteration 1959, Loss: 1.3071273565292358\n",
            "Training Iteration 1960, Loss: 4.161838054656982\n",
            "Training Iteration 1961, Loss: 10.927980422973633\n",
            "Training Iteration 1962, Loss: 5.742437362670898\n",
            "Training Iteration 1963, Loss: 2.871044635772705\n",
            "Training Iteration 1964, Loss: 6.732683181762695\n",
            "Training Iteration 1965, Loss: 3.935940742492676\n",
            "Training Iteration 1966, Loss: 5.660285949707031\n",
            "Training Iteration 1967, Loss: 4.861032485961914\n",
            "Training Iteration 1968, Loss: 5.245889663696289\n",
            "Training Iteration 1969, Loss: 4.780351638793945\n",
            "Training Iteration 1970, Loss: 3.0462024211883545\n",
            "Training Iteration 1971, Loss: 5.875965118408203\n",
            "Training Iteration 1972, Loss: 4.1454362869262695\n",
            "Training Iteration 1973, Loss: 2.3344573974609375\n",
            "Training Iteration 1974, Loss: 2.123100996017456\n",
            "Training Iteration 1975, Loss: 5.402897834777832\n",
            "Training Iteration 1976, Loss: 2.199563503265381\n",
            "Training Iteration 1977, Loss: 5.535577774047852\n",
            "Training Iteration 1978, Loss: 3.6486454010009766\n",
            "Training Iteration 1979, Loss: 6.160018444061279\n",
            "Training Iteration 1980, Loss: 4.751219272613525\n",
            "Training Iteration 1981, Loss: 4.129302501678467\n",
            "Training Iteration 1982, Loss: 4.493314743041992\n",
            "Training Iteration 1983, Loss: 3.2019143104553223\n",
            "Training Iteration 1984, Loss: 2.6169986724853516\n",
            "Training Iteration 1985, Loss: 5.742084503173828\n",
            "Training Iteration 1986, Loss: 4.865972518920898\n",
            "Training Iteration 1987, Loss: 3.2857506275177\n",
            "Training Iteration 1988, Loss: 4.259003639221191\n",
            "Training Iteration 1989, Loss: 3.5311038494110107\n",
            "Training Iteration 1990, Loss: 2.963592052459717\n",
            "Training Iteration 1991, Loss: 3.6438043117523193\n",
            "Training Iteration 1992, Loss: 5.996621131896973\n",
            "Training Iteration 1993, Loss: 3.428192138671875\n",
            "Training Iteration 1994, Loss: 5.948482513427734\n",
            "Training Iteration 1995, Loss: 1.3867131471633911\n",
            "Training Iteration 1996, Loss: 2.2219655513763428\n",
            "Training Iteration 1997, Loss: 3.965837001800537\n",
            "Training Iteration 1998, Loss: 2.724538564682007\n",
            "Training Iteration 1999, Loss: 3.886301040649414\n",
            "Training Iteration 2000, Loss: 5.836472988128662\n",
            "Training Iteration 2001, Loss: 4.595954895019531\n",
            "Training Iteration 2002, Loss: 4.602437496185303\n",
            "Training Iteration 2003, Loss: 3.4929769039154053\n",
            "Training Iteration 2004, Loss: 3.094374656677246\n",
            "Training Iteration 2005, Loss: 5.615140438079834\n",
            "Training Iteration 2006, Loss: 5.726605415344238\n",
            "Training Iteration 2007, Loss: 3.10312819480896\n",
            "Training Iteration 2008, Loss: 4.26503849029541\n",
            "Training Iteration 2009, Loss: 4.5632405281066895\n",
            "Training Iteration 2010, Loss: 3.3209354877471924\n",
            "Training Iteration 2011, Loss: 6.325899600982666\n",
            "Training Iteration 2012, Loss: 3.8041765689849854\n",
            "Training Iteration 2013, Loss: 3.9385504722595215\n",
            "Training Iteration 2014, Loss: 6.430490493774414\n",
            "Training Iteration 2015, Loss: 5.256799697875977\n",
            "Training Iteration 2016, Loss: 3.7715463638305664\n",
            "Training Iteration 2017, Loss: 6.5866007804870605\n",
            "Training Iteration 2018, Loss: 4.475558757781982\n",
            "Training Iteration 2019, Loss: 5.622827529907227\n",
            "Training Iteration 2020, Loss: 1.627232313156128\n",
            "Training Iteration 2021, Loss: 5.168034553527832\n",
            "Training Iteration 2022, Loss: 6.969684600830078\n",
            "Training Iteration 2023, Loss: 4.296149730682373\n",
            "Training Iteration 2024, Loss: 6.824128150939941\n",
            "Training Iteration 2025, Loss: 2.7479896545410156\n",
            "Training Iteration 2026, Loss: 3.0071961879730225\n",
            "Training Iteration 2027, Loss: 4.586765289306641\n",
            "Training Iteration 2028, Loss: 7.071443557739258\n",
            "Training Iteration 2029, Loss: 3.605095624923706\n",
            "Training Iteration 2030, Loss: 3.174210786819458\n",
            "Training Iteration 2031, Loss: 5.217855453491211\n",
            "Training Iteration 2032, Loss: 11.076271057128906\n",
            "Training Iteration 2033, Loss: 2.874004364013672\n",
            "Training Iteration 2034, Loss: 3.523322820663452\n",
            "Training Iteration 2035, Loss: 2.8356003761291504\n",
            "Training Iteration 2036, Loss: 4.624920845031738\n",
            "Training Iteration 2037, Loss: 9.684432983398438\n",
            "Training Iteration 2038, Loss: 5.595958232879639\n",
            "Training Iteration 2039, Loss: 6.9772539138793945\n",
            "Training Iteration 2040, Loss: 7.141478538513184\n",
            "Training Iteration 2041, Loss: 3.7547242641448975\n",
            "Training Iteration 2042, Loss: 2.61360764503479\n",
            "Training Iteration 2043, Loss: 4.045557022094727\n",
            "Training Iteration 2044, Loss: 4.255677223205566\n",
            "Training Iteration 2045, Loss: 5.02286434173584\n",
            "Training Iteration 2046, Loss: 2.620976209640503\n",
            "Training Iteration 2047, Loss: 2.9741101264953613\n",
            "Training Iteration 2048, Loss: 3.169659376144409\n",
            "Training Iteration 2049, Loss: 4.491386413574219\n",
            "Training Iteration 2050, Loss: 4.586870193481445\n",
            "Training Iteration 2051, Loss: 2.681623697280884\n",
            "Training Iteration 2052, Loss: 2.8651373386383057\n",
            "Training Iteration 2053, Loss: 3.2172694206237793\n",
            "Training Iteration 2054, Loss: 2.3483738899230957\n",
            "Training Iteration 2055, Loss: 7.182351112365723\n",
            "Training Iteration 2056, Loss: 3.5694351196289062\n",
            "Training Iteration 2057, Loss: 5.5211896896362305\n",
            "Training Iteration 2058, Loss: 3.5749151706695557\n",
            "Training Iteration 2059, Loss: 6.179074287414551\n",
            "Training Iteration 2060, Loss: 6.469281196594238\n",
            "Training Iteration 2061, Loss: 3.3461849689483643\n",
            "Training Iteration 2062, Loss: 6.088513374328613\n",
            "Training Iteration 2063, Loss: 5.1779046058654785\n",
            "Training Iteration 2064, Loss: 2.345258951187134\n",
            "Training Iteration 2065, Loss: 4.945279121398926\n",
            "Training Iteration 2066, Loss: 5.7232136726379395\n",
            "Training Iteration 2067, Loss: 6.384158611297607\n",
            "Training Iteration 2068, Loss: 5.009030342102051\n",
            "Training Iteration 2069, Loss: 7.41419792175293\n",
            "Training Iteration 2070, Loss: 3.986536979675293\n",
            "Training Iteration 2071, Loss: 2.722935676574707\n",
            "Training Iteration 2072, Loss: 4.456637382507324\n",
            "Training Iteration 2073, Loss: 7.321399688720703\n",
            "Training Iteration 2074, Loss: 3.5675477981567383\n",
            "Training Iteration 2075, Loss: 6.3114447593688965\n",
            "Training Iteration 2076, Loss: 4.29148530960083\n",
            "Training Iteration 2077, Loss: 2.611211061477661\n",
            "Training Iteration 2078, Loss: 8.56084156036377\n",
            "Training Iteration 2079, Loss: 3.035602569580078\n",
            "Training Iteration 2080, Loss: 5.035375118255615\n",
            "Training Iteration 2081, Loss: 4.679270267486572\n",
            "Training Iteration 2082, Loss: 7.2429680824279785\n",
            "Training Iteration 2083, Loss: 6.147823333740234\n",
            "Training Iteration 2084, Loss: 5.18930196762085\n",
            "Training Iteration 2085, Loss: 4.255460262298584\n",
            "Training Iteration 2086, Loss: 4.847361087799072\n",
            "Training Iteration 2087, Loss: 6.000340461730957\n",
            "Training Iteration 2088, Loss: 4.595002174377441\n",
            "Training Iteration 2089, Loss: 5.155448913574219\n",
            "Training Iteration 2090, Loss: 4.026006698608398\n",
            "Training Iteration 2091, Loss: 4.2197160720825195\n",
            "Training Iteration 2092, Loss: 3.4408137798309326\n",
            "Training Iteration 2093, Loss: 5.1913275718688965\n",
            "Training Iteration 2094, Loss: 5.820916175842285\n",
            "Training Iteration 2095, Loss: 3.9018516540527344\n",
            "Training Iteration 2096, Loss: 3.954634189605713\n",
            "Training Iteration 2097, Loss: 3.7091176509857178\n",
            "Training Iteration 2098, Loss: 3.4865498542785645\n",
            "Training Iteration 2099, Loss: 3.819767713546753\n",
            "Training Iteration 2100, Loss: 3.2002081871032715\n",
            "Training Iteration 2101, Loss: 3.1827657222747803\n",
            "Training Iteration 2102, Loss: 2.2463717460632324\n",
            "Training Iteration 2103, Loss: 8.116178512573242\n",
            "Training Iteration 2104, Loss: 1.263951301574707\n",
            "Training Iteration 2105, Loss: 3.7267961502075195\n",
            "Training Iteration 2106, Loss: 3.7202625274658203\n",
            "Training Iteration 2107, Loss: 2.7960448265075684\n",
            "Training Iteration 2108, Loss: 4.664888858795166\n",
            "Training Iteration 2109, Loss: 4.7461113929748535\n",
            "Training Iteration 2110, Loss: 4.344600200653076\n",
            "Training Iteration 2111, Loss: 4.687683582305908\n",
            "Training Iteration 2112, Loss: 3.2826757431030273\n",
            "Training Iteration 2113, Loss: 5.185842514038086\n",
            "Training Iteration 2114, Loss: 3.1576156616210938\n",
            "Training Iteration 2115, Loss: 4.802654266357422\n",
            "Training Iteration 2116, Loss: 6.037531852722168\n",
            "Training Iteration 2117, Loss: 3.991182565689087\n",
            "Training Iteration 2118, Loss: 5.037207126617432\n",
            "Training Iteration 2119, Loss: 6.119154453277588\n",
            "Training Iteration 2120, Loss: 4.296817302703857\n",
            "Training Iteration 2121, Loss: 3.7839038372039795\n",
            "Training Iteration 2122, Loss: 5.052489757537842\n",
            "Training Iteration 2123, Loss: 4.3757452964782715\n",
            "Training Iteration 2124, Loss: 7.046952724456787\n",
            "Training Iteration 2125, Loss: 5.008676528930664\n",
            "Training Iteration 2126, Loss: 4.103113174438477\n",
            "Training Iteration 2127, Loss: 5.93668270111084\n",
            "Training Iteration 2128, Loss: 5.587875843048096\n",
            "Training Iteration 2129, Loss: 2.9728269577026367\n",
            "Training Iteration 2130, Loss: 4.797144412994385\n",
            "Training Iteration 2131, Loss: 4.293054103851318\n",
            "Training Iteration 2132, Loss: 6.897276878356934\n",
            "Training Iteration 2133, Loss: 2.6517341136932373\n",
            "Training Iteration 2134, Loss: 4.227426528930664\n",
            "Training Iteration 2135, Loss: 6.898754596710205\n",
            "Training Iteration 2136, Loss: 4.280330657958984\n",
            "Training Iteration 2137, Loss: 6.8798112869262695\n",
            "Training Iteration 2138, Loss: 5.900965690612793\n",
            "Training Iteration 2139, Loss: 3.123720169067383\n",
            "Training Iteration 2140, Loss: 5.264540195465088\n",
            "Training Iteration 2141, Loss: 5.002516746520996\n",
            "Training Iteration 2142, Loss: 2.9085230827331543\n",
            "Training Iteration 2143, Loss: 3.7591805458068848\n",
            "Training Iteration 2144, Loss: 6.088551998138428\n",
            "Training Iteration 2145, Loss: 3.8187055587768555\n",
            "Training Iteration 2146, Loss: 3.926544427871704\n",
            "Training Iteration 2147, Loss: 3.842040538787842\n",
            "Training Iteration 2148, Loss: 6.213948726654053\n",
            "Training Iteration 2149, Loss: 3.5016281604766846\n",
            "Training Iteration 2150, Loss: 5.1510114669799805\n",
            "Training Iteration 2151, Loss: 3.8170268535614014\n",
            "Training Iteration 2152, Loss: 3.972942590713501\n",
            "Training Iteration 2153, Loss: 4.2034010887146\n",
            "Training Iteration 2154, Loss: 3.417703151702881\n",
            "Training Iteration 2155, Loss: 3.7206614017486572\n",
            "Training Iteration 2156, Loss: 7.880922317504883\n",
            "Training Iteration 2157, Loss: 4.91525936126709\n",
            "Training Iteration 2158, Loss: 6.807962894439697\n",
            "Training Iteration 2159, Loss: 6.38442850112915\n",
            "Training Iteration 2160, Loss: 4.479393482208252\n",
            "Training Iteration 2161, Loss: 3.808892250061035\n",
            "Training Iteration 2162, Loss: 4.444090843200684\n",
            "Training Iteration 2163, Loss: 8.300952911376953\n",
            "Training Iteration 2164, Loss: 4.180562973022461\n",
            "Training Iteration 2165, Loss: 3.9498326778411865\n",
            "Training Iteration 2166, Loss: 2.507087230682373\n",
            "Training Iteration 2167, Loss: 4.179575443267822\n",
            "Training Iteration 2168, Loss: 4.231736183166504\n",
            "Training Iteration 2169, Loss: 5.126831531524658\n",
            "Training Iteration 2170, Loss: 4.190289497375488\n",
            "Training Iteration 2171, Loss: 3.1845099925994873\n",
            "Training Iteration 2172, Loss: 4.5304646492004395\n",
            "Training Iteration 2173, Loss: 2.819436550140381\n",
            "Training Iteration 2174, Loss: 6.323091506958008\n",
            "Training Iteration 2175, Loss: 4.982163429260254\n",
            "Training Iteration 2176, Loss: 3.5954790115356445\n",
            "Training Iteration 2177, Loss: 5.115973472595215\n",
            "Training Iteration 2178, Loss: 5.053833484649658\n",
            "Training Iteration 2179, Loss: 8.329602241516113\n",
            "Training Iteration 2180, Loss: 5.211215019226074\n",
            "Training Iteration 2181, Loss: 4.605447769165039\n",
            "Training Iteration 2182, Loss: 3.65602707862854\n",
            "Training Iteration 2183, Loss: 4.5908637046813965\n",
            "Training Iteration 2184, Loss: 7.658333778381348\n",
            "Training Iteration 2185, Loss: 9.033947944641113\n",
            "Training Iteration 2186, Loss: 4.7590861320495605\n",
            "Training Iteration 2187, Loss: 3.6373794078826904\n",
            "Training Iteration 2188, Loss: 2.1259045600891113\n",
            "Training Iteration 2189, Loss: 4.279615879058838\n",
            "Training Iteration 2190, Loss: 3.818406343460083\n",
            "Training Iteration 2191, Loss: 4.048920631408691\n",
            "Training Iteration 2192, Loss: 3.3635425567626953\n",
            "Training Iteration 2193, Loss: 3.1068215370178223\n",
            "Training Iteration 2194, Loss: 2.4073169231414795\n",
            "Training Iteration 2195, Loss: 2.552093982696533\n",
            "Training Iteration 2196, Loss: 7.237759113311768\n",
            "Training Iteration 2197, Loss: 3.185793161392212\n",
            "Training Iteration 2198, Loss: 2.99953556060791\n",
            "Training Iteration 2199, Loss: 6.222633361816406\n",
            "Training Iteration 2200, Loss: 3.702970504760742\n",
            "Training Iteration 2201, Loss: 5.552805423736572\n",
            "Training Iteration 2202, Loss: 7.707522392272949\n",
            "Training Iteration 2203, Loss: 5.527003288269043\n",
            "Training Iteration 2204, Loss: 5.713902473449707\n",
            "Training Iteration 2205, Loss: 2.178401470184326\n",
            "Training Iteration 2206, Loss: 0.6559757590293884\n",
            "Training Iteration 2207, Loss: 7.11680793762207\n",
            "Training Iteration 2208, Loss: 5.6872029304504395\n",
            "Training Iteration 2209, Loss: 5.66521692276001\n",
            "Training Iteration 2210, Loss: 1.8615703582763672\n",
            "Training Iteration 2211, Loss: 5.851009368896484\n",
            "Training Iteration 2212, Loss: 3.7283411026000977\n",
            "Training Iteration 2213, Loss: 4.373469352722168\n",
            "Training Iteration 2214, Loss: 2.122730016708374\n",
            "Training Iteration 2215, Loss: 5.032631874084473\n",
            "Training Iteration 2216, Loss: 6.6731367111206055\n",
            "Training Iteration 2217, Loss: 3.043856143951416\n",
            "Training Iteration 2218, Loss: 2.2323834896087646\n",
            "Training Iteration 2219, Loss: 3.4446191787719727\n",
            "Training Iteration 2220, Loss: 1.8569539785385132\n",
            "Training Iteration 2221, Loss: 5.2351298332214355\n",
            "Training Iteration 2222, Loss: 4.112322807312012\n",
            "Training Iteration 2223, Loss: 6.023582458496094\n",
            "Training Iteration 2224, Loss: 2.7466907501220703\n",
            "Training Iteration 2225, Loss: 4.096982002258301\n",
            "Training Iteration 2226, Loss: 4.222731113433838\n",
            "Training Iteration 2227, Loss: 3.031149387359619\n",
            "Training Iteration 2228, Loss: 2.314387321472168\n",
            "Training Iteration 2229, Loss: 3.0862746238708496\n",
            "Training Iteration 2230, Loss: 3.942392349243164\n",
            "Training Iteration 2231, Loss: 7.613473892211914\n",
            "Training Iteration 2232, Loss: 4.332302570343018\n",
            "Training Iteration 2233, Loss: 2.7281529903411865\n",
            "Training Iteration 2234, Loss: 6.085313320159912\n",
            "Training Iteration 2235, Loss: 3.6310813426971436\n",
            "Training Iteration 2236, Loss: 5.221399307250977\n",
            "Training Iteration 2237, Loss: 4.3596577644348145\n",
            "Training Iteration 2238, Loss: 2.8960862159729004\n",
            "Training Iteration 2239, Loss: 2.261615037918091\n",
            "Training Iteration 2240, Loss: 3.203132152557373\n",
            "Training Iteration 2241, Loss: 2.3122153282165527\n",
            "Training Iteration 2242, Loss: 3.8526039123535156\n",
            "Training Iteration 2243, Loss: 6.113038063049316\n",
            "Training Iteration 2244, Loss: 4.172802925109863\n",
            "Training Iteration 2245, Loss: 4.184088230133057\n",
            "Training Iteration 2246, Loss: 5.355685234069824\n",
            "Training Iteration 2247, Loss: 6.575474262237549\n",
            "Training Iteration 2248, Loss: 4.111121654510498\n",
            "Training Iteration 2249, Loss: 2.812591552734375\n",
            "Training Iteration 2250, Loss: 4.919229507446289\n",
            "Training Iteration 2251, Loss: 6.470202922821045\n",
            "Training Iteration 2252, Loss: 2.4710865020751953\n",
            "Training Iteration 2253, Loss: 2.0870819091796875\n",
            "Training Iteration 2254, Loss: 3.094952344894409\n",
            "Training Iteration 2255, Loss: 2.0809147357940674\n",
            "Training Iteration 2256, Loss: 4.458510398864746\n",
            "Training Iteration 2257, Loss: 3.6149063110351562\n",
            "Training Iteration 2258, Loss: 3.1302309036254883\n",
            "Training Iteration 2259, Loss: 3.715183973312378\n",
            "Training Iteration 2260, Loss: 4.603382110595703\n",
            "Training Iteration 2261, Loss: 6.171121597290039\n",
            "Training Iteration 2262, Loss: 3.8442223072052\n",
            "Training Iteration 2263, Loss: 4.5194411277771\n",
            "Training Iteration 2264, Loss: 5.070204257965088\n",
            "Training Iteration 2265, Loss: 2.3943614959716797\n",
            "Training Iteration 2266, Loss: 3.889741897583008\n",
            "Training Iteration 2267, Loss: 5.426233768463135\n",
            "Training Iteration 2268, Loss: 4.117368698120117\n",
            "Training Iteration 2269, Loss: 3.7733120918273926\n",
            "Training Iteration 2270, Loss: 2.983572006225586\n",
            "Training Iteration 2271, Loss: 3.6688950061798096\n",
            "Training Iteration 2272, Loss: 1.9874539375305176\n",
            "Training Iteration 2273, Loss: 2.3887040615081787\n",
            "Training Iteration 2274, Loss: 3.872098922729492\n",
            "Training Iteration 2275, Loss: 4.307315349578857\n",
            "Training Iteration 2276, Loss: 6.9196648597717285\n",
            "Training Iteration 2277, Loss: 6.23150634765625\n",
            "Training Iteration 2278, Loss: 3.9890999794006348\n",
            "Training Iteration 2279, Loss: 3.3186473846435547\n",
            "Training Iteration 2280, Loss: 5.286909103393555\n",
            "Training Iteration 2281, Loss: 9.033251762390137\n",
            "Training Iteration 2282, Loss: 4.557960033416748\n",
            "Training Iteration 2283, Loss: 6.176551342010498\n",
            "Training Iteration 2284, Loss: 4.351704120635986\n",
            "Training Iteration 2285, Loss: 7.23838472366333\n",
            "Training Iteration 2286, Loss: 5.756533145904541\n",
            "Training Iteration 2287, Loss: 3.685312271118164\n",
            "Training Iteration 2288, Loss: 4.384224891662598\n",
            "Training Iteration 2289, Loss: 4.398802757263184\n",
            "Training Iteration 2290, Loss: 3.346275568008423\n",
            "Training Iteration 2291, Loss: 2.6420626640319824\n",
            "Training Iteration 2292, Loss: 5.7761945724487305\n",
            "Training Iteration 2293, Loss: 6.142824172973633\n",
            "Training Iteration 2294, Loss: 3.7303521633148193\n",
            "Training Iteration 2295, Loss: 4.407674312591553\n",
            "Training Iteration 2296, Loss: 2.4760258197784424\n",
            "Training Iteration 2297, Loss: 2.6717121601104736\n",
            "Training Iteration 2298, Loss: 3.553968906402588\n",
            "Training Iteration 2299, Loss: 4.074090003967285\n",
            "Training Iteration 2300, Loss: 2.3140854835510254\n",
            "Training Iteration 2301, Loss: 4.034631252288818\n",
            "Training Iteration 2302, Loss: 5.749788284301758\n",
            "Training Iteration 2303, Loss: 3.943060874938965\n",
            "Training Iteration 2304, Loss: 4.041208267211914\n",
            "Training Iteration 2305, Loss: 3.1305627822875977\n",
            "Training Iteration 2306, Loss: 3.807194948196411\n",
            "Training Iteration 2307, Loss: 4.865365982055664\n",
            "Training Iteration 2308, Loss: 2.326608657836914\n",
            "Training Iteration 2309, Loss: 5.006377220153809\n",
            "Training Iteration 2310, Loss: 5.159658908843994\n",
            "Training Iteration 2311, Loss: 5.652158737182617\n",
            "Training Iteration 2312, Loss: 2.613699197769165\n",
            "Training Iteration 2313, Loss: 3.8663268089294434\n",
            "Training Iteration 2314, Loss: 1.9343721866607666\n",
            "Training Iteration 2315, Loss: 4.5962653160095215\n",
            "Training Iteration 2316, Loss: 4.872659683227539\n",
            "Training Iteration 2317, Loss: 4.303994178771973\n",
            "Training Iteration 2318, Loss: 4.126170635223389\n",
            "Training Iteration 2319, Loss: 3.674363136291504\n",
            "Training Iteration 2320, Loss: 3.8086109161376953\n",
            "Training Iteration 2321, Loss: 3.5450525283813477\n",
            "Training Iteration 2322, Loss: 3.1580262184143066\n",
            "Training Iteration 2323, Loss: 5.27180290222168\n",
            "Training Iteration 2324, Loss: 1.366118311882019\n",
            "Training Iteration 2325, Loss: 5.420957565307617\n",
            "Training Iteration 2326, Loss: 3.9693145751953125\n",
            "Training Iteration 2327, Loss: 3.5829834938049316\n",
            "Training Iteration 2328, Loss: 3.6526670455932617\n",
            "Training Iteration 2329, Loss: 5.884092330932617\n",
            "Training Iteration 2330, Loss: 5.121195316314697\n",
            "Training Iteration 2331, Loss: 7.165215015411377\n",
            "Training Iteration 2332, Loss: 3.314732074737549\n",
            "Training Iteration 2333, Loss: 5.880741119384766\n",
            "Training Iteration 2334, Loss: 6.144411087036133\n",
            "Training Iteration 2335, Loss: 6.728015899658203\n",
            "Training Iteration 2336, Loss: 5.266067981719971\n",
            "Training Iteration 2337, Loss: 6.4638800621032715\n",
            "Training Iteration 2338, Loss: 2.952235460281372\n",
            "Training Iteration 2339, Loss: 4.052069664001465\n",
            "Training Iteration 2340, Loss: 3.1284339427948\n",
            "Training Iteration 2341, Loss: 3.366276741027832\n",
            "Training Iteration 2342, Loss: 8.980196952819824\n",
            "Training Iteration 2343, Loss: 4.7117815017700195\n",
            "Training Iteration 2344, Loss: 2.0100932121276855\n",
            "Training Iteration 2345, Loss: 1.1662819385528564\n",
            "Training Iteration 2346, Loss: 5.740789413452148\n",
            "Training Iteration 2347, Loss: 2.894415855407715\n",
            "Training Iteration 2348, Loss: 4.6357197761535645\n",
            "Training Iteration 2349, Loss: 7.280614376068115\n",
            "Training Iteration 2350, Loss: 2.7617058753967285\n",
            "Training Iteration 2351, Loss: 2.2485008239746094\n",
            "Training Iteration 2352, Loss: 6.620682716369629\n",
            "Training Iteration 2353, Loss: 6.532792568206787\n",
            "Training Iteration 2354, Loss: 6.112216472625732\n",
            "Training Iteration 2355, Loss: 5.631401062011719\n",
            "Training Iteration 2356, Loss: 4.282891273498535\n",
            "Training Iteration 2357, Loss: 3.666668176651001\n",
            "Training Iteration 2358, Loss: 4.011506080627441\n",
            "Training Iteration 2359, Loss: 5.508482456207275\n",
            "Training Iteration 2360, Loss: 6.647328853607178\n",
            "Training Iteration 2361, Loss: 5.961663246154785\n",
            "Training Iteration 2362, Loss: 2.1327998638153076\n",
            "Training Iteration 2363, Loss: 2.41398549079895\n",
            "Training Iteration 2364, Loss: 3.4208827018737793\n",
            "Training Iteration 2365, Loss: 4.87999153137207\n",
            "Training Iteration 2366, Loss: 4.310118675231934\n",
            "Training Iteration 2367, Loss: 4.588972568511963\n",
            "Training Iteration 2368, Loss: 4.572597980499268\n",
            "Training Iteration 2369, Loss: 5.318576812744141\n",
            "Training Iteration 2370, Loss: 0.9135642051696777\n",
            "Training Iteration 2371, Loss: 6.861385822296143\n",
            "Training Iteration 2372, Loss: 4.907829284667969\n",
            "Training Iteration 2373, Loss: 4.114327907562256\n",
            "Training Iteration 2374, Loss: 2.4315073490142822\n",
            "Training Iteration 2375, Loss: 3.883389711380005\n",
            "Training Iteration 2376, Loss: 4.758867263793945\n",
            "Training Iteration 2377, Loss: 4.443382263183594\n",
            "Training Iteration 2378, Loss: 6.056005954742432\n",
            "Training Iteration 2379, Loss: 3.8168063163757324\n",
            "Training Iteration 2380, Loss: 4.382088661193848\n",
            "Training Iteration 2381, Loss: 3.1582508087158203\n",
            "Training Iteration 2382, Loss: 4.007782936096191\n",
            "Training Iteration 2383, Loss: 1.9847508668899536\n",
            "Training Iteration 2384, Loss: 3.6886520385742188\n",
            "Training Iteration 2385, Loss: 5.0467209815979\n",
            "Training Iteration 2386, Loss: 3.7719180583953857\n",
            "Training Iteration 2387, Loss: 3.4435863494873047\n",
            "Training Iteration 2388, Loss: 4.21793794631958\n",
            "Training Iteration 2389, Loss: 4.759973526000977\n",
            "Training Iteration 2390, Loss: 12.851213455200195\n",
            "Training Iteration 2391, Loss: 8.00017261505127\n",
            "Training Iteration 2392, Loss: 2.6489944458007812\n",
            "Training Iteration 2393, Loss: 3.21685791015625\n",
            "Training Iteration 2394, Loss: 5.012598037719727\n",
            "Training Iteration 2395, Loss: 4.495944023132324\n",
            "Training Iteration 2396, Loss: 4.936894416809082\n",
            "Training Iteration 2397, Loss: 1.4981672763824463\n",
            "Training Iteration 2398, Loss: 4.199225425720215\n",
            "Training Iteration 2399, Loss: 4.000663757324219\n",
            "Training Iteration 2400, Loss: 3.194913625717163\n",
            "Training Iteration 2401, Loss: 3.351999044418335\n",
            "Training Iteration 2402, Loss: 7.234040260314941\n",
            "Training Iteration 2403, Loss: 6.044999122619629\n",
            "Training Iteration 2404, Loss: 5.735610485076904\n",
            "Training Iteration 2405, Loss: 4.381249904632568\n",
            "Training Iteration 2406, Loss: 4.715851783752441\n",
            "Training Iteration 2407, Loss: 2.848116159439087\n",
            "Training Iteration 2408, Loss: 2.865971803665161\n",
            "Training Iteration 2409, Loss: 4.031649112701416\n",
            "Training Iteration 2410, Loss: 8.167520523071289\n",
            "Training Iteration 2411, Loss: 3.4162025451660156\n",
            "Training Iteration 2412, Loss: 4.18278694152832\n",
            "Training Iteration 2413, Loss: 7.268800735473633\n",
            "Training Iteration 2414, Loss: 4.741640090942383\n",
            "Training Iteration 2415, Loss: 4.322574138641357\n",
            "Training Iteration 2416, Loss: 4.256818771362305\n",
            "Training Iteration 2417, Loss: 6.638332366943359\n",
            "Training Iteration 2418, Loss: 8.926960945129395\n",
            "Training Iteration 2419, Loss: 4.3317179679870605\n",
            "Training Iteration 2420, Loss: 4.837435245513916\n",
            "Training Iteration 2421, Loss: 6.533073425292969\n",
            "Training Iteration 2422, Loss: 4.941372394561768\n",
            "Training Iteration 2423, Loss: 3.017780303955078\n",
            "Training Iteration 2424, Loss: 3.7429111003875732\n",
            "Training Iteration 2425, Loss: 6.819882869720459\n",
            "Training Iteration 2426, Loss: 2.134073257446289\n",
            "Training Iteration 2427, Loss: 3.3309524059295654\n",
            "Training Iteration 2428, Loss: 3.400954246520996\n",
            "Training Iteration 2429, Loss: 2.562225818634033\n",
            "Training Iteration 2430, Loss: 3.905984401702881\n",
            "Training Iteration 2431, Loss: 3.7313618659973145\n",
            "Training Iteration 2432, Loss: 3.3554275035858154\n",
            "Training Iteration 2433, Loss: 3.571916103363037\n",
            "Training Iteration 2434, Loss: 5.342783451080322\n",
            "Training Iteration 2435, Loss: 5.3643622398376465\n",
            "Training Iteration 2436, Loss: 6.944047451019287\n",
            "Training Iteration 2437, Loss: 3.4239959716796875\n",
            "Training Iteration 2438, Loss: 3.2199935913085938\n",
            "Training Iteration 2439, Loss: 4.023435115814209\n",
            "Training Iteration 2440, Loss: 6.508109092712402\n",
            "Training Iteration 2441, Loss: 5.0162200927734375\n",
            "Training Iteration 2442, Loss: 8.897978782653809\n",
            "Training Iteration 2443, Loss: 4.060515403747559\n",
            "Training Iteration 2444, Loss: 5.237873554229736\n",
            "Training Iteration 2445, Loss: 2.606553077697754\n",
            "Training Iteration 2446, Loss: 3.1977930068969727\n",
            "Training Iteration 2447, Loss: 3.976146936416626\n",
            "Training Iteration 2448, Loss: 5.34394645690918\n",
            "Training Iteration 2449, Loss: 6.8471832275390625\n",
            "Training Iteration 2450, Loss: 5.294482707977295\n",
            "Training Iteration 2451, Loss: 4.6510186195373535\n",
            "Training Iteration 2452, Loss: 2.967067241668701\n",
            "Training Iteration 2453, Loss: 1.9553605318069458\n",
            "Training Iteration 2454, Loss: 3.330195903778076\n",
            "Training Iteration 2455, Loss: 5.913665771484375\n",
            "Training Iteration 2456, Loss: 2.329007387161255\n",
            "Training Iteration 2457, Loss: 5.35964822769165\n",
            "Training Iteration 2458, Loss: 4.841905117034912\n",
            "Training Iteration 2459, Loss: 3.7445993423461914\n",
            "Training Iteration 2460, Loss: 2.818972587585449\n",
            "Training Iteration 2461, Loss: 3.9660253524780273\n",
            "Training Iteration 2462, Loss: 4.445871829986572\n",
            "Training Iteration 2463, Loss: 2.5828685760498047\n",
            "Training Iteration 2464, Loss: 5.537548542022705\n",
            "Training Iteration 2465, Loss: 3.8745248317718506\n",
            "Training Iteration 2466, Loss: 3.3757150173187256\n",
            "Training Iteration 2467, Loss: 2.4720382690429688\n",
            "Training Iteration 2468, Loss: 5.076037406921387\n",
            "Training Iteration 2469, Loss: 3.4282190799713135\n",
            "Training Iteration 2470, Loss: 3.8359851837158203\n",
            "Training Iteration 2471, Loss: 4.787721157073975\n",
            "Training Iteration 2472, Loss: 1.6367260217666626\n",
            "Training Iteration 2473, Loss: 4.954835891723633\n",
            "Training Iteration 2474, Loss: 5.6569318771362305\n",
            "Training Iteration 2475, Loss: 3.278724431991577\n",
            "Training Iteration 2476, Loss: 3.344228744506836\n",
            "Training Iteration 2477, Loss: 3.006612777709961\n",
            "Training Iteration 2478, Loss: 2.5369739532470703\n",
            "Training Iteration 2479, Loss: 3.330263614654541\n",
            "Training Iteration 2480, Loss: 3.0309715270996094\n",
            "Training Iteration 2481, Loss: 4.1556267738342285\n",
            "Training Iteration 2482, Loss: 3.5134639739990234\n",
            "Training Iteration 2483, Loss: 2.4318768978118896\n",
            "Training Iteration 2484, Loss: 4.230937480926514\n",
            "Training Iteration 2485, Loss: 4.411057949066162\n",
            "Training Iteration 2486, Loss: 5.300026893615723\n",
            "Training Iteration 2487, Loss: 5.275052070617676\n",
            "Training Iteration 2488, Loss: 2.842808723449707\n",
            "Training Iteration 2489, Loss: 4.424164295196533\n",
            "Training Iteration 2490, Loss: 3.288133144378662\n",
            "Training Iteration 2491, Loss: 3.4548206329345703\n",
            "Training Iteration 2492, Loss: 6.034674167633057\n",
            "Training Iteration 2493, Loss: 3.289940595626831\n",
            "Training Iteration 2494, Loss: 2.554408550262451\n",
            "Training Iteration 2495, Loss: 4.521800994873047\n",
            "Training Iteration 2496, Loss: 6.485708713531494\n",
            "Training Iteration 2497, Loss: 3.309396505355835\n",
            "Training Iteration 2498, Loss: 4.0436811447143555\n",
            "Training Iteration 2499, Loss: 9.498022079467773\n",
            "Training Iteration 2500, Loss: 9.839153289794922\n",
            "Training Iteration 2501, Loss: 2.6562232971191406\n",
            "Training Iteration 2502, Loss: 5.026283264160156\n",
            "Training Iteration 2503, Loss: 4.996875762939453\n",
            "Training Iteration 2504, Loss: 4.503413677215576\n",
            "Training Iteration 2505, Loss: 1.9332478046417236\n",
            "Training Iteration 2506, Loss: 4.3201775550842285\n",
            "Training Iteration 2507, Loss: 2.8564891815185547\n",
            "Training Iteration 2508, Loss: 4.04141902923584\n",
            "Training Iteration 2509, Loss: 4.521322727203369\n",
            "Training Iteration 2510, Loss: 3.9074347019195557\n",
            "Training Iteration 2511, Loss: 5.033762454986572\n",
            "Training Iteration 2512, Loss: 7.855076789855957\n",
            "Training Iteration 2513, Loss: 5.803690433502197\n",
            "Training Iteration 2514, Loss: 4.15786600112915\n",
            "Training Iteration 2515, Loss: 4.561612606048584\n",
            "Training Iteration 2516, Loss: 2.646900177001953\n",
            "Training Iteration 2517, Loss: 3.217261552810669\n",
            "Training Iteration 2518, Loss: 4.05000114440918\n",
            "Training Iteration 2519, Loss: 4.0727009773254395\n",
            "Training Iteration 2520, Loss: 1.6419848203659058\n",
            "Training Iteration 2521, Loss: 3.629737138748169\n",
            "Training Iteration 2522, Loss: 2.4846696853637695\n",
            "Training Iteration 2523, Loss: 4.470124244689941\n",
            "Training Iteration 2524, Loss: 2.574230194091797\n",
            "Training Iteration 2525, Loss: 2.673318386077881\n",
            "Training Iteration 2526, Loss: 4.252839088439941\n",
            "Training Iteration 2527, Loss: 5.869200706481934\n",
            "Training Iteration 2528, Loss: 4.63896369934082\n",
            "Training Iteration 2529, Loss: 5.21120548248291\n",
            "Training Iteration 2530, Loss: 4.854447841644287\n",
            "Training Iteration 2531, Loss: 2.59155535697937\n",
            "Training Iteration 2532, Loss: 5.162163734436035\n",
            "Training Iteration 2533, Loss: 7.624011516571045\n",
            "Training Iteration 2534, Loss: 4.333434104919434\n",
            "Training Iteration 2535, Loss: 5.8127970695495605\n",
            "Training Iteration 2536, Loss: 3.1024603843688965\n",
            "Training Iteration 2537, Loss: 4.594296455383301\n",
            "Training Iteration 2538, Loss: 6.331633567810059\n",
            "Training Iteration 2539, Loss: 5.226159572601318\n",
            "Training Iteration 2540, Loss: 3.996358871459961\n",
            "Training Iteration 2541, Loss: 3.7118303775787354\n",
            "Training Iteration 2542, Loss: 4.805227279663086\n",
            "Training Iteration 2543, Loss: 3.4346487522125244\n",
            "Training Iteration 2544, Loss: 3.380659341812134\n",
            "Training Iteration 2545, Loss: 4.494091510772705\n",
            "Training Iteration 2546, Loss: 2.6286606788635254\n",
            "Training Iteration 2547, Loss: 4.857728004455566\n",
            "Training Iteration 2548, Loss: 4.799529075622559\n",
            "Training Iteration 2549, Loss: 3.8500804901123047\n",
            "Training Iteration 2550, Loss: 7.676466941833496\n",
            "Training Iteration 2551, Loss: 5.273406982421875\n",
            "Training Iteration 2552, Loss: 4.724156379699707\n",
            "Training Iteration 2553, Loss: 4.808930397033691\n",
            "Training Iteration 2554, Loss: 3.457746982574463\n",
            "Training Iteration 2555, Loss: 4.553359508514404\n",
            "Training Iteration 2556, Loss: 4.746625900268555\n",
            "Training Iteration 2557, Loss: 2.0971603393554688\n",
            "Training Iteration 2558, Loss: 5.822772979736328\n",
            "Training Iteration 2559, Loss: 2.8380067348480225\n",
            "Training Iteration 2560, Loss: 5.641839027404785\n",
            "Training Iteration 2561, Loss: 1.9203455448150635\n",
            "Training Iteration 2562, Loss: 6.163595199584961\n",
            "Training Iteration 2563, Loss: 1.9190679788589478\n",
            "Training Iteration 2564, Loss: 4.2561821937561035\n",
            "Training Iteration 2565, Loss: 3.339003562927246\n",
            "Training Iteration 2566, Loss: 3.1459479331970215\n",
            "Training Iteration 2567, Loss: 3.3773193359375\n",
            "Training Iteration 2568, Loss: 5.791964530944824\n",
            "Training Iteration 2569, Loss: 4.453665733337402\n",
            "Training Iteration 2570, Loss: 4.361596584320068\n",
            "Training Iteration 2571, Loss: 9.455218315124512\n",
            "Training Iteration 2572, Loss: 4.714479923248291\n",
            "Training Iteration 2573, Loss: 3.5369038581848145\n",
            "Training Iteration 2574, Loss: 7.224735260009766\n",
            "Training Iteration 2575, Loss: 4.025990009307861\n",
            "Training Iteration 2576, Loss: 2.927529811859131\n",
            "Training Iteration 2577, Loss: 4.753625869750977\n",
            "Training Iteration 2578, Loss: 5.1042799949646\n",
            "Training Iteration 2579, Loss: 6.542893886566162\n",
            "Training Iteration 2580, Loss: 3.496492624282837\n",
            "Training Iteration 2581, Loss: 6.112430095672607\n",
            "Training Iteration 2582, Loss: 5.573984146118164\n",
            "Training Iteration 2583, Loss: 6.434868812561035\n",
            "Training Iteration 2584, Loss: 4.017243385314941\n",
            "Training Iteration 2585, Loss: 5.040112495422363\n",
            "Training Iteration 2586, Loss: 2.4799153804779053\n",
            "Training Iteration 2587, Loss: 3.7666258811950684\n",
            "Training Iteration 2588, Loss: 6.427176475524902\n",
            "Training Iteration 2589, Loss: 4.186144828796387\n",
            "Training Iteration 2590, Loss: 4.064174652099609\n",
            "Training Iteration 2591, Loss: 4.874463081359863\n",
            "Training Iteration 2592, Loss: 3.4408648014068604\n",
            "Training Iteration 2593, Loss: 3.0183892250061035\n",
            "Training Iteration 2594, Loss: 2.7824835777282715\n",
            "Training Iteration 2595, Loss: 4.680222988128662\n",
            "Training Iteration 2596, Loss: 4.288430213928223\n",
            "Training Iteration 2597, Loss: 2.318702459335327\n",
            "Training Iteration 2598, Loss: 6.507935047149658\n",
            "Training Iteration 2599, Loss: 4.320817947387695\n",
            "Training Iteration 2600, Loss: 3.029168128967285\n",
            "Training Iteration 2601, Loss: 4.99409818649292\n",
            "Training Iteration 2602, Loss: 2.828420639038086\n",
            "Training Iteration 2603, Loss: 2.6444005966186523\n",
            "Training Iteration 2604, Loss: 3.38135027885437\n",
            "Training Iteration 2605, Loss: 6.405337333679199\n",
            "Training Iteration 2606, Loss: 4.849601745605469\n",
            "Training Iteration 2607, Loss: 4.6714372634887695\n",
            "Training Iteration 2608, Loss: 5.048863887786865\n",
            "Training Iteration 2609, Loss: 3.2332918643951416\n",
            "Training Iteration 2610, Loss: 6.864989280700684\n",
            "Training Iteration 2611, Loss: 3.393022060394287\n",
            "Training Iteration 2612, Loss: 2.863252639770508\n",
            "Training Iteration 2613, Loss: 4.165126323699951\n",
            "Training Iteration 2614, Loss: 6.486841678619385\n",
            "Training Iteration 2615, Loss: 6.731085777282715\n",
            "Training Iteration 2616, Loss: 1.3610336780548096\n",
            "Training Iteration 2617, Loss: 9.500699043273926\n",
            "Training Iteration 2618, Loss: 1.2045483589172363\n",
            "Training Iteration 2619, Loss: 6.213714122772217\n",
            "Training Iteration 2620, Loss: 3.971924304962158\n",
            "Training Iteration 2621, Loss: 7.424172878265381\n",
            "Training Iteration 2622, Loss: 7.175058841705322\n",
            "Training Iteration 2623, Loss: 5.337869167327881\n",
            "Training Iteration 2624, Loss: 4.260821342468262\n",
            "Training Iteration 2625, Loss: 4.39985990524292\n",
            "Training Iteration 2626, Loss: 6.240355968475342\n",
            "Training Iteration 2627, Loss: 5.565898895263672\n",
            "Training Iteration 2628, Loss: 6.44566011428833\n",
            "Training Iteration 2629, Loss: 3.985895872116089\n",
            "Training Iteration 2630, Loss: 6.286290168762207\n",
            "Training Iteration 2631, Loss: 6.240578651428223\n",
            "Training Iteration 2632, Loss: 5.065110206604004\n",
            "Training Iteration 2633, Loss: 2.9329988956451416\n",
            "Training Iteration 2634, Loss: 3.7874889373779297\n",
            "Training Iteration 2635, Loss: 2.2457833290100098\n",
            "Training Iteration 2636, Loss: 7.225529670715332\n",
            "Training Iteration 2637, Loss: 3.403193235397339\n",
            "Training Iteration 2638, Loss: 3.4325928688049316\n",
            "Training Iteration 2639, Loss: 2.4864661693573\n",
            "Training Iteration 2640, Loss: 4.054267883300781\n",
            "Training Iteration 2641, Loss: 3.284058094024658\n",
            "Training Iteration 2642, Loss: 4.585831165313721\n",
            "Training Iteration 2643, Loss: 3.501969814300537\n",
            "Training Iteration 2644, Loss: 4.741746425628662\n",
            "Training Iteration 2645, Loss: 4.888195991516113\n",
            "Training Iteration 2646, Loss: 1.8470778465270996\n",
            "Training Iteration 2647, Loss: 4.1583147048950195\n",
            "Training Iteration 2648, Loss: 6.317497253417969\n",
            "Training Iteration 2649, Loss: 4.517159461975098\n",
            "Training Iteration 2650, Loss: 2.2139148712158203\n",
            "Training Iteration 2651, Loss: 5.510379314422607\n",
            "Training Iteration 2652, Loss: 6.538025856018066\n",
            "Training Iteration 2653, Loss: 9.56422233581543\n",
            "Training Iteration 2654, Loss: 3.6330201625823975\n",
            "Training Iteration 2655, Loss: 5.248597145080566\n",
            "Training Iteration 2656, Loss: 1.7921468019485474\n",
            "Training Iteration 2657, Loss: 4.502613544464111\n",
            "Training Iteration 2658, Loss: 6.6088714599609375\n",
            "Training Iteration 2659, Loss: 6.751959800720215\n",
            "Training Iteration 2660, Loss: 4.056976318359375\n",
            "Training Iteration 2661, Loss: 6.959512233734131\n",
            "Training Iteration 2662, Loss: 5.605001926422119\n",
            "Training Iteration 2663, Loss: 4.463227272033691\n",
            "Training Iteration 2664, Loss: 4.110133171081543\n",
            "Training Iteration 2665, Loss: 3.1796631813049316\n",
            "Training Iteration 2666, Loss: 4.4647626876831055\n",
            "Training Iteration 2667, Loss: 4.4252543449401855\n",
            "Training Iteration 2668, Loss: 10.071219444274902\n",
            "Training Iteration 2669, Loss: 5.123227596282959\n",
            "Training Iteration 2670, Loss: 4.903927326202393\n",
            "Training Iteration 2671, Loss: 4.476475238800049\n",
            "Training Iteration 2672, Loss: 4.231086254119873\n",
            "Training Iteration 2673, Loss: 5.731224536895752\n",
            "Training Iteration 2674, Loss: 3.277998447418213\n",
            "Training Iteration 2675, Loss: 6.603482246398926\n",
            "Training Iteration 2676, Loss: 4.655704975128174\n",
            "Training Iteration 2677, Loss: 2.902432441711426\n",
            "Training Iteration 2678, Loss: 4.025153636932373\n",
            "Training Iteration 2679, Loss: 5.109229564666748\n",
            "Training Iteration 2680, Loss: 3.795588970184326\n",
            "Training Iteration 2681, Loss: 2.4042866230010986\n",
            "Training Iteration 2682, Loss: 3.1422343254089355\n",
            "Training Iteration 2683, Loss: 4.485208988189697\n",
            "Training Iteration 2684, Loss: 9.048807144165039\n",
            "Training Iteration 2685, Loss: 4.273205280303955\n",
            "Training Iteration 2686, Loss: 2.421334743499756\n",
            "Training Iteration 2687, Loss: 2.9772231578826904\n",
            "Training Iteration 2688, Loss: 5.4759979248046875\n",
            "Training Iteration 2689, Loss: 4.979907989501953\n",
            "Training Iteration 2690, Loss: 1.4495635032653809\n",
            "Training Iteration 2691, Loss: 1.947786569595337\n",
            "Training Iteration 2692, Loss: 4.769151210784912\n",
            "Training Iteration 2693, Loss: 4.807428359985352\n",
            "Training Iteration 2694, Loss: 7.536432266235352\n",
            "Training Iteration 2695, Loss: 5.474560737609863\n",
            "Training Iteration 2696, Loss: 3.5499422550201416\n",
            "Training Iteration 2697, Loss: 5.644057750701904\n",
            "Training Iteration 2698, Loss: 3.2074949741363525\n",
            "Training Iteration 2699, Loss: 3.0403265953063965\n",
            "Training Iteration 2700, Loss: 8.597831726074219\n",
            "Training Iteration 2701, Loss: 7.557901859283447\n",
            "Training Iteration 2702, Loss: 7.6308183670043945\n",
            "Training Iteration 2703, Loss: 9.081198692321777\n",
            "Training Iteration 2704, Loss: 6.713401794433594\n",
            "Training Iteration 2705, Loss: 3.137960195541382\n",
            "Training Iteration 2706, Loss: 3.1018593311309814\n",
            "Training Iteration 2707, Loss: 4.3419413566589355\n",
            "Training Iteration 2708, Loss: 3.4644060134887695\n",
            "Training Iteration 2709, Loss: 3.2429230213165283\n",
            "Training Iteration 2710, Loss: 3.9667015075683594\n",
            "Training Iteration 2711, Loss: 5.730035305023193\n",
            "Training Iteration 2712, Loss: 3.1809873580932617\n",
            "Training Iteration 2713, Loss: 2.712825298309326\n",
            "Training Iteration 2714, Loss: 5.548559188842773\n",
            "Training Iteration 2715, Loss: 5.858703136444092\n",
            "Training Iteration 2716, Loss: 5.2760396003723145\n",
            "Training Iteration 2717, Loss: 5.026176452636719\n",
            "Training Iteration 2718, Loss: 4.706089496612549\n",
            "Training Iteration 2719, Loss: 4.712855339050293\n",
            "Training Iteration 2720, Loss: 3.7341184616088867\n",
            "Training Iteration 2721, Loss: 2.4459831714630127\n",
            "Training Iteration 2722, Loss: 3.276641845703125\n",
            "Training Iteration 2723, Loss: 2.9390487670898438\n",
            "Training Iteration 2724, Loss: 3.1506659984588623\n",
            "Training Iteration 2725, Loss: 3.4256696701049805\n",
            "Training Iteration 2726, Loss: 3.553656578063965\n",
            "Training Iteration 2727, Loss: 5.7453718185424805\n",
            "Training Iteration 2728, Loss: 5.778522491455078\n",
            "Training Iteration 2729, Loss: 1.8502044677734375\n",
            "Training Iteration 2730, Loss: 2.7618839740753174\n",
            "Training Iteration 2731, Loss: 6.401474952697754\n",
            "Training Iteration 2732, Loss: 2.7271440029144287\n",
            "Training Iteration 2733, Loss: 3.6002447605133057\n",
            "Training Iteration 2734, Loss: 2.403517007827759\n",
            "Training Iteration 2735, Loss: 3.434006690979004\n",
            "Training Iteration 2736, Loss: 3.2638609409332275\n",
            "Training Iteration 2737, Loss: 4.466963768005371\n",
            "Training Iteration 2738, Loss: 5.975146293640137\n",
            "Training Iteration 2739, Loss: 5.827086448669434\n",
            "Training Iteration 2740, Loss: 6.278402805328369\n",
            "Training Iteration 2741, Loss: 3.0694973468780518\n",
            "Training Iteration 2742, Loss: 2.6315457820892334\n",
            "Training Iteration 2743, Loss: 6.246393203735352\n",
            "Training Iteration 2744, Loss: 5.791900634765625\n",
            "Training Iteration 2745, Loss: 5.8352861404418945\n",
            "Training Iteration 2746, Loss: 8.144590377807617\n",
            "Training Iteration 2747, Loss: 4.945713043212891\n",
            "Training Iteration 2748, Loss: 4.0359086990356445\n",
            "Training Iteration 2749, Loss: 4.065779209136963\n",
            "Training Iteration 2750, Loss: 7.589511871337891\n",
            "Training Iteration 2751, Loss: 2.986861228942871\n",
            "Training Iteration 2752, Loss: 3.232776165008545\n",
            "Training Iteration 2753, Loss: 4.265818119049072\n",
            "Training Iteration 2754, Loss: 4.258401393890381\n",
            "Training Iteration 2755, Loss: 2.608074426651001\n",
            "Training Iteration 2756, Loss: 1.6721163988113403\n",
            "Training Iteration 2757, Loss: 4.368374824523926\n",
            "Training Iteration 2758, Loss: 6.259256839752197\n",
            "Training Iteration 2759, Loss: 4.2654829025268555\n",
            "Training Iteration 2760, Loss: 1.168879747390747\n",
            "Training Iteration 2761, Loss: 8.563504219055176\n",
            "Training Iteration 2762, Loss: 4.637022972106934\n",
            "Training Iteration 2763, Loss: 3.1276047229766846\n",
            "Training Iteration 2764, Loss: 3.0373263359069824\n",
            "Training Iteration 2765, Loss: 3.229804039001465\n",
            "Training Iteration 2766, Loss: 6.997757911682129\n",
            "Training Iteration 2767, Loss: 2.936135768890381\n",
            "Training Iteration 2768, Loss: 4.598564624786377\n",
            "Training Iteration 2769, Loss: 2.8432891368865967\n",
            "Training Iteration 2770, Loss: 6.782742023468018\n",
            "Training Iteration 2771, Loss: 3.402907133102417\n",
            "Training Iteration 2772, Loss: 3.2766942977905273\n",
            "Training Iteration 2773, Loss: 3.3197672367095947\n",
            "Training Iteration 2774, Loss: 3.940627336502075\n",
            "Training Iteration 2775, Loss: 3.271912097930908\n",
            "Training Iteration 2776, Loss: 5.72233247756958\n",
            "Training Iteration 2777, Loss: 4.349826335906982\n",
            "Training Iteration 2778, Loss: 4.637567043304443\n",
            "Training Iteration 2779, Loss: 5.338913917541504\n",
            "Training Iteration 2780, Loss: 4.986893653869629\n",
            "Training Iteration 2781, Loss: 4.611195087432861\n",
            "Training Iteration 2782, Loss: 5.843889236450195\n",
            "Training Iteration 2783, Loss: 4.309576034545898\n",
            "Training Iteration 2784, Loss: 4.3128662109375\n",
            "Training Iteration 2785, Loss: 3.4787404537200928\n",
            "Training Iteration 2786, Loss: 4.568387985229492\n",
            "Training Iteration 2787, Loss: 7.11152458190918\n",
            "Training Iteration 2788, Loss: 4.759268760681152\n",
            "Training Iteration 2789, Loss: 4.168351650238037\n",
            "Training Iteration 2790, Loss: 3.5965776443481445\n",
            "Training Iteration 2791, Loss: 3.411780595779419\n",
            "Training Iteration 2792, Loss: 3.189216136932373\n",
            "Training Iteration 2793, Loss: 3.7691357135772705\n",
            "Training Iteration 2794, Loss: 6.841432571411133\n",
            "Training Iteration 2795, Loss: 5.602597236633301\n",
            "Training Iteration 2796, Loss: 2.320687770843506\n",
            "Training Iteration 2797, Loss: 4.982456684112549\n",
            "Training Iteration 2798, Loss: 2.4774696826934814\n",
            "Training Iteration 2799, Loss: 4.142271995544434\n",
            "Training Iteration 2800, Loss: 3.7647600173950195\n",
            "Training Iteration 2801, Loss: 4.223045825958252\n",
            "Training Iteration 2802, Loss: 5.826049327850342\n",
            "Training Iteration 2803, Loss: 1.6684868335723877\n",
            "Training Iteration 2804, Loss: 4.877046585083008\n",
            "Training Iteration 2805, Loss: 1.7057921886444092\n",
            "Training Iteration 2806, Loss: 6.746826171875\n",
            "Training Iteration 2807, Loss: 3.382998466491699\n",
            "Training Iteration 2808, Loss: 3.699878454208374\n",
            "Training Iteration 2809, Loss: 3.2821667194366455\n",
            "Training Iteration 2810, Loss: 4.139422416687012\n",
            "Training Iteration 2811, Loss: 8.634208679199219\n",
            "Training Iteration 2812, Loss: 3.663646697998047\n",
            "Training Iteration 2813, Loss: 1.7028899192810059\n",
            "Training Iteration 2814, Loss: 4.3523383140563965\n",
            "Training Iteration 2815, Loss: 5.2129058837890625\n",
            "Training Iteration 2816, Loss: 5.857021808624268\n",
            "Training Iteration 2817, Loss: 2.6315083503723145\n",
            "Training Iteration 2818, Loss: 4.667651176452637\n",
            "Training Iteration 2819, Loss: 4.0268707275390625\n",
            "Training Iteration 2820, Loss: 3.644948959350586\n",
            "Training Iteration 2821, Loss: 5.314220428466797\n",
            "Training Iteration 2822, Loss: 3.8207902908325195\n",
            "Training Iteration 2823, Loss: 5.763197898864746\n",
            "Training Iteration 2824, Loss: 2.531266212463379\n",
            "Training Iteration 2825, Loss: 2.7244184017181396\n",
            "Training Iteration 2826, Loss: 3.876056671142578\n",
            "Training Iteration 2827, Loss: 7.047562599182129\n",
            "Training Iteration 2828, Loss: 2.6100575923919678\n",
            "Training Iteration 2829, Loss: 3.4474728107452393\n",
            "Training Iteration 2830, Loss: 5.6763505935668945\n",
            "Training Iteration 2831, Loss: 4.181288242340088\n",
            "Training Iteration 2832, Loss: 3.7025301456451416\n",
            "Training Iteration 2833, Loss: 4.410964012145996\n",
            "Training Iteration 2834, Loss: 4.720328330993652\n",
            "Training Iteration 2835, Loss: 2.9204659461975098\n",
            "Training Iteration 2836, Loss: 1.0024906396865845\n",
            "Training Iteration 2837, Loss: 8.905698776245117\n",
            "Training Iteration 2838, Loss: 5.07181978225708\n",
            "Training Iteration 2839, Loss: 4.472243309020996\n",
            "Training Iteration 2840, Loss: 5.515346050262451\n",
            "Training Iteration 2841, Loss: 1.8213304281234741\n",
            "Training Iteration 2842, Loss: 5.693441867828369\n",
            "Training Iteration 2843, Loss: 5.0028581619262695\n",
            "Training Iteration 2844, Loss: 5.80220365524292\n",
            "Training Iteration 2845, Loss: 2.6239964962005615\n",
            "Training Iteration 2846, Loss: 4.978279113769531\n",
            "Training Iteration 2847, Loss: 2.128889560699463\n",
            "Training Iteration 2848, Loss: 4.933229923248291\n",
            "Training Iteration 2849, Loss: 4.218543529510498\n",
            "Training Iteration 2850, Loss: 2.060659885406494\n",
            "Training Iteration 2851, Loss: 4.164694786071777\n",
            "Training Iteration 2852, Loss: 4.468653202056885\n",
            "Training Iteration 2853, Loss: 5.328200340270996\n",
            "Training Iteration 2854, Loss: 3.5339102745056152\n",
            "Training Iteration 2855, Loss: 3.9362411499023438\n",
            "Training Iteration 2856, Loss: 5.124982833862305\n",
            "Training Iteration 2857, Loss: 4.3203444480896\n",
            "Training Iteration 2858, Loss: 4.791511058807373\n",
            "Training Iteration 2859, Loss: 6.948080062866211\n",
            "Training Iteration 2860, Loss: 3.630582571029663\n",
            "Training Iteration 2861, Loss: 5.062247276306152\n",
            "Training Iteration 2862, Loss: 2.6582183837890625\n",
            "Training Iteration 2863, Loss: 4.051743030548096\n",
            "Training Iteration 2864, Loss: 2.3009488582611084\n",
            "Training Iteration 2865, Loss: 3.0093815326690674\n",
            "Training Iteration 2866, Loss: 4.668492317199707\n",
            "Training Iteration 2867, Loss: 9.375317573547363\n",
            "Training Iteration 2868, Loss: 8.46052360534668\n",
            "Training Iteration 2869, Loss: 4.3513946533203125\n",
            "Training Iteration 2870, Loss: 3.8726322650909424\n",
            "Training Iteration 2871, Loss: 4.4064483642578125\n",
            "Training Iteration 2872, Loss: 3.833444356918335\n",
            "Training Iteration 2873, Loss: 4.5209197998046875\n",
            "Training Iteration 2874, Loss: 3.3060641288757324\n",
            "Training Iteration 2875, Loss: 6.624031066894531\n",
            "Training Iteration 2876, Loss: 4.816448211669922\n",
            "Training Iteration 2877, Loss: 4.774918556213379\n",
            "Training Iteration 2878, Loss: 3.081000804901123\n",
            "Training Iteration 2879, Loss: 5.411831855773926\n",
            "Training Iteration 2880, Loss: 5.161362648010254\n",
            "Training Iteration 2881, Loss: 3.214024066925049\n",
            "Training Iteration 2882, Loss: 2.758761405944824\n",
            "Training Iteration 2883, Loss: 5.31009578704834\n",
            "Training Iteration 2884, Loss: 2.2837071418762207\n",
            "Training Iteration 2885, Loss: 2.7101893424987793\n",
            "Training Iteration 2886, Loss: 2.1846699714660645\n",
            "Training Iteration 2887, Loss: 0.9743474721908569\n",
            "Training Iteration 2888, Loss: 5.736102104187012\n",
            "Training Iteration 2889, Loss: 7.079756259918213\n",
            "Training Iteration 2890, Loss: 4.8242268562316895\n",
            "Training Iteration 2891, Loss: 3.4869210720062256\n",
            "Training Iteration 2892, Loss: 4.321629047393799\n",
            "Training Iteration 2893, Loss: 2.4250922203063965\n",
            "Training Iteration 2894, Loss: 2.9565415382385254\n",
            "Training Iteration 2895, Loss: 5.034667015075684\n",
            "Training Iteration 2896, Loss: 4.906585216522217\n",
            "Training Iteration 2897, Loss: 3.212568521499634\n",
            "Training Iteration 2898, Loss: 2.5817160606384277\n",
            "Training Iteration 2899, Loss: 5.438600540161133\n",
            "Training Iteration 2900, Loss: 2.2861194610595703\n",
            "Training Iteration 2901, Loss: 4.788303375244141\n",
            "Training Iteration 2902, Loss: 3.4502806663513184\n",
            "Training Iteration 2903, Loss: 1.9668304920196533\n",
            "Training Iteration 2904, Loss: 3.3895230293273926\n",
            "Training Iteration 2905, Loss: 7.717165470123291\n",
            "Training Iteration 2906, Loss: 4.017106533050537\n",
            "Training Iteration 2907, Loss: 4.621464252471924\n",
            "Training Iteration 2908, Loss: 5.703573703765869\n",
            "Training Iteration 2909, Loss: 7.885764122009277\n",
            "Training Iteration 2910, Loss: 3.9733150005340576\n",
            "Training Iteration 2911, Loss: 6.135041236877441\n",
            "Training Iteration 2912, Loss: 3.2590906620025635\n",
            "Training Iteration 2913, Loss: 1.7704484462738037\n",
            "Training Iteration 2914, Loss: 1.8042851686477661\n",
            "Training Iteration 2915, Loss: 4.146059513092041\n",
            "Training Iteration 2916, Loss: 3.9942150115966797\n",
            "Training Iteration 2917, Loss: 2.081075668334961\n",
            "Training Iteration 2918, Loss: 7.280315399169922\n",
            "Training Iteration 2919, Loss: 7.028149127960205\n",
            "Training Iteration 2920, Loss: 4.767611980438232\n",
            "Training Iteration 2921, Loss: 3.6269986629486084\n",
            "Training Iteration 2922, Loss: 2.468883991241455\n",
            "Training Iteration 2923, Loss: 3.0319771766662598\n",
            "Training Iteration 2924, Loss: 3.0292229652404785\n",
            "Training Iteration 2925, Loss: 5.376791954040527\n",
            "Training Iteration 2926, Loss: 3.9574198722839355\n",
            "Training Iteration 2927, Loss: 3.8020739555358887\n",
            "Training Iteration 2928, Loss: 3.7137668132781982\n",
            "Training Iteration 2929, Loss: 3.7045741081237793\n",
            "Training Iteration 2930, Loss: 6.168923377990723\n",
            "Training Iteration 2931, Loss: 1.9325485229492188\n",
            "Training Iteration 2932, Loss: 2.833179473876953\n",
            "Training Iteration 2933, Loss: 3.4200611114501953\n",
            "Training Iteration 2934, Loss: 5.775730133056641\n",
            "Training Iteration 2935, Loss: 2.0906870365142822\n",
            "Training Iteration 2936, Loss: 5.773456573486328\n",
            "Training Iteration 2937, Loss: 4.382080554962158\n",
            "Training Iteration 2938, Loss: 3.0101425647735596\n",
            "Training Iteration 2939, Loss: 5.425282001495361\n",
            "Training Iteration 2940, Loss: 4.556838512420654\n",
            "Training Iteration 2941, Loss: 4.797724723815918\n",
            "Training Iteration 2942, Loss: 2.680025577545166\n",
            "Training Iteration 2943, Loss: 6.257638454437256\n",
            "Training Iteration 2944, Loss: 3.2524025440216064\n",
            "Training Iteration 2945, Loss: 2.7849042415618896\n",
            "Training Iteration 2946, Loss: 3.354423999786377\n",
            "Training Iteration 2947, Loss: 7.889050006866455\n",
            "Training Iteration 2948, Loss: 4.318902969360352\n",
            "Training Iteration 2949, Loss: 3.6076149940490723\n",
            "Training Iteration 2950, Loss: 3.628185272216797\n",
            "Training Iteration 2951, Loss: 2.9283971786499023\n",
            "Training Iteration 2952, Loss: 4.359026908874512\n",
            "Training Iteration 2953, Loss: 3.475964307785034\n",
            "Training Iteration 2954, Loss: 4.2410807609558105\n",
            "Training Iteration 2955, Loss: 5.428738117218018\n",
            "Training Iteration 2956, Loss: 2.5873029232025146\n",
            "Training Iteration 2957, Loss: 3.2424960136413574\n",
            "Training Iteration 2958, Loss: 4.528602600097656\n",
            "Training Iteration 2959, Loss: 3.3453240394592285\n",
            "Training Iteration 2960, Loss: 2.3170788288116455\n",
            "Training Iteration 2961, Loss: 3.7991676330566406\n",
            "Training Iteration 2962, Loss: 5.008668422698975\n",
            "Training Iteration 2963, Loss: 5.035003662109375\n",
            "Training Iteration 2964, Loss: 5.030287742614746\n",
            "Training Iteration 2965, Loss: 5.151185035705566\n",
            "Training Iteration 2966, Loss: 3.8657567501068115\n",
            "Training Iteration 2967, Loss: 3.7910165786743164\n",
            "Training Iteration 2968, Loss: 4.703686237335205\n",
            "Training Iteration 2969, Loss: 5.037840843200684\n",
            "Training Iteration 2970, Loss: 5.258211612701416\n",
            "Training Iteration 2971, Loss: 7.093594551086426\n",
            "Training Iteration 2972, Loss: 2.333552122116089\n",
            "Training Iteration 2973, Loss: 6.2873921394348145\n",
            "Training Iteration 2974, Loss: 4.457339763641357\n",
            "Training Iteration 2975, Loss: 4.487338066101074\n",
            "Training Iteration 2976, Loss: 6.314798831939697\n",
            "Training Iteration 2977, Loss: 2.725609302520752\n",
            "Training Iteration 2978, Loss: 2.3299992084503174\n",
            "Training Iteration 2979, Loss: 3.25717830657959\n",
            "Training Iteration 2980, Loss: 5.2849955558776855\n",
            "Training Iteration 2981, Loss: 3.976909637451172\n",
            "Training Iteration 2982, Loss: 6.809125900268555\n",
            "Training Iteration 2983, Loss: 3.376678943634033\n",
            "Training Iteration 2984, Loss: 5.612325191497803\n",
            "Training Iteration 2985, Loss: 4.051244258880615\n",
            "Training Iteration 2986, Loss: 6.241439342498779\n",
            "Training Iteration 2987, Loss: 6.067485809326172\n",
            "Training Iteration 2988, Loss: 2.367920398712158\n",
            "Training Iteration 2989, Loss: 6.025279521942139\n",
            "Training Iteration 2990, Loss: 5.177859783172607\n",
            "Training Iteration 2991, Loss: 3.751800060272217\n",
            "Training Iteration 2992, Loss: 5.152485370635986\n",
            "Training Iteration 2993, Loss: 4.189494609832764\n",
            "Training Iteration 2994, Loss: 6.052529811859131\n",
            "Training Iteration 2995, Loss: 3.510371685028076\n",
            "Training Iteration 2996, Loss: 5.218658447265625\n",
            "Training Iteration 2997, Loss: 4.137629985809326\n",
            "Training Iteration 2998, Loss: 4.848672389984131\n",
            "Training Iteration 2999, Loss: 4.5219268798828125\n",
            "Training Iteration 3000, Loss: 7.121119499206543\n",
            "Training Iteration 3001, Loss: 2.662236452102661\n",
            "Training Iteration 3002, Loss: 3.711272954940796\n",
            "Training Iteration 3003, Loss: 3.1677589416503906\n",
            "Training Iteration 3004, Loss: 3.5405569076538086\n",
            "Training Iteration 3005, Loss: 3.466513156890869\n",
            "Training Iteration 3006, Loss: 3.4226818084716797\n",
            "Training Iteration 3007, Loss: 6.353176593780518\n",
            "Training Iteration 3008, Loss: 3.6976194381713867\n",
            "Training Iteration 3009, Loss: 2.932692527770996\n",
            "Training Iteration 3010, Loss: 4.00282621383667\n",
            "Training Iteration 3011, Loss: 3.0222764015197754\n",
            "Training Iteration 3012, Loss: 3.704153537750244\n",
            "Training Iteration 3013, Loss: 2.4218642711639404\n",
            "Training Iteration 3014, Loss: 3.561619997024536\n",
            "Training Iteration 3015, Loss: 3.93668794631958\n",
            "Training Iteration 3016, Loss: 2.7269177436828613\n",
            "Training Iteration 3017, Loss: 4.84234094619751\n",
            "Training Iteration 3018, Loss: 4.532764911651611\n",
            "Training Iteration 3019, Loss: 2.5311026573181152\n",
            "Training Iteration 3020, Loss: 2.2688705921173096\n",
            "Training Iteration 3021, Loss: 5.051380157470703\n",
            "Training Iteration 3022, Loss: 3.297483205795288\n",
            "Training Iteration 3023, Loss: 4.226545333862305\n",
            "Training Iteration 3024, Loss: 5.5450310707092285\n",
            "Training Iteration 3025, Loss: 4.4256591796875\n",
            "Training Iteration 3026, Loss: 3.0247950553894043\n",
            "Training Iteration 3027, Loss: 5.351433753967285\n",
            "Training Iteration 3028, Loss: 4.837022304534912\n",
            "Training Iteration 3029, Loss: 3.81701922416687\n",
            "Training Iteration 3030, Loss: 5.2548909187316895\n",
            "Training Iteration 3031, Loss: 5.571300983428955\n",
            "Training Iteration 3032, Loss: 4.923275947570801\n",
            "Training Iteration 3033, Loss: 5.388632774353027\n",
            "Training Iteration 3034, Loss: 1.0047580003738403\n",
            "Training Iteration 3035, Loss: 4.608908176422119\n",
            "Training Iteration 3036, Loss: 5.8266191482543945\n",
            "Training Iteration 3037, Loss: 3.3607475757598877\n",
            "Training Iteration 3038, Loss: 3.9188644886016846\n",
            "Training Iteration 3039, Loss: 4.284455299377441\n",
            "Training Iteration 3040, Loss: 4.911479473114014\n",
            "Training Iteration 3041, Loss: 3.6206564903259277\n",
            "Training Iteration 3042, Loss: 2.9461312294006348\n",
            "Training Iteration 3043, Loss: 5.038480281829834\n",
            "Training Iteration 3044, Loss: 5.943021774291992\n",
            "Training Iteration 3045, Loss: 3.660787343978882\n",
            "Training Iteration 3046, Loss: 5.804537296295166\n",
            "Training Iteration 3047, Loss: 6.05632209777832\n",
            "Training Iteration 3048, Loss: 4.535738468170166\n",
            "Training Iteration 3049, Loss: 4.947862148284912\n",
            "Training Iteration 3050, Loss: 4.465649604797363\n",
            "Training Iteration 3051, Loss: 7.538944721221924\n",
            "Training Iteration 3052, Loss: 8.317441940307617\n",
            "Training Iteration 3053, Loss: 10.929771423339844\n",
            "Training Iteration 3054, Loss: 3.4963231086730957\n",
            "Training Iteration 3055, Loss: 3.992377281188965\n",
            "Training Iteration 3056, Loss: 8.363945007324219\n",
            "Training Iteration 3057, Loss: 5.429505348205566\n",
            "Training Iteration 3058, Loss: 3.3020150661468506\n",
            "Training Iteration 3059, Loss: 4.5496649742126465\n",
            "Training Iteration 3060, Loss: 3.806182384490967\n",
            "Training Iteration 3061, Loss: 10.957159042358398\n",
            "Training Iteration 3062, Loss: 5.774324417114258\n",
            "Training Iteration 3063, Loss: 5.334052562713623\n",
            "Training Iteration 3064, Loss: 6.746415615081787\n",
            "Training Iteration 3065, Loss: 7.622694492340088\n",
            "Training Iteration 3066, Loss: 3.103235960006714\n",
            "Training Iteration 3067, Loss: 6.7505340576171875\n",
            "Training Iteration 3068, Loss: 6.682979106903076\n",
            "Training Iteration 3069, Loss: 4.50302267074585\n",
            "Training Iteration 3070, Loss: 5.058069705963135\n",
            "Training Iteration 3071, Loss: 6.685749530792236\n",
            "Training Iteration 3072, Loss: 4.047588348388672\n",
            "Training Iteration 3073, Loss: 1.7448997497558594\n",
            "Training Iteration 3074, Loss: 3.505105495452881\n",
            "Training Iteration 3075, Loss: 3.795975923538208\n",
            "Training Iteration 3076, Loss: 2.3781471252441406\n",
            "Training Iteration 3077, Loss: 9.185526847839355\n",
            "Training Iteration 3078, Loss: 3.319535970687866\n",
            "Training Iteration 3079, Loss: 8.375711441040039\n",
            "Training Iteration 3080, Loss: 7.157537937164307\n",
            "Training Iteration 3081, Loss: 5.63861608505249\n",
            "Training Iteration 3082, Loss: 8.768506050109863\n",
            "Training Iteration 3083, Loss: 5.253927230834961\n",
            "Training Iteration 3084, Loss: 5.656979084014893\n",
            "Training Iteration 3085, Loss: 5.218985080718994\n",
            "Training Iteration 3086, Loss: 5.423300743103027\n",
            "Training Iteration 3087, Loss: 4.223351955413818\n",
            "Training Iteration 3088, Loss: 4.104898452758789\n",
            "Training Iteration 3089, Loss: 4.179567813873291\n",
            "Training Iteration 3090, Loss: 4.046416282653809\n",
            "Training Iteration 3091, Loss: 4.688882350921631\n",
            "Training Iteration 3092, Loss: 3.5499887466430664\n",
            "Training Iteration 3093, Loss: 5.1455535888671875\n",
            "Training Iteration 3094, Loss: 2.2387588024139404\n",
            "Training Iteration 3095, Loss: 6.3916754722595215\n",
            "Training Iteration 3096, Loss: 2.798332929611206\n",
            "Training Iteration 3097, Loss: 8.340169906616211\n",
            "Training Iteration 3098, Loss: 6.181267261505127\n",
            "Training Iteration 3099, Loss: 3.947692394256592\n",
            "Training Iteration 3100, Loss: 3.8242290019989014\n",
            "Training Iteration 3101, Loss: 4.7244462966918945\n",
            "Training Iteration 3102, Loss: 3.827995777130127\n",
            "Training Iteration 3103, Loss: 5.396417140960693\n",
            "Training Iteration 3104, Loss: 3.857534885406494\n",
            "Training Iteration 3105, Loss: 6.143362998962402\n",
            "Training Iteration 3106, Loss: 4.75147819519043\n",
            "Training Iteration 3107, Loss: 4.36572265625\n",
            "Training Iteration 3108, Loss: 4.360421657562256\n",
            "Training Iteration 3109, Loss: 3.736400604248047\n",
            "Training Iteration 3110, Loss: 3.1285324096679688\n",
            "Training Iteration 3111, Loss: 4.63759708404541\n",
            "Training Iteration 3112, Loss: 5.486202716827393\n",
            "Training Iteration 3113, Loss: 4.625453472137451\n",
            "Training Iteration 3114, Loss: 3.141251802444458\n",
            "Training Iteration 3115, Loss: 6.538073539733887\n",
            "Training Iteration 3116, Loss: 6.171689510345459\n",
            "Training Iteration 3117, Loss: 5.26820182800293\n",
            "Training Iteration 3118, Loss: 4.503589630126953\n",
            "Training Iteration 3119, Loss: 5.150149345397949\n",
            "Training Iteration 3120, Loss: 3.679651975631714\n",
            "Training Iteration 3121, Loss: 6.123231887817383\n",
            "Training Iteration 3122, Loss: 5.01979923248291\n",
            "Training Iteration 3123, Loss: 5.580414295196533\n",
            "Training Iteration 3124, Loss: 3.681729316711426\n",
            "Training Iteration 3125, Loss: 4.18968391418457\n",
            "Training Iteration 3126, Loss: 6.483257293701172\n",
            "Training Iteration 3127, Loss: 5.860222816467285\n",
            "Training Iteration 3128, Loss: 6.744524955749512\n",
            "Training Iteration 3129, Loss: 4.492806911468506\n",
            "Training Iteration 3130, Loss: 7.599354267120361\n",
            "Training Iteration 3131, Loss: 4.451695442199707\n",
            "Training Iteration 3132, Loss: 8.30533504486084\n",
            "Training Iteration 3133, Loss: 7.062138080596924\n",
            "Training Iteration 3134, Loss: 6.458806037902832\n",
            "Training Iteration 3135, Loss: 1.2956279516220093\n",
            "Training Iteration 3136, Loss: 6.904957294464111\n",
            "Training Iteration 3137, Loss: 2.3319389820098877\n",
            "Training Iteration 3138, Loss: 3.0942790508270264\n",
            "Training Iteration 3139, Loss: 3.9621641635894775\n",
            "Training Iteration 3140, Loss: 6.890135288238525\n",
            "Training Iteration 3141, Loss: 6.712380409240723\n",
            "Training Iteration 3142, Loss: 6.502686500549316\n",
            "Training Iteration 3143, Loss: 4.173859596252441\n",
            "Training Iteration 3144, Loss: 3.4033968448638916\n",
            "Training Iteration 3145, Loss: 5.090115070343018\n",
            "Training Iteration 3146, Loss: 3.564159870147705\n",
            "Training Iteration 3147, Loss: 5.650693893432617\n",
            "Training Iteration 3148, Loss: 4.2997894287109375\n",
            "Training Iteration 3149, Loss: 4.418268203735352\n",
            "Training Iteration 3150, Loss: 2.938627004623413\n",
            "Training Iteration 3151, Loss: 5.0958733558654785\n",
            "Training Iteration 3152, Loss: 5.160675525665283\n",
            "Training Iteration 3153, Loss: 4.6067094802856445\n",
            "Training Iteration 3154, Loss: 2.1077115535736084\n",
            "Training Iteration 3155, Loss: 4.932616710662842\n",
            "Training Iteration 3156, Loss: 4.405884265899658\n",
            "Training Iteration 3157, Loss: 4.673914909362793\n",
            "Training Iteration 3158, Loss: 5.119564056396484\n",
            "Training Iteration 3159, Loss: 3.0018470287323\n",
            "Training Iteration 3160, Loss: 5.757138252258301\n",
            "Training Iteration 3161, Loss: 4.638766765594482\n",
            "Training Iteration 3162, Loss: 7.188216686248779\n",
            "Training Iteration 3163, Loss: 5.202592372894287\n",
            "Training Iteration 3164, Loss: 6.287621974945068\n",
            "Training Iteration 3165, Loss: 4.290767669677734\n",
            "Training Iteration 3166, Loss: 5.176943302154541\n",
            "Training Iteration 3167, Loss: 2.843249797821045\n",
            "Training Iteration 3168, Loss: 4.90286922454834\n",
            "Training Iteration 3169, Loss: 3.282576322555542\n",
            "Training Iteration 3170, Loss: 7.343275547027588\n",
            "Training Iteration 3171, Loss: 4.932380676269531\n",
            "Training Iteration 3172, Loss: 5.351624011993408\n",
            "Training Iteration 3173, Loss: 3.5412373542785645\n",
            "Training Iteration 3174, Loss: 4.149281024932861\n",
            "Training Iteration 3175, Loss: 3.8729875087738037\n",
            "Training Iteration 3176, Loss: 2.302880048751831\n",
            "Training Iteration 3177, Loss: 5.021261215209961\n",
            "Training Iteration 3178, Loss: 2.4875917434692383\n",
            "Training Iteration 3179, Loss: 4.767340183258057\n",
            "Training Iteration 3180, Loss: 2.8744707107543945\n",
            "Training Iteration 3181, Loss: 3.5573415756225586\n",
            "Training Iteration 3182, Loss: 1.9491112232208252\n",
            "Training Iteration 3183, Loss: 3.1018614768981934\n",
            "Training Iteration 3184, Loss: 3.5915327072143555\n",
            "Training Iteration 3185, Loss: 5.090945243835449\n",
            "Training Iteration 3186, Loss: 2.9742884635925293\n",
            "Training Iteration 3187, Loss: 3.179281711578369\n",
            "Training Iteration 3188, Loss: 3.9355292320251465\n",
            "Training Iteration 3189, Loss: 7.846096515655518\n",
            "Training Iteration 3190, Loss: 3.8045787811279297\n",
            "Training Iteration 3191, Loss: 3.7307181358337402\n",
            "Training Iteration 3192, Loss: 2.694230318069458\n",
            "Training Iteration 3193, Loss: 4.379840850830078\n",
            "Training Iteration 3194, Loss: 6.748173713684082\n",
            "Training Iteration 3195, Loss: 4.699467658996582\n",
            "Training Iteration 3196, Loss: 1.4881742000579834\n",
            "Training Iteration 3197, Loss: 5.741013526916504\n",
            "Training Iteration 3198, Loss: 2.4686577320098877\n",
            "Training Iteration 3199, Loss: 4.736196041107178\n",
            "Training Iteration 3200, Loss: 2.9868061542510986\n",
            "Training Iteration 3201, Loss: 3.500849723815918\n",
            "Training Iteration 3202, Loss: 7.139708042144775\n",
            "Training Iteration 3203, Loss: 4.088671684265137\n",
            "Training Iteration 3204, Loss: 2.98459529876709\n",
            "Training Iteration 3205, Loss: 3.519007921218872\n",
            "Training Iteration 3206, Loss: 4.453947067260742\n",
            "Training Iteration 3207, Loss: 3.3722691535949707\n",
            "Training Iteration 3208, Loss: 6.096433162689209\n",
            "Training Iteration 3209, Loss: 5.0290093421936035\n",
            "Training Iteration 3210, Loss: 3.431788682937622\n",
            "Training Iteration 3211, Loss: 6.515319347381592\n",
            "Training Iteration 3212, Loss: 5.121192455291748\n",
            "Training Iteration 3213, Loss: 4.549057960510254\n",
            "Training Iteration 3214, Loss: 5.307922840118408\n",
            "Training Iteration 3215, Loss: 3.7388033866882324\n",
            "Training Iteration 3216, Loss: 7.257617950439453\n",
            "Training Iteration 3217, Loss: 0.9176454544067383\n",
            "Training Iteration 3218, Loss: 5.087986469268799\n",
            "Training Iteration 3219, Loss: 6.678997993469238\n",
            "Training Iteration 3220, Loss: 3.765695810317993\n",
            "Training Iteration 3221, Loss: 3.1341280937194824\n",
            "Training Iteration 3222, Loss: 4.363481521606445\n",
            "Training Iteration 3223, Loss: 3.9221930503845215\n",
            "Training Iteration 3224, Loss: 3.407463312149048\n",
            "Training Iteration 3225, Loss: 3.799393892288208\n",
            "Training Iteration 3226, Loss: 3.76922607421875\n",
            "Training Iteration 3227, Loss: 3.7736401557922363\n",
            "Training Iteration 3228, Loss: 2.3666553497314453\n",
            "Training Iteration 3229, Loss: 1.783546805381775\n",
            "Training Iteration 3230, Loss: 5.9003753662109375\n",
            "Training Iteration 3231, Loss: 4.3199567794799805\n",
            "Training Iteration 3232, Loss: 4.0720534324646\n",
            "Training Iteration 3233, Loss: 1.597482442855835\n",
            "Training Iteration 3234, Loss: 3.436642646789551\n",
            "Training Iteration 3235, Loss: 5.6175360679626465\n",
            "Training Iteration 3236, Loss: 6.308060169219971\n",
            "Training Iteration 3237, Loss: 3.8952929973602295\n",
            "Training Iteration 3238, Loss: 1.7542071342468262\n",
            "Training Iteration 3239, Loss: 2.8484039306640625\n",
            "Training Iteration 3240, Loss: 3.92162823677063\n",
            "Training Iteration 3241, Loss: 3.9454166889190674\n",
            "Training Iteration 3242, Loss: 5.020473480224609\n",
            "Training Iteration 3243, Loss: 3.9276785850524902\n",
            "Training Iteration 3244, Loss: 5.841522216796875\n",
            "Training Iteration 3245, Loss: 4.443655014038086\n",
            "Training Iteration 3246, Loss: 2.6861586570739746\n",
            "Training Iteration 3247, Loss: 9.20286750793457\n",
            "Training Iteration 3248, Loss: 5.6194610595703125\n",
            "Training Iteration 3249, Loss: 2.855958938598633\n",
            "Training Iteration 3250, Loss: 3.099705457687378\n",
            "Training Iteration 3251, Loss: 4.673100471496582\n",
            "Training Iteration 3252, Loss: 4.372872829437256\n",
            "Training Iteration 3253, Loss: 5.321115016937256\n",
            "Training Iteration 3254, Loss: 3.657780170440674\n",
            "Training Iteration 3255, Loss: 1.9084062576293945\n",
            "Training Iteration 3256, Loss: 4.980629920959473\n",
            "Training Iteration 3257, Loss: 3.2746264934539795\n",
            "Training Iteration 3258, Loss: 4.583017826080322\n",
            "Training Iteration 3259, Loss: 2.893643379211426\n",
            "Training Iteration 3260, Loss: 6.896050453186035\n",
            "Training Iteration 3261, Loss: 4.961663722991943\n",
            "Training Iteration 3262, Loss: 2.636662006378174\n",
            "Training Iteration 3263, Loss: 3.122615098953247\n",
            "Training Iteration 3264, Loss: 3.531778335571289\n",
            "Training Iteration 3265, Loss: 4.170709133148193\n",
            "Training Iteration 3266, Loss: 3.1434004306793213\n",
            "Training Iteration 3267, Loss: 7.52635383605957\n",
            "Training Iteration 3268, Loss: 3.48201322555542\n",
            "Training Iteration 3269, Loss: 5.805693626403809\n",
            "Training Iteration 3270, Loss: 3.011751413345337\n",
            "Training Iteration 3271, Loss: 5.782480716705322\n",
            "Training Iteration 3272, Loss: 4.155058860778809\n",
            "Training Iteration 3273, Loss: 3.5027987957000732\n",
            "Training Iteration 3274, Loss: 1.7759432792663574\n",
            "Training Iteration 3275, Loss: 5.721583366394043\n",
            "Training Iteration 3276, Loss: 4.157368183135986\n",
            "Training Iteration 3277, Loss: 4.244977951049805\n",
            "Training Iteration 3278, Loss: 6.310627460479736\n",
            "Training Iteration 3279, Loss: 4.067453861236572\n",
            "Training Iteration 3280, Loss: 2.085296869277954\n",
            "Training Iteration 3281, Loss: 7.602866172790527\n",
            "Training Iteration 3282, Loss: 7.118313312530518\n",
            "Training Iteration 3283, Loss: 2.617973804473877\n",
            "Training Iteration 3284, Loss: 6.087472915649414\n",
            "Training Iteration 3285, Loss: 5.382219314575195\n",
            "Training Iteration 3286, Loss: 6.83040714263916\n",
            "Training Iteration 3287, Loss: 5.34505033493042\n",
            "Training Iteration 3288, Loss: 4.887943744659424\n",
            "Training Iteration 3289, Loss: 4.338809013366699\n",
            "Training Iteration 3290, Loss: 8.751632690429688\n",
            "Training Iteration 3291, Loss: 4.053382873535156\n",
            "Training Iteration 3292, Loss: 5.856412410736084\n",
            "Training Iteration 3293, Loss: 5.99454927444458\n",
            "Training Iteration 3294, Loss: 5.1123223304748535\n",
            "Training Iteration 3295, Loss: 6.080881595611572\n",
            "Training Iteration 3296, Loss: 4.885129928588867\n",
            "Training Iteration 3297, Loss: 2.6202051639556885\n",
            "Training Iteration 3298, Loss: 5.587742805480957\n",
            "Training Iteration 3299, Loss: 5.565576553344727\n",
            "Training Iteration 3300, Loss: 4.371805191040039\n",
            "Training Iteration 3301, Loss: 6.20864200592041\n",
            "Training Iteration 3302, Loss: 2.994504690170288\n",
            "Training Iteration 3303, Loss: 6.848507404327393\n",
            "Training Iteration 3304, Loss: 8.04620361328125\n",
            "Training Iteration 3305, Loss: 8.883829116821289\n",
            "Training Iteration 3306, Loss: 2.1474685668945312\n",
            "Training Iteration 3307, Loss: 5.4469733238220215\n",
            "Training Iteration 3308, Loss: 7.7717132568359375\n",
            "Training Iteration 3309, Loss: 3.9259443283081055\n",
            "Training Iteration 3310, Loss: 5.341716289520264\n",
            "Training Iteration 3311, Loss: 1.6535359621047974\n",
            "Training Iteration 3312, Loss: 3.0612034797668457\n",
            "Training Iteration 3313, Loss: 4.2490410804748535\n",
            "Training Iteration 3314, Loss: 4.025028705596924\n",
            "Training Iteration 3315, Loss: 5.607893466949463\n",
            "Training Iteration 3316, Loss: 4.891939163208008\n",
            "Training Iteration 3317, Loss: 4.277358531951904\n",
            "Training Iteration 3318, Loss: 4.550628185272217\n",
            "Training Iteration 3319, Loss: 2.984874963760376\n",
            "Training Iteration 3320, Loss: 3.634695291519165\n",
            "Training Iteration 3321, Loss: 6.421032428741455\n",
            "Training Iteration 3322, Loss: 4.369022369384766\n",
            "Training Iteration 3323, Loss: 3.0498735904693604\n",
            "Training Iteration 3324, Loss: 3.782031297683716\n",
            "Training Iteration 3325, Loss: 3.517099380493164\n",
            "Training Iteration 3326, Loss: 2.0722060203552246\n",
            "Training Iteration 3327, Loss: 7.207416534423828\n",
            "Training Iteration 3328, Loss: 5.447155475616455\n",
            "Training Iteration 3329, Loss: 5.166621208190918\n",
            "Training Iteration 3330, Loss: 4.870181083679199\n",
            "Training Iteration 3331, Loss: 2.8176445960998535\n",
            "Training Iteration 3332, Loss: 6.30841588973999\n",
            "Training Iteration 3333, Loss: 4.366880893707275\n",
            "Training Iteration 3334, Loss: 3.6463747024536133\n",
            "Training Iteration 3335, Loss: 6.932552814483643\n",
            "Training Iteration 3336, Loss: 5.0961689949035645\n",
            "Training Iteration 3337, Loss: 4.483615875244141\n",
            "Training Iteration 3338, Loss: 2.7790024280548096\n",
            "Training Iteration 3339, Loss: 4.9637770652771\n",
            "Training Iteration 3340, Loss: 7.881518363952637\n",
            "Training Iteration 3341, Loss: 3.460700035095215\n",
            "Training Iteration 3342, Loss: 3.2680513858795166\n",
            "Training Iteration 3343, Loss: 3.5315098762512207\n",
            "Training Iteration 3344, Loss: 2.8570494651794434\n",
            "Training Iteration 3345, Loss: 1.5847793817520142\n",
            "Training Iteration 3346, Loss: 6.229851722717285\n",
            "Training Iteration 3347, Loss: 2.8944664001464844\n",
            "Training Iteration 3348, Loss: 2.810957193374634\n",
            "Training Iteration 3349, Loss: 3.97795033454895\n",
            "Training Iteration 3350, Loss: 1.8488967418670654\n",
            "Training Iteration 3351, Loss: 4.470925807952881\n",
            "Training Iteration 3352, Loss: 4.87325382232666\n",
            "Training Iteration 3353, Loss: 6.1756815910339355\n",
            "Training Iteration 3354, Loss: 4.764650344848633\n",
            "Training Iteration 3355, Loss: 2.8118486404418945\n",
            "Training Iteration 3356, Loss: 1.7015151977539062\n",
            "Training Iteration 3357, Loss: 1.8361411094665527\n",
            "Training Iteration 3358, Loss: 4.321583271026611\n",
            "Training Iteration 3359, Loss: 1.4255074262619019\n",
            "Training Iteration 3360, Loss: 4.124245643615723\n",
            "Training Iteration 3361, Loss: 5.515389442443848\n",
            "Training Iteration 3362, Loss: 3.588566780090332\n",
            "Training Iteration 3363, Loss: 3.356032609939575\n",
            "Training Iteration 3364, Loss: 3.2831573486328125\n",
            "Training Iteration 3365, Loss: 3.128722906112671\n",
            "Training Iteration 3366, Loss: 5.1249003410339355\n",
            "Training Iteration 3367, Loss: 2.402175188064575\n",
            "Training Iteration 3368, Loss: 6.914730072021484\n",
            "Training Iteration 3369, Loss: 3.3967742919921875\n",
            "Training Iteration 3370, Loss: 5.265893459320068\n",
            "Training Iteration 3371, Loss: 3.937471866607666\n",
            "Training Iteration 3372, Loss: 4.954320430755615\n",
            "Training Iteration 3373, Loss: 2.9586808681488037\n",
            "Training Iteration 3374, Loss: 3.351372718811035\n",
            "Training Iteration 3375, Loss: 4.579346656799316\n",
            "Training Iteration 3376, Loss: 2.4148459434509277\n",
            "Training Iteration 3377, Loss: 3.148071527481079\n",
            "Training Iteration 3378, Loss: 3.470047950744629\n",
            "Training Iteration 3379, Loss: 1.5972232818603516\n",
            "Training Iteration 3380, Loss: 5.035262584686279\n",
            "Training Iteration 3381, Loss: 3.9639594554901123\n",
            "Training Iteration 3382, Loss: 3.642927885055542\n",
            "Training Iteration 3383, Loss: 3.7039926052093506\n",
            "Training Iteration 3384, Loss: 2.0064077377319336\n",
            "Training Iteration 3385, Loss: 3.2327873706817627\n",
            "Training Iteration 3386, Loss: 4.3882951736450195\n",
            "Training Iteration 3387, Loss: 5.029634475708008\n",
            "Training Iteration 3388, Loss: 6.828880786895752\n",
            "Training Iteration 3389, Loss: 3.2252278327941895\n",
            "Training Iteration 3390, Loss: 3.7379348278045654\n",
            "Training Iteration 3391, Loss: 4.298323631286621\n",
            "Training Iteration 3392, Loss: 5.3863396644592285\n",
            "Training Iteration 3393, Loss: 3.271836757659912\n",
            "Training Iteration 3394, Loss: 4.662973403930664\n",
            "Training Iteration 3395, Loss: 6.296510219573975\n",
            "Training Iteration 3396, Loss: 3.718381881713867\n",
            "Training Iteration 3397, Loss: 4.340543746948242\n",
            "Training Iteration 3398, Loss: 4.914392948150635\n",
            "Training Iteration 3399, Loss: 5.12883186340332\n",
            "Training Iteration 3400, Loss: 4.660288333892822\n",
            "Training Iteration 3401, Loss: 2.7700324058532715\n",
            "Training Iteration 3402, Loss: 5.238497734069824\n",
            "Training Iteration 3403, Loss: 3.197401523590088\n",
            "Training Iteration 3404, Loss: 5.93637752532959\n",
            "Training Iteration 3405, Loss: 6.078205585479736\n",
            "Training Iteration 3406, Loss: 6.786736488342285\n",
            "Training Iteration 3407, Loss: 7.3772053718566895\n",
            "Training Iteration 3408, Loss: 7.043814659118652\n",
            "Training Iteration 3409, Loss: 2.1921868324279785\n",
            "Training Iteration 3410, Loss: 4.492304801940918\n",
            "Training Iteration 3411, Loss: 4.293632507324219\n",
            "Training Iteration 3412, Loss: 3.8168234825134277\n",
            "Training Iteration 3413, Loss: 3.0812125205993652\n",
            "Training Iteration 3414, Loss: 2.2265655994415283\n",
            "Training Iteration 3415, Loss: 2.4671711921691895\n",
            "Training Iteration 3416, Loss: 4.399655818939209\n",
            "Training Iteration 3417, Loss: 5.1606764793396\n",
            "Training Iteration 3418, Loss: 3.895307779312134\n",
            "Training Iteration 3419, Loss: 4.347005844116211\n",
            "Training Iteration 3420, Loss: 3.9479589462280273\n",
            "Training Iteration 3421, Loss: 5.381394386291504\n",
            "Training Iteration 3422, Loss: 3.766378879547119\n",
            "Training Iteration 3423, Loss: 4.968959331512451\n",
            "Training Iteration 3424, Loss: 4.151131629943848\n",
            "Training Iteration 3425, Loss: 2.8028757572174072\n",
            "Training Iteration 3426, Loss: 2.2777633666992188\n",
            "Training Iteration 3427, Loss: 3.854544162750244\n",
            "Training Iteration 3428, Loss: 7.9425835609436035\n",
            "Training Iteration 3429, Loss: 7.522200584411621\n",
            "Training Iteration 3430, Loss: 8.011783599853516\n",
            "Training Iteration 3431, Loss: 4.2720842361450195\n",
            "Training Iteration 3432, Loss: 3.0567169189453125\n",
            "Training Iteration 3433, Loss: 4.5354790687561035\n",
            "Training Iteration 3434, Loss: 3.611248254776001\n",
            "Training Iteration 3435, Loss: 4.399447441101074\n",
            "Training Iteration 3436, Loss: 7.2891340255737305\n",
            "Training Iteration 3437, Loss: 5.323209285736084\n",
            "Training Iteration 3438, Loss: 5.698663234710693\n",
            "Training Iteration 3439, Loss: 6.283940315246582\n",
            "Training Iteration 3440, Loss: 5.972132205963135\n",
            "Training Iteration 3441, Loss: 5.120373725891113\n",
            "Training Iteration 3442, Loss: 5.630514144897461\n",
            "Training Iteration 3443, Loss: 5.100716590881348\n",
            "Training Iteration 3444, Loss: 6.973584175109863\n",
            "Training Iteration 3445, Loss: 7.527924060821533\n",
            "Training Iteration 3446, Loss: 7.453068256378174\n",
            "Training Iteration 3447, Loss: 7.4463324546813965\n",
            "Training Iteration 3448, Loss: 9.670279502868652\n",
            "Training Iteration 3449, Loss: 3.46172833442688\n",
            "Training Iteration 3450, Loss: 3.595684051513672\n",
            "Training Iteration 3451, Loss: 8.864139556884766\n",
            "Training Iteration 3452, Loss: 4.6938323974609375\n",
            "Training Iteration 3453, Loss: 5.224512100219727\n",
            "Training Iteration 3454, Loss: 6.041163921356201\n",
            "Training Iteration 3455, Loss: 6.082029819488525\n",
            "Training Iteration 3456, Loss: 4.102725505828857\n",
            "Training Iteration 3457, Loss: 9.982274055480957\n",
            "Training Iteration 3458, Loss: 3.1773037910461426\n",
            "Training Iteration 3459, Loss: 6.056533336639404\n",
            "Training Iteration 3460, Loss: 13.178448677062988\n",
            "Training Iteration 3461, Loss: 5.490340232849121\n",
            "Training Iteration 3462, Loss: 3.985475778579712\n",
            "Training Iteration 3463, Loss: 2.8518526554107666\n",
            "Training Iteration 3464, Loss: 3.515120029449463\n",
            "Training Iteration 3465, Loss: 1.3353445529937744\n",
            "Training Iteration 3466, Loss: 3.8519792556762695\n",
            "Training Iteration 3467, Loss: 3.027686595916748\n",
            "Training Iteration 3468, Loss: 7.342990875244141\n",
            "Training Iteration 3469, Loss: 6.222249507904053\n",
            "Training Iteration 3470, Loss: 4.8193440437316895\n",
            "Training Iteration 3471, Loss: 4.693401336669922\n",
            "Training Iteration 3472, Loss: 5.634695053100586\n",
            "Training Iteration 3473, Loss: 14.609978675842285\n",
            "Training Iteration 3474, Loss: 1.9162906408309937\n",
            "Training Iteration 3475, Loss: 6.491730690002441\n",
            "Training Iteration 3476, Loss: 8.474081039428711\n",
            "Training Iteration 3477, Loss: 5.3957695960998535\n",
            "Training Iteration 3478, Loss: 0.9746034741401672\n",
            "Training Iteration 3479, Loss: 3.6607322692871094\n",
            "Training Iteration 3480, Loss: 3.2931437492370605\n",
            "Training Iteration 3481, Loss: 9.804742813110352\n",
            "Training Iteration 3482, Loss: 4.156759262084961\n",
            "Training Iteration 3483, Loss: 5.769612789154053\n",
            "Training Iteration 3484, Loss: 3.000271797180176\n",
            "Training Iteration 3485, Loss: 11.038176536560059\n",
            "Training Iteration 3486, Loss: 5.5103912353515625\n",
            "Training Iteration 3487, Loss: 3.1994123458862305\n",
            "Training Iteration 3488, Loss: 2.881619930267334\n",
            "Training Iteration 3489, Loss: 2.630402088165283\n",
            "Training Iteration 3490, Loss: 6.019089698791504\n",
            "Training Iteration 3491, Loss: 3.4613795280456543\n",
            "Training Iteration 3492, Loss: 5.208630561828613\n",
            "Training Iteration 3493, Loss: 5.4889960289001465\n",
            "Training Iteration 3494, Loss: 6.57617712020874\n",
            "Training Iteration 3495, Loss: 4.8383564949035645\n",
            "Training Iteration 3496, Loss: 3.712352991104126\n",
            "Training Iteration 3497, Loss: 3.1816296577453613\n",
            "Training Iteration 3498, Loss: 2.3879189491271973\n",
            "Training Iteration 3499, Loss: 4.322556018829346\n",
            "Training Iteration 3500, Loss: 3.5594871044158936\n",
            "Training Iteration 3501, Loss: 4.220195770263672\n",
            "Training Iteration 3502, Loss: 5.502470016479492\n",
            "Training Iteration 3503, Loss: 5.306917667388916\n",
            "Training Iteration 3504, Loss: 4.858192443847656\n",
            "Training Iteration 3505, Loss: 1.6388787031173706\n",
            "Training Iteration 3506, Loss: 4.630443572998047\n",
            "Training Iteration 3507, Loss: 3.30564546585083\n",
            "Training Iteration 3508, Loss: 8.978693962097168\n",
            "Training Iteration 3509, Loss: 3.4231150150299072\n",
            "Training Iteration 3510, Loss: 8.86308479309082\n",
            "Training Iteration 3511, Loss: 3.0370969772338867\n",
            "Training Iteration 3512, Loss: 5.833945274353027\n",
            "Training Iteration 3513, Loss: 7.933111190795898\n",
            "Training Iteration 3514, Loss: 6.953399181365967\n",
            "Training Iteration 3515, Loss: 1.169183611869812\n",
            "Training Iteration 3516, Loss: 3.8605966567993164\n",
            "Training Iteration 3517, Loss: 5.731673240661621\n",
            "Training Iteration 3518, Loss: 4.6901702880859375\n",
            "Training Iteration 3519, Loss: 5.618647575378418\n",
            "Training Iteration 3520, Loss: 7.225615501403809\n",
            "Training Iteration 3521, Loss: 7.337122917175293\n",
            "Training Iteration 3522, Loss: 4.922624588012695\n",
            "Training Iteration 3523, Loss: 1.4150309562683105\n",
            "Training Iteration 3524, Loss: 4.307107925415039\n",
            "Training Iteration 3525, Loss: 2.280017614364624\n",
            "Training Iteration 3526, Loss: 5.177356243133545\n",
            "Training Iteration 3527, Loss: 6.2264084815979\n",
            "Training Iteration 3528, Loss: 5.9083662033081055\n",
            "Training Iteration 3529, Loss: 9.870471000671387\n",
            "Training Iteration 3530, Loss: 6.898040294647217\n",
            "Training Iteration 3531, Loss: 3.624455690383911\n",
            "Training Iteration 3532, Loss: 6.600803852081299\n",
            "Training Iteration 3533, Loss: 5.091508865356445\n",
            "Training Iteration 3534, Loss: 7.673341751098633\n",
            "Training Iteration 3535, Loss: 1.901168704032898\n",
            "Training Iteration 3536, Loss: 5.8063435554504395\n",
            "Training Iteration 3537, Loss: 4.480678558349609\n",
            "Training Iteration 3538, Loss: 3.6534476280212402\n",
            "Training Iteration 3539, Loss: 4.378410816192627\n",
            "Training Iteration 3540, Loss: 4.0281500816345215\n",
            "Training Iteration 3541, Loss: 3.0380377769470215\n",
            "Training Iteration 3542, Loss: 5.055374622344971\n",
            "Training Iteration 3543, Loss: 3.290546417236328\n",
            "Training Iteration 3544, Loss: 5.596696853637695\n",
            "Training Iteration 3545, Loss: 5.059725761413574\n",
            "Training Iteration 3546, Loss: 4.833489894866943\n",
            "Training Iteration 3547, Loss: 5.862442970275879\n",
            "Training Iteration 3548, Loss: 4.865756034851074\n",
            "Training Iteration 3549, Loss: 6.705950736999512\n",
            "Training Iteration 3550, Loss: 2.3969500064849854\n",
            "Training Iteration 3551, Loss: 6.778722763061523\n",
            "Training Iteration 3552, Loss: 5.250306606292725\n",
            "Training Iteration 3553, Loss: 2.8468432426452637\n",
            "Training Iteration 3554, Loss: 6.007632255554199\n",
            "Training Iteration 3555, Loss: 3.3052115440368652\n",
            "Training Iteration 3556, Loss: 4.712151050567627\n",
            "Training Iteration 3557, Loss: 1.459816813468933\n",
            "Training Iteration 3558, Loss: 5.784210205078125\n",
            "Training Iteration 3559, Loss: 4.163825988769531\n",
            "Training Iteration 3560, Loss: 4.1103739738464355\n",
            "Training Iteration 3561, Loss: 4.35270357131958\n",
            "Training Iteration 3562, Loss: 2.796736001968384\n",
            "Training Iteration 3563, Loss: 3.2663023471832275\n",
            "Training Iteration 3564, Loss: 2.901095390319824\n",
            "Training Iteration 3565, Loss: 2.44757342338562\n",
            "Training Iteration 3566, Loss: 7.4677042961120605\n",
            "Training Iteration 3567, Loss: 4.302973747253418\n",
            "Training Iteration 3568, Loss: 2.0264158248901367\n",
            "Training Iteration 3569, Loss: 4.042183876037598\n",
            "Training Iteration 3570, Loss: 3.431713104248047\n",
            "Training Iteration 3571, Loss: 2.930558681488037\n",
            "Training Iteration 3572, Loss: 1.4862644672393799\n",
            "Training Iteration 3573, Loss: 1.6002662181854248\n",
            "Training Iteration 3574, Loss: 2.4122745990753174\n",
            "Training Iteration 3575, Loss: 5.230535984039307\n",
            "Training Iteration 3576, Loss: 4.560516834259033\n",
            "Training Iteration 3577, Loss: 3.1415822505950928\n",
            "Training Iteration 3578, Loss: 6.976162433624268\n",
            "Training Iteration 3579, Loss: 7.191795349121094\n",
            "Training Iteration 3580, Loss: 2.729635000228882\n",
            "Training Iteration 3581, Loss: 2.295135259628296\n",
            "Training Iteration 3582, Loss: 2.544945240020752\n",
            "Training Iteration 3583, Loss: 4.380437850952148\n",
            "Training Iteration 3584, Loss: 4.54563045501709\n",
            "Training Iteration 3585, Loss: 3.1453194618225098\n",
            "Training Iteration 3586, Loss: 4.482531547546387\n",
            "Training Iteration 3587, Loss: 3.974738836288452\n",
            "Training Iteration 3588, Loss: 2.9705402851104736\n",
            "Training Iteration 3589, Loss: 4.94893217086792\n",
            "Training Iteration 3590, Loss: 7.015296936035156\n",
            "Training Iteration 3591, Loss: 5.3394389152526855\n",
            "Training Iteration 3592, Loss: 4.230246067047119\n",
            "Training Iteration 3593, Loss: 2.2959485054016113\n",
            "Training Iteration 3594, Loss: 9.126167297363281\n",
            "Training Iteration 3595, Loss: 5.6563873291015625\n",
            "Training Iteration 3596, Loss: 6.514389991760254\n",
            "Training Iteration 3597, Loss: 4.715900421142578\n",
            "Training Iteration 3598, Loss: 2.0632708072662354\n",
            "Training Iteration 3599, Loss: 3.9920804500579834\n",
            "Training Iteration 3600, Loss: 5.925588130950928\n",
            "Training Iteration 3601, Loss: 6.919217109680176\n",
            "Training Iteration 3602, Loss: 6.915313720703125\n",
            "Training Iteration 3603, Loss: 6.304119110107422\n",
            "Training Iteration 3604, Loss: 6.927056789398193\n",
            "Training Iteration 3605, Loss: 7.574481010437012\n",
            "Training Iteration 3606, Loss: 4.509259223937988\n",
            "Training Iteration 3607, Loss: 4.343729496002197\n",
            "Training Iteration 3608, Loss: 6.76065731048584\n",
            "Training Iteration 3609, Loss: 3.4399895668029785\n",
            "Training Iteration 3610, Loss: 2.4866228103637695\n",
            "Training Iteration 3611, Loss: 3.025160074234009\n",
            "Training Iteration 3612, Loss: 3.0259952545166016\n",
            "Training Iteration 3613, Loss: 3.719370126724243\n",
            "Training Iteration 3614, Loss: 2.9708213806152344\n",
            "Training Iteration 3615, Loss: 2.7643895149230957\n",
            "Training Iteration 3616, Loss: 3.5755488872528076\n",
            "Training Iteration 3617, Loss: 6.374435901641846\n",
            "Training Iteration 3618, Loss: 5.634675979614258\n",
            "Training Iteration 3619, Loss: 4.430938720703125\n",
            "Training Iteration 3620, Loss: 4.214501857757568\n",
            "Training Iteration 3621, Loss: 5.466739177703857\n",
            "Training Iteration 3622, Loss: 3.45709490776062\n",
            "Training Iteration 3623, Loss: 6.186493396759033\n",
            "Training Iteration 3624, Loss: 3.5515968799591064\n",
            "Training Iteration 3625, Loss: 5.133835315704346\n",
            "Training Iteration 3626, Loss: 5.195887565612793\n",
            "Training Iteration 3627, Loss: 4.975753307342529\n",
            "Training Iteration 3628, Loss: 7.927116394042969\n",
            "Training Iteration 3629, Loss: 4.93066930770874\n",
            "Training Iteration 3630, Loss: 6.292100429534912\n",
            "Training Iteration 3631, Loss: 6.111210346221924\n",
            "Training Iteration 3632, Loss: 10.670106887817383\n",
            "Training Iteration 3633, Loss: 4.977561950683594\n",
            "Training Iteration 3634, Loss: 4.05463171005249\n",
            "Training Iteration 3635, Loss: 3.9100332260131836\n",
            "Training Iteration 3636, Loss: 3.102581262588501\n",
            "Training Iteration 3637, Loss: 2.4446358680725098\n",
            "Training Iteration 3638, Loss: 5.869493007659912\n",
            "Training Iteration 3639, Loss: 5.589093208312988\n",
            "Training Iteration 3640, Loss: 3.7960894107818604\n",
            "Training Iteration 3641, Loss: 3.3500001430511475\n",
            "Training Iteration 3642, Loss: 4.2308669090271\n",
            "Training Iteration 3643, Loss: 3.445566177368164\n",
            "Training Iteration 3644, Loss: 5.473254680633545\n",
            "Training Iteration 3645, Loss: 10.266231536865234\n",
            "Training Iteration 3646, Loss: 11.946380615234375\n",
            "Training Iteration 3647, Loss: 6.627071380615234\n",
            "Training Iteration 3648, Loss: 5.132741928100586\n",
            "Training Iteration 3649, Loss: 6.895163536071777\n",
            "Training Iteration 3650, Loss: 3.137869358062744\n",
            "Training Iteration 3651, Loss: 5.175731182098389\n",
            "Training Iteration 3652, Loss: 3.2313029766082764\n",
            "Training Iteration 3653, Loss: 3.294238805770874\n",
            "Training Iteration 3654, Loss: 9.831924438476562\n",
            "Training Iteration 3655, Loss: 3.1482789516448975\n",
            "Training Iteration 3656, Loss: 5.969070911407471\n",
            "Training Iteration 3657, Loss: 4.764627456665039\n",
            "Training Iteration 3658, Loss: 4.807351112365723\n",
            "Training Iteration 3659, Loss: 6.511136054992676\n",
            "Training Iteration 3660, Loss: 3.084216356277466\n",
            "Training Iteration 3661, Loss: 8.358274459838867\n",
            "Training Iteration 3662, Loss: 7.209716320037842\n",
            "Training Iteration 3663, Loss: 6.0355634689331055\n",
            "Training Iteration 3664, Loss: 7.376804351806641\n",
            "Training Iteration 3665, Loss: 3.227583885192871\n",
            "Training Iteration 3666, Loss: 6.766834735870361\n",
            "Training Iteration 3667, Loss: 4.558216094970703\n",
            "Training Iteration 3668, Loss: 3.4903762340545654\n",
            "Training Iteration 3669, Loss: 5.832942485809326\n",
            "Training Iteration 3670, Loss: 5.552137851715088\n",
            "Training Iteration 3671, Loss: 3.493720531463623\n",
            "Training Iteration 3672, Loss: 3.945327043533325\n",
            "Training Iteration 3673, Loss: 4.102268695831299\n",
            "Training Iteration 3674, Loss: 4.04192590713501\n",
            "Training Iteration 3675, Loss: 4.739561080932617\n",
            "Training Iteration 3676, Loss: 3.870112895965576\n",
            "Training Iteration 3677, Loss: 7.415007591247559\n",
            "Training Iteration 3678, Loss: 2.2588136196136475\n",
            "Training Iteration 3679, Loss: 5.397398948669434\n",
            "Training Iteration 3680, Loss: 3.7742762565612793\n",
            "Training Iteration 3681, Loss: 3.8778679370880127\n",
            "Training Iteration 3682, Loss: 4.647623062133789\n",
            "Training Iteration 3683, Loss: 2.884755849838257\n",
            "Training Iteration 3684, Loss: 3.959097385406494\n",
            "Training Iteration 3685, Loss: 5.262744903564453\n",
            "Training Iteration 3686, Loss: 7.547255992889404\n",
            "Training Iteration 3687, Loss: 4.9041666984558105\n",
            "Training Iteration 3688, Loss: 6.4481120109558105\n",
            "Training Iteration 3689, Loss: 3.9958748817443848\n",
            "Training Iteration 3690, Loss: 2.7255773544311523\n",
            "Training Iteration 3691, Loss: 7.707200050354004\n",
            "Training Iteration 3692, Loss: 5.768216609954834\n",
            "Training Iteration 3693, Loss: 5.571071147918701\n",
            "Training Iteration 3694, Loss: 8.387846946716309\n",
            "Training Iteration 3695, Loss: 2.580082416534424\n",
            "Training Iteration 3696, Loss: 8.382582664489746\n",
            "Training Iteration 3697, Loss: 5.223298072814941\n",
            "Training Iteration 3698, Loss: 6.10837984085083\n",
            "Training Iteration 3699, Loss: 5.006627082824707\n",
            "Training Iteration 3700, Loss: 5.2737836837768555\n",
            "Training Iteration 3701, Loss: 4.918143272399902\n",
            "Training Iteration 3702, Loss: 5.029892921447754\n",
            "Training Iteration 3703, Loss: 3.5401835441589355\n",
            "Training Iteration 3704, Loss: 6.3010077476501465\n",
            "Training Iteration 3705, Loss: 6.335572242736816\n",
            "Training Iteration 3706, Loss: 7.438176155090332\n",
            "Training Iteration 3707, Loss: 6.825993061065674\n",
            "Training Iteration 3708, Loss: 4.590562343597412\n",
            "Training Iteration 3709, Loss: 4.932657241821289\n",
            "Training Iteration 3710, Loss: 4.220934867858887\n",
            "Training Iteration 3711, Loss: 5.129700183868408\n",
            "Training Iteration 3712, Loss: 2.7687854766845703\n",
            "Training Iteration 3713, Loss: 6.894179344177246\n",
            "Training Iteration 3714, Loss: 4.8657026290893555\n",
            "Training Iteration 3715, Loss: 3.757878065109253\n",
            "Training Iteration 3716, Loss: 5.107952117919922\n",
            "Training Iteration 3717, Loss: 4.109618663787842\n",
            "Training Iteration 3718, Loss: 6.997259140014648\n",
            "Training Iteration 3719, Loss: 5.028165817260742\n",
            "Training Iteration 3720, Loss: 4.1972880363464355\n",
            "Training Iteration 3721, Loss: 0.9950826168060303\n",
            "Training Iteration 3722, Loss: 6.666158676147461\n",
            "Training Iteration 3723, Loss: 4.522409915924072\n",
            "Training Iteration 3724, Loss: 6.451256275177002\n",
            "Training Iteration 3725, Loss: 5.467423915863037\n",
            "Training Iteration 3726, Loss: 5.112749099731445\n",
            "Training Iteration 3727, Loss: 5.3500566482543945\n",
            "Training Iteration 3728, Loss: 3.8273136615753174\n",
            "Training Iteration 3729, Loss: 2.6900298595428467\n",
            "Training Iteration 3730, Loss: 2.8637027740478516\n",
            "Training Iteration 3731, Loss: 2.83439040184021\n",
            "Training Iteration 3732, Loss: 5.642928123474121\n",
            "Training Iteration 3733, Loss: 5.942421913146973\n",
            "Training Iteration 3734, Loss: 5.173036098480225\n",
            "Training Iteration 3735, Loss: 6.396848201751709\n",
            "Training Iteration 3736, Loss: 5.256135940551758\n",
            "Training Iteration 3737, Loss: 1.510983943939209\n",
            "Training Iteration 3738, Loss: 3.961088180541992\n",
            "Training Iteration 3739, Loss: 4.6097307205200195\n",
            "Training Iteration 3740, Loss: 5.666543006896973\n",
            "Training Iteration 3741, Loss: 4.533718109130859\n",
            "Training Iteration 3742, Loss: 9.347028732299805\n",
            "Training Iteration 3743, Loss: 4.176675319671631\n",
            "Training Iteration 3744, Loss: 4.682301998138428\n",
            "Training Iteration 3745, Loss: 6.907820224761963\n",
            "Training Iteration 3746, Loss: 5.120645999908447\n",
            "Training Iteration 3747, Loss: 3.648629665374756\n",
            "Training Iteration 3748, Loss: 2.4501795768737793\n",
            "Training Iteration 3749, Loss: 3.7671422958374023\n",
            "Training Iteration 3750, Loss: 4.7621355056762695\n",
            "Training Iteration 3751, Loss: 4.642951965332031\n",
            "Training Iteration 3752, Loss: 2.884864568710327\n",
            "Training Iteration 3753, Loss: 3.4908008575439453\n",
            "Training Iteration 3754, Loss: 1.4216015338897705\n",
            "Training Iteration 3755, Loss: 7.601625442504883\n",
            "Training Iteration 3756, Loss: 7.692800521850586\n",
            "Training Iteration 3757, Loss: 8.965859413146973\n",
            "Training Iteration 3758, Loss: 3.028360605239868\n",
            "Training Iteration 3759, Loss: 4.9601335525512695\n",
            "Training Iteration 3760, Loss: 5.0140533447265625\n",
            "Training Iteration 3761, Loss: 7.8074541091918945\n",
            "Training Iteration 3762, Loss: 3.133223295211792\n",
            "Training Iteration 3763, Loss: 5.065900802612305\n",
            "Training Iteration 3764, Loss: 4.764308929443359\n",
            "Training Iteration 3765, Loss: 6.247279644012451\n",
            "Training Iteration 3766, Loss: 3.8432345390319824\n",
            "Training Iteration 3767, Loss: 4.032361030578613\n",
            "Training Iteration 3768, Loss: 2.4213616847991943\n",
            "Training Iteration 3769, Loss: 3.025961399078369\n",
            "Training Iteration 3770, Loss: 4.301746845245361\n",
            "Training Iteration 3771, Loss: 2.4505937099456787\n",
            "Training Iteration 3772, Loss: 3.1691133975982666\n",
            "Training Iteration 3773, Loss: 4.938343048095703\n",
            "Training Iteration 3774, Loss: 3.297750473022461\n",
            "Training Iteration 3775, Loss: 3.9762606620788574\n",
            "Training Iteration 3776, Loss: 6.176474094390869\n",
            "Training Iteration 3777, Loss: 5.015493869781494\n",
            "Training Iteration 3778, Loss: 4.816161632537842\n",
            "Training Iteration 3779, Loss: 5.441647052764893\n",
            "Training Iteration 3780, Loss: 4.228786945343018\n",
            "Training Iteration 3781, Loss: 5.73112154006958\n",
            "Training Iteration 3782, Loss: 3.5387353897094727\n",
            "Training Iteration 3783, Loss: 4.393788814544678\n",
            "Training Iteration 3784, Loss: 5.163008689880371\n",
            "Training Iteration 3785, Loss: 3.608353614807129\n",
            "Training Iteration 3786, Loss: 5.970367431640625\n",
            "Training Iteration 3787, Loss: 4.008875846862793\n",
            "Training Iteration 3788, Loss: 8.154888153076172\n",
            "Training Iteration 3789, Loss: 6.966245651245117\n",
            "Training Iteration 3790, Loss: 5.761574745178223\n",
            "Training Iteration 3791, Loss: 5.5168352127075195\n",
            "Training Iteration 3792, Loss: 6.21923828125\n",
            "Training Iteration 3793, Loss: 4.449926853179932\n",
            "Training Iteration 3794, Loss: 5.6413397789001465\n",
            "Training Iteration 3795, Loss: 9.3194580078125\n",
            "Training Iteration 3796, Loss: 3.990673780441284\n",
            "Training Iteration 3797, Loss: 11.338479995727539\n",
            "Training Iteration 3798, Loss: 3.2981913089752197\n",
            "Training Iteration 3799, Loss: 6.869820594787598\n",
            "Training Iteration 3800, Loss: 3.358522891998291\n",
            "Training Iteration 3801, Loss: 5.9481706619262695\n",
            "Training Iteration 3802, Loss: 3.2514071464538574\n",
            "Training Iteration 3803, Loss: 3.6954922676086426\n",
            "Training Iteration 3804, Loss: 4.618285655975342\n",
            "Training Iteration 3805, Loss: 3.701341390609741\n",
            "Training Iteration 3806, Loss: 4.160669803619385\n",
            "Training Iteration 3807, Loss: 4.936577320098877\n",
            "Training Iteration 3808, Loss: 6.890726566314697\n",
            "Training Iteration 3809, Loss: 2.6972854137420654\n",
            "Training Iteration 3810, Loss: 6.1708903312683105\n",
            "Training Iteration 3811, Loss: 4.851676940917969\n",
            "Training Iteration 3812, Loss: 3.1730947494506836\n",
            "Training Iteration 3813, Loss: 2.055840253829956\n",
            "Training Iteration 3814, Loss: 4.492713928222656\n",
            "Training Iteration 3815, Loss: 8.908342361450195\n",
            "Training Iteration 3816, Loss: 3.1413278579711914\n",
            "Training Iteration 3817, Loss: 9.396501541137695\n",
            "Training Iteration 3818, Loss: 6.225610256195068\n",
            "Training Iteration 3819, Loss: 3.5133509635925293\n",
            "Training Iteration 3820, Loss: 4.408146858215332\n",
            "Training Iteration 3821, Loss: 2.4752144813537598\n",
            "Training Iteration 3822, Loss: 7.9105353355407715\n",
            "Training Iteration 3823, Loss: 4.295672416687012\n",
            "Training Iteration 3824, Loss: 7.109572410583496\n",
            "Training Iteration 3825, Loss: 11.669532775878906\n",
            "Training Iteration 3826, Loss: 4.459691047668457\n",
            "Training Iteration 3827, Loss: 5.3191328048706055\n",
            "Training Iteration 3828, Loss: 4.511919975280762\n",
            "Training Iteration 3829, Loss: 4.633482456207275\n",
            "Training Iteration 3830, Loss: 3.6296119689941406\n",
            "Training Iteration 3831, Loss: 2.7956387996673584\n",
            "Training Iteration 3832, Loss: 4.954419136047363\n",
            "Training Iteration 3833, Loss: 2.406961441040039\n",
            "Training Iteration 3834, Loss: 2.8082852363586426\n",
            "Training Iteration 3835, Loss: 6.362628936767578\n",
            "Training Iteration 3836, Loss: 6.3817949295043945\n",
            "Training Iteration 3837, Loss: 6.572897434234619\n",
            "Training Iteration 3838, Loss: 1.952404499053955\n",
            "Training Iteration 3839, Loss: 8.28170394897461\n",
            "Training Iteration 3840, Loss: 4.14089298248291\n",
            "Training Iteration 3841, Loss: 4.091455936431885\n",
            "Training Iteration 3842, Loss: 3.854827404022217\n",
            "Training Iteration 3843, Loss: 6.408231735229492\n",
            "Training Iteration 3844, Loss: 5.054117202758789\n",
            "Training Iteration 3845, Loss: 2.3718578815460205\n",
            "Training Iteration 3846, Loss: 4.148743629455566\n",
            "Training Iteration 3847, Loss: 5.1257734298706055\n",
            "Training Iteration 3848, Loss: 3.378204584121704\n",
            "Training Iteration 3849, Loss: 3.475379705429077\n",
            "Training Iteration 3850, Loss: 3.3919246196746826\n",
            "Training Iteration 3851, Loss: 5.941776275634766\n",
            "Training Iteration 3852, Loss: 2.970029592514038\n",
            "Training Iteration 3853, Loss: 3.1779937744140625\n",
            "Training Iteration 3854, Loss: 6.181107044219971\n",
            "Training Iteration 3855, Loss: 7.843122959136963\n",
            "Training Iteration 3856, Loss: 4.012585639953613\n",
            "Training Iteration 3857, Loss: 4.549560070037842\n",
            "Training Iteration 3858, Loss: 3.977198600769043\n",
            "Training Iteration 3859, Loss: 4.341582298278809\n",
            "Training Iteration 3860, Loss: 4.0956711769104\n",
            "Training Iteration 3861, Loss: 4.567245960235596\n",
            "Training Iteration 3862, Loss: 2.736011505126953\n",
            "Training Iteration 3863, Loss: 3.2808902263641357\n",
            "Training Iteration 3864, Loss: 1.6703490018844604\n",
            "Training Iteration 3865, Loss: 1.2125041484832764\n",
            "Training Iteration 3866, Loss: 3.341776132583618\n",
            "Training Iteration 3867, Loss: 4.4862141609191895\n",
            "Training Iteration 3868, Loss: 5.146420478820801\n",
            "Training Iteration 3869, Loss: 5.470163822174072\n",
            "Training Iteration 3870, Loss: 4.005697727203369\n",
            "Training Iteration 3871, Loss: 2.253657102584839\n",
            "Training Iteration 3872, Loss: 6.6321539878845215\n",
            "Training Iteration 3873, Loss: 3.0617623329162598\n",
            "Training Iteration 3874, Loss: 6.924911975860596\n",
            "Training Iteration 3875, Loss: 5.240964412689209\n",
            "Training Iteration 3876, Loss: 9.07175350189209\n",
            "Training Iteration 3877, Loss: 2.3247697353363037\n",
            "Training Iteration 3878, Loss: 3.174119472503662\n",
            "Training Iteration 3879, Loss: 2.0739247798919678\n",
            "Training Iteration 3880, Loss: 4.793450355529785\n",
            "Training Iteration 3881, Loss: 0.6082170009613037\n",
            "Training Iteration 3882, Loss: 2.549403429031372\n",
            "Training Iteration 3883, Loss: 6.532773494720459\n",
            "Training Iteration 3884, Loss: 3.7502591609954834\n",
            "Training Iteration 3885, Loss: 14.017362594604492\n",
            "Training Iteration 3886, Loss: 3.6368205547332764\n",
            "Training Iteration 3887, Loss: 8.955131530761719\n",
            "Training Iteration 3888, Loss: 3.3045434951782227\n",
            "Training Iteration 3889, Loss: 4.380526542663574\n",
            "Training Iteration 3890, Loss: 7.753555774688721\n",
            "Training Iteration 3891, Loss: 4.631160259246826\n",
            "Training Iteration 3892, Loss: 7.160146713256836\n",
            "Training Iteration 3893, Loss: 3.127335786819458\n",
            "Training Iteration 3894, Loss: 2.6455373764038086\n",
            "Training Iteration 3895, Loss: 4.565999984741211\n",
            "Training Iteration 3896, Loss: 1.2683985233306885\n",
            "Training Iteration 3897, Loss: 4.791714191436768\n",
            "Training Iteration 3898, Loss: 5.4136433601379395\n",
            "Training Iteration 3899, Loss: 3.446784734725952\n",
            "Training Iteration 3900, Loss: 6.487415790557861\n",
            "Training Iteration 3901, Loss: 5.357357978820801\n",
            "Training Iteration 3902, Loss: 1.2538414001464844\n",
            "Training Iteration 3903, Loss: 4.854870319366455\n",
            "Training Iteration 3904, Loss: 3.0024545192718506\n",
            "Training Iteration 3905, Loss: 5.824591636657715\n",
            "Training Iteration 3906, Loss: 4.175066947937012\n",
            "Training Iteration 3907, Loss: 4.315472602844238\n",
            "Training Iteration 3908, Loss: 2.4411535263061523\n",
            "Training Iteration 3909, Loss: 3.4288337230682373\n",
            "Training Iteration 3910, Loss: 4.516854763031006\n",
            "Training Iteration 3911, Loss: 2.9397287368774414\n",
            "Training Iteration 3912, Loss: 4.589717864990234\n",
            "Training Iteration 3913, Loss: 4.3553338050842285\n",
            "Training Iteration 3914, Loss: 3.425530433654785\n",
            "Training Iteration 3915, Loss: 2.0394437313079834\n",
            "Training Iteration 3916, Loss: 5.106247425079346\n",
            "Training Iteration 3917, Loss: 8.43288516998291\n",
            "Training Iteration 3918, Loss: 1.2764924764633179\n",
            "Training Iteration 3919, Loss: 1.910097360610962\n",
            "Training Iteration 3920, Loss: 4.693073272705078\n",
            "Training Iteration 3921, Loss: 4.826231479644775\n",
            "Training Iteration 3922, Loss: 5.095778942108154\n",
            "Training Iteration 3923, Loss: 6.9506049156188965\n",
            "Training Iteration 3924, Loss: 3.3940482139587402\n",
            "Training Iteration 3925, Loss: 4.706251621246338\n",
            "Training Iteration 3926, Loss: 3.4164605140686035\n",
            "Training Iteration 3927, Loss: 3.212202310562134\n",
            "Training Iteration 3928, Loss: 4.1700029373168945\n",
            "Training Iteration 3929, Loss: 2.0710082054138184\n",
            "Training Iteration 3930, Loss: 3.583141565322876\n",
            "Training Iteration 3931, Loss: 3.863635778427124\n",
            "Training Iteration 3932, Loss: 5.55052375793457\n",
            "Training Iteration 3933, Loss: 7.049643516540527\n",
            "Training Iteration 3934, Loss: 5.605501174926758\n",
            "Training Iteration 3935, Loss: 4.694098949432373\n",
            "Training Iteration 3936, Loss: 1.9257867336273193\n",
            "Training Iteration 3937, Loss: 5.20713996887207\n",
            "Training Iteration 3938, Loss: 3.4881703853607178\n",
            "Training Iteration 3939, Loss: 6.952394485473633\n",
            "Training Iteration 3940, Loss: 5.5211944580078125\n",
            "Training Iteration 3941, Loss: 7.892395973205566\n",
            "Training Iteration 3942, Loss: 3.92230486869812\n",
            "Training Iteration 3943, Loss: 1.4582749605178833\n",
            "Training Iteration 3944, Loss: 4.696342468261719\n",
            "Training Iteration 3945, Loss: 7.545009613037109\n",
            "Training Iteration 3946, Loss: 7.769800186157227\n",
            "Training Iteration 3947, Loss: 8.937297821044922\n",
            "Training Iteration 3948, Loss: 4.188803672790527\n",
            "Training Iteration 3949, Loss: 6.113738059997559\n",
            "Training Iteration 3950, Loss: 4.684340000152588\n",
            "Training Iteration 3951, Loss: 5.155803203582764\n",
            "Training Iteration 3952, Loss: 3.1130740642547607\n",
            "Training Iteration 3953, Loss: 2.4523398876190186\n",
            "Training Iteration 3954, Loss: 3.3117258548736572\n",
            "Training Iteration 3955, Loss: 3.937807321548462\n",
            "Training Iteration 3956, Loss: 6.773885726928711\n",
            "Training Iteration 3957, Loss: 8.41130542755127\n",
            "Training Iteration 3958, Loss: 7.453543186187744\n",
            "Training Iteration 3959, Loss: 4.432339668273926\n",
            "Training Iteration 3960, Loss: 2.3860738277435303\n",
            "Training Iteration 3961, Loss: 8.32017707824707\n",
            "Training Iteration 3962, Loss: 3.613492965698242\n",
            "Training Iteration 3963, Loss: 8.145817756652832\n",
            "Training Iteration 3964, Loss: 3.487945079803467\n",
            "Training Iteration 3965, Loss: 4.101929187774658\n",
            "Training Iteration 3966, Loss: 5.09483528137207\n",
            "Training Iteration 3967, Loss: 0.8004239797592163\n",
            "Training Iteration 3968, Loss: 5.3765668869018555\n",
            "Training Iteration 3969, Loss: 3.730923652648926\n",
            "Training Iteration 3970, Loss: 4.29622745513916\n",
            "Training Iteration 3971, Loss: 4.114616394042969\n",
            "Training Iteration 3972, Loss: 3.668238639831543\n",
            "Training Iteration 3973, Loss: 4.245729446411133\n",
            "Training Iteration 3974, Loss: 7.516299724578857\n",
            "Training Iteration 3975, Loss: 9.712903022766113\n",
            "Training Iteration 3976, Loss: 4.213201999664307\n",
            "Training Iteration 3977, Loss: 4.104846954345703\n",
            "Training Iteration 3978, Loss: 3.416447162628174\n",
            "Training Iteration 3979, Loss: 2.2711832523345947\n",
            "Training Iteration 3980, Loss: 2.705031156539917\n",
            "Training Iteration 3981, Loss: 3.0315632820129395\n",
            "Training Iteration 3982, Loss: 7.164443492889404\n",
            "Training Iteration 3983, Loss: 2.369154930114746\n",
            "Training Iteration 3984, Loss: 5.493186950683594\n",
            "Training Iteration 3985, Loss: 4.55696439743042\n",
            "Training Iteration 3986, Loss: 7.512709617614746\n",
            "Training Iteration 3987, Loss: 5.5521626472473145\n",
            "Training Iteration 3988, Loss: 10.791515350341797\n",
            "Training Iteration 3989, Loss: 5.4417314529418945\n",
            "Training Iteration 3990, Loss: 4.78912353515625\n",
            "Training Iteration 3991, Loss: 9.870262145996094\n",
            "Training Iteration 3992, Loss: 6.428278923034668\n",
            "Training Iteration 3993, Loss: 4.6543073654174805\n",
            "Training Iteration 3994, Loss: 6.360830783843994\n",
            "Training Iteration 3995, Loss: 6.489498615264893\n",
            "Training Iteration 3996, Loss: 3.8686563968658447\n",
            "Training Iteration 3997, Loss: 4.429422378540039\n",
            "Training Iteration 3998, Loss: 2.4419050216674805\n",
            "Training Iteration 3999, Loss: 2.096360206604004\n",
            "Training Iteration 4000, Loss: 4.1320624351501465\n",
            "Training Iteration 4001, Loss: 7.803691864013672\n",
            "Training Iteration 4002, Loss: 6.939302921295166\n",
            "Training Iteration 4003, Loss: 3.7498507499694824\n",
            "Training Iteration 4004, Loss: 3.4182844161987305\n",
            "Training Iteration 4005, Loss: 5.460712432861328\n",
            "Training Iteration 4006, Loss: 7.968555927276611\n",
            "Training Iteration 4007, Loss: 6.6505632400512695\n",
            "Training Iteration 4008, Loss: 3.3160178661346436\n",
            "Training Iteration 4009, Loss: 6.437023162841797\n",
            "Training Iteration 4010, Loss: 2.484851360321045\n",
            "Training Iteration 4011, Loss: 1.8694945573806763\n",
            "Training Iteration 4012, Loss: 4.5509796142578125\n",
            "Training Iteration 4013, Loss: 6.117964744567871\n",
            "Training Iteration 4014, Loss: 3.5566396713256836\n",
            "Training Iteration 4015, Loss: 2.2123043537139893\n",
            "Training Iteration 4016, Loss: 2.257458209991455\n",
            "Training Iteration 4017, Loss: 2.1601486206054688\n",
            "Training Iteration 4018, Loss: 4.368485927581787\n",
            "Training Iteration 4019, Loss: 6.1776885986328125\n",
            "Training Iteration 4020, Loss: 5.890400409698486\n",
            "Training Iteration 4021, Loss: 4.015169620513916\n",
            "Training Iteration 4022, Loss: 4.17052698135376\n",
            "Training Iteration 4023, Loss: 2.6043357849121094\n",
            "Training Iteration 4024, Loss: 5.108433723449707\n",
            "Training Iteration 4025, Loss: 1.9492161273956299\n",
            "Training Iteration 4026, Loss: 4.21878719329834\n",
            "Training Iteration 4027, Loss: 6.516395092010498\n",
            "Training Iteration 4028, Loss: 4.769989013671875\n",
            "Training Iteration 4029, Loss: 8.97320556640625\n",
            "Training Iteration 4030, Loss: 4.651583194732666\n",
            "Training Iteration 4031, Loss: 2.131911039352417\n",
            "Training Iteration 4032, Loss: 5.380756855010986\n",
            "Training Iteration 4033, Loss: 2.828400135040283\n",
            "Training Iteration 4034, Loss: 5.429182052612305\n",
            "Training Iteration 4035, Loss: 2.124025583267212\n",
            "Training Iteration 4036, Loss: 2.707601308822632\n",
            "Training Iteration 4037, Loss: 2.0020692348480225\n",
            "Training Iteration 4038, Loss: 2.1154634952545166\n",
            "Training Iteration 4039, Loss: 4.320609092712402\n",
            "Training Iteration 4040, Loss: 3.3087692260742188\n",
            "Training Iteration 4041, Loss: 4.727086544036865\n",
            "Training Iteration 4042, Loss: 6.103926658630371\n",
            "Training Iteration 4043, Loss: 2.444129467010498\n",
            "Training Iteration 4044, Loss: 4.31897497177124\n",
            "Training Iteration 4045, Loss: 5.906447410583496\n",
            "Training Iteration 4046, Loss: 5.287647724151611\n",
            "Training Iteration 4047, Loss: 4.446622848510742\n",
            "Training Iteration 4048, Loss: 3.3789596557617188\n",
            "Training Iteration 4049, Loss: 4.452247142791748\n",
            "Training Iteration 4050, Loss: 4.954121112823486\n",
            "Training Iteration 4051, Loss: 4.787053108215332\n",
            "Training Iteration 4052, Loss: 2.7660069465637207\n",
            "Training Iteration 4053, Loss: 4.2838215827941895\n",
            "Training Iteration 4054, Loss: 7.767764568328857\n",
            "Training Iteration 4055, Loss: 5.0850396156311035\n",
            "Training Iteration 4056, Loss: 3.6338722705841064\n",
            "Training Iteration 4057, Loss: 3.9869096279144287\n",
            "Training Iteration 4058, Loss: 6.8274383544921875\n",
            "Training Iteration 4059, Loss: 9.240272521972656\n",
            "Training Iteration 4060, Loss: 4.931938648223877\n",
            "Training Iteration 4061, Loss: 3.862790107727051\n",
            "Training Iteration 4062, Loss: 3.1660242080688477\n",
            "Training Iteration 4063, Loss: 5.28449010848999\n",
            "Training Iteration 4064, Loss: 7.732929229736328\n",
            "Training Iteration 4065, Loss: 7.629506587982178\n",
            "Training Iteration 4066, Loss: 6.271699905395508\n",
            "Training Iteration 4067, Loss: 7.045916557312012\n",
            "Training Iteration 4068, Loss: 5.602461814880371\n",
            "Training Iteration 4069, Loss: 2.403414726257324\n",
            "Training Iteration 4070, Loss: 3.995180606842041\n",
            "Training Iteration 4071, Loss: 6.0413947105407715\n",
            "Training Iteration 4072, Loss: 6.722551345825195\n",
            "Training Iteration 4073, Loss: 2.5794713497161865\n",
            "Training Iteration 4074, Loss: 3.524164915084839\n",
            "Training Iteration 4075, Loss: 5.743843078613281\n",
            "Training Iteration 4076, Loss: 3.262833595275879\n",
            "Training Iteration 4077, Loss: 5.18993616104126\n",
            "Training Iteration 4078, Loss: 10.082328796386719\n",
            "Training Iteration 4079, Loss: 5.421914100646973\n",
            "Training Iteration 4080, Loss: 4.127485275268555\n",
            "Training Iteration 4081, Loss: 3.5100436210632324\n",
            "Training Iteration 4082, Loss: 2.6522438526153564\n",
            "Training Iteration 4083, Loss: 5.600039482116699\n",
            "Training Iteration 4084, Loss: 4.583671569824219\n",
            "Training Iteration 4085, Loss: 2.5615615844726562\n",
            "Training Iteration 4086, Loss: 3.8657193183898926\n",
            "Training Iteration 4087, Loss: 3.9885363578796387\n",
            "Training Iteration 4088, Loss: 4.643741607666016\n",
            "Training Iteration 4089, Loss: 4.688913822174072\n",
            "Training Iteration 4090, Loss: 3.0760653018951416\n",
            "Training Iteration 4091, Loss: 3.01922345161438\n",
            "Training Iteration 4092, Loss: 4.274326801300049\n",
            "Training Iteration 4093, Loss: 5.55770206451416\n",
            "Training Iteration 4094, Loss: 3.93644642829895\n",
            "Training Iteration 4095, Loss: 3.636296033859253\n",
            "Training Iteration 4096, Loss: 5.0544962882995605\n",
            "Training Iteration 4097, Loss: 2.5050439834594727\n",
            "Training Iteration 4098, Loss: 6.421087265014648\n",
            "Training Iteration 4099, Loss: 5.836163520812988\n",
            "Training Iteration 4100, Loss: 4.216659069061279\n",
            "Training Iteration 4101, Loss: 8.61277961730957\n",
            "Training Iteration 4102, Loss: 5.788661479949951\n",
            "Training Iteration 4103, Loss: 6.703516960144043\n",
            "Training Iteration 4104, Loss: 8.270320892333984\n",
            "Training Iteration 4105, Loss: 3.3060178756713867\n",
            "Training Iteration 4106, Loss: 2.7129127979278564\n",
            "Training Iteration 4107, Loss: 3.5398223400115967\n",
            "Training Iteration 4108, Loss: 3.389072895050049\n",
            "Training Iteration 4109, Loss: 2.151754140853882\n",
            "Training Iteration 4110, Loss: 5.072060585021973\n",
            "Training Iteration 4111, Loss: 1.6876351833343506\n",
            "Training Iteration 4112, Loss: 3.002037763595581\n",
            "Training Iteration 4113, Loss: 5.233515739440918\n",
            "Training Iteration 4114, Loss: 3.9179675579071045\n",
            "Training Iteration 4115, Loss: 3.1979587078094482\n",
            "Training Iteration 4116, Loss: 4.302298545837402\n",
            "Training Iteration 4117, Loss: 2.9419755935668945\n",
            "Training Iteration 4118, Loss: 4.934701442718506\n",
            "Training Iteration 4119, Loss: 4.393568992614746\n",
            "Training Iteration 4120, Loss: 2.4487111568450928\n",
            "Training Iteration 4121, Loss: 2.6362664699554443\n",
            "Training Iteration 4122, Loss: 3.7809629440307617\n",
            "Training Iteration 4123, Loss: 2.942720413208008\n",
            "Training Iteration 4124, Loss: 3.78604793548584\n",
            "Training Iteration 4125, Loss: 4.493824481964111\n",
            "Training Iteration 4126, Loss: 6.409429550170898\n",
            "Training Iteration 4127, Loss: 4.981210231781006\n",
            "Training Iteration 4128, Loss: 5.006326675415039\n",
            "Training Iteration 4129, Loss: 4.588510990142822\n",
            "Training Iteration 4130, Loss: 3.774118661880493\n",
            "Training Iteration 4131, Loss: 4.283445835113525\n",
            "Training Iteration 4132, Loss: 5.810103416442871\n",
            "Training Iteration 4133, Loss: 1.656305193901062\n",
            "Training Iteration 4134, Loss: 2.738715410232544\n",
            "Training Iteration 4135, Loss: 3.688829183578491\n",
            "Training Iteration 4136, Loss: 5.0571088790893555\n",
            "Training Iteration 4137, Loss: 3.18058180809021\n",
            "Training Iteration 4138, Loss: 4.540597438812256\n",
            "Training Iteration 4139, Loss: 5.840954780578613\n",
            "Training Iteration 4140, Loss: 2.96798038482666\n",
            "Training Iteration 4141, Loss: 1.915168285369873\n",
            "Training Iteration 4142, Loss: 5.498299598693848\n",
            "Training Iteration 4143, Loss: 3.3699803352355957\n",
            "Training Iteration 4144, Loss: 1.562927484512329\n",
            "Training Iteration 4145, Loss: 2.9510412216186523\n",
            "Training Iteration 4146, Loss: 2.7853150367736816\n",
            "Training Iteration 4147, Loss: 6.635152339935303\n",
            "Training Iteration 4148, Loss: 4.793616771697998\n",
            "Training Iteration 4149, Loss: 5.293583393096924\n",
            "Training Iteration 4150, Loss: 3.969719171524048\n",
            "Training Iteration 4151, Loss: 3.376155376434326\n",
            "Training Iteration 4152, Loss: 6.211813449859619\n",
            "Training Iteration 4153, Loss: 8.223331451416016\n",
            "Training Iteration 4154, Loss: 5.20151424407959\n",
            "Training Iteration 4155, Loss: 5.615990161895752\n",
            "Training Iteration 4156, Loss: 5.630531311035156\n",
            "Training Iteration 4157, Loss: 4.393182754516602\n",
            "Training Iteration 4158, Loss: 4.957542896270752\n",
            "Training Iteration 4159, Loss: 5.195340633392334\n",
            "Training Iteration 4160, Loss: 4.064574718475342\n",
            "Training Iteration 4161, Loss: 6.896565914154053\n",
            "Training Iteration 4162, Loss: 4.2404303550720215\n",
            "Training Iteration 4163, Loss: 6.297792434692383\n",
            "Training Iteration 4164, Loss: 3.14064884185791\n",
            "Training Iteration 4165, Loss: 6.584736347198486\n",
            "Training Iteration 4166, Loss: 5.164558410644531\n",
            "Training Iteration 4167, Loss: 7.366507530212402\n",
            "Training Iteration 4168, Loss: 6.278753280639648\n",
            "Training Iteration 4169, Loss: 3.4576687812805176\n",
            "Training Iteration 4170, Loss: 5.180370807647705\n",
            "Training Iteration 4171, Loss: 6.224242687225342\n",
            "Training Iteration 4172, Loss: 2.578280448913574\n",
            "Training Iteration 4173, Loss: 7.995100975036621\n",
            "Training Iteration 4174, Loss: 5.834782123565674\n",
            "Training Iteration 4175, Loss: 9.648037910461426\n",
            "Training Iteration 4176, Loss: 4.858242988586426\n",
            "Training Iteration 4177, Loss: 4.614030838012695\n",
            "Training Iteration 4178, Loss: 3.1888179779052734\n",
            "Training Iteration 4179, Loss: 6.162011623382568\n",
            "Training Iteration 4180, Loss: 4.729132652282715\n",
            "Training Iteration 4181, Loss: 2.596571445465088\n",
            "Training Iteration 4182, Loss: 7.660334587097168\n",
            "Training Iteration 4183, Loss: 6.173583507537842\n",
            "Training Iteration 4184, Loss: 4.312742233276367\n",
            "Training Iteration 4185, Loss: 4.773924827575684\n",
            "Training Iteration 4186, Loss: 4.1408514976501465\n",
            "Training Iteration 4187, Loss: 3.08162260055542\n",
            "Training Iteration 4188, Loss: 3.562023878097534\n",
            "Training Iteration 4189, Loss: 5.750964641571045\n",
            "Training Iteration 4190, Loss: 7.649086952209473\n",
            "Training Iteration 4191, Loss: 6.544511318206787\n",
            "Training Iteration 4192, Loss: 5.637486457824707\n",
            "Training Iteration 4193, Loss: 6.075648784637451\n",
            "Training Iteration 4194, Loss: 5.490140438079834\n",
            "Training Iteration 4195, Loss: 5.05307674407959\n",
            "Training Iteration 4196, Loss: 4.140477657318115\n",
            "Training Iteration 4197, Loss: 3.626406669616699\n",
            "Training Iteration 4198, Loss: 6.5202789306640625\n",
            "Training Iteration 4199, Loss: 5.621037483215332\n",
            "Training Iteration 4200, Loss: 3.4893429279327393\n",
            "Training Iteration 4201, Loss: 6.071479797363281\n",
            "Training Iteration 4202, Loss: 5.00898551940918\n",
            "Training Iteration 4203, Loss: 5.676587104797363\n",
            "Training Iteration 4204, Loss: 7.328577518463135\n",
            "Training Iteration 4205, Loss: 4.170287132263184\n",
            "Training Iteration 4206, Loss: 2.0618698596954346\n",
            "Training Iteration 4207, Loss: 4.158943176269531\n",
            "Training Iteration 4208, Loss: 4.271973609924316\n",
            "Training Iteration 4209, Loss: 5.592483043670654\n",
            "Training Iteration 4210, Loss: 3.426483154296875\n",
            "Training Iteration 4211, Loss: 2.8672783374786377\n",
            "Training Iteration 4212, Loss: 4.658393859863281\n",
            "Training Iteration 4213, Loss: 3.951664924621582\n",
            "Training Iteration 4214, Loss: 3.9935967922210693\n",
            "Training Iteration 4215, Loss: 3.1142349243164062\n",
            "Training Iteration 4216, Loss: 3.9533519744873047\n",
            "Training Iteration 4217, Loss: 5.982460021972656\n",
            "Training Iteration 4218, Loss: 3.1274592876434326\n",
            "Training Iteration 4219, Loss: 4.997092247009277\n",
            "Training Iteration 4220, Loss: 2.719953775405884\n",
            "Training Iteration 4221, Loss: 3.5271944999694824\n",
            "Training Iteration 4222, Loss: 4.115354061126709\n",
            "Training Iteration 4223, Loss: 5.485316276550293\n",
            "Training Iteration 4224, Loss: 4.5242414474487305\n",
            "Training Iteration 4225, Loss: 4.528435707092285\n",
            "Training Iteration 4226, Loss: 7.0071210861206055\n",
            "Training Iteration 4227, Loss: 3.374345541000366\n",
            "Training Iteration 4228, Loss: 2.8940987586975098\n",
            "Training Iteration 4229, Loss: 6.565867900848389\n",
            "Training Iteration 4230, Loss: 4.31202507019043\n",
            "Training Iteration 4231, Loss: 7.609464645385742\n",
            "Training Iteration 4232, Loss: 4.059270858764648\n",
            "Training Iteration 4233, Loss: 2.9191224575042725\n",
            "Training Iteration 4234, Loss: 2.84002947807312\n",
            "Training Iteration 4235, Loss: 5.810205459594727\n",
            "Training Iteration 4236, Loss: 3.84321928024292\n",
            "Training Iteration 4237, Loss: 5.92723274230957\n",
            "Training Iteration 4238, Loss: 4.078207969665527\n",
            "Training Iteration 4239, Loss: 5.316825866699219\n",
            "Training Iteration 4240, Loss: 2.158722400665283\n",
            "Training Iteration 4241, Loss: 4.0281147956848145\n",
            "Training Iteration 4242, Loss: 6.216268539428711\n",
            "Training Iteration 4243, Loss: 7.148997783660889\n",
            "Training Iteration 4244, Loss: 3.6059911251068115\n",
            "Training Iteration 4245, Loss: 4.429703712463379\n",
            "Training Iteration 4246, Loss: 1.9547374248504639\n",
            "Training Iteration 4247, Loss: 4.884798049926758\n",
            "Training Iteration 4248, Loss: 4.275244235992432\n",
            "Training Iteration 4249, Loss: 3.557570695877075\n",
            "Training Iteration 4250, Loss: 5.488473892211914\n",
            "Training Iteration 4251, Loss: 3.734776258468628\n",
            "Training Iteration 4252, Loss: 3.3975462913513184\n",
            "Training Iteration 4253, Loss: 2.082977771759033\n",
            "Training Iteration 4254, Loss: 3.885781764984131\n",
            "Training Iteration 4255, Loss: 3.2049176692962646\n",
            "Training Iteration 4256, Loss: 3.4923949241638184\n",
            "Training Iteration 4257, Loss: 6.777083396911621\n",
            "Training Iteration 4258, Loss: 3.113297700881958\n",
            "Training Iteration 4259, Loss: 5.341175079345703\n",
            "Training Iteration 4260, Loss: 3.4484639167785645\n",
            "Training Iteration 4261, Loss: 1.9924747943878174\n",
            "Training Iteration 4262, Loss: 3.737218141555786\n",
            "Training Iteration 4263, Loss: 3.17500901222229\n",
            "Training Iteration 4264, Loss: 2.9914186000823975\n",
            "Training Iteration 4265, Loss: 2.9569897651672363\n",
            "Training Iteration 4266, Loss: 3.001511573791504\n",
            "Training Iteration 4267, Loss: 5.311285972595215\n",
            "Training Iteration 4268, Loss: 3.5504403114318848\n",
            "Training Iteration 4269, Loss: 3.8325228691101074\n",
            "Training Iteration 4270, Loss: 5.244096755981445\n",
            "Training Iteration 4271, Loss: 4.2769880294799805\n",
            "Training Iteration 4272, Loss: 5.402408123016357\n",
            "Training Iteration 4273, Loss: 4.094442844390869\n",
            "Training Iteration 4274, Loss: 3.35662841796875\n",
            "Training Iteration 4275, Loss: 3.8515164852142334\n",
            "Training Iteration 4276, Loss: 3.2340049743652344\n",
            "Training Iteration 4277, Loss: 3.913822650909424\n",
            "Training Iteration 4278, Loss: 3.5193910598754883\n",
            "Training Iteration 4279, Loss: 4.319931983947754\n",
            "Training Iteration 4280, Loss: 3.397820234298706\n",
            "Training Iteration 4281, Loss: 6.8721184730529785\n",
            "Training Iteration 4282, Loss: 4.211221694946289\n",
            "Training Iteration 4283, Loss: 4.5842461585998535\n",
            "Training Iteration 4284, Loss: 2.5549023151397705\n",
            "Training Iteration 4285, Loss: 7.854824542999268\n",
            "Training Iteration 4286, Loss: 5.419058799743652\n",
            "Training Iteration 4287, Loss: 1.7375856637954712\n",
            "Training Iteration 4288, Loss: 3.382871627807617\n",
            "Training Iteration 4289, Loss: 4.112318515777588\n",
            "Training Iteration 4290, Loss: 5.602454662322998\n",
            "Training Iteration 4291, Loss: 3.8927183151245117\n",
            "Training Iteration 4292, Loss: 5.71035099029541\n",
            "Training Iteration 4293, Loss: 6.92088508605957\n",
            "Training Iteration 4294, Loss: 5.570030212402344\n",
            "Training Iteration 4295, Loss: 2.6613268852233887\n",
            "Training Iteration 4296, Loss: 5.907216548919678\n",
            "Training Iteration 4297, Loss: 4.272643089294434\n",
            "Training Iteration 4298, Loss: 3.429262399673462\n",
            "Training Iteration 4299, Loss: 6.361921310424805\n",
            "Training Iteration 4300, Loss: 4.637381553649902\n",
            "Training Iteration 4301, Loss: 3.379460573196411\n",
            "Training Iteration 4302, Loss: 5.933079719543457\n",
            "Training Iteration 4303, Loss: 4.831063747406006\n",
            "Training Iteration 4304, Loss: 6.084611415863037\n",
            "Training Iteration 4305, Loss: 7.5054521560668945\n",
            "Training Iteration 4306, Loss: 6.6656107902526855\n",
            "Training Iteration 4307, Loss: 5.6519575119018555\n",
            "Training Iteration 4308, Loss: 5.288765907287598\n",
            "Training Iteration 4309, Loss: 3.18060302734375\n",
            "Training Iteration 4310, Loss: 3.026682138442993\n",
            "Training Iteration 4311, Loss: 5.572368144989014\n",
            "Training Iteration 4312, Loss: 4.670906066894531\n",
            "Training Iteration 4313, Loss: 5.373473644256592\n",
            "Training Iteration 4314, Loss: 5.275925636291504\n",
            "Training Iteration 4315, Loss: 3.318662405014038\n",
            "Training Iteration 4316, Loss: 1.464756727218628\n",
            "Training Iteration 4317, Loss: 4.124433517456055\n",
            "Training Iteration 4318, Loss: 5.993983745574951\n",
            "Training Iteration 4319, Loss: 2.736034870147705\n",
            "Training Iteration 4320, Loss: 3.140890121459961\n",
            "Training Iteration 4321, Loss: 4.991189956665039\n",
            "Training Iteration 4322, Loss: 3.821084976196289\n",
            "Training Iteration 4323, Loss: 4.368170738220215\n",
            "Training Iteration 4324, Loss: 3.298492908477783\n",
            "Training Iteration 4325, Loss: 2.7229912281036377\n",
            "Training Iteration 4326, Loss: 5.81878137588501\n",
            "Training Iteration 4327, Loss: 1.1157563924789429\n",
            "Training Iteration 4328, Loss: 4.696570873260498\n",
            "Training Iteration 4329, Loss: 1.322608232498169\n",
            "Training Iteration 4330, Loss: 8.308563232421875\n",
            "Training Iteration 4331, Loss: 5.648038864135742\n",
            "Training Iteration 4332, Loss: 1.8351103067398071\n",
            "Training Iteration 4333, Loss: 1.951812982559204\n",
            "Training Iteration 4334, Loss: 3.1022958755493164\n",
            "Training Iteration 4335, Loss: 4.489285469055176\n",
            "Training Iteration 4336, Loss: 4.979278087615967\n",
            "Training Iteration 4337, Loss: 4.428330421447754\n",
            "Training Iteration 4338, Loss: 3.4196197986602783\n",
            "Training Iteration 4339, Loss: 3.5024640560150146\n",
            "Training Iteration 4340, Loss: 2.9861161708831787\n",
            "Training Iteration 4341, Loss: 5.177576065063477\n",
            "Training Iteration 4342, Loss: 2.3043367862701416\n",
            "Training Iteration 4343, Loss: 2.2372193336486816\n",
            "Training Iteration 4344, Loss: 3.347296714782715\n",
            "Training Iteration 4345, Loss: 2.2399818897247314\n",
            "Training Iteration 4346, Loss: 5.508935451507568\n",
            "Training Iteration 4347, Loss: 3.439685583114624\n",
            "Training Iteration 4348, Loss: 3.7689099311828613\n",
            "Training Iteration 4349, Loss: 4.068829536437988\n",
            "Training Iteration 4350, Loss: 4.336894989013672\n",
            "Training Iteration 4351, Loss: 5.03415060043335\n",
            "Training Iteration 4352, Loss: 8.581241607666016\n",
            "Training Iteration 4353, Loss: 4.323458671569824\n",
            "Training Iteration 4354, Loss: 7.399606704711914\n",
            "Training Iteration 4355, Loss: 4.335639953613281\n",
            "Training Iteration 4356, Loss: 2.516512393951416\n",
            "Training Iteration 4357, Loss: 3.8019142150878906\n",
            "Training Iteration 4358, Loss: 5.150540828704834\n",
            "Training Iteration 4359, Loss: 4.464104652404785\n",
            "Training Iteration 4360, Loss: 4.846859455108643\n",
            "Training Iteration 4361, Loss: 5.954270839691162\n",
            "Training Iteration 4362, Loss: 5.666375160217285\n",
            "Training Iteration 4363, Loss: 2.9371628761291504\n",
            "Training Iteration 4364, Loss: 2.6560776233673096\n",
            "Training Iteration 4365, Loss: 5.489315032958984\n",
            "Training Iteration 4366, Loss: 4.163562297821045\n",
            "Training Iteration 4367, Loss: 1.9508739709854126\n",
            "Training Iteration 4368, Loss: 3.391111373901367\n",
            "Training Iteration 4369, Loss: 9.027688980102539\n",
            "Training Iteration 4370, Loss: 2.2306742668151855\n",
            "Training Iteration 4371, Loss: 3.0901694297790527\n",
            "Training Iteration 4372, Loss: 5.930728912353516\n",
            "Training Iteration 4373, Loss: 4.2859907150268555\n",
            "Training Iteration 4374, Loss: 4.141625881195068\n",
            "Training Iteration 4375, Loss: 5.799530029296875\n",
            "Training Iteration 4376, Loss: 4.805328845977783\n",
            "Training Iteration 4377, Loss: 5.711825847625732\n",
            "Training Iteration 4378, Loss: 3.223310947418213\n",
            "Training Iteration 4379, Loss: 4.226124286651611\n",
            "Training Iteration 4380, Loss: 4.5867462158203125\n",
            "Training Iteration 4381, Loss: 7.8228559494018555\n",
            "Training Iteration 4382, Loss: 4.529341697692871\n",
            "Training Iteration 4383, Loss: 4.514444351196289\n",
            "Training Iteration 4384, Loss: 4.738127708435059\n",
            "Training Iteration 4385, Loss: 2.860146999359131\n",
            "Training Iteration 4386, Loss: 4.641946315765381\n",
            "Training Iteration 4387, Loss: 2.3776674270629883\n",
            "Training Iteration 4388, Loss: 4.498085021972656\n",
            "Training Iteration 4389, Loss: 5.017026901245117\n",
            "Training Iteration 4390, Loss: 8.634532928466797\n",
            "Training Iteration 4391, Loss: 5.9308319091796875\n",
            "Training Iteration 4392, Loss: 6.3728718757629395\n",
            "Training Iteration 4393, Loss: 4.744369029998779\n",
            "Training Iteration 4394, Loss: 3.499535083770752\n",
            "Training Iteration 4395, Loss: 10.249102592468262\n",
            "Training Iteration 4396, Loss: 3.5986242294311523\n",
            "Training Iteration 4397, Loss: 3.1426262855529785\n",
            "Training Iteration 4398, Loss: 2.4488134384155273\n",
            "Training Iteration 4399, Loss: 7.127801418304443\n",
            "Training Iteration 4400, Loss: 5.824388027191162\n",
            "Training Iteration 4401, Loss: 6.282804489135742\n",
            "Training Iteration 4402, Loss: 4.184466361999512\n",
            "Training Iteration 4403, Loss: 4.843090534210205\n",
            "Training Iteration 4404, Loss: 5.899096965789795\n",
            "Training Iteration 4405, Loss: 6.455356597900391\n",
            "Training Iteration 4406, Loss: 2.760030508041382\n",
            "Training Iteration 4407, Loss: 4.711192607879639\n",
            "Training Iteration 4408, Loss: 4.755056858062744\n",
            "Training Iteration 4409, Loss: 6.144268989562988\n",
            "Training Iteration 4410, Loss: 6.07015323638916\n",
            "Training Iteration 4411, Loss: 4.615195274353027\n",
            "Training Iteration 4412, Loss: 4.468066215515137\n",
            "Training Iteration 4413, Loss: 3.9931485652923584\n",
            "Training Iteration 4414, Loss: 4.925582408905029\n",
            "Training Iteration 4415, Loss: 5.724029541015625\n",
            "Training Iteration 4416, Loss: 5.95678186416626\n",
            "Training Iteration 4417, Loss: 6.321287155151367\n",
            "Training Iteration 4418, Loss: 4.6537041664123535\n",
            "Training Iteration 4419, Loss: 5.206882953643799\n",
            "Training Iteration 4420, Loss: 3.194391965866089\n",
            "Training Iteration 4421, Loss: 8.503744125366211\n",
            "Training Iteration 4422, Loss: 2.913161277770996\n",
            "Training Iteration 4423, Loss: 3.462602138519287\n",
            "Training Iteration 4424, Loss: 4.221352577209473\n",
            "Training Iteration 4425, Loss: 2.582153797149658\n",
            "Training Iteration 4426, Loss: 1.8441295623779297\n",
            "Training Iteration 4427, Loss: 3.5994722843170166\n",
            "Training Iteration 4428, Loss: 3.683354616165161\n",
            "Training Iteration 4429, Loss: 5.686857223510742\n",
            "Training Iteration 4430, Loss: 6.062045097351074\n",
            "Training Iteration 4431, Loss: 3.6440341472625732\n",
            "Training Iteration 4432, Loss: 6.532958030700684\n",
            "Training Iteration 4433, Loss: 4.572813034057617\n",
            "Training Iteration 4434, Loss: 2.921492099761963\n",
            "Training Iteration 4435, Loss: 8.370950698852539\n",
            "Training Iteration 4436, Loss: 6.879330158233643\n",
            "Training Iteration 4437, Loss: 4.383639812469482\n",
            "Training Iteration 4438, Loss: 6.436046600341797\n",
            "Training Iteration 4439, Loss: 7.674583911895752\n",
            "Training Iteration 4440, Loss: 5.007096767425537\n",
            "Training Iteration 4441, Loss: 3.0926401615142822\n",
            "Training Iteration 4442, Loss: 2.9774158000946045\n",
            "Training Iteration 4443, Loss: 3.0414013862609863\n",
            "Training Iteration 4444, Loss: 3.265742778778076\n",
            "Training Iteration 4445, Loss: 5.294668197631836\n",
            "Training Iteration 4446, Loss: 7.889404296875\n",
            "Training Iteration 4447, Loss: 10.46929931640625\n",
            "Training Iteration 4448, Loss: 5.639944076538086\n",
            "Training Iteration 4449, Loss: 6.143982887268066\n",
            "Training Iteration 4450, Loss: 4.675410747528076\n",
            "Training Iteration 4451, Loss: 3.409893751144409\n",
            "Training Iteration 4452, Loss: 6.823619842529297\n",
            "Training Iteration 4453, Loss: 8.259087562561035\n",
            "Training Iteration 4454, Loss: 6.963626861572266\n",
            "Training Iteration 4455, Loss: 6.038638591766357\n",
            "Training Iteration 4456, Loss: 2.386106252670288\n",
            "Training Iteration 4457, Loss: 4.434929370880127\n",
            "Training Iteration 4458, Loss: 2.4122610092163086\n",
            "Training Iteration 4459, Loss: 3.531224250793457\n",
            "Training Iteration 4460, Loss: 6.000754356384277\n",
            "Training Iteration 4461, Loss: 4.392075061798096\n",
            "Training Iteration 4462, Loss: 2.898491621017456\n",
            "Training Iteration 4463, Loss: 4.557223796844482\n",
            "Training Iteration 4464, Loss: 6.137909412384033\n",
            "Training Iteration 4465, Loss: 3.415252685546875\n",
            "Training Iteration 4466, Loss: 5.187857151031494\n",
            "Training Iteration 4467, Loss: 3.5568277835845947\n",
            "Training Iteration 4468, Loss: 4.174459457397461\n",
            "Training Iteration 4469, Loss: 4.402834892272949\n",
            "Training Iteration 4470, Loss: 5.395308971405029\n",
            "Training Iteration 4471, Loss: 7.111011505126953\n",
            "Training Iteration 4472, Loss: 6.756984710693359\n",
            "Training Iteration 4473, Loss: 8.349002838134766\n",
            "Training Iteration 4474, Loss: 9.553186416625977\n",
            "Training Iteration 4475, Loss: 8.539481163024902\n",
            "Training Iteration 4476, Loss: 5.98138427734375\n",
            "Training Iteration 4477, Loss: 3.1263227462768555\n",
            "Training Iteration 4478, Loss: 2.464824914932251\n",
            "Training Iteration 4479, Loss: 2.9649569988250732\n",
            "Training Iteration 4480, Loss: 4.275516033172607\n",
            "Training Iteration 4481, Loss: 3.7648894786834717\n",
            "Training Iteration 4482, Loss: 4.194719314575195\n",
            "Training Iteration 4483, Loss: 8.383814811706543\n",
            "Training Iteration 4484, Loss: 4.504397869110107\n",
            "Training Iteration 4485, Loss: 3.6011147499084473\n",
            "Training Iteration 4486, Loss: 5.136322975158691\n",
            "Training Iteration 4487, Loss: 3.912471294403076\n",
            "Training Iteration 4488, Loss: 9.123138427734375\n",
            "Training Iteration 4489, Loss: 3.6050291061401367\n",
            "Training Iteration 4490, Loss: 5.255456447601318\n",
            "Training Iteration 4491, Loss: 4.016158103942871\n",
            "Training Iteration 4492, Loss: 7.74984073638916\n",
            "Training Iteration 4493, Loss: 4.479235649108887\n",
            "Training Iteration 4494, Loss: 1.837205171585083\n",
            "Training Iteration 4495, Loss: 5.58233642578125\n",
            "Training Iteration 4496, Loss: 5.334868431091309\n",
            "Training Iteration 4497, Loss: 7.651609420776367\n",
            "Training Iteration 4498, Loss: 5.83254337310791\n",
            "Training Iteration 4499, Loss: 5.208998203277588\n",
            "Training Iteration 4500, Loss: 5.309825420379639\n",
            "Training Iteration 4501, Loss: 3.1748087406158447\n",
            "Training Iteration 4502, Loss: 2.320554256439209\n",
            "Training Iteration 4503, Loss: 1.7423912286758423\n",
            "Training Iteration 4504, Loss: 7.633254528045654\n",
            "Training Iteration 4505, Loss: 3.8591060638427734\n",
            "Training Iteration 4506, Loss: 1.183891773223877\n",
            "Training Iteration 4507, Loss: 2.7731666564941406\n",
            "Training Iteration 4508, Loss: 2.1692605018615723\n",
            "Training Iteration 4509, Loss: 4.494133949279785\n",
            "Training Iteration 4510, Loss: 5.622497081756592\n",
            "Training Iteration 4511, Loss: 2.014063835144043\n",
            "Training Iteration 4512, Loss: 3.3386106491088867\n",
            "Training Iteration 4513, Loss: 2.910069465637207\n",
            "Training Iteration 4514, Loss: 3.3283815383911133\n",
            "Training Iteration 4515, Loss: 6.335729598999023\n",
            "Training Iteration 4516, Loss: 2.7440433502197266\n",
            "Training Iteration 4517, Loss: 4.483026027679443\n",
            "Training Iteration 4518, Loss: 7.4604082107543945\n",
            "Training Iteration 4519, Loss: 3.7744548320770264\n",
            "Training Iteration 4520, Loss: 2.719006061553955\n",
            "Training Iteration 4521, Loss: 4.486958026885986\n",
            "Training Iteration 4522, Loss: 2.032799482345581\n",
            "Training Iteration 4523, Loss: 2.903719902038574\n",
            "Training Iteration 4524, Loss: 5.979094505310059\n",
            "Training Iteration 4525, Loss: 3.0583620071411133\n",
            "Training Iteration 4526, Loss: 5.087130069732666\n",
            "Training Iteration 4527, Loss: 1.5022307634353638\n",
            "Training Iteration 4528, Loss: 2.995384454727173\n",
            "Training Iteration 4529, Loss: 3.1512186527252197\n",
            "Training Iteration 4530, Loss: 2.577401638031006\n",
            "Training Iteration 4531, Loss: 6.712498188018799\n",
            "Training Iteration 4532, Loss: 5.969818592071533\n",
            "Training Iteration 4533, Loss: 2.3202860355377197\n",
            "Training Iteration 4534, Loss: 3.654747486114502\n",
            "Training Iteration 4535, Loss: 7.090753078460693\n",
            "Training Iteration 4536, Loss: 2.2796897888183594\n",
            "Training Iteration 4537, Loss: 3.760756015777588\n",
            "Training Iteration 4538, Loss: 5.9249491691589355\n",
            "Training Iteration 4539, Loss: 5.786088943481445\n",
            "Training Iteration 4540, Loss: 2.598238945007324\n",
            "Training Iteration 4541, Loss: 3.676438808441162\n",
            "Training Iteration 4542, Loss: 4.881312370300293\n",
            "Training Iteration 4543, Loss: 1.8570799827575684\n",
            "Training Iteration 4544, Loss: 4.623502731323242\n",
            "Training Iteration 4545, Loss: 3.1625616550445557\n",
            "Training Iteration 4546, Loss: 3.342374801635742\n",
            "Training Iteration 4547, Loss: 2.4374630451202393\n",
            "Training Iteration 4548, Loss: 3.016798496246338\n",
            "Training Iteration 4549, Loss: 3.7344770431518555\n",
            "Training Iteration 4550, Loss: 4.466142177581787\n",
            "Training Iteration 4551, Loss: 4.922770977020264\n",
            "Training Iteration 4552, Loss: 4.021533012390137\n",
            "Training Iteration 4553, Loss: 7.203224182128906\n",
            "Training Iteration 4554, Loss: 3.1990957260131836\n",
            "Training Iteration 4555, Loss: 4.311028957366943\n",
            "Training Iteration 4556, Loss: 5.16988468170166\n",
            "Training Iteration 4557, Loss: 3.8101489543914795\n",
            "Training Iteration 4558, Loss: 2.894573211669922\n",
            "Training Iteration 4559, Loss: 3.57098126411438\n",
            "Training Iteration 4560, Loss: 3.717768669128418\n",
            "Training Iteration 4561, Loss: 3.3006479740142822\n",
            "Training Iteration 4562, Loss: 2.71394681930542\n",
            "Training Iteration 4563, Loss: 5.832675933837891\n",
            "Training Iteration 4564, Loss: 4.26602840423584\n",
            "Training Iteration 4565, Loss: 6.9383368492126465\n",
            "Training Iteration 4566, Loss: 7.6335577964782715\n",
            "Training Iteration 4567, Loss: 7.531620979309082\n",
            "Training Iteration 4568, Loss: 6.097989559173584\n",
            "Training Iteration 4569, Loss: 4.326724529266357\n",
            "Training Iteration 4570, Loss: 6.586202621459961\n",
            "Training Iteration 4571, Loss: 2.4675612449645996\n",
            "Training Iteration 4572, Loss: 7.013299942016602\n",
            "Training Iteration 4573, Loss: 9.031033515930176\n",
            "Training Iteration 4574, Loss: 4.778562545776367\n",
            "Training Iteration 4575, Loss: 4.666560649871826\n",
            "Training Iteration 4576, Loss: 3.737403392791748\n",
            "Training Iteration 4577, Loss: 4.928307056427002\n",
            "Training Iteration 4578, Loss: 5.948390007019043\n",
            "Training Iteration 4579, Loss: 7.229602813720703\n",
            "Training Iteration 4580, Loss: 9.106630325317383\n",
            "Training Iteration 4581, Loss: 6.9033918380737305\n",
            "Training Iteration 4582, Loss: 6.6928277015686035\n",
            "Training Iteration 4583, Loss: 6.2265214920043945\n",
            "Training Iteration 4584, Loss: 6.954230308532715\n",
            "Training Iteration 4585, Loss: 4.805542945861816\n",
            "Training Iteration 4586, Loss: 3.7008228302001953\n",
            "Training Iteration 4587, Loss: 3.0778589248657227\n",
            "Training Iteration 4588, Loss: 10.801937103271484\n",
            "Training Iteration 4589, Loss: 7.740478992462158\n",
            "Training Iteration 4590, Loss: 4.907556533813477\n",
            "Training Iteration 4591, Loss: 4.866207122802734\n",
            "Training Iteration 4592, Loss: 3.8270020484924316\n",
            "Training Iteration 4593, Loss: 7.4532551765441895\n",
            "Training Iteration 4594, Loss: 4.742744445800781\n",
            "Training Iteration 4595, Loss: 6.841716766357422\n",
            "Training Iteration 4596, Loss: 6.049641132354736\n",
            "Training Iteration 4597, Loss: 6.687332630157471\n",
            "Training Iteration 4598, Loss: 5.796110153198242\n",
            "Training Iteration 4599, Loss: 6.470332622528076\n",
            "Training Iteration 4600, Loss: 3.287226676940918\n",
            "Training Iteration 4601, Loss: 4.885529518127441\n",
            "Training Iteration 4602, Loss: 4.616511821746826\n",
            "Training Iteration 4603, Loss: 7.622509479522705\n",
            "Training Iteration 4604, Loss: 4.564484119415283\n",
            "Training Iteration 4605, Loss: 6.707972526550293\n",
            "Training Iteration 4606, Loss: 4.690977573394775\n",
            "Training Iteration 4607, Loss: 4.828864574432373\n",
            "Training Iteration 4608, Loss: 4.879426002502441\n",
            "Training Iteration 4609, Loss: 4.340970039367676\n",
            "Training Iteration 4610, Loss: 5.112412452697754\n",
            "Training Iteration 4611, Loss: 6.420969009399414\n",
            "Training Iteration 4612, Loss: 4.545201301574707\n",
            "Training Iteration 4613, Loss: 4.479252815246582\n",
            "Training Iteration 4614, Loss: 4.924161434173584\n",
            "Training Iteration 4615, Loss: 5.9488959312438965\n",
            "Training Iteration 4616, Loss: 4.208131313323975\n",
            "Training Iteration 4617, Loss: 3.7604269981384277\n",
            "Training Iteration 4618, Loss: 5.915939807891846\n",
            "Training Iteration 4619, Loss: 2.8266167640686035\n",
            "Training Iteration 4620, Loss: 3.887274980545044\n",
            "Training Iteration 4621, Loss: 3.9496982097625732\n",
            "Training Iteration 4622, Loss: 2.8995261192321777\n",
            "Training Iteration 4623, Loss: 6.0269670486450195\n",
            "Training Iteration 4624, Loss: 2.619478225708008\n",
            "Training Iteration 4625, Loss: 5.729853630065918\n",
            "Training Iteration 4626, Loss: 7.211590766906738\n",
            "Training Iteration 4627, Loss: 9.7239351272583\n",
            "Training Iteration 4628, Loss: 9.070523262023926\n",
            "Training Iteration 4629, Loss: 6.106753826141357\n",
            "Training Iteration 4630, Loss: 5.097470283508301\n",
            "Training Iteration 4631, Loss: 3.576070785522461\n",
            "Training Iteration 4632, Loss: 3.9648029804229736\n",
            "Training Iteration 4633, Loss: 7.303699970245361\n",
            "Training Iteration 4634, Loss: 4.067673683166504\n",
            "Training Iteration 4635, Loss: 3.6589231491088867\n",
            "Training Iteration 4636, Loss: 4.603518962860107\n",
            "Training Iteration 4637, Loss: 3.8656005859375\n",
            "Training Iteration 4638, Loss: 5.612812519073486\n",
            "Training Iteration 4639, Loss: 5.466465473175049\n",
            "Training Iteration 4640, Loss: 9.8112154006958\n",
            "Training Iteration 4641, Loss: 5.3517255783081055\n",
            "Training Iteration 4642, Loss: 6.314736843109131\n",
            "Training Iteration 4643, Loss: 3.436114549636841\n",
            "Training Iteration 4644, Loss: 3.5040926933288574\n",
            "Training Iteration 4645, Loss: 4.820534706115723\n",
            "Training Iteration 4646, Loss: 3.944866180419922\n",
            "Training Iteration 4647, Loss: 4.320517063140869\n",
            "Training Iteration 4648, Loss: 3.7165675163269043\n",
            "Training Iteration 4649, Loss: 4.075416564941406\n",
            "Training Iteration 4650, Loss: 1.8548274040222168\n",
            "Training Iteration 4651, Loss: 4.411002159118652\n",
            "Training Iteration 4652, Loss: 4.5880889892578125\n",
            "Training Iteration 4653, Loss: 4.834082126617432\n",
            "Training Iteration 4654, Loss: 3.0174660682678223\n",
            "Training Iteration 4655, Loss: 4.235103607177734\n",
            "Training Iteration 4656, Loss: 6.274860382080078\n",
            "Training Iteration 4657, Loss: 6.410953044891357\n",
            "Training Iteration 4658, Loss: 4.909598350524902\n",
            "Training Iteration 4659, Loss: 4.4236979484558105\n",
            "Training Iteration 4660, Loss: 4.4905314445495605\n",
            "Training Iteration 4661, Loss: 3.878941059112549\n",
            "Training Iteration 4662, Loss: 5.0357279777526855\n",
            "Training Iteration 4663, Loss: 5.773721218109131\n",
            "Training Iteration 4664, Loss: 4.054066181182861\n",
            "Training Iteration 4665, Loss: 4.143898010253906\n",
            "Training Iteration 4666, Loss: 5.454896450042725\n",
            "Training Iteration 4667, Loss: 3.920248031616211\n",
            "Training Iteration 4668, Loss: 4.483572006225586\n",
            "Training Iteration 4669, Loss: 3.152866840362549\n",
            "Training Iteration 4670, Loss: 6.043718338012695\n",
            "Training Iteration 4671, Loss: 3.4577486515045166\n",
            "Training Iteration 4672, Loss: 4.608546257019043\n",
            "Training Iteration 4673, Loss: 3.9315905570983887\n",
            "Training Iteration 4674, Loss: 5.754809856414795\n",
            "Training Iteration 4675, Loss: 8.177882194519043\n",
            "Training Iteration 4676, Loss: 4.457958221435547\n",
            "Training Iteration 4677, Loss: 5.678027629852295\n",
            "Training Iteration 4678, Loss: 3.454554557800293\n",
            "Training Iteration 4679, Loss: 4.5818867683410645\n",
            "Training Iteration 4680, Loss: 4.499696731567383\n",
            "Training Iteration 4681, Loss: 4.233050346374512\n",
            "Training Iteration 4682, Loss: 5.190946578979492\n",
            "Training Iteration 4683, Loss: 4.308166980743408\n",
            "Training Iteration 4684, Loss: 3.405123710632324\n",
            "Training Iteration 4685, Loss: 4.575135707855225\n",
            "Training Iteration 4686, Loss: 4.470235824584961\n",
            "Training Iteration 4687, Loss: 5.074261665344238\n",
            "Training Iteration 4688, Loss: 3.980498790740967\n",
            "Training Iteration 4689, Loss: 1.7031800746917725\n",
            "Training Iteration 4690, Loss: 5.2424468994140625\n",
            "Training Iteration 4691, Loss: 5.189206123352051\n",
            "Training Iteration 4692, Loss: 8.25606918334961\n",
            "Training Iteration 4693, Loss: 3.3563544750213623\n",
            "Training Iteration 4694, Loss: 3.970174789428711\n",
            "Training Iteration 4695, Loss: 4.90681791305542\n",
            "Training Iteration 4696, Loss: 7.912949562072754\n",
            "Training Iteration 4697, Loss: 6.1229329109191895\n",
            "Training Iteration 4698, Loss: 9.494832992553711\n",
            "Training Iteration 4699, Loss: 3.641291618347168\n",
            "Training Iteration 4700, Loss: 2.9184048175811768\n",
            "Training Iteration 4701, Loss: 3.5317673683166504\n",
            "Training Iteration 4702, Loss: 3.5830118656158447\n",
            "Training Iteration 4703, Loss: 1.7173902988433838\n",
            "Training Iteration 4704, Loss: 7.829851150512695\n",
            "Training Iteration 4705, Loss: 4.468384742736816\n",
            "Training Iteration 4706, Loss: 6.448607921600342\n",
            "Training Iteration 4707, Loss: 5.466955661773682\n",
            "Training Iteration 4708, Loss: 2.337517261505127\n",
            "Training Iteration 4709, Loss: 1.7520995140075684\n",
            "Training Iteration 4710, Loss: 3.2336223125457764\n",
            "Training Iteration 4711, Loss: 3.749445915222168\n",
            "Training Iteration 4712, Loss: 5.323113918304443\n",
            "Training Iteration 4713, Loss: 2.8102049827575684\n",
            "Training Iteration 4714, Loss: 3.9677412509918213\n",
            "Training Iteration 4715, Loss: 3.1832950115203857\n",
            "Training Iteration 4716, Loss: 1.9079678058624268\n",
            "Training Iteration 4717, Loss: 2.484589099884033\n",
            "Training Iteration 4718, Loss: 3.722921133041382\n",
            "Training Iteration 4719, Loss: 7.339852333068848\n",
            "Training Iteration 4720, Loss: 2.3564958572387695\n",
            "Training Iteration 4721, Loss: 4.199807643890381\n",
            "Training Iteration 4722, Loss: 5.106762409210205\n",
            "Training Iteration 4723, Loss: 3.583711862564087\n",
            "Training Iteration 4724, Loss: 3.0433733463287354\n",
            "Training Iteration 4725, Loss: 1.379259705543518\n",
            "Training Iteration 4726, Loss: 3.5240771770477295\n",
            "Training Iteration 4727, Loss: 4.227788925170898\n",
            "Training Iteration 4728, Loss: 5.8194146156311035\n",
            "Training Iteration 4729, Loss: 5.671780109405518\n",
            "Training Iteration 4730, Loss: 2.930759906768799\n",
            "Training Iteration 4731, Loss: 2.097149133682251\n",
            "Training Iteration 4732, Loss: 4.059918403625488\n",
            "Training Iteration 4733, Loss: 6.946866035461426\n",
            "Training Iteration 4734, Loss: 4.233290672302246\n",
            "Training Iteration 4735, Loss: 3.468484878540039\n",
            "Training Iteration 4736, Loss: 6.0206217765808105\n",
            "Training Iteration 4737, Loss: 2.400277853012085\n",
            "Training Iteration 4738, Loss: 4.57729434967041\n",
            "Training Iteration 4739, Loss: 3.2375588417053223\n",
            "Training Iteration 4740, Loss: 4.440335273742676\n",
            "Training Iteration 4741, Loss: 1.0942778587341309\n",
            "Training Iteration 4742, Loss: 6.1309309005737305\n",
            "Training Iteration 4743, Loss: 5.500430583953857\n",
            "Training Iteration 4744, Loss: 6.4780426025390625\n",
            "Training Iteration 4745, Loss: 7.830471038818359\n",
            "Training Iteration 4746, Loss: 6.749671936035156\n",
            "Training Iteration 4747, Loss: 5.84700345993042\n",
            "Training Iteration 4748, Loss: 5.367289066314697\n",
            "Training Iteration 4749, Loss: 4.316659450531006\n",
            "Training Iteration 4750, Loss: 5.715240955352783\n",
            "Training Iteration 4751, Loss: 6.006316661834717\n",
            "Training Iteration 4752, Loss: 5.57767915725708\n",
            "Training Iteration 4753, Loss: 5.205580234527588\n",
            "Training Iteration 4754, Loss: 5.276818752288818\n",
            "Training Iteration 4755, Loss: 4.776780128479004\n",
            "Training Iteration 4756, Loss: 5.254673480987549\n",
            "Training Iteration 4757, Loss: 5.329590320587158\n",
            "Training Iteration 4758, Loss: 7.681734561920166\n",
            "Training Iteration 4759, Loss: 2.3016672134399414\n",
            "Training Iteration 4760, Loss: 4.9912872314453125\n",
            "Training Iteration 4761, Loss: 5.54827880859375\n",
            "Training Iteration 4762, Loss: 5.24581241607666\n",
            "Training Iteration 4763, Loss: 10.679204940795898\n",
            "Training Iteration 4764, Loss: 4.194618225097656\n",
            "Training Iteration 4765, Loss: 6.04937219619751\n",
            "Training Iteration 4766, Loss: 2.986985683441162\n",
            "Training Iteration 4767, Loss: 3.2628676891326904\n",
            "Training Iteration 4768, Loss: 5.605499744415283\n",
            "Training Iteration 4769, Loss: 4.004661560058594\n",
            "Training Iteration 4770, Loss: 6.6673712730407715\n",
            "Training Iteration 4771, Loss: 6.659612655639648\n",
            "Training Iteration 4772, Loss: 3.321852207183838\n",
            "Training Iteration 4773, Loss: 3.2125027179718018\n",
            "Training Iteration 4774, Loss: 4.590391159057617\n",
            "Training Iteration 4775, Loss: 3.3330190181732178\n",
            "Training Iteration 4776, Loss: 4.536725997924805\n",
            "Training Iteration 4777, Loss: 4.528626441955566\n",
            "Training Iteration 4778, Loss: 5.418448448181152\n",
            "Training Iteration 4779, Loss: 3.344254493713379\n",
            "Training Iteration 4780, Loss: 6.0007147789001465\n",
            "Training Iteration 4781, Loss: 3.296769142150879\n",
            "Training Iteration 4782, Loss: 4.104475975036621\n",
            "Training Iteration 4783, Loss: 5.07696533203125\n",
            "Training Iteration 4784, Loss: 4.403409481048584\n",
            "Training Iteration 4785, Loss: 4.131067752838135\n",
            "Training Iteration 4786, Loss: 4.751736164093018\n",
            "Training Iteration 4787, Loss: 2.4368984699249268\n",
            "Training Iteration 4788, Loss: 4.843815326690674\n",
            "Training Iteration 4789, Loss: 2.2866628170013428\n",
            "Training Iteration 4790, Loss: 3.018655776977539\n",
            "Training Iteration 4791, Loss: 9.754351615905762\n",
            "Training Iteration 4792, Loss: 5.9789204597473145\n",
            "Training Iteration 4793, Loss: 3.9239888191223145\n",
            "Training Iteration 4794, Loss: 4.823003768920898\n",
            "Training Iteration 4795, Loss: 2.9327332973480225\n",
            "Training Iteration 4796, Loss: 6.447808265686035\n",
            "Training Iteration 4797, Loss: 2.9980170726776123\n",
            "Training Iteration 4798, Loss: 3.6073379516601562\n",
            "Training Iteration 4799, Loss: 6.936430931091309\n",
            "Training Iteration 4800, Loss: 5.463555812835693\n",
            "Training Iteration 4801, Loss: 6.335083961486816\n",
            "Training Iteration 4802, Loss: 3.26859712600708\n",
            "Training Iteration 4803, Loss: 2.518951654434204\n",
            "Training Iteration 4804, Loss: 4.116841793060303\n",
            "Training Iteration 4805, Loss: 4.078105926513672\n",
            "Training Iteration 4806, Loss: 3.8215854167938232\n",
            "Training Iteration 4807, Loss: 2.403590679168701\n",
            "Training Iteration 4808, Loss: 4.017126560211182\n",
            "Training Iteration 4809, Loss: 3.54910945892334\n",
            "Training Iteration 4810, Loss: 4.335891246795654\n",
            "Training Iteration 4811, Loss: 7.293459415435791\n",
            "Training Iteration 4812, Loss: 2.4850914478302\n",
            "Training Iteration 4813, Loss: 1.5873563289642334\n",
            "Training Iteration 4814, Loss: 3.204625129699707\n",
            "Training Iteration 4815, Loss: 3.3728723526000977\n",
            "Training Iteration 4816, Loss: 4.4912848472595215\n",
            "Training Iteration 4817, Loss: 6.851529121398926\n",
            "Training Iteration 4818, Loss: 1.8662140369415283\n",
            "Training Iteration 4819, Loss: 2.9206016063690186\n",
            "Training Iteration 4820, Loss: 6.285140514373779\n",
            "Training Iteration 4821, Loss: 3.840737819671631\n",
            "Training Iteration 4822, Loss: 2.0355069637298584\n",
            "Training Iteration 4823, Loss: 5.867090225219727\n",
            "Training Iteration 4824, Loss: 5.516374111175537\n",
            "Training Iteration 4825, Loss: 3.752393960952759\n",
            "Training Iteration 4826, Loss: 4.965404510498047\n",
            "Training Iteration 4827, Loss: 4.1545586585998535\n",
            "Training Iteration 4828, Loss: 4.9014506340026855\n",
            "Training Iteration 4829, Loss: 5.7404375076293945\n",
            "Training Iteration 4830, Loss: 3.929014205932617\n",
            "Training Iteration 4831, Loss: 3.550450325012207\n",
            "Training Iteration 4832, Loss: 4.622148036956787\n",
            "Training Iteration 4833, Loss: 4.684691429138184\n",
            "Training Iteration 4834, Loss: 4.254158020019531\n",
            "Training Iteration 4835, Loss: 4.793833255767822\n",
            "Training Iteration 4836, Loss: 3.6454217433929443\n",
            "Training Iteration 4837, Loss: 2.467895984649658\n",
            "Training Iteration 4838, Loss: 3.4571211338043213\n",
            "Training Iteration 4839, Loss: 2.266371250152588\n",
            "Training Iteration 4840, Loss: 2.9435389041900635\n",
            "Training Iteration 4841, Loss: 5.264272689819336\n",
            "Training Iteration 4842, Loss: 5.743061065673828\n",
            "Training Iteration 4843, Loss: 3.6288819313049316\n",
            "Training Iteration 4844, Loss: 1.9982399940490723\n",
            "Training Iteration 4845, Loss: 3.232233762741089\n",
            "Training Iteration 4846, Loss: 3.711118698120117\n",
            "Training Iteration 4847, Loss: 3.269232749938965\n",
            "Training Iteration 4848, Loss: 4.864117622375488\n",
            "Training Iteration 4849, Loss: 3.9698688983917236\n",
            "Training Iteration 4850, Loss: 6.932607650756836\n",
            "Training Iteration 4851, Loss: 5.6361212730407715\n",
            "Training Iteration 4852, Loss: 5.582548141479492\n",
            "Training Iteration 4853, Loss: 2.6351683139801025\n",
            "Training Iteration 4854, Loss: 2.6554107666015625\n",
            "Training Iteration 4855, Loss: 6.213574409484863\n",
            "Training Iteration 4856, Loss: 4.175747394561768\n",
            "Training Iteration 4857, Loss: 3.9117112159729004\n",
            "Training Iteration 4858, Loss: 6.660279273986816\n",
            "Training Iteration 4859, Loss: 6.434712886810303\n",
            "Training Iteration 4860, Loss: 5.620466232299805\n",
            "Training Iteration 4861, Loss: 3.282010078430176\n",
            "Training Iteration 4862, Loss: 8.460380554199219\n",
            "Training Iteration 4863, Loss: 4.839610576629639\n",
            "Training Iteration 4864, Loss: 1.8252391815185547\n",
            "Training Iteration 4865, Loss: 4.166090965270996\n",
            "Training Iteration 4866, Loss: 4.673264980316162\n",
            "Training Iteration 4867, Loss: 8.743257522583008\n",
            "Training Iteration 4868, Loss: 6.763469696044922\n",
            "Training Iteration 4869, Loss: 2.667896032333374\n",
            "Training Iteration 4870, Loss: 10.761541366577148\n",
            "Training Iteration 4871, Loss: 4.138153076171875\n",
            "Training Iteration 4872, Loss: 3.3573851585388184\n",
            "Training Iteration 4873, Loss: 1.6359981298446655\n",
            "Training Iteration 4874, Loss: 5.366629600524902\n",
            "Training Iteration 4875, Loss: 5.093469142913818\n",
            "Training Iteration 4876, Loss: 3.3427798748016357\n",
            "Training Iteration 4877, Loss: 4.768008232116699\n",
            "Training Iteration 4878, Loss: 5.677488803863525\n",
            "Training Iteration 4879, Loss: 5.9319376945495605\n",
            "Training Iteration 4880, Loss: 3.9926304817199707\n",
            "Training Iteration 4881, Loss: 5.525107383728027\n",
            "Training Iteration 4882, Loss: 5.768564224243164\n",
            "Training Iteration 4883, Loss: 5.263913631439209\n",
            "Training Iteration 4884, Loss: 4.5116472244262695\n",
            "Training Iteration 4885, Loss: 2.8240737915039062\n",
            "Training Iteration 4886, Loss: 6.330178260803223\n",
            "Training Iteration 4887, Loss: 3.948986291885376\n",
            "Training Iteration 4888, Loss: 3.88028621673584\n",
            "Training Iteration 4889, Loss: 2.6951329708099365\n",
            "Training Iteration 4890, Loss: 5.3669586181640625\n",
            "Training Iteration 4891, Loss: 4.106171607971191\n",
            "Training Iteration 4892, Loss: 7.274639129638672\n",
            "Training Iteration 4893, Loss: 5.685689926147461\n",
            "Training Iteration 4894, Loss: 4.501821041107178\n",
            "Training Iteration 4895, Loss: 6.953962326049805\n",
            "Training Iteration 4896, Loss: 7.002713203430176\n",
            "Training Iteration 4897, Loss: 3.7228598594665527\n",
            "Training Iteration 4898, Loss: 7.068705081939697\n",
            "Training Iteration 4899, Loss: 3.8341615200042725\n",
            "Training Iteration 4900, Loss: 4.570789813995361\n",
            "Training Iteration 4901, Loss: 6.290641784667969\n",
            "Training Iteration 4902, Loss: 3.9906346797943115\n",
            "Training Iteration 4903, Loss: 5.304148197174072\n",
            "Training Iteration 4904, Loss: 3.02032208442688\n",
            "Training Iteration 4905, Loss: 3.88547945022583\n",
            "Training Iteration 4906, Loss: 3.8648719787597656\n",
            "Training Iteration 4907, Loss: 5.794491291046143\n",
            "Training Iteration 4908, Loss: 3.3583953380584717\n",
            "Training Iteration 4909, Loss: 4.260909557342529\n",
            "Training Iteration 4910, Loss: 6.697011470794678\n",
            "Training Iteration 4911, Loss: 2.4694764614105225\n",
            "Training Iteration 4912, Loss: 4.347269535064697\n",
            "Training Iteration 4913, Loss: 2.1369247436523438\n",
            "Training Iteration 4914, Loss: 6.647461891174316\n",
            "Training Iteration 4915, Loss: 4.084693908691406\n",
            "Training Iteration 4916, Loss: 2.0657739639282227\n",
            "Training Iteration 4917, Loss: 3.655268430709839\n",
            "Training Iteration 4918, Loss: 3.3596882820129395\n",
            "Training Iteration 4919, Loss: 3.43843674659729\n",
            "Training Iteration 4920, Loss: 4.636509895324707\n",
            "Training Iteration 4921, Loss: 6.7196807861328125\n",
            "Training Iteration 4922, Loss: 6.7122721672058105\n",
            "Training Iteration 4923, Loss: 2.312889575958252\n",
            "Training Iteration 4924, Loss: 4.688416957855225\n",
            "Training Iteration 4925, Loss: 2.29526686668396\n",
            "Training Iteration 4926, Loss: 5.168412208557129\n",
            "Training Iteration 4927, Loss: 3.943488121032715\n",
            "Training Iteration 4928, Loss: 2.1169872283935547\n",
            "Training Iteration 4929, Loss: 3.1860952377319336\n",
            "Training Iteration 4930, Loss: 3.6932411193847656\n",
            "Training Iteration 4931, Loss: 2.3583314418792725\n",
            "Training Iteration 4932, Loss: 5.337790489196777\n",
            "Training Iteration 4933, Loss: 2.3986990451812744\n",
            "Training Iteration 4934, Loss: 4.212579250335693\n",
            "Training Iteration 4935, Loss: 4.646261692047119\n",
            "Training Iteration 4936, Loss: 4.434096813201904\n",
            "Training Iteration 4937, Loss: 3.4100332260131836\n",
            "Training Iteration 4938, Loss: 1.7443434000015259\n",
            "Training Iteration 4939, Loss: 3.432513952255249\n",
            "Training Iteration 4940, Loss: 4.074746131896973\n",
            "Training Iteration 4941, Loss: 8.550612449645996\n",
            "Training Iteration 4942, Loss: 2.1377134323120117\n",
            "Training Iteration 4943, Loss: 4.376031875610352\n",
            "Training Iteration 4944, Loss: 5.769780158996582\n",
            "Training Iteration 4945, Loss: 5.88498067855835\n",
            "Training Iteration 4946, Loss: 5.559969425201416\n",
            "Training Iteration 4947, Loss: 3.4198484420776367\n",
            "Training Iteration 4948, Loss: 3.3630969524383545\n",
            "Training Iteration 4949, Loss: 3.2502706050872803\n",
            "Training Iteration 4950, Loss: 3.3311307430267334\n",
            "Training Iteration 4951, Loss: 4.217878818511963\n",
            "Training Iteration 4952, Loss: 3.68564510345459\n",
            "Training Iteration 4953, Loss: 4.327104568481445\n",
            "Training Iteration 4954, Loss: 4.300784587860107\n",
            "Training Iteration 4955, Loss: 2.33896803855896\n",
            "Training Iteration 4956, Loss: 7.776638984680176\n",
            "Training Iteration 4957, Loss: 5.054844856262207\n",
            "Training Iteration 4958, Loss: 4.918407440185547\n",
            "Training Iteration 4959, Loss: 2.041806221008301\n",
            "Training Iteration 4960, Loss: 3.085402011871338\n",
            "Training Iteration 4961, Loss: 6.698757171630859\n",
            "Training Iteration 4962, Loss: 3.6999640464782715\n",
            "Training Iteration 4963, Loss: 3.592543840408325\n",
            "Training Iteration 4964, Loss: 7.133458614349365\n",
            "Training Iteration 4965, Loss: 3.6594860553741455\n",
            "Training Iteration 4966, Loss: 9.025479316711426\n",
            "Training Iteration 4967, Loss: 4.269766807556152\n",
            "Training Iteration 4968, Loss: 4.46723747253418\n",
            "Training Iteration 4969, Loss: 7.061744689941406\n",
            "Training Iteration 4970, Loss: 7.297427654266357\n",
            "Training Iteration 4971, Loss: 4.995719909667969\n",
            "Training Iteration 4972, Loss: 4.989224910736084\n",
            "Training Iteration 4973, Loss: 3.952969789505005\n",
            "Training Iteration 4974, Loss: 4.926338195800781\n",
            "Training Iteration 4975, Loss: 4.410521507263184\n",
            "Training Iteration 4976, Loss: 5.356001377105713\n",
            "Training Iteration 4977, Loss: 2.44939923286438\n",
            "Training Iteration 4978, Loss: 2.579591989517212\n",
            "Training Iteration 4979, Loss: 5.657838344573975\n",
            "Training Iteration 4980, Loss: 2.055778741836548\n",
            "Training Iteration 4981, Loss: 4.144178867340088\n",
            "Training Iteration 4982, Loss: 3.709380626678467\n",
            "Training Iteration 4983, Loss: 3.644503116607666\n",
            "Training Iteration 4984, Loss: 5.161562442779541\n",
            "Training Iteration 4985, Loss: 10.51639461517334\n",
            "Training Iteration 4986, Loss: 11.46218490600586\n",
            "Training Iteration 4987, Loss: 3.359888792037964\n",
            "Training Iteration 4988, Loss: 9.63383960723877\n",
            "Training Iteration 4989, Loss: 3.0997424125671387\n",
            "Training Iteration 4990, Loss: 3.9831252098083496\n",
            "Training Iteration 4991, Loss: 1.746110439300537\n",
            "Training Iteration 4992, Loss: 5.636086463928223\n",
            "Training Iteration 4993, Loss: 4.654855251312256\n",
            "Training Iteration 4994, Loss: 4.247511863708496\n",
            "Training Iteration 4995, Loss: 2.9735987186431885\n",
            "Training Iteration 4996, Loss: 4.634110450744629\n",
            "Training Iteration 4997, Loss: 2.971182346343994\n",
            "Training Iteration 4998, Loss: 4.497681140899658\n",
            "Training Iteration 4999, Loss: 6.19877815246582\n",
            "Training Iteration 5000, Loss: 2.733668327331543\n",
            "Training Iteration 5001, Loss: 3.970118284225464\n",
            "Training Iteration 5002, Loss: 2.4301397800445557\n",
            "Training Iteration 5003, Loss: 6.877338409423828\n",
            "Training Iteration 5004, Loss: 5.552878379821777\n",
            "Training Iteration 5005, Loss: 3.649409055709839\n",
            "Training Iteration 5006, Loss: 8.142091751098633\n",
            "Training Iteration 5007, Loss: 3.4736452102661133\n",
            "Training Iteration 5008, Loss: 2.5975382328033447\n",
            "Training Iteration 5009, Loss: 6.2551774978637695\n",
            "Training Iteration 5010, Loss: 6.501829624176025\n",
            "Training Iteration 5011, Loss: 3.364703893661499\n",
            "Training Iteration 5012, Loss: 4.26561164855957\n",
            "Training Iteration 5013, Loss: 4.351003646850586\n",
            "Training Iteration 5014, Loss: 4.9626078605651855\n",
            "Training Iteration 5015, Loss: 5.579145431518555\n",
            "Training Iteration 5016, Loss: 4.947458267211914\n",
            "Training Iteration 5017, Loss: 3.320892572402954\n",
            "Training Iteration 5018, Loss: 6.869450092315674\n",
            "Training Iteration 5019, Loss: 3.8682665824890137\n",
            "Training Iteration 5020, Loss: 3.4543442726135254\n",
            "Training Iteration 5021, Loss: 4.2010393142700195\n",
            "Training Iteration 5022, Loss: 3.140324592590332\n",
            "Training Iteration 5023, Loss: 6.498934745788574\n",
            "Training Iteration 5024, Loss: 2.6589794158935547\n",
            "Training Iteration 5025, Loss: 3.5308308601379395\n",
            "Training Iteration 5026, Loss: 3.2533118724823\n",
            "Training Iteration 5027, Loss: 3.902010679244995\n",
            "Training Iteration 5028, Loss: 5.378208160400391\n",
            "Training Iteration 5029, Loss: 3.3909380435943604\n",
            "Training Iteration 5030, Loss: 5.013247013092041\n",
            "Training Iteration 5031, Loss: 4.56301212310791\n",
            "Training Iteration 5032, Loss: 5.992819786071777\n",
            "Training Iteration 5033, Loss: 7.2468438148498535\n",
            "Training Iteration 5034, Loss: 4.763315200805664\n",
            "Training Iteration 5035, Loss: 1.6638448238372803\n",
            "Training Iteration 5036, Loss: 2.989619493484497\n",
            "Training Iteration 5037, Loss: 2.956937551498413\n",
            "Training Iteration 5038, Loss: 4.749501705169678\n",
            "Training Iteration 5039, Loss: 4.616363048553467\n",
            "Training Iteration 5040, Loss: 3.2640480995178223\n",
            "Training Iteration 5041, Loss: 2.5288054943084717\n",
            "Training Iteration 5042, Loss: 4.350462436676025\n",
            "Training Iteration 5043, Loss: 7.555089950561523\n",
            "Training Iteration 5044, Loss: 4.260535717010498\n",
            "Training Iteration 5045, Loss: 4.323179244995117\n",
            "Training Iteration 5046, Loss: 1.937964677810669\n",
            "Training Iteration 5047, Loss: 3.5760462284088135\n",
            "Training Iteration 5048, Loss: 2.6291773319244385\n",
            "Training Iteration 5049, Loss: 2.1959829330444336\n",
            "Training Iteration 5050, Loss: 4.978285789489746\n",
            "Training Iteration 5051, Loss: 1.8605706691741943\n",
            "Training Iteration 5052, Loss: 5.853206157684326\n",
            "Training Iteration 5053, Loss: 2.8262131214141846\n",
            "Training Iteration 5054, Loss: 4.789712429046631\n",
            "Training Iteration 5055, Loss: 3.568254232406616\n",
            "Training Iteration 5056, Loss: 4.097711086273193\n",
            "Training Iteration 5057, Loss: 4.533087253570557\n",
            "Training Iteration 5058, Loss: 1.8112149238586426\n",
            "Training Iteration 5059, Loss: 3.442096710205078\n",
            "Training Iteration 5060, Loss: 2.284043550491333\n",
            "Training Iteration 5061, Loss: 7.3559417724609375\n",
            "Training Iteration 5062, Loss: 9.034147262573242\n",
            "Training Iteration 5063, Loss: 2.54872727394104\n",
            "Training Iteration 5064, Loss: 5.346508026123047\n",
            "Training Iteration 5065, Loss: 3.391659736633301\n",
            "Training Iteration 5066, Loss: 3.273005723953247\n",
            "Training Iteration 5067, Loss: 5.730555534362793\n",
            "Training Iteration 5068, Loss: 1.7502119541168213\n",
            "Training Iteration 5069, Loss: 2.9727160930633545\n",
            "Training Iteration 5070, Loss: 9.204780578613281\n",
            "Training Iteration 5071, Loss: 6.670546054840088\n",
            "Training Iteration 5072, Loss: 8.227898597717285\n",
            "Training Iteration 5073, Loss: 2.860675811767578\n",
            "Training Iteration 5074, Loss: 4.3850507736206055\n",
            "Training Iteration 5075, Loss: 6.291976451873779\n",
            "Training Iteration 5076, Loss: 2.537360668182373\n",
            "Training Iteration 5077, Loss: 4.164775371551514\n",
            "Training Iteration 5078, Loss: 7.264270782470703\n",
            "Training Iteration 5079, Loss: 6.4948811531066895\n",
            "Training Iteration 5080, Loss: 5.32427453994751\n",
            "Training Iteration 5081, Loss: 3.0644123554229736\n",
            "Training Iteration 5082, Loss: 4.880828857421875\n",
            "Training Iteration 5083, Loss: 7.156381130218506\n",
            "Training Iteration 5084, Loss: 4.878707408905029\n",
            "Training Iteration 5085, Loss: 2.7402029037475586\n",
            "Training Iteration 5086, Loss: 3.759368419647217\n",
            "Training Iteration 5087, Loss: 3.372114658355713\n",
            "Training Iteration 5088, Loss: 2.449383020401001\n",
            "Training Iteration 5089, Loss: 5.247398376464844\n",
            "Training Iteration 5090, Loss: 4.360561370849609\n",
            "Training Iteration 5091, Loss: 4.146187782287598\n",
            "Training Iteration 5092, Loss: 2.6172521114349365\n",
            "Training Iteration 5093, Loss: 5.187844276428223\n",
            "Training Iteration 5094, Loss: 4.733736515045166\n",
            "Training Iteration 5095, Loss: 3.537442445755005\n",
            "Training Iteration 5096, Loss: 1.7708361148834229\n",
            "Training Iteration 5097, Loss: 3.4202890396118164\n",
            "Training Iteration 5098, Loss: 5.050098896026611\n",
            "Training Iteration 5099, Loss: 4.820825576782227\n",
            "Training Iteration 5100, Loss: 7.825540542602539\n",
            "Training Iteration 5101, Loss: 4.993165969848633\n",
            "Training Iteration 5102, Loss: 7.071730613708496\n",
            "Training Iteration 5103, Loss: 1.0082664489746094\n",
            "Training Iteration 5104, Loss: 5.4690093994140625\n",
            "Training Iteration 5105, Loss: 1.5641627311706543\n",
            "Training Iteration 5106, Loss: 2.119544744491577\n",
            "Training Iteration 5107, Loss: 4.613925933837891\n",
            "Training Iteration 5108, Loss: 5.183207988739014\n",
            "Training Iteration 5109, Loss: 1.6639893054962158\n",
            "Training Iteration 5110, Loss: 4.621366024017334\n",
            "Training Iteration 5111, Loss: 3.3057336807250977\n",
            "Training Iteration 5112, Loss: 4.240607261657715\n",
            "Training Iteration 5113, Loss: 6.6332807540893555\n",
            "Training Iteration 5114, Loss: 3.179046392440796\n",
            "Training Iteration 5115, Loss: 6.101857662200928\n",
            "Training Iteration 5116, Loss: 3.3305091857910156\n",
            "Training Iteration 5117, Loss: 4.032740592956543\n",
            "Training Iteration 5118, Loss: 7.098430633544922\n",
            "Training Iteration 5119, Loss: 2.3063831329345703\n",
            "Training Iteration 5120, Loss: 3.0736982822418213\n",
            "Training Iteration 5121, Loss: 6.449517250061035\n",
            "Training Iteration 5122, Loss: 3.37882661819458\n",
            "Training Iteration 5123, Loss: 5.438813209533691\n",
            "Training Iteration 5124, Loss: 5.200921058654785\n",
            "Training Iteration 5125, Loss: 4.4198102951049805\n",
            "Training Iteration 5126, Loss: 3.6371209621429443\n",
            "Training Iteration 5127, Loss: 5.999965667724609\n",
            "Training Iteration 5128, Loss: 6.065506935119629\n",
            "Training Iteration 5129, Loss: 4.299383163452148\n",
            "Training Iteration 5130, Loss: 3.11095929145813\n",
            "Training Iteration 5131, Loss: 4.330264091491699\n",
            "Training Iteration 5132, Loss: 5.34958553314209\n",
            "Training Iteration 5133, Loss: 3.5664687156677246\n",
            "Training Iteration 5134, Loss: 6.886129856109619\n",
            "Training Iteration 5135, Loss: 4.486195087432861\n",
            "Training Iteration 5136, Loss: 3.5306737422943115\n",
            "Training Iteration 5137, Loss: 5.156761646270752\n",
            "Training Iteration 5138, Loss: 2.434736967086792\n",
            "Training Iteration 5139, Loss: 2.962641954421997\n",
            "Training Iteration 5140, Loss: 4.072607040405273\n",
            "Training Iteration 5141, Loss: 3.781024694442749\n",
            "Training Iteration 5142, Loss: 4.778146743774414\n",
            "Training Iteration 5143, Loss: 3.6419248580932617\n",
            "Training Iteration 5144, Loss: 5.339264392852783\n",
            "Training Iteration 5145, Loss: 1.896590232849121\n",
            "Training Iteration 5146, Loss: 2.651564121246338\n",
            "Training Iteration 5147, Loss: 2.9684722423553467\n",
            "Training Iteration 5148, Loss: 3.8416261672973633\n",
            "Training Iteration 5149, Loss: 4.468216896057129\n",
            "Training Iteration 5150, Loss: 4.99564266204834\n",
            "Training Iteration 5151, Loss: 3.7256364822387695\n",
            "Training Iteration 5152, Loss: 4.106741428375244\n",
            "Training Iteration 5153, Loss: 5.0140061378479\n",
            "Training Iteration 5154, Loss: 2.823153495788574\n",
            "Training Iteration 5155, Loss: 2.7011685371398926\n",
            "Training Iteration 5156, Loss: 4.1812849044799805\n",
            "Training Iteration 5157, Loss: 4.031671047210693\n",
            "Training Iteration 5158, Loss: 4.782106399536133\n",
            "Training Iteration 5159, Loss: 5.109172344207764\n",
            "Training Iteration 5160, Loss: 6.4089765548706055\n",
            "Training Iteration 5161, Loss: 4.179116249084473\n",
            "Training Iteration 5162, Loss: 4.770517349243164\n",
            "Training Iteration 5163, Loss: 4.804190635681152\n",
            "Training Iteration 5164, Loss: 3.731346368789673\n",
            "Training Iteration 5165, Loss: 4.54995584487915\n",
            "Training Iteration 5166, Loss: 5.344693660736084\n",
            "Training Iteration 5167, Loss: 3.6375746726989746\n",
            "Training Iteration 5168, Loss: 6.595934867858887\n",
            "Training Iteration 5169, Loss: 4.058547019958496\n",
            "Training Iteration 5170, Loss: 4.1211113929748535\n",
            "Training Iteration 5171, Loss: 2.438067674636841\n",
            "Training Iteration 5172, Loss: 4.5764007568359375\n",
            "Training Iteration 5173, Loss: 5.468251705169678\n",
            "Training Iteration 5174, Loss: 2.3958728313446045\n",
            "Training Iteration 5175, Loss: 5.137466907501221\n",
            "Training Iteration 5176, Loss: 6.198452949523926\n",
            "Training Iteration 5177, Loss: 5.141191005706787\n",
            "Training Iteration 5178, Loss: 2.5873868465423584\n",
            "Training Iteration 5179, Loss: 4.8201165199279785\n",
            "Training Iteration 5180, Loss: 4.818936347961426\n",
            "Training Iteration 5181, Loss: 7.412441253662109\n",
            "Training Iteration 5182, Loss: 3.7240800857543945\n",
            "Training Iteration 5183, Loss: 3.2351880073547363\n",
            "Training Iteration 5184, Loss: 4.672459125518799\n",
            "Training Iteration 5185, Loss: 4.12413215637207\n",
            "Training Iteration 5186, Loss: 6.2275824546813965\n",
            "Training Iteration 5187, Loss: 4.4946489334106445\n",
            "Training Iteration 5188, Loss: 6.670809745788574\n",
            "Training Iteration 5189, Loss: 2.876884937286377\n",
            "Training Iteration 5190, Loss: 8.424888610839844\n",
            "Training Iteration 5191, Loss: 6.354583263397217\n",
            "Training Iteration 5192, Loss: 2.691293954849243\n",
            "Training Iteration 5193, Loss: 4.363368034362793\n",
            "Training Iteration 5194, Loss: 4.613291263580322\n",
            "Training Iteration 5195, Loss: 1.831518292427063\n",
            "Training Iteration 5196, Loss: 2.8226490020751953\n",
            "Training Iteration 5197, Loss: 4.943641662597656\n",
            "Training Iteration 5198, Loss: 4.073589324951172\n",
            "Training Iteration 5199, Loss: 6.456883430480957\n",
            "Training Iteration 5200, Loss: 3.5176680088043213\n",
            "Training Iteration 5201, Loss: 4.887441635131836\n",
            "Training Iteration 5202, Loss: 3.560537576675415\n",
            "Training Iteration 5203, Loss: 4.791669845581055\n",
            "Training Iteration 5204, Loss: 5.175262451171875\n",
            "Training Iteration 5205, Loss: 2.8065505027770996\n",
            "Training Iteration 5206, Loss: 2.836243152618408\n",
            "Training Iteration 5207, Loss: 4.701233386993408\n",
            "Training Iteration 5208, Loss: 5.680097579956055\n",
            "Training Iteration 5209, Loss: 4.187975883483887\n",
            "Training Iteration 5210, Loss: 4.291586399078369\n",
            "Training Iteration 5211, Loss: 2.429548740386963\n",
            "Training Iteration 5212, Loss: 3.823700428009033\n",
            "Training Iteration 5213, Loss: 6.567299842834473\n",
            "Training Iteration 5214, Loss: 3.974130630493164\n",
            "Training Iteration 5215, Loss: 4.991852760314941\n",
            "Training Iteration 5216, Loss: 6.540900230407715\n",
            "Training Iteration 5217, Loss: 4.996409893035889\n",
            "Training Iteration 5218, Loss: 5.090373992919922\n",
            "Training Iteration 5219, Loss: 7.037574768066406\n",
            "Training Iteration 5220, Loss: 2.702723979949951\n",
            "Training Iteration 5221, Loss: 5.695061206817627\n",
            "Training Iteration 5222, Loss: 6.136898994445801\n",
            "Training Iteration 5223, Loss: 4.470287799835205\n",
            "Training Iteration 5224, Loss: 4.018835544586182\n",
            "Training Iteration 5225, Loss: 7.139837741851807\n",
            "Training Iteration 5226, Loss: 6.670315265655518\n",
            "Training Iteration 5227, Loss: 2.3780434131622314\n",
            "Training Iteration 5228, Loss: 6.096304893493652\n",
            "Training Iteration 5229, Loss: 3.825514078140259\n",
            "Training Iteration 5230, Loss: 3.809333562850952\n",
            "Training Iteration 5231, Loss: 7.5061187744140625\n",
            "Training Iteration 5232, Loss: 4.773892402648926\n",
            "Training Iteration 5233, Loss: 4.934003829956055\n",
            "Training Iteration 5234, Loss: 6.0302605628967285\n",
            "Training Iteration 5235, Loss: 5.01596736907959\n",
            "Training Iteration 5236, Loss: 2.815520763397217\n",
            "Training Iteration 5237, Loss: 1.6447420120239258\n",
            "Training Iteration 5238, Loss: 4.150543689727783\n",
            "Training Iteration 5239, Loss: 2.241995096206665\n",
            "Training Iteration 5240, Loss: 11.351149559020996\n",
            "Training Iteration 5241, Loss: 6.366982460021973\n",
            "Training Iteration 5242, Loss: 6.239377498626709\n",
            "Training Iteration 5243, Loss: 3.150630474090576\n",
            "Training Iteration 5244, Loss: 3.9385876655578613\n",
            "Training Iteration 5245, Loss: 4.721796989440918\n",
            "Training Iteration 5246, Loss: 9.923367500305176\n",
            "Training Iteration 5247, Loss: 10.367876052856445\n",
            "Training Iteration 5248, Loss: 8.02677059173584\n",
            "Training Iteration 5249, Loss: 2.8220934867858887\n",
            "Training Iteration 5250, Loss: 5.3904852867126465\n",
            "Training Iteration 5251, Loss: 8.404515266418457\n",
            "Training Iteration 5252, Loss: 5.456699371337891\n",
            "Training Iteration 5253, Loss: 6.35676383972168\n",
            "Training Iteration 5254, Loss: 3.964120864868164\n",
            "Training Iteration 5255, Loss: 4.671929359436035\n",
            "Training Iteration 5256, Loss: 7.565494060516357\n",
            "Training Iteration 5257, Loss: 10.781441688537598\n",
            "Training Iteration 5258, Loss: 6.867964744567871\n",
            "Training Iteration 5259, Loss: 7.811326026916504\n",
            "Training Iteration 5260, Loss: 7.065774440765381\n",
            "Training Iteration 5261, Loss: 6.7143073081970215\n",
            "Training Iteration 5262, Loss: 4.156317710876465\n",
            "Training Iteration 5263, Loss: 1.496559977531433\n",
            "Training Iteration 5264, Loss: 4.096070766448975\n",
            "Training Iteration 5265, Loss: 7.169177055358887\n",
            "Training Iteration 5266, Loss: 7.3436784744262695\n",
            "Training Iteration 5267, Loss: 10.509735107421875\n",
            "Training Iteration 5268, Loss: 1.8696342706680298\n",
            "Training Iteration 5269, Loss: 3.2264280319213867\n",
            "Training Iteration 5270, Loss: 9.39190673828125\n",
            "Training Iteration 5271, Loss: 3.0293710231781006\n",
            "Training Iteration 5272, Loss: 7.151276588439941\n",
            "Training Iteration 5273, Loss: 4.107306003570557\n",
            "Training Iteration 5274, Loss: 3.401533842086792\n",
            "Training Iteration 5275, Loss: 5.6147589683532715\n",
            "Training Iteration 5276, Loss: 6.573712348937988\n",
            "Training Iteration 5277, Loss: 8.11743450164795\n",
            "Training Iteration 5278, Loss: 8.833786010742188\n",
            "Training Iteration 5279, Loss: 2.5495028495788574\n",
            "Training Iteration 5280, Loss: 1.8097901344299316\n",
            "Training Iteration 5281, Loss: 6.736635208129883\n",
            "Training Iteration 5282, Loss: 2.175618886947632\n",
            "Training Iteration 5283, Loss: 3.598238468170166\n",
            "Training Iteration 5284, Loss: 4.8627095222473145\n",
            "Training Iteration 5285, Loss: 2.21167254447937\n",
            "Training Iteration 5286, Loss: 6.612444877624512\n",
            "Training Iteration 5287, Loss: 5.7555460929870605\n",
            "Training Iteration 5288, Loss: 4.7872467041015625\n",
            "Training Iteration 5289, Loss: 3.3283283710479736\n",
            "Training Iteration 5290, Loss: 5.3467559814453125\n",
            "Training Iteration 5291, Loss: 3.271331787109375\n",
            "Training Iteration 5292, Loss: 5.122713565826416\n",
            "Training Iteration 5293, Loss: 3.827212333679199\n",
            "Training Iteration 5294, Loss: 3.2569386959075928\n",
            "Training Iteration 5295, Loss: 8.156503677368164\n",
            "Training Iteration 5296, Loss: 0.657588005065918\n",
            "Training Iteration 5297, Loss: 4.972863674163818\n",
            "Training Iteration 5298, Loss: 8.380573272705078\n",
            "Training Iteration 5299, Loss: 5.029301643371582\n",
            "Training Iteration 5300, Loss: 6.3817057609558105\n",
            "Training Iteration 5301, Loss: 4.8956475257873535\n",
            "Training Iteration 5302, Loss: 4.808872699737549\n",
            "Training Iteration 5303, Loss: 3.7227840423583984\n",
            "Training Iteration 5304, Loss: 3.252655267715454\n",
            "Training Iteration 5305, Loss: 7.51820182800293\n",
            "Training Iteration 5306, Loss: 2.633425712585449\n",
            "Training Iteration 5307, Loss: 8.437993049621582\n",
            "Training Iteration 5308, Loss: 4.722797393798828\n",
            "Training Iteration 5309, Loss: 3.905956745147705\n",
            "Training Iteration 5310, Loss: 8.48957347869873\n",
            "Training Iteration 5311, Loss: 7.768561363220215\n",
            "Training Iteration 5312, Loss: 5.833827018737793\n",
            "Training Iteration 5313, Loss: 3.7640554904937744\n",
            "Training Iteration 5314, Loss: 5.254467964172363\n",
            "Training Iteration 5315, Loss: 2.251380681991577\n",
            "Training Iteration 5316, Loss: 4.861625671386719\n",
            "Training Iteration 5317, Loss: 3.9068243503570557\n",
            "Training Iteration 5318, Loss: 4.7491960525512695\n",
            "Training Iteration 5319, Loss: 3.801309108734131\n",
            "Training Iteration 5320, Loss: 3.044813871383667\n",
            "Training Iteration 5321, Loss: 5.544445991516113\n",
            "Training Iteration 5322, Loss: 3.7295315265655518\n",
            "Training Iteration 5323, Loss: 5.011476993560791\n",
            "Training Iteration 5324, Loss: 4.106736660003662\n",
            "Training Iteration 5325, Loss: 4.368497371673584\n",
            "Training Iteration 5326, Loss: 1.0742990970611572\n",
            "Training Iteration 5327, Loss: 3.4226644039154053\n",
            "Training Iteration 5328, Loss: 7.168461799621582\n",
            "Training Iteration 5329, Loss: 6.0787835121154785\n",
            "Training Iteration 5330, Loss: 5.315047740936279\n",
            "Training Iteration 5331, Loss: 4.97485876083374\n",
            "Training Iteration 5332, Loss: 6.414271354675293\n",
            "Training Iteration 5333, Loss: 5.788037300109863\n",
            "Training Iteration 5334, Loss: 5.726473808288574\n",
            "Training Iteration 5335, Loss: 5.809467315673828\n",
            "Training Iteration 5336, Loss: 7.001437664031982\n",
            "Training Iteration 5337, Loss: 5.221793174743652\n",
            "Training Iteration 5338, Loss: 2.1298234462738037\n",
            "Training Iteration 5339, Loss: 4.734365463256836\n",
            "Training Iteration 5340, Loss: 4.685924530029297\n",
            "Training Iteration 5341, Loss: 5.665607929229736\n",
            "Training Iteration 5342, Loss: 3.9457132816314697\n",
            "Training Iteration 5343, Loss: 4.201233863830566\n",
            "Training Iteration 5344, Loss: 6.059303283691406\n",
            "Training Iteration 5345, Loss: 3.8577983379364014\n",
            "Training Iteration 5346, Loss: 3.638504981994629\n",
            "Training Iteration 5347, Loss: 3.2957077026367188\n",
            "Training Iteration 5348, Loss: 2.8026957511901855\n",
            "Training Iteration 5349, Loss: 4.08779764175415\n",
            "Training Iteration 5350, Loss: 4.865515232086182\n",
            "Training Iteration 5351, Loss: 3.620563507080078\n",
            "Training Iteration 5352, Loss: 6.057971477508545\n",
            "Training Iteration 5353, Loss: 3.6316299438476562\n",
            "Training Iteration 5354, Loss: 4.104056358337402\n",
            "Training Iteration 5355, Loss: 4.557356357574463\n",
            "Training Iteration 5356, Loss: 4.6570353507995605\n",
            "Training Iteration 5357, Loss: 3.0087890625\n",
            "Training Iteration 5358, Loss: 2.736564874649048\n",
            "Training Iteration 5359, Loss: 5.928678035736084\n",
            "Training Iteration 5360, Loss: 7.945785999298096\n",
            "Training Iteration 5361, Loss: 3.8149921894073486\n",
            "Training Iteration 5362, Loss: 4.248445987701416\n",
            "Training Iteration 5363, Loss: 6.263012886047363\n",
            "Training Iteration 5364, Loss: 1.4599542617797852\n",
            "Training Iteration 5365, Loss: 3.0628910064697266\n",
            "Training Iteration 5366, Loss: 5.118887424468994\n",
            "Training Iteration 5367, Loss: 4.956152439117432\n",
            "Training Iteration 5368, Loss: 7.488884449005127\n",
            "Training Iteration 5369, Loss: 2.96488881111145\n",
            "Training Iteration 5370, Loss: 3.781497001647949\n",
            "Training Iteration 5371, Loss: 1.552695870399475\n",
            "Training Iteration 5372, Loss: 3.5735249519348145\n",
            "Training Iteration 5373, Loss: 7.491847038269043\n",
            "Training Iteration 5374, Loss: 5.189938545227051\n",
            "Training Iteration 5375, Loss: 4.463334083557129\n",
            "Training Iteration 5376, Loss: 4.420839786529541\n",
            "Training Iteration 5377, Loss: 2.453392267227173\n",
            "Training Iteration 5378, Loss: 6.297998905181885\n",
            "Training Iteration 5379, Loss: 4.934093952178955\n",
            "Training Iteration 5380, Loss: 5.615048408508301\n",
            "Training Iteration 5381, Loss: 2.8425965309143066\n",
            "Training Iteration 5382, Loss: 2.641934633255005\n",
            "Training Iteration 5383, Loss: 6.424388885498047\n",
            "Training Iteration 5384, Loss: 5.688748359680176\n",
            "Training Iteration 5385, Loss: 3.506908655166626\n",
            "Training Iteration 5386, Loss: 3.7147982120513916\n",
            "Training Iteration 5387, Loss: 3.2260096073150635\n",
            "Training Iteration 5388, Loss: 3.863973379135132\n",
            "Training Iteration 5389, Loss: 4.220766544342041\n",
            "Training Iteration 5390, Loss: 3.85129976272583\n",
            "Training Iteration 5391, Loss: 4.7639617919921875\n",
            "Training Iteration 5392, Loss: 5.827249526977539\n",
            "Training Iteration 5393, Loss: 4.950782299041748\n",
            "Training Iteration 5394, Loss: 3.205993413925171\n",
            "Training Iteration 5395, Loss: 5.871253490447998\n",
            "Training Iteration 5396, Loss: 2.3949124813079834\n",
            "Training Iteration 5397, Loss: 5.218622207641602\n",
            "Training Iteration 5398, Loss: 7.376148223876953\n",
            "Training Iteration 5399, Loss: 3.8422181606292725\n",
            "Training Iteration 5400, Loss: 6.968081474304199\n",
            "Training Iteration 5401, Loss: 5.301327705383301\n",
            "Training Iteration 5402, Loss: 2.7084693908691406\n",
            "Training Iteration 5403, Loss: 6.952142238616943\n",
            "Training Iteration 5404, Loss: 7.401924133300781\n",
            "Training Iteration 5405, Loss: 5.517893314361572\n",
            "Training Iteration 5406, Loss: 4.280514240264893\n",
            "Training Iteration 5407, Loss: 5.753979206085205\n",
            "Training Iteration 5408, Loss: 4.4682159423828125\n",
            "Training Iteration 5409, Loss: 2.645352363586426\n",
            "Training Iteration 5410, Loss: 3.576244592666626\n",
            "Training Iteration 5411, Loss: 4.964290142059326\n",
            "Training Iteration 5412, Loss: 2.833732843399048\n",
            "Training Iteration 5413, Loss: 2.116799831390381\n",
            "Training Iteration 5414, Loss: 2.4309439659118652\n",
            "Training Iteration 5415, Loss: 2.4500317573547363\n",
            "Training Iteration 5416, Loss: 4.307975769042969\n",
            "Training Iteration 5417, Loss: 5.775739669799805\n",
            "Training Iteration 5418, Loss: 3.167581558227539\n",
            "Training Iteration 5419, Loss: 3.3709259033203125\n",
            "Training Iteration 5420, Loss: 4.937372207641602\n",
            "Training Iteration 5421, Loss: 1.411653757095337\n",
            "Training Iteration 5422, Loss: 2.5667824745178223\n",
            "Training Iteration 5423, Loss: 6.685919284820557\n",
            "Training Iteration 5424, Loss: 3.911283493041992\n",
            "Training Iteration 5425, Loss: 4.598208904266357\n",
            "Training Iteration 5426, Loss: 2.7467727661132812\n",
            "Training Iteration 5427, Loss: 2.0102429389953613\n",
            "Training Iteration 5428, Loss: 3.0255770683288574\n",
            "Training Iteration 5429, Loss: 3.9199411869049072\n",
            "Training Iteration 5430, Loss: 3.352663516998291\n",
            "Training Iteration 5431, Loss: 4.13993501663208\n",
            "Training Iteration 5432, Loss: 4.081723690032959\n",
            "Training Iteration 5433, Loss: 3.8086483478546143\n",
            "Training Iteration 5434, Loss: 6.601036548614502\n",
            "Training Iteration 5435, Loss: 2.878347158432007\n",
            "Training Iteration 5436, Loss: 4.542004108428955\n",
            "Training Iteration 5437, Loss: 6.667046070098877\n",
            "Training Iteration 5438, Loss: 3.405789852142334\n",
            "Training Iteration 5439, Loss: 4.191098690032959\n",
            "Training Iteration 5440, Loss: 3.6392388343811035\n",
            "Training Iteration 5441, Loss: 5.0561203956604\n",
            "Training Iteration 5442, Loss: 3.8647689819335938\n",
            "Training Iteration 5443, Loss: 4.8852219581604\n",
            "Training Iteration 5444, Loss: 4.0543012619018555\n",
            "Training Iteration 5445, Loss: 4.122344493865967\n",
            "Training Iteration 5446, Loss: 1.6021524667739868\n",
            "Training Iteration 5447, Loss: 2.5588715076446533\n",
            "Training Iteration 5448, Loss: 3.8235645294189453\n",
            "Training Iteration 5449, Loss: 3.5777227878570557\n",
            "Training Iteration 5450, Loss: 2.1541080474853516\n",
            "Training Iteration 5451, Loss: 1.3559716939926147\n",
            "Training Iteration 5452, Loss: 7.108302116394043\n",
            "Training Iteration 5453, Loss: 5.502058029174805\n",
            "Training Iteration 5454, Loss: 4.277273178100586\n",
            "Training Iteration 5455, Loss: 4.944692611694336\n",
            "Training Iteration 5456, Loss: 4.742033958435059\n",
            "Training Iteration 5457, Loss: 3.4846270084381104\n",
            "Training Iteration 5458, Loss: 4.074741363525391\n",
            "Training Iteration 5459, Loss: 2.7154698371887207\n",
            "Training Iteration 5460, Loss: 3.850210428237915\n",
            "Training Iteration 5461, Loss: 5.1173624992370605\n",
            "Training Iteration 5462, Loss: 4.252878189086914\n",
            "Training Iteration 5463, Loss: 2.2225852012634277\n",
            "Training Iteration 5464, Loss: 4.352052688598633\n",
            "Training Iteration 5465, Loss: 5.030838966369629\n",
            "Training Iteration 5466, Loss: 3.475264549255371\n",
            "Training Iteration 5467, Loss: 4.167784214019775\n",
            "Training Iteration 5468, Loss: 4.116776466369629\n",
            "Training Iteration 5469, Loss: 1.9878599643707275\n",
            "Training Iteration 5470, Loss: 3.533409595489502\n",
            "Training Iteration 5471, Loss: 4.734963417053223\n",
            "Training Iteration 5472, Loss: 4.426214694976807\n",
            "Training Iteration 5473, Loss: 5.2933526039123535\n",
            "Training Iteration 5474, Loss: 3.8086698055267334\n",
            "Training Iteration 5475, Loss: 2.534240245819092\n",
            "Training Iteration 5476, Loss: 4.9211344718933105\n",
            "Training Iteration 5477, Loss: 3.51436448097229\n",
            "Training Iteration 5478, Loss: 5.287781238555908\n",
            "Training Iteration 5479, Loss: 6.222332954406738\n",
            "Training Iteration 5480, Loss: 6.213715553283691\n",
            "Training Iteration 5481, Loss: 3.507925510406494\n",
            "Training Iteration 5482, Loss: 3.4160361289978027\n",
            "Training Iteration 5483, Loss: 5.953822612762451\n",
            "Training Iteration 5484, Loss: 5.905821323394775\n",
            "Training Iteration 5485, Loss: 4.637187957763672\n",
            "Training Iteration 5486, Loss: 2.3850111961364746\n",
            "Training Iteration 5487, Loss: 4.412730693817139\n",
            "Training Iteration 5488, Loss: 4.950562477111816\n",
            "Training Iteration 5489, Loss: 4.2110514640808105\n",
            "Training Iteration 5490, Loss: 2.151179075241089\n",
            "Training Iteration 5491, Loss: 5.378898620605469\n",
            "Training Iteration 5492, Loss: 3.5212082862854004\n",
            "Training Iteration 5493, Loss: 7.717506408691406\n",
            "Training Iteration 5494, Loss: 2.7966976165771484\n",
            "Training Iteration 5495, Loss: 5.35404634475708\n",
            "Training Iteration 5496, Loss: 3.372255802154541\n",
            "Training Iteration 5497, Loss: 5.685549259185791\n",
            "Training Iteration 5498, Loss: 6.101652145385742\n",
            "Training Iteration 5499, Loss: 5.898966312408447\n",
            "Training Iteration 5500, Loss: 4.082634449005127\n",
            "Training Iteration 5501, Loss: 4.083362102508545\n",
            "Training Iteration 5502, Loss: 4.813268661499023\n",
            "Training Iteration 5503, Loss: 5.059281349182129\n",
            "Training Iteration 5504, Loss: 6.271259784698486\n",
            "Training Iteration 5505, Loss: 4.438344478607178\n",
            "Training Iteration 5506, Loss: 3.499258518218994\n",
            "Training Iteration 5507, Loss: 4.835521697998047\n",
            "Training Iteration 5508, Loss: 2.469222068786621\n",
            "Training Iteration 5509, Loss: 5.511812210083008\n",
            "Training Iteration 5510, Loss: 6.378257751464844\n",
            "Training Iteration 5511, Loss: 3.355353593826294\n",
            "Training Iteration 5512, Loss: 6.827833652496338\n",
            "Training Iteration 5513, Loss: 4.307612419128418\n",
            "Training Iteration 5514, Loss: 3.819100856781006\n",
            "Training Iteration 5515, Loss: 2.392164707183838\n",
            "Training Iteration 5516, Loss: 4.218736171722412\n",
            "Training Iteration 5517, Loss: 7.047990322113037\n",
            "Training Iteration 5518, Loss: 3.575803756713867\n",
            "Training Iteration 5519, Loss: 6.300200462341309\n",
            "Training Iteration 5520, Loss: 7.765498161315918\n",
            "Training Iteration 5521, Loss: 7.321184158325195\n",
            "Training Iteration 5522, Loss: 4.239382266998291\n",
            "Training Iteration 5523, Loss: 1.9722095727920532\n",
            "Training Iteration 5524, Loss: 4.113643169403076\n",
            "Training Iteration 5525, Loss: 2.950645923614502\n",
            "Training Iteration 5526, Loss: 3.768624782562256\n",
            "Training Iteration 5527, Loss: 4.939581871032715\n",
            "Training Iteration 5528, Loss: 5.887597560882568\n",
            "Training Iteration 5529, Loss: 5.489065647125244\n",
            "Training Iteration 5530, Loss: 3.6209065914154053\n",
            "Training Iteration 5531, Loss: 3.9189887046813965\n",
            "Training Iteration 5532, Loss: 3.8841054439544678\n",
            "Training Iteration 5533, Loss: 4.415958881378174\n",
            "Training Iteration 5534, Loss: 4.0262603759765625\n",
            "Training Iteration 5535, Loss: 5.982310771942139\n",
            "Training Iteration 5536, Loss: 8.554193496704102\n",
            "Training Iteration 5537, Loss: 4.113819122314453\n",
            "Training Iteration 5538, Loss: 4.429874897003174\n",
            "Training Iteration 5539, Loss: 5.668755531311035\n",
            "Training Iteration 5540, Loss: 6.2291998863220215\n",
            "Training Iteration 5541, Loss: 4.0015692710876465\n",
            "Training Iteration 5542, Loss: 4.629251480102539\n",
            "Training Iteration 5543, Loss: 3.5432896614074707\n",
            "Training Iteration 5544, Loss: 5.787449836730957\n",
            "Training Iteration 5545, Loss: 7.711055278778076\n",
            "Training Iteration 5546, Loss: 8.155945777893066\n",
            "Training Iteration 5547, Loss: 3.2993874549865723\n",
            "Training Iteration 5548, Loss: 6.672619342803955\n",
            "Training Iteration 5549, Loss: 6.20808219909668\n",
            "Training Iteration 5550, Loss: 2.616522789001465\n",
            "Training Iteration 5551, Loss: 2.218801736831665\n",
            "Training Iteration 5552, Loss: 4.668985366821289\n",
            "Training Iteration 5553, Loss: 4.291871547698975\n",
            "Training Iteration 5554, Loss: 2.267838478088379\n",
            "Training Iteration 5555, Loss: 5.268762111663818\n",
            "Training Iteration 5556, Loss: 2.302982807159424\n",
            "Training Iteration 5557, Loss: 3.465738296508789\n",
            "Training Iteration 5558, Loss: 2.7443106174468994\n",
            "Training Iteration 5559, Loss: 2.327070474624634\n",
            "Training Iteration 5560, Loss: 5.589305877685547\n",
            "Training Iteration 5561, Loss: 8.170821189880371\n",
            "Training Iteration 5562, Loss: 3.612154483795166\n",
            "Training Iteration 5563, Loss: 5.48337459564209\n",
            "Training Iteration 5564, Loss: 6.334098815917969\n",
            "Training Iteration 5565, Loss: 5.555483341217041\n",
            "Training Iteration 5566, Loss: 3.9162521362304688\n",
            "Training Iteration 5567, Loss: 2.8573336601257324\n",
            "Training Iteration 5568, Loss: 2.927952289581299\n",
            "Training Iteration 5569, Loss: 5.005587577819824\n",
            "Training Iteration 5570, Loss: 3.1492977142333984\n",
            "Training Iteration 5571, Loss: 6.058497428894043\n",
            "Training Iteration 5572, Loss: 5.36566686630249\n",
            "Training Iteration 5573, Loss: 5.144070625305176\n",
            "Training Iteration 5574, Loss: 1.693881630897522\n",
            "Training Iteration 5575, Loss: 2.6100351810455322\n",
            "Training Iteration 5576, Loss: 7.593301296234131\n",
            "Training Iteration 5577, Loss: 7.893477439880371\n",
            "Training Iteration 5578, Loss: 7.394221305847168\n",
            "Training Iteration 5579, Loss: 6.7149553298950195\n",
            "Training Iteration 5580, Loss: 5.894057750701904\n",
            "Training Iteration 5581, Loss: 1.52853524684906\n",
            "Training Iteration 5582, Loss: 5.870968341827393\n",
            "Training Iteration 5583, Loss: 3.525975227355957\n",
            "Training Iteration 5584, Loss: 4.112696170806885\n",
            "Training Iteration 5585, Loss: 5.05347204208374\n",
            "Training Iteration 5586, Loss: 5.830796241760254\n",
            "Training Iteration 5587, Loss: 7.045536994934082\n",
            "Training Iteration 5588, Loss: 6.233553886413574\n",
            "Training Iteration 5589, Loss: 2.46425461769104\n",
            "Training Iteration 5590, Loss: 9.202879905700684\n",
            "Training Iteration 5591, Loss: 8.37744426727295\n",
            "Training Iteration 5592, Loss: 9.408124923706055\n",
            "Training Iteration 5593, Loss: 5.7434983253479\n",
            "Training Iteration 5594, Loss: 5.601411819458008\n",
            "Training Iteration 5595, Loss: 2.0101730823516846\n",
            "Training Iteration 5596, Loss: 6.805113792419434\n",
            "Training Iteration 5597, Loss: 7.980983734130859\n",
            "Training Iteration 5598, Loss: 6.209808349609375\n",
            "Training Iteration 5599, Loss: 9.420357704162598\n",
            "Training Iteration 5600, Loss: 10.645718574523926\n",
            "Training Iteration 5601, Loss: 3.419006824493408\n",
            "Training Iteration 5602, Loss: 2.5041897296905518\n",
            "Training Iteration 5603, Loss: 6.730827808380127\n",
            "Training Iteration 5604, Loss: 3.8309459686279297\n",
            "Training Iteration 5605, Loss: 4.818893909454346\n",
            "Training Iteration 5606, Loss: 4.703421592712402\n",
            "Training Iteration 5607, Loss: 3.7502589225769043\n",
            "Training Iteration 5608, Loss: 2.049373149871826\n",
            "Training Iteration 5609, Loss: 3.9078640937805176\n",
            "Training Iteration 5610, Loss: 8.733698844909668\n",
            "Training Iteration 5611, Loss: 5.687870502471924\n",
            "Training Iteration 5612, Loss: 0.6346201300621033\n",
            "Training Iteration 5613, Loss: 10.230085372924805\n",
            "Training Iteration 5614, Loss: 4.632856369018555\n",
            "Training Iteration 5615, Loss: 7.365265369415283\n",
            "Training Iteration 5616, Loss: 5.774532318115234\n",
            "Training Iteration 5617, Loss: 5.769162654876709\n",
            "Training Iteration 5618, Loss: 5.9377827644348145\n",
            "Training Iteration 5619, Loss: 6.096944808959961\n",
            "Training Iteration 5620, Loss: 3.3652098178863525\n",
            "Training Iteration 5621, Loss: 2.8840925693511963\n",
            "Training Iteration 5622, Loss: 3.7172892093658447\n",
            "Training Iteration 5623, Loss: 5.717714786529541\n",
            "Training Iteration 5624, Loss: 2.185243844985962\n",
            "Training Iteration 5625, Loss: 4.278789043426514\n",
            "Training Iteration 5626, Loss: 3.699939727783203\n",
            "Training Iteration 5627, Loss: 4.127495765686035\n",
            "Training Iteration 5628, Loss: 4.880707740783691\n",
            "Training Iteration 5629, Loss: 4.756676197052002\n",
            "Training Iteration 5630, Loss: 3.174351692199707\n",
            "Training Iteration 5631, Loss: 5.053640842437744\n",
            "Training Iteration 5632, Loss: 3.8659534454345703\n",
            "Training Iteration 5633, Loss: 3.3279693126678467\n",
            "Training Iteration 5634, Loss: 2.013894557952881\n",
            "Training Iteration 5635, Loss: 4.474803924560547\n",
            "Training Iteration 5636, Loss: 3.7963547706604004\n",
            "Training Iteration 5637, Loss: 6.337076187133789\n",
            "Training Iteration 5638, Loss: 3.67604923248291\n",
            "Training Iteration 5639, Loss: 1.1803680658340454\n",
            "Training Iteration 5640, Loss: 6.998510360717773\n",
            "Training Iteration 5641, Loss: 3.0668370723724365\n",
            "Training Iteration 5642, Loss: 2.620525360107422\n",
            "Training Iteration 5643, Loss: 6.82698917388916\n",
            "Training Iteration 5644, Loss: 4.766981601715088\n",
            "Training Iteration 5645, Loss: 10.213432312011719\n",
            "Training Iteration 5646, Loss: 4.964188098907471\n",
            "Training Iteration 5647, Loss: 5.257214546203613\n",
            "Training Iteration 5648, Loss: 4.4374494552612305\n",
            "Training Iteration 5649, Loss: 5.930846691131592\n",
            "Training Iteration 5650, Loss: 3.3558037281036377\n",
            "Training Iteration 5651, Loss: 4.781672477722168\n",
            "Training Iteration 5652, Loss: 3.653046131134033\n",
            "Training Iteration 5653, Loss: 5.821627616882324\n",
            "Training Iteration 5654, Loss: 7.8091888427734375\n",
            "Training Iteration 5655, Loss: 4.066609859466553\n",
            "Training Iteration 5656, Loss: 4.8862833976745605\n",
            "Training Iteration 5657, Loss: 5.069616794586182\n",
            "Training Iteration 5658, Loss: 7.322176933288574\n",
            "Training Iteration 5659, Loss: 4.604738235473633\n",
            "Training Iteration 5660, Loss: 5.790360450744629\n",
            "Training Iteration 5661, Loss: 5.2237138748168945\n",
            "Training Iteration 5662, Loss: 6.438767910003662\n",
            "Training Iteration 5663, Loss: 5.1528143882751465\n",
            "Training Iteration 5664, Loss: 4.380080223083496\n",
            "Training Iteration 5665, Loss: 5.809376239776611\n",
            "Training Iteration 5666, Loss: 3.7223637104034424\n",
            "Training Iteration 5667, Loss: 4.437246322631836\n",
            "Training Iteration 5668, Loss: 4.635146617889404\n",
            "Training Iteration 5669, Loss: 3.0622589588165283\n",
            "Training Iteration 5670, Loss: 5.432585716247559\n",
            "Training Iteration 5671, Loss: 5.701106071472168\n",
            "Training Iteration 5672, Loss: 4.624783515930176\n",
            "Training Iteration 5673, Loss: 3.6546361446380615\n",
            "Training Iteration 5674, Loss: 4.719996929168701\n",
            "Training Iteration 5675, Loss: 1.853817105293274\n",
            "Training Iteration 5676, Loss: 3.2573375701904297\n",
            "Training Iteration 5677, Loss: 6.77195930480957\n",
            "Training Iteration 5678, Loss: 3.988205909729004\n",
            "Training Iteration 5679, Loss: 3.487947940826416\n",
            "Training Iteration 5680, Loss: 5.756712913513184\n",
            "Training Iteration 5681, Loss: 4.060171604156494\n",
            "Training Iteration 5682, Loss: 2.9161460399627686\n",
            "Training Iteration 5683, Loss: 2.673856258392334\n",
            "Training Iteration 5684, Loss: 4.512961387634277\n",
            "Training Iteration 5685, Loss: 6.728186130523682\n",
            "Training Iteration 5686, Loss: 4.844352722167969\n",
            "Training Iteration 5687, Loss: 5.135011672973633\n",
            "Training Iteration 5688, Loss: 3.9593100547790527\n",
            "Training Iteration 5689, Loss: 4.566629409790039\n",
            "Training Iteration 5690, Loss: 6.555082321166992\n",
            "Training Iteration 5691, Loss: 3.3809242248535156\n",
            "Training Iteration 5692, Loss: 5.11928129196167\n",
            "Training Iteration 5693, Loss: 4.381982803344727\n",
            "Training Iteration 5694, Loss: 4.058809280395508\n",
            "Training Iteration 5695, Loss: 4.538727283477783\n",
            "Training Iteration 5696, Loss: 2.164675235748291\n",
            "Training Iteration 5697, Loss: 5.112936496734619\n",
            "Training Iteration 5698, Loss: 4.340935230255127\n",
            "Training Iteration 5699, Loss: 1.6349132061004639\n",
            "Training Iteration 5700, Loss: 2.1038658618927\n",
            "Training Iteration 5701, Loss: 3.3882522583007812\n",
            "Training Iteration 5702, Loss: 3.227165460586548\n",
            "Training Iteration 5703, Loss: 4.813699245452881\n",
            "Training Iteration 5704, Loss: 4.7408246994018555\n",
            "Training Iteration 5705, Loss: 4.178843975067139\n",
            "Training Iteration 5706, Loss: 5.55593204498291\n",
            "Training Iteration 5707, Loss: 3.565842390060425\n",
            "Training Iteration 5708, Loss: 5.2281365394592285\n",
            "Training Iteration 5709, Loss: 6.490869522094727\n",
            "Training Iteration 5710, Loss: 8.871533393859863\n",
            "Training Iteration 5711, Loss: 5.69393253326416\n",
            "Training Iteration 5712, Loss: 2.6844098567962646\n",
            "Training Iteration 5713, Loss: 4.8631696701049805\n",
            "Training Iteration 5714, Loss: 3.2476296424865723\n",
            "Training Iteration 5715, Loss: 3.8772919178009033\n",
            "Training Iteration 5716, Loss: 3.1215898990631104\n",
            "Training Iteration 5717, Loss: 2.9321129322052\n",
            "Training Iteration 5718, Loss: 1.632131576538086\n",
            "Training Iteration 5719, Loss: 3.3239448070526123\n",
            "Training Iteration 5720, Loss: 3.724069595336914\n",
            "Training Iteration 5721, Loss: 2.1313860416412354\n",
            "Training Iteration 5722, Loss: 3.618513822555542\n",
            "Training Iteration 5723, Loss: 2.3443968296051025\n",
            "Training Iteration 5724, Loss: 3.2265748977661133\n",
            "Training Iteration 5725, Loss: 5.594461441040039\n",
            "Training Iteration 5726, Loss: 3.6000380516052246\n",
            "Training Iteration 5727, Loss: 6.015217304229736\n",
            "Training Iteration 5728, Loss: 3.4430394172668457\n",
            "Training Iteration 5729, Loss: 3.742737293243408\n",
            "Training Iteration 5730, Loss: 4.716640949249268\n",
            "Training Iteration 5731, Loss: 4.860980987548828\n",
            "Training Iteration 5732, Loss: 3.5987346172332764\n",
            "Training Iteration 5733, Loss: 1.9913537502288818\n",
            "Training Iteration 5734, Loss: 3.6216001510620117\n",
            "Training Iteration 5735, Loss: 4.724867820739746\n",
            "Training Iteration 5736, Loss: 7.028343200683594\n",
            "Training Iteration 5737, Loss: 5.306586265563965\n",
            "Training Iteration 5738, Loss: 4.705425262451172\n",
            "Training Iteration 5739, Loss: 3.6196224689483643\n",
            "Training Iteration 5740, Loss: 3.2673559188842773\n",
            "Training Iteration 5741, Loss: 4.081772804260254\n",
            "Training Iteration 5742, Loss: 3.1262481212615967\n",
            "Training Iteration 5743, Loss: 7.778944969177246\n",
            "Training Iteration 5744, Loss: 2.8207600116729736\n",
            "Training Iteration 5745, Loss: 5.738687515258789\n",
            "Training Iteration 5746, Loss: 5.976228713989258\n",
            "Training Iteration 5747, Loss: 4.640737056732178\n",
            "Training Iteration 5748, Loss: 3.461383819580078\n",
            "Training Iteration 5749, Loss: 2.475414514541626\n",
            "Training Iteration 5750, Loss: 7.084948539733887\n",
            "Training Iteration 5751, Loss: 3.3331196308135986\n",
            "Training Iteration 5752, Loss: 3.1995129585266113\n",
            "Training Iteration 5753, Loss: 4.203457832336426\n",
            "Training Iteration 5754, Loss: 3.2956326007843018\n",
            "Training Iteration 5755, Loss: 2.5616838932037354\n",
            "Training Iteration 5756, Loss: 3.7499499320983887\n",
            "Training Iteration 5757, Loss: 4.672426223754883\n",
            "Training Iteration 5758, Loss: 3.6355133056640625\n",
            "Training Iteration 5759, Loss: 2.7738664150238037\n",
            "Training Iteration 5760, Loss: 5.450159072875977\n",
            "Training Iteration 5761, Loss: 7.21645450592041\n",
            "Training Iteration 5762, Loss: 4.5929059982299805\n",
            "Training Iteration 5763, Loss: 5.600826263427734\n",
            "Training Iteration 5764, Loss: 1.82713782787323\n",
            "Training Iteration 5765, Loss: 1.2026903629302979\n",
            "Training Iteration 5766, Loss: 5.710031509399414\n",
            "Training Iteration 5767, Loss: 8.576043128967285\n",
            "Training Iteration 5768, Loss: 6.150844573974609\n",
            "Training Iteration 5769, Loss: 4.547088623046875\n",
            "Training Iteration 5770, Loss: 4.929462909698486\n",
            "Training Iteration 5771, Loss: 5.053754806518555\n",
            "Training Iteration 5772, Loss: 6.013568878173828\n",
            "Training Iteration 5773, Loss: 7.6877522468566895\n",
            "Training Iteration 5774, Loss: 7.908687114715576\n",
            "Training Iteration 5775, Loss: 4.968090534210205\n",
            "Training Iteration 5776, Loss: 3.795973300933838\n",
            "Training Iteration 5777, Loss: 3.052584648132324\n",
            "Training Iteration 5778, Loss: 4.6826019287109375\n",
            "Training Iteration 5779, Loss: 3.672513484954834\n",
            "Training Iteration 5780, Loss: 5.9338788986206055\n",
            "Training Iteration 5781, Loss: 5.199136257171631\n",
            "Training Iteration 5782, Loss: 4.842007160186768\n",
            "Training Iteration 5783, Loss: 4.716421127319336\n",
            "Training Iteration 5784, Loss: 7.161884784698486\n",
            "Training Iteration 5785, Loss: 7.55501127243042\n",
            "Training Iteration 5786, Loss: 5.962823390960693\n",
            "Training Iteration 5787, Loss: 6.197761535644531\n",
            "Training Iteration 5788, Loss: 3.7550876140594482\n",
            "Training Iteration 5789, Loss: 3.2696352005004883\n",
            "Training Iteration 5790, Loss: 3.007460594177246\n",
            "Training Iteration 5791, Loss: 4.825342178344727\n",
            "Training Iteration 5792, Loss: 2.929659843444824\n",
            "Training Iteration 5793, Loss: 3.7139644622802734\n",
            "Training Iteration 5794, Loss: 5.685090065002441\n",
            "Training Iteration 5795, Loss: 3.051645517349243\n",
            "Training Iteration 5796, Loss: 6.2135844230651855\n",
            "Training Iteration 5797, Loss: 6.297567844390869\n",
            "Training Iteration 5798, Loss: 3.986732006072998\n",
            "Training Iteration 5799, Loss: 3.048884868621826\n",
            "Training Iteration 5800, Loss: 5.449882507324219\n",
            "Training Iteration 5801, Loss: 3.5843427181243896\n",
            "Training Iteration 5802, Loss: 2.693342447280884\n",
            "Training Iteration 5803, Loss: 2.6550159454345703\n",
            "Training Iteration 5804, Loss: 4.880892276763916\n",
            "Training Iteration 5805, Loss: 3.5680594444274902\n",
            "Training Iteration 5806, Loss: 3.1605074405670166\n",
            "Training Iteration 5807, Loss: 5.755143642425537\n",
            "Training Iteration 5808, Loss: 2.765986680984497\n",
            "Training Iteration 5809, Loss: 3.3873391151428223\n",
            "Training Iteration 5810, Loss: 4.702018737792969\n",
            "Training Iteration 5811, Loss: 3.2649600505828857\n",
            "Training Iteration 5812, Loss: 4.430342197418213\n",
            "Training Iteration 5813, Loss: 4.1867170333862305\n",
            "Training Iteration 5814, Loss: 3.6703217029571533\n",
            "Training Iteration 5815, Loss: 3.796455144882202\n",
            "Training Iteration 5816, Loss: 5.535722255706787\n",
            "Training Iteration 5817, Loss: 6.94650936126709\n",
            "Training Iteration 5818, Loss: 3.439086675643921\n",
            "Training Iteration 5819, Loss: 3.707430362701416\n",
            "Training Iteration 5820, Loss: 5.769021034240723\n",
            "Training Iteration 5821, Loss: 4.398126602172852\n",
            "Training Iteration 5822, Loss: 6.386850357055664\n",
            "Training Iteration 5823, Loss: 3.298501491546631\n",
            "Training Iteration 5824, Loss: 6.093466758728027\n",
            "Training Iteration 5825, Loss: 1.1426421403884888\n",
            "Training Iteration 5826, Loss: 6.367549896240234\n",
            "Training Iteration 5827, Loss: 7.272432327270508\n",
            "Training Iteration 5828, Loss: 4.051638603210449\n",
            "Training Iteration 5829, Loss: 3.6936769485473633\n",
            "Training Iteration 5830, Loss: 7.182582855224609\n",
            "Training Iteration 5831, Loss: 3.0464460849761963\n",
            "Training Iteration 5832, Loss: 4.6163482666015625\n",
            "Training Iteration 5833, Loss: 2.6941516399383545\n",
            "Training Iteration 5834, Loss: 4.044626235961914\n",
            "Training Iteration 5835, Loss: 3.579460859298706\n",
            "Training Iteration 5836, Loss: 6.9483642578125\n",
            "Training Iteration 5837, Loss: 6.025967121124268\n",
            "Training Iteration 5838, Loss: 5.182736873626709\n",
            "Training Iteration 5839, Loss: 4.557344436645508\n",
            "Training Iteration 5840, Loss: 6.839930057525635\n",
            "Training Iteration 5841, Loss: 3.9140725135803223\n",
            "Training Iteration 5842, Loss: 3.3480472564697266\n",
            "Training Iteration 5843, Loss: 3.4706008434295654\n",
            "Training Iteration 5844, Loss: 5.681641101837158\n",
            "Training Iteration 5845, Loss: 5.640605926513672\n",
            "Training Iteration 5846, Loss: 3.2082862854003906\n",
            "Training Iteration 5847, Loss: 5.026637077331543\n",
            "Training Iteration 5848, Loss: 4.5287699699401855\n",
            "Training Iteration 5849, Loss: 2.8570141792297363\n",
            "Training Iteration 5850, Loss: 6.077112197875977\n",
            "Training Iteration 5851, Loss: 4.590033054351807\n",
            "Training Iteration 5852, Loss: 5.1894121170043945\n",
            "Training Iteration 5853, Loss: 5.338010311126709\n",
            "Training Iteration 5854, Loss: 3.0759198665618896\n",
            "Training Iteration 5855, Loss: 3.8255443572998047\n",
            "Training Iteration 5856, Loss: 5.02564001083374\n",
            "Training Iteration 5857, Loss: 3.297750949859619\n",
            "Training Iteration 5858, Loss: 6.748314380645752\n",
            "Training Iteration 5859, Loss: 6.474540710449219\n",
            "Training Iteration 5860, Loss: 3.481429100036621\n",
            "Training Iteration 5861, Loss: 2.7369441986083984\n",
            "Training Iteration 5862, Loss: 5.65432071685791\n",
            "Training Iteration 5863, Loss: 3.5510199069976807\n",
            "Training Iteration 5864, Loss: 5.942014217376709\n",
            "Training Iteration 5865, Loss: 6.549245834350586\n",
            "Training Iteration 5866, Loss: 4.429702281951904\n",
            "Training Iteration 5867, Loss: 6.790752410888672\n",
            "Training Iteration 5868, Loss: 2.2155745029449463\n",
            "Training Iteration 5869, Loss: 6.77423620223999\n",
            "Training Iteration 5870, Loss: 4.3121418952941895\n",
            "Training Iteration 5871, Loss: 5.928857326507568\n",
            "Training Iteration 5872, Loss: 4.988844871520996\n",
            "Training Iteration 5873, Loss: 4.140002727508545\n",
            "Training Iteration 5874, Loss: 1.7001761198043823\n",
            "Training Iteration 5875, Loss: 7.725556373596191\n",
            "Training Iteration 5876, Loss: 2.8247735500335693\n",
            "Training Iteration 5877, Loss: 6.308053970336914\n",
            "Training Iteration 5878, Loss: 5.4742302894592285\n",
            "Training Iteration 5879, Loss: 7.736969947814941\n",
            "Training Iteration 5880, Loss: 3.8571512699127197\n",
            "Training Iteration 5881, Loss: 3.7339186668395996\n",
            "Training Iteration 5882, Loss: 5.30928897857666\n",
            "Training Iteration 5883, Loss: 6.185224533081055\n",
            "Training Iteration 5884, Loss: 7.295339584350586\n",
            "Training Iteration 5885, Loss: 3.2288966178894043\n",
            "Training Iteration 5886, Loss: 2.2800660133361816\n",
            "Training Iteration 5887, Loss: 2.2701172828674316\n",
            "Training Iteration 5888, Loss: 2.5907974243164062\n",
            "Training Iteration 5889, Loss: 9.130646705627441\n",
            "Training Iteration 5890, Loss: 6.210422515869141\n",
            "Training Iteration 5891, Loss: 5.651856899261475\n",
            "Training Iteration 5892, Loss: 6.687629222869873\n",
            "Training Iteration 5893, Loss: 6.283720016479492\n",
            "Training Iteration 5894, Loss: 3.37638258934021\n",
            "Training Iteration 5895, Loss: 4.773451805114746\n",
            "Training Iteration 5896, Loss: 4.0901713371276855\n",
            "Training Iteration 5897, Loss: 4.945429801940918\n",
            "Training Iteration 5898, Loss: 5.508079528808594\n",
            "Training Iteration 5899, Loss: 3.291053056716919\n",
            "Training Iteration 5900, Loss: 4.0565009117126465\n",
            "Training Iteration 5901, Loss: 5.4313249588012695\n",
            "Training Iteration 5902, Loss: 7.769688129425049\n",
            "Training Iteration 5903, Loss: 6.503735065460205\n",
            "Training Iteration 5904, Loss: 2.3577942848205566\n",
            "Training Iteration 5905, Loss: 4.379236221313477\n",
            "Training Iteration 5906, Loss: 6.586065769195557\n",
            "Training Iteration 5907, Loss: 6.7949981689453125\n",
            "Training Iteration 5908, Loss: 3.4471452236175537\n",
            "Training Iteration 5909, Loss: 2.710869312286377\n",
            "Training Iteration 5910, Loss: 3.8249447345733643\n",
            "Training Iteration 5911, Loss: 1.7250105142593384\n",
            "Training Iteration 5912, Loss: 5.541387557983398\n",
            "Training Iteration 5913, Loss: 3.3694252967834473\n",
            "Training Iteration 5914, Loss: 5.0853190422058105\n",
            "Training Iteration 5915, Loss: 4.644050121307373\n",
            "Training Iteration 5916, Loss: 3.824342966079712\n",
            "Training Iteration 5917, Loss: 5.668628215789795\n",
            "Training Iteration 5918, Loss: 9.395978927612305\n",
            "Training Iteration 5919, Loss: 7.675083637237549\n",
            "Training Iteration 5920, Loss: 2.4885199069976807\n",
            "Training Iteration 5921, Loss: 4.541913032531738\n",
            "Training Iteration 5922, Loss: 4.5299201011657715\n",
            "Training Iteration 5923, Loss: 8.07866096496582\n",
            "Training Iteration 5924, Loss: 5.360383033752441\n",
            "Training Iteration 5925, Loss: 5.670742988586426\n",
            "Training Iteration 5926, Loss: 6.006828308105469\n",
            "Training Iteration 5927, Loss: 5.962498188018799\n",
            "Training Iteration 5928, Loss: 6.3308868408203125\n",
            "Training Iteration 5929, Loss: 13.246183395385742\n",
            "Training Iteration 5930, Loss: 7.492962837219238\n",
            "Training Iteration 5931, Loss: 11.503433227539062\n",
            "Training Iteration 5932, Loss: 5.618741989135742\n",
            "Training Iteration 5933, Loss: 4.392993450164795\n",
            "Training Iteration 5934, Loss: 3.205641984939575\n",
            "Training Iteration 5935, Loss: 4.210572242736816\n",
            "Training Iteration 5936, Loss: 3.418865203857422\n",
            "Training Iteration 5937, Loss: 6.6320390701293945\n",
            "Training Iteration 5938, Loss: 4.959346294403076\n",
            "Training Iteration 5939, Loss: 3.659675121307373\n",
            "Training Iteration 5940, Loss: 3.423985242843628\n",
            "Training Iteration 5941, Loss: 7.316922664642334\n",
            "Training Iteration 5942, Loss: 5.985162734985352\n",
            "Training Iteration 5943, Loss: 1.6216672658920288\n",
            "Training Iteration 5944, Loss: 6.949104309082031\n",
            "Training Iteration 5945, Loss: 2.1285061836242676\n",
            "Training Iteration 5946, Loss: 7.135316848754883\n",
            "Training Iteration 5947, Loss: 4.139535903930664\n",
            "Training Iteration 5948, Loss: 3.9613380432128906\n",
            "Training Iteration 5949, Loss: 4.124605178833008\n",
            "Training Iteration 5950, Loss: 8.241869926452637\n",
            "Training Iteration 5951, Loss: 2.228689432144165\n",
            "Training Iteration 5952, Loss: 4.315367221832275\n",
            "Training Iteration 5953, Loss: 3.827711343765259\n",
            "Training Iteration 5954, Loss: 3.7840113639831543\n",
            "Training Iteration 5955, Loss: 4.565064430236816\n",
            "Training Iteration 5956, Loss: 4.1027069091796875\n",
            "Training Iteration 5957, Loss: 4.9473700523376465\n",
            "Training Iteration 5958, Loss: 8.219533920288086\n",
            "Training Iteration 5959, Loss: 5.744222164154053\n",
            "Training Iteration 5960, Loss: 4.9318928718566895\n",
            "Training Iteration 5961, Loss: 6.066896438598633\n",
            "Training Iteration 5962, Loss: 8.467326164245605\n",
            "Training Iteration 5963, Loss: 3.1091887950897217\n",
            "Training Iteration 5964, Loss: 4.625898361206055\n",
            "Training Iteration 5965, Loss: 4.449024200439453\n",
            "Training Iteration 5966, Loss: 3.8894596099853516\n",
            "Training Iteration 5967, Loss: 4.159652233123779\n",
            "Training Iteration 5968, Loss: 2.0766360759735107\n",
            "Training Iteration 5969, Loss: 6.378280162811279\n",
            "Training Iteration 5970, Loss: 4.122062683105469\n",
            "Training Iteration 5971, Loss: 5.455739974975586\n",
            "Training Iteration 5972, Loss: 2.994703531265259\n",
            "Training Iteration 5973, Loss: 5.72211217880249\n",
            "Training Iteration 5974, Loss: 3.9215352535247803\n",
            "Training Iteration 5975, Loss: 5.926535129547119\n",
            "Training Iteration 5976, Loss: 3.9943485260009766\n",
            "Training Iteration 5977, Loss: 4.196502685546875\n",
            "Training Iteration 5978, Loss: 6.33009147644043\n",
            "Training Iteration 5979, Loss: 1.885569453239441\n",
            "Training Iteration 5980, Loss: 6.266188144683838\n",
            "Training Iteration 5981, Loss: 4.24092435836792\n",
            "Training Iteration 5982, Loss: 7.909111022949219\n",
            "Training Iteration 5983, Loss: 4.999109745025635\n",
            "Training Iteration 5984, Loss: 6.222814559936523\n",
            "Training Iteration 5985, Loss: 6.637521266937256\n",
            "Training Iteration 5986, Loss: 5.651891231536865\n",
            "Training Iteration 5987, Loss: 3.7111544609069824\n",
            "Training Iteration 5988, Loss: 1.9906452894210815\n",
            "Training Iteration 5989, Loss: 3.687654733657837\n",
            "Training Iteration 5990, Loss: 2.6935043334960938\n",
            "Training Iteration 5991, Loss: 3.4800710678100586\n",
            "Training Iteration 5992, Loss: 5.360258102416992\n",
            "Training Iteration 5993, Loss: 3.4759397506713867\n",
            "Training Iteration 5994, Loss: 4.1867876052856445\n",
            "Training Iteration 5995, Loss: 4.857787132263184\n",
            "Training Iteration 5996, Loss: 4.090332984924316\n",
            "Training Iteration 5997, Loss: 3.0873477458953857\n",
            "Training Iteration 5998, Loss: 4.199045658111572\n",
            "Training Iteration 5999, Loss: 3.2271194458007812\n",
            "Training Iteration 6000, Loss: 5.061692237854004\n",
            "Training Iteration 6001, Loss: 4.572548866271973\n",
            "Training Iteration 6002, Loss: 5.159768104553223\n",
            "Training Iteration 6003, Loss: 4.598872184753418\n",
            "Training Iteration 6004, Loss: 4.394631385803223\n",
            "Training Iteration 6005, Loss: 2.8757214546203613\n",
            "Training Iteration 6006, Loss: 3.8369529247283936\n",
            "Training Iteration 6007, Loss: 2.058109998703003\n",
            "Training Iteration 6008, Loss: 3.284158706665039\n",
            "Training Iteration 6009, Loss: 2.1214094161987305\n",
            "Training Iteration 6010, Loss: 3.5571858882904053\n",
            "Training Iteration 6011, Loss: 4.362156391143799\n",
            "Training Iteration 6012, Loss: 2.990015983581543\n",
            "Training Iteration 6013, Loss: 5.047975540161133\n",
            "Training Iteration 6014, Loss: 5.187182903289795\n",
            "Training Iteration 6015, Loss: 2.866454601287842\n",
            "Training Iteration 6016, Loss: 3.7516469955444336\n",
            "Training Iteration 6017, Loss: 4.022017002105713\n",
            "Training Iteration 6018, Loss: 3.1431751251220703\n",
            "Training Iteration 6019, Loss: 2.0248262882232666\n",
            "Training Iteration 6020, Loss: 2.2163405418395996\n",
            "Training Iteration 6021, Loss: 3.502997636795044\n",
            "Training Iteration 6022, Loss: 3.3368711471557617\n",
            "Training Iteration 6023, Loss: 6.4597368240356445\n",
            "Training Iteration 6024, Loss: 4.76554012298584\n",
            "Training Iteration 6025, Loss: 3.3763861656188965\n",
            "Training Iteration 6026, Loss: 5.572044372558594\n",
            "Training Iteration 6027, Loss: 1.318524718284607\n",
            "Training Iteration 6028, Loss: 4.002758026123047\n",
            "Training Iteration 6029, Loss: 2.8176162242889404\n",
            "Training Iteration 6030, Loss: 4.697279930114746\n",
            "Training Iteration 6031, Loss: 5.143533229827881\n",
            "Training Iteration 6032, Loss: 4.376299858093262\n",
            "Training Iteration 6033, Loss: 2.9556710720062256\n",
            "Training Iteration 6034, Loss: 3.2153637409210205\n",
            "Training Iteration 6035, Loss: 2.7471466064453125\n",
            "Training Iteration 6036, Loss: 5.94865608215332\n",
            "Training Iteration 6037, Loss: 3.954333782196045\n",
            "Training Iteration 6038, Loss: 2.801126480102539\n",
            "Training Iteration 6039, Loss: 4.461679458618164\n",
            "Training Iteration 6040, Loss: 3.4367129802703857\n",
            "Training Iteration 6041, Loss: 5.61721658706665\n",
            "Training Iteration 6042, Loss: 4.848443508148193\n",
            "Training Iteration 6043, Loss: 4.076801776885986\n",
            "Training Iteration 6044, Loss: 3.8875579833984375\n",
            "Training Iteration 6045, Loss: 6.970056533813477\n",
            "Training Iteration 6046, Loss: 4.113880157470703\n",
            "Training Iteration 6047, Loss: 7.235119819641113\n",
            "Training Iteration 6048, Loss: 4.734377861022949\n",
            "Training Iteration 6049, Loss: 7.178671360015869\n",
            "Training Iteration 6050, Loss: 3.9122567176818848\n",
            "Training Iteration 6051, Loss: 6.123128890991211\n",
            "Training Iteration 6052, Loss: 5.593623161315918\n",
            "Training Iteration 6053, Loss: 4.966801166534424\n",
            "Training Iteration 6054, Loss: 1.7989864349365234\n",
            "Training Iteration 6055, Loss: 6.338057518005371\n",
            "Training Iteration 6056, Loss: 5.1308183670043945\n",
            "Training Iteration 6057, Loss: 3.3287925720214844\n",
            "Training Iteration 6058, Loss: 4.813486099243164\n",
            "Training Iteration 6059, Loss: 3.4165825843811035\n",
            "Training Iteration 6060, Loss: 2.868096113204956\n",
            "Training Iteration 6061, Loss: 4.839885711669922\n",
            "Training Iteration 6062, Loss: 3.7018189430236816\n",
            "Training Iteration 6063, Loss: 5.126581192016602\n",
            "Training Iteration 6064, Loss: 3.9285449981689453\n",
            "Training Iteration 6065, Loss: 8.3145112991333\n",
            "Training Iteration 6066, Loss: 6.825239181518555\n",
            "Training Iteration 6067, Loss: 5.959128379821777\n",
            "Training Iteration 6068, Loss: 1.8162670135498047\n",
            "Training Iteration 6069, Loss: 4.035511493682861\n",
            "Training Iteration 6070, Loss: 5.932551383972168\n",
            "Training Iteration 6071, Loss: 3.309080123901367\n",
            "Training Iteration 6072, Loss: 4.091062068939209\n",
            "Training Iteration 6073, Loss: 2.511176824569702\n",
            "Training Iteration 6074, Loss: 5.308366298675537\n",
            "Training Iteration 6075, Loss: 7.628012180328369\n",
            "Training Iteration 6076, Loss: 3.100872039794922\n",
            "Training Iteration 6077, Loss: 2.907843828201294\n",
            "Training Iteration 6078, Loss: 3.9782466888427734\n",
            "Training Iteration 6079, Loss: 4.263078689575195\n",
            "Training Iteration 6080, Loss: 5.422581195831299\n",
            "Training Iteration 6081, Loss: 5.9103899002075195\n",
            "Training Iteration 6082, Loss: 5.042346954345703\n",
            "Training Iteration 6083, Loss: 3.3581786155700684\n",
            "Training Iteration 6084, Loss: 6.240997791290283\n",
            "Training Iteration 6085, Loss: 3.094203233718872\n",
            "Training Iteration 6086, Loss: 2.1521546840667725\n",
            "Training Iteration 6087, Loss: 5.165609359741211\n",
            "Training Iteration 6088, Loss: 1.6826856136322021\n",
            "Training Iteration 6089, Loss: 4.928918838500977\n",
            "Training Iteration 6090, Loss: 5.345300197601318\n",
            "Training Iteration 6091, Loss: 4.090954303741455\n",
            "Training Iteration 6092, Loss: 2.7340402603149414\n",
            "Training Iteration 6093, Loss: 4.988801002502441\n",
            "Training Iteration 6094, Loss: 5.355124473571777\n",
            "Training Iteration 6095, Loss: 1.839482069015503\n",
            "Training Iteration 6096, Loss: 4.193702220916748\n",
            "Training Iteration 6097, Loss: 2.4686689376831055\n",
            "Training Iteration 6098, Loss: 4.117194175720215\n",
            "Training Iteration 6099, Loss: 8.798348426818848\n",
            "Training Iteration 6100, Loss: 2.8738222122192383\n",
            "Training Iteration 6101, Loss: 1.4762150049209595\n",
            "Training Iteration 6102, Loss: 4.21207857131958\n",
            "Training Iteration 6103, Loss: 7.059909820556641\n",
            "Training Iteration 6104, Loss: 3.087751865386963\n",
            "Training Iteration 6105, Loss: 5.8579607009887695\n",
            "Training Iteration 6106, Loss: 2.5145702362060547\n",
            "Training Iteration 6107, Loss: 6.219391822814941\n",
            "Training Iteration 6108, Loss: 5.316588878631592\n",
            "Training Iteration 6109, Loss: 2.917452812194824\n",
            "Training Iteration 6110, Loss: 5.877372741699219\n",
            "Training Iteration 6111, Loss: 5.697403907775879\n",
            "Training Iteration 6112, Loss: 5.9901533126831055\n",
            "Training Iteration 6113, Loss: 5.050948143005371\n",
            "Training Iteration 6114, Loss: 3.9112558364868164\n",
            "Training Iteration 6115, Loss: 7.01522159576416\n",
            "Training Iteration 6116, Loss: 4.848470211029053\n",
            "Training Iteration 6117, Loss: 6.298534870147705\n",
            "Training Iteration 6118, Loss: 4.785494327545166\n",
            "Training Iteration 6119, Loss: 3.719224452972412\n",
            "Training Iteration 6120, Loss: 4.769057750701904\n",
            "Training Iteration 6121, Loss: 3.341049909591675\n",
            "Training Iteration 6122, Loss: 3.386653423309326\n",
            "Training Iteration 6123, Loss: 8.819372177124023\n",
            "Training Iteration 6124, Loss: 8.02097225189209\n",
            "Training Iteration 6125, Loss: 6.167484760284424\n",
            "Training Iteration 6126, Loss: 1.7167302370071411\n",
            "Training Iteration 6127, Loss: 2.5160772800445557\n",
            "Training Iteration 6128, Loss: 3.9487805366516113\n",
            "Training Iteration 6129, Loss: 3.8249034881591797\n",
            "Training Iteration 6130, Loss: 4.599264144897461\n",
            "Training Iteration 6131, Loss: 3.3023200035095215\n",
            "Training Iteration 6132, Loss: 3.5496201515197754\n",
            "Training Iteration 6133, Loss: 3.1604349613189697\n",
            "Training Iteration 6134, Loss: 8.671735763549805\n",
            "Training Iteration 6135, Loss: 4.838716983795166\n",
            "Training Iteration 6136, Loss: 4.739720821380615\n",
            "Training Iteration 6137, Loss: 2.5935676097869873\n",
            "Training Iteration 6138, Loss: 4.080132007598877\n",
            "Training Iteration 6139, Loss: 4.546276569366455\n",
            "Training Iteration 6140, Loss: 3.2612082958221436\n",
            "Training Iteration 6141, Loss: 1.5750658512115479\n",
            "Training Iteration 6142, Loss: 2.908595323562622\n",
            "Training Iteration 6143, Loss: 3.705296754837036\n",
            "Training Iteration 6144, Loss: 2.4930522441864014\n",
            "Training Iteration 6145, Loss: 4.940842628479004\n",
            "Training Iteration 6146, Loss: 2.1553163528442383\n",
            "Training Iteration 6147, Loss: 4.563750267028809\n",
            "Training Iteration 6148, Loss: 4.6730570793151855\n",
            "Training Iteration 6149, Loss: 4.9923624992370605\n",
            "Training Iteration 6150, Loss: 3.7680344581604004\n",
            "Training Iteration 6151, Loss: 3.867311954498291\n",
            "Training Iteration 6152, Loss: 4.160154819488525\n",
            "Training Iteration 6153, Loss: 2.194093704223633\n",
            "Training Iteration 6154, Loss: 3.6346359252929688\n",
            "Training Iteration 6155, Loss: 2.547516345977783\n",
            "Training Iteration 6156, Loss: 5.652536392211914\n",
            "Training Iteration 6157, Loss: 3.7966620922088623\n",
            "Training Iteration 6158, Loss: 4.36898136138916\n",
            "Training Iteration 6159, Loss: 6.3898820877075195\n",
            "Training Iteration 6160, Loss: 3.5532448291778564\n",
            "Training Iteration 6161, Loss: 5.894801616668701\n",
            "Training Iteration 6162, Loss: 7.8313140869140625\n",
            "Training Iteration 6163, Loss: 4.385422229766846\n",
            "Training Iteration 6164, Loss: 3.984158992767334\n",
            "Training Iteration 6165, Loss: 3.6581287384033203\n",
            "Training Iteration 6166, Loss: 4.438329696655273\n",
            "Training Iteration 6167, Loss: 3.422966480255127\n",
            "Training Iteration 6168, Loss: 6.313103199005127\n",
            "Training Iteration 6169, Loss: 6.07445764541626\n",
            "Training Iteration 6170, Loss: 6.140084266662598\n",
            "Training Iteration 6171, Loss: 9.861236572265625\n",
            "Training Iteration 6172, Loss: 3.3358120918273926\n",
            "Training Iteration 6173, Loss: 2.822199583053589\n",
            "Training Iteration 6174, Loss: 5.631017208099365\n",
            "Training Iteration 6175, Loss: 6.788661479949951\n",
            "Training Iteration 6176, Loss: 6.461004257202148\n",
            "Training Iteration 6177, Loss: 6.885091304779053\n",
            "Training Iteration 6178, Loss: 2.4606263637542725\n",
            "Training Iteration 6179, Loss: 5.168563365936279\n",
            "Training Iteration 6180, Loss: 5.753361701965332\n",
            "Training Iteration 6181, Loss: 2.1845574378967285\n",
            "Training Iteration 6182, Loss: 6.159307479858398\n",
            "Training Iteration 6183, Loss: 5.610499858856201\n",
            "Training Iteration 6184, Loss: 1.369484305381775\n",
            "Training Iteration 6185, Loss: 3.9475138187408447\n",
            "Training Iteration 6186, Loss: 5.047754287719727\n",
            "Training Iteration 6187, Loss: 7.655075550079346\n",
            "Training Iteration 6188, Loss: 6.012096881866455\n",
            "Training Iteration 6189, Loss: 4.225368499755859\n",
            "Training Iteration 6190, Loss: 4.0101494789123535\n",
            "Training Iteration 6191, Loss: 2.2148642539978027\n",
            "Training Iteration 6192, Loss: 6.554723739624023\n",
            "Training Iteration 6193, Loss: 5.177217960357666\n",
            "Training Iteration 6194, Loss: 3.46462345123291\n",
            "Training Iteration 6195, Loss: 7.064179420471191\n",
            "Training Iteration 6196, Loss: 4.349219799041748\n",
            "Training Iteration 6197, Loss: 3.206042528152466\n",
            "Training Iteration 6198, Loss: 5.710622310638428\n",
            "Training Iteration 6199, Loss: 4.215802192687988\n",
            "Training Iteration 6200, Loss: 4.065107345581055\n",
            "Training Iteration 6201, Loss: 4.811680316925049\n",
            "Training Iteration 6202, Loss: 3.654099941253662\n",
            "Training Iteration 6203, Loss: 3.8030190467834473\n",
            "Training Iteration 6204, Loss: 4.171218395233154\n",
            "Training Iteration 6205, Loss: 6.3635382652282715\n",
            "Training Iteration 6206, Loss: 4.1169047355651855\n",
            "Training Iteration 6207, Loss: 3.7604618072509766\n",
            "Training Iteration 6208, Loss: 3.746610403060913\n",
            "Training Iteration 6209, Loss: 4.807310104370117\n",
            "Training Iteration 6210, Loss: 4.526803970336914\n",
            "Training Iteration 6211, Loss: 5.749485969543457\n",
            "Training Iteration 6212, Loss: 4.721561908721924\n",
            "Training Iteration 6213, Loss: 2.6778199672698975\n",
            "Training Iteration 6214, Loss: 3.755200147628784\n",
            "Training Iteration 6215, Loss: 4.318409442901611\n",
            "Training Iteration 6216, Loss: 5.013205528259277\n",
            "Training Iteration 6217, Loss: 4.524900436401367\n",
            "Training Iteration 6218, Loss: 5.006826400756836\n",
            "Training Iteration 6219, Loss: 1.7080557346343994\n",
            "Training Iteration 6220, Loss: 2.8307833671569824\n",
            "Training Iteration 6221, Loss: 6.781348705291748\n",
            "Training Iteration 6222, Loss: 4.988184452056885\n",
            "Training Iteration 6223, Loss: 4.936303615570068\n",
            "Training Iteration 6224, Loss: 1.2561547756195068\n",
            "Training Iteration 6225, Loss: 2.9231317043304443\n",
            "Training Iteration 6226, Loss: 4.06878662109375\n",
            "Training Iteration 6227, Loss: 3.6525704860687256\n",
            "Training Iteration 6228, Loss: 3.2657127380371094\n",
            "Training Iteration 6229, Loss: 4.344634532928467\n",
            "Training Iteration 6230, Loss: 10.316056251525879\n",
            "Training Iteration 6231, Loss: 7.019789695739746\n",
            "Training Iteration 6232, Loss: 1.9867911338806152\n",
            "Training Iteration 6233, Loss: 1.795654296875\n",
            "Training Iteration 6234, Loss: 2.372690439224243\n",
            "Training Iteration 6235, Loss: 4.787929058074951\n",
            "Training Iteration 6236, Loss: 5.78515100479126\n",
            "Training Iteration 6237, Loss: 4.469236850738525\n",
            "Training Iteration 6238, Loss: 4.257342338562012\n",
            "Training Iteration 6239, Loss: 4.653156757354736\n",
            "Training Iteration 6240, Loss: 3.640259027481079\n",
            "Training Iteration 6241, Loss: 3.9022927284240723\n",
            "Training Iteration 6242, Loss: 3.150397300720215\n",
            "Training Iteration 6243, Loss: 3.5029208660125732\n",
            "Training Iteration 6244, Loss: 5.075428009033203\n",
            "Training Iteration 6245, Loss: 4.987144470214844\n",
            "Training Iteration 6246, Loss: 2.258476972579956\n",
            "Training Iteration 6247, Loss: 5.471100330352783\n",
            "Training Iteration 6248, Loss: 1.854041576385498\n",
            "Training Iteration 6249, Loss: 1.9455138444900513\n",
            "Training Iteration 6250, Loss: 5.277582168579102\n",
            "Training Iteration 6251, Loss: 4.840288162231445\n",
            "Training Iteration 6252, Loss: 4.4206624031066895\n",
            "Training Iteration 6253, Loss: 4.144209861755371\n",
            "Training Iteration 6254, Loss: 3.144942283630371\n",
            "Training Iteration 6255, Loss: 3.25249981880188\n",
            "Training Iteration 6256, Loss: 7.261176586151123\n",
            "Training Iteration 6257, Loss: 6.434570789337158\n",
            "Training Iteration 6258, Loss: 5.903395175933838\n",
            "Training Iteration 6259, Loss: 4.364917755126953\n",
            "Training Iteration 6260, Loss: 3.8067290782928467\n",
            "Training Iteration 6261, Loss: 3.5694339275360107\n",
            "Training Iteration 6262, Loss: 4.987554550170898\n",
            "Training Iteration 6263, Loss: 0.9401118159294128\n",
            "Training Iteration 6264, Loss: 3.569676160812378\n",
            "Training Iteration 6265, Loss: 7.584965229034424\n",
            "Training Iteration 6266, Loss: 4.498468399047852\n",
            "Training Iteration 6267, Loss: 7.538270473480225\n",
            "Training Iteration 6268, Loss: 4.287872314453125\n",
            "Training Iteration 6269, Loss: 4.296201705932617\n",
            "Training Iteration 6270, Loss: 4.003340244293213\n",
            "Training Iteration 6271, Loss: 8.532207489013672\n",
            "Training Iteration 6272, Loss: 6.7695746421813965\n",
            "Training Iteration 6273, Loss: 9.923903465270996\n",
            "Training Iteration 6274, Loss: 10.264647483825684\n",
            "Training Iteration 6275, Loss: 7.432169437408447\n",
            "Training Iteration 6276, Loss: 9.124958992004395\n",
            "Training Iteration 6277, Loss: 5.926940441131592\n",
            "Training Iteration 6278, Loss: 3.0916664600372314\n",
            "Training Iteration 6279, Loss: 5.772838115692139\n",
            "Training Iteration 6280, Loss: 3.137082815170288\n",
            "Training Iteration 6281, Loss: 3.7976672649383545\n",
            "Training Iteration 6282, Loss: 7.917268753051758\n",
            "Training Iteration 6283, Loss: 6.1282734870910645\n",
            "Training Iteration 6284, Loss: 6.161883354187012\n",
            "Training Iteration 6285, Loss: 2.306220531463623\n",
            "Training Iteration 6286, Loss: 6.871068477630615\n",
            "Training Iteration 6287, Loss: 5.472503185272217\n",
            "Training Iteration 6288, Loss: 5.430180072784424\n",
            "Training Iteration 6289, Loss: 5.0667524337768555\n",
            "Training Iteration 6290, Loss: 4.689281463623047\n",
            "Training Iteration 6291, Loss: 10.406265258789062\n",
            "Training Iteration 6292, Loss: 7.842077732086182\n",
            "Training Iteration 6293, Loss: 4.260322570800781\n",
            "Training Iteration 6294, Loss: 2.7095909118652344\n",
            "Training Iteration 6295, Loss: 7.074031352996826\n",
            "Training Iteration 6296, Loss: 7.743720531463623\n",
            "Training Iteration 6297, Loss: 5.767454147338867\n",
            "Training Iteration 6298, Loss: 4.634675025939941\n",
            "Training Iteration 6299, Loss: 4.8386006355285645\n",
            "Training Iteration 6300, Loss: 1.9194045066833496\n",
            "Training Iteration 6301, Loss: 4.815193176269531\n",
            "Training Iteration 6302, Loss: 3.5858824253082275\n",
            "Training Iteration 6303, Loss: 3.5983474254608154\n",
            "Training Iteration 6304, Loss: 3.894686222076416\n",
            "Training Iteration 6305, Loss: 5.320066452026367\n",
            "Training Iteration 6306, Loss: 4.921468257904053\n",
            "Training Iteration 6307, Loss: 1.197624921798706\n",
            "Training Iteration 6308, Loss: 5.24502420425415\n",
            "Training Iteration 6309, Loss: 3.0503158569335938\n",
            "Training Iteration 6310, Loss: 3.2889316082000732\n",
            "Training Iteration 6311, Loss: 4.247586250305176\n",
            "Training Iteration 6312, Loss: 6.232216835021973\n",
            "Training Iteration 6313, Loss: 3.0857644081115723\n",
            "Training Iteration 6314, Loss: 4.911900043487549\n",
            "Training Iteration 6315, Loss: 4.98958158493042\n",
            "Training Iteration 6316, Loss: 3.7093918323516846\n",
            "Training Iteration 6317, Loss: 3.9325993061065674\n",
            "Training Iteration 6318, Loss: 6.983974456787109\n",
            "Training Iteration 6319, Loss: 3.0052483081817627\n",
            "Training Iteration 6320, Loss: 6.051558494567871\n",
            "Training Iteration 6321, Loss: 5.072735786437988\n",
            "Training Iteration 6322, Loss: 2.7785816192626953\n",
            "Training Iteration 6323, Loss: 8.345305442810059\n",
            "Training Iteration 6324, Loss: 7.147387504577637\n",
            "Training Iteration 6325, Loss: 3.2848968505859375\n",
            "Training Iteration 6326, Loss: 4.122386932373047\n",
            "Training Iteration 6327, Loss: 4.115636348724365\n",
            "Training Iteration 6328, Loss: 0.8000995516777039\n",
            "Training Iteration 6329, Loss: 1.697509527206421\n",
            "Training Iteration 6330, Loss: 6.116944313049316\n",
            "Training Iteration 6331, Loss: 3.5066683292388916\n",
            "Training Iteration 6332, Loss: 5.512042999267578\n",
            "Training Iteration 6333, Loss: 3.658930778503418\n",
            "Training Iteration 6334, Loss: 1.8801935911178589\n",
            "Training Iteration 6335, Loss: 3.880082607269287\n",
            "Training Iteration 6336, Loss: 5.848856449127197\n",
            "Training Iteration 6337, Loss: 5.474822044372559\n",
            "Training Iteration 6338, Loss: 5.26472806930542\n",
            "Training Iteration 6339, Loss: 6.6447672843933105\n",
            "Training Iteration 6340, Loss: 4.524898529052734\n",
            "Training Iteration 6341, Loss: 6.239270210266113\n",
            "Training Iteration 6342, Loss: 6.797590255737305\n",
            "Training Iteration 6343, Loss: 5.0996856689453125\n",
            "Training Iteration 6344, Loss: 3.5767555236816406\n",
            "Training Iteration 6345, Loss: 5.026293754577637\n",
            "Training Iteration 6346, Loss: 3.5585267543792725\n",
            "Training Iteration 6347, Loss: 3.0844082832336426\n",
            "Training Iteration 6348, Loss: 5.732381820678711\n",
            "Training Iteration 6349, Loss: 5.421813011169434\n",
            "Training Iteration 6350, Loss: 4.33892822265625\n",
            "Training Iteration 6351, Loss: 5.873968124389648\n",
            "Training Iteration 6352, Loss: 3.105273723602295\n",
            "Training Iteration 6353, Loss: 2.652527093887329\n",
            "Training Iteration 6354, Loss: 5.061932563781738\n",
            "Training Iteration 6355, Loss: 3.3620569705963135\n",
            "Training Iteration 6356, Loss: 2.4425878524780273\n",
            "Training Iteration 6357, Loss: 4.653670787811279\n",
            "Training Iteration 6358, Loss: 5.447333335876465\n",
            "Training Iteration 6359, Loss: 4.826591491699219\n",
            "Training Iteration 6360, Loss: 4.442897319793701\n",
            "Training Iteration 6361, Loss: 4.663458347320557\n",
            "Training Iteration 6362, Loss: 5.109024524688721\n",
            "Training Iteration 6363, Loss: 3.8844292163848877\n",
            "Training Iteration 6364, Loss: 4.327032089233398\n",
            "Training Iteration 6365, Loss: 4.1354169845581055\n",
            "Training Iteration 6366, Loss: 5.116652965545654\n",
            "Training Iteration 6367, Loss: 4.167994022369385\n",
            "Training Iteration 6368, Loss: 6.581467628479004\n",
            "Training Iteration 6369, Loss: 3.3822240829467773\n",
            "Training Iteration 6370, Loss: 4.3188581466674805\n",
            "Training Iteration 6371, Loss: 9.372122764587402\n",
            "Training Iteration 6372, Loss: 5.050905704498291\n",
            "Training Iteration 6373, Loss: 4.764418601989746\n",
            "Training Iteration 6374, Loss: 8.797060012817383\n",
            "Training Iteration 6375, Loss: 4.011946678161621\n",
            "Training Iteration 6376, Loss: 6.532877445220947\n",
            "Training Iteration 6377, Loss: 5.002033710479736\n",
            "Training Iteration 6378, Loss: 6.684706687927246\n",
            "Training Iteration 6379, Loss: 4.68832540512085\n",
            "Training Iteration 6380, Loss: 4.147431373596191\n",
            "Training Iteration 6381, Loss: 5.446798324584961\n",
            "Training Iteration 6382, Loss: 5.398927211761475\n",
            "Training Iteration 6383, Loss: 2.6803786754608154\n",
            "Training Iteration 6384, Loss: 4.6559295654296875\n",
            "Training Iteration 6385, Loss: 5.102563858032227\n",
            "Training Iteration 6386, Loss: 5.435008525848389\n",
            "Training Iteration 6387, Loss: 4.697479248046875\n",
            "Training Iteration 6388, Loss: 5.175436973571777\n",
            "Training Iteration 6389, Loss: 3.269071340560913\n",
            "Training Iteration 6390, Loss: 5.1533403396606445\n",
            "Training Iteration 6391, Loss: 1.5406975746154785\n",
            "Training Iteration 6392, Loss: 1.8623613119125366\n",
            "Training Iteration 6393, Loss: 7.287615776062012\n",
            "Training Iteration 6394, Loss: 7.119212627410889\n",
            "Training Iteration 6395, Loss: 3.081521987915039\n",
            "Training Iteration 6396, Loss: 5.416492938995361\n",
            "Training Iteration 6397, Loss: 4.869710922241211\n",
            "Training Iteration 6398, Loss: 1.7088592052459717\n",
            "Training Iteration 6399, Loss: 3.992910861968994\n",
            "Training Iteration 6400, Loss: 3.931581735610962\n",
            "Training Iteration 6401, Loss: 4.627322196960449\n",
            "Training Iteration 6402, Loss: 4.934783935546875\n",
            "Training Iteration 6403, Loss: 3.7209935188293457\n",
            "Training Iteration 6404, Loss: 4.675340175628662\n",
            "Training Iteration 6405, Loss: 3.7149746417999268\n",
            "Training Iteration 6406, Loss: 3.494069814682007\n",
            "Training Iteration 6407, Loss: 4.03870153427124\n",
            "Training Iteration 6408, Loss: 3.320723056793213\n",
            "Training Iteration 6409, Loss: 7.182730674743652\n",
            "Training Iteration 6410, Loss: 3.6001458168029785\n",
            "Training Iteration 6411, Loss: 7.1527581214904785\n",
            "Training Iteration 6412, Loss: 1.0227837562561035\n",
            "Training Iteration 6413, Loss: 3.008746385574341\n",
            "Training Iteration 6414, Loss: 3.901848077774048\n",
            "Training Iteration 6415, Loss: 4.069246768951416\n",
            "Training Iteration 6416, Loss: 6.742867469787598\n",
            "Training Iteration 6417, Loss: 3.706521511077881\n",
            "Training Iteration 6418, Loss: 5.820678234100342\n",
            "Training Iteration 6419, Loss: 4.731082916259766\n",
            "Training Iteration 6420, Loss: 1.5447685718536377\n",
            "Training Iteration 6421, Loss: 2.428694486618042\n",
            "Training Iteration 6422, Loss: 5.726462364196777\n",
            "Training Iteration 6423, Loss: 4.1796722412109375\n",
            "Training Iteration 6424, Loss: 3.882150173187256\n",
            "Training Iteration 6425, Loss: 3.506007432937622\n",
            "Training Iteration 6426, Loss: 6.095851898193359\n",
            "Training Iteration 6427, Loss: 3.4882524013519287\n",
            "Training Iteration 6428, Loss: 4.50052547454834\n",
            "Training Iteration 6429, Loss: 3.8286216259002686\n",
            "Training Iteration 6430, Loss: 4.8142852783203125\n",
            "Training Iteration 6431, Loss: 4.321180820465088\n",
            "Training Iteration 6432, Loss: 5.020583629608154\n",
            "Training Iteration 6433, Loss: 3.5317485332489014\n",
            "Training Iteration 6434, Loss: 3.6757547855377197\n",
            "Training Iteration 6435, Loss: 3.1918721199035645\n",
            "Training Iteration 6436, Loss: 7.098072052001953\n",
            "Training Iteration 6437, Loss: 1.9119973182678223\n",
            "Training Iteration 6438, Loss: 4.280943870544434\n",
            "Training Iteration 6439, Loss: 4.11161470413208\n",
            "Training Iteration 6440, Loss: 3.451796531677246\n",
            "Training Iteration 6441, Loss: 5.050980091094971\n",
            "Training Iteration 6442, Loss: 1.6729339361190796\n",
            "Training Iteration 6443, Loss: 7.174488067626953\n",
            "Training Iteration 6444, Loss: 3.0485925674438477\n",
            "Training Iteration 6445, Loss: 3.8490185737609863\n",
            "Training Iteration 6446, Loss: 1.1544504165649414\n",
            "Training Iteration 6447, Loss: 3.8372936248779297\n",
            "Training Iteration 6448, Loss: 5.459808826446533\n",
            "Training Iteration 6449, Loss: 4.8220906257629395\n",
            "Training Iteration 6450, Loss: 4.564441204071045\n",
            "Training Iteration 6451, Loss: 4.746566295623779\n",
            "Training Iteration 6452, Loss: 4.098569869995117\n",
            "Training Iteration 6453, Loss: 5.8252105712890625\n",
            "Training Iteration 6454, Loss: 3.7219715118408203\n",
            "Training Iteration 6455, Loss: 3.4726128578186035\n",
            "Training Iteration 6456, Loss: 5.769346714019775\n",
            "Training Iteration 6457, Loss: 9.296489715576172\n",
            "Training Iteration 6458, Loss: 6.758563041687012\n",
            "Training Iteration 6459, Loss: 3.5017452239990234\n",
            "Training Iteration 6460, Loss: 4.782135009765625\n",
            "Training Iteration 6461, Loss: 6.003478050231934\n",
            "Training Iteration 6462, Loss: 6.469748497009277\n",
            "Training Iteration 6463, Loss: 7.099801063537598\n",
            "Training Iteration 6464, Loss: 7.234012126922607\n",
            "Training Iteration 6465, Loss: 7.660985946655273\n",
            "Training Iteration 6466, Loss: 2.6931071281433105\n",
            "Training Iteration 6467, Loss: 6.348118782043457\n",
            "Training Iteration 6468, Loss: 7.095542907714844\n",
            "Training Iteration 6469, Loss: 3.630809783935547\n",
            "Training Iteration 6470, Loss: 3.211177349090576\n",
            "Training Iteration 6471, Loss: 2.7042593955993652\n",
            "Training Iteration 6472, Loss: 5.466367721557617\n",
            "Training Iteration 6473, Loss: 2.1589763164520264\n",
            "Training Iteration 6474, Loss: 3.586921215057373\n",
            "Training Iteration 6475, Loss: 4.549928188323975\n",
            "Training Iteration 6476, Loss: 6.3667988777160645\n",
            "Training Iteration 6477, Loss: 7.909310340881348\n",
            "Training Iteration 6478, Loss: 6.873567581176758\n",
            "Training Iteration 6479, Loss: 4.896759510040283\n",
            "Training Iteration 6480, Loss: 5.913508892059326\n",
            "Training Iteration 6481, Loss: 7.6668596267700195\n",
            "Training Iteration 6482, Loss: 8.261378288269043\n",
            "Training Iteration 6483, Loss: 3.0461974143981934\n",
            "Training Iteration 6484, Loss: 5.740211486816406\n",
            "Training Iteration 6485, Loss: 3.069181203842163\n",
            "Training Iteration 6486, Loss: 4.025770664215088\n",
            "Training Iteration 6487, Loss: 4.904001235961914\n",
            "Training Iteration 6488, Loss: 6.320155620574951\n",
            "Training Iteration 6489, Loss: 3.1827139854431152\n",
            "Training Iteration 6490, Loss: 7.731423377990723\n",
            "Training Iteration 6491, Loss: 3.5675196647644043\n",
            "Training Iteration 6492, Loss: 7.102099895477295\n",
            "Training Iteration 6493, Loss: 6.026475429534912\n",
            "Training Iteration 6494, Loss: 7.930635452270508\n",
            "Training Iteration 6495, Loss: 3.207329273223877\n",
            "Training Iteration 6496, Loss: 6.116057395935059\n",
            "Training Iteration 6497, Loss: 6.006117343902588\n",
            "Training Iteration 6498, Loss: 2.277284622192383\n",
            "Training Iteration 6499, Loss: 5.289344787597656\n",
            "Training Iteration 6500, Loss: 8.567742347717285\n",
            "Training Iteration 6501, Loss: 5.443102836608887\n",
            "Training Iteration 6502, Loss: 2.4778027534484863\n",
            "Training Iteration 6503, Loss: 2.9994144439697266\n",
            "Training Iteration 6504, Loss: 4.778986930847168\n",
            "Training Iteration 6505, Loss: 7.205896377563477\n",
            "Training Iteration 6506, Loss: 8.421910285949707\n",
            "Training Iteration 6507, Loss: 3.938568115234375\n",
            "Training Iteration 6508, Loss: 4.550235271453857\n",
            "Training Iteration 6509, Loss: 2.563793420791626\n",
            "Training Iteration 6510, Loss: 3.0424787998199463\n",
            "Training Iteration 6511, Loss: 5.689496040344238\n",
            "Training Iteration 6512, Loss: 5.428354740142822\n",
            "Training Iteration 6513, Loss: 4.9051079750061035\n",
            "Training Iteration 6514, Loss: 8.866108894348145\n",
            "Training Iteration 6515, Loss: 6.159780502319336\n",
            "Training Iteration 6516, Loss: 3.428610324859619\n",
            "Training Iteration 6517, Loss: 2.925492286682129\n",
            "Training Iteration 6518, Loss: 6.725512504577637\n",
            "Training Iteration 6519, Loss: 6.061683654785156\n",
            "Training Iteration 6520, Loss: 2.853675603866577\n",
            "Training Iteration 6521, Loss: 5.290907859802246\n",
            "Training Iteration 6522, Loss: 6.890311241149902\n",
            "Training Iteration 6523, Loss: 2.056668519973755\n",
            "Training Iteration 6524, Loss: 3.298938274383545\n",
            "Training Iteration 6525, Loss: 4.829919338226318\n",
            "Training Iteration 6526, Loss: 4.888401508331299\n",
            "Training Iteration 6527, Loss: 4.076321125030518\n",
            "Training Iteration 6528, Loss: 3.8493080139160156\n",
            "Training Iteration 6529, Loss: 4.4031267166137695\n",
            "Training Iteration 6530, Loss: 2.5872421264648438\n",
            "Training Iteration 6531, Loss: 3.1571526527404785\n",
            "Training Iteration 6532, Loss: 6.740420341491699\n",
            "Training Iteration 6533, Loss: 5.008515357971191\n",
            "Training Iteration 6534, Loss: 5.375424385070801\n",
            "Training Iteration 6535, Loss: 6.831591606140137\n",
            "Training Iteration 6536, Loss: 2.8109817504882812\n",
            "Training Iteration 6537, Loss: 3.8177263736724854\n",
            "Training Iteration 6538, Loss: 5.950302600860596\n",
            "Training Iteration 6539, Loss: 8.123340606689453\n",
            "Training Iteration 6540, Loss: 3.2663257122039795\n",
            "Training Iteration 6541, Loss: 3.449178695678711\n",
            "Training Iteration 6542, Loss: 3.9289262294769287\n",
            "Training Iteration 6543, Loss: 4.604667663574219\n",
            "Training Iteration 6544, Loss: 7.221240520477295\n",
            "Training Iteration 6545, Loss: 5.616459846496582\n",
            "Training Iteration 6546, Loss: 5.696230411529541\n",
            "Training Iteration 6547, Loss: 4.679384708404541\n",
            "Training Iteration 6548, Loss: 3.9345154762268066\n",
            "Training Iteration 6549, Loss: 4.027021884918213\n",
            "Training Iteration 6550, Loss: 5.044227600097656\n",
            "Training Iteration 6551, Loss: 6.337344169616699\n",
            "Training Iteration 6552, Loss: 7.326345920562744\n",
            "Training Iteration 6553, Loss: 3.3531482219696045\n",
            "Training Iteration 6554, Loss: 2.545206069946289\n",
            "tensor([[4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        ...,\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03]])\n",
            "Training loss for epcoh 8: 3.459984961591336\n",
            "Training accuracy for epoch 8: 0.08770457406630298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        ...,\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03],\n",
            "        [4.5346e-02, 2.0875e-01, 5.2027e-04, 7.1009e-01, 2.8726e-02, 6.5668e-03]])\n",
            "Validation loss for epcoh 8: 3.450575482197983\n",
            "Test accuracy for epoch 8: 0.08926527809567407\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch No: 9:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1a0588431ab41b48f2ddb645fce31f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Iteration 1, Loss: 5.747020244598389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 2.6631147861480713\n",
            "Training Iteration 1565, Loss: 3.7439329624176025\n",
            "Training Iteration 1566, Loss: 4.349673748016357\n",
            "Training Iteration 1567, Loss: 6.606026649475098\n",
            "Training Iteration 1568, Loss: 3.561584949493408\n",
            "Training Iteration 1569, Loss: 3.2483580112457275\n",
            "Training Iteration 1570, Loss: 2.9761509895324707\n",
            "Training Iteration 1571, Loss: 7.219094276428223\n",
            "Training Iteration 1572, Loss: 3.3908884525299072\n",
            "Training Iteration 1573, Loss: 5.715595722198486\n",
            "Training Iteration 1574, Loss: 3.1278510093688965\n",
            "Training Iteration 1575, Loss: 3.1547305583953857\n",
            "Training Iteration 1576, Loss: 3.160489320755005\n",
            "Training Iteration 1577, Loss: 4.125607967376709\n",
            "Training Iteration 1578, Loss: 3.7197885513305664\n",
            "Training Iteration 1579, Loss: 4.261526107788086\n",
            "Training Iteration 1580, Loss: 3.3946499824523926\n",
            "Training Iteration 1581, Loss: 3.576620101928711\n",
            "Training Iteration 1582, Loss: 4.229910373687744\n",
            "Training Iteration 1583, Loss: 2.5962531566619873\n",
            "Training Iteration 1584, Loss: 6.5638427734375\n",
            "Training Iteration 1585, Loss: 2.9396302700042725\n",
            "Training Iteration 1586, Loss: 5.676872253417969\n",
            "Training Iteration 1587, Loss: 3.9404823780059814\n",
            "Training Iteration 1588, Loss: 3.178222179412842\n",
            "Training Iteration 1589, Loss: 4.187337875366211\n",
            "Training Iteration 1590, Loss: 4.942232131958008\n",
            "Training Iteration 1591, Loss: 4.3136372566223145\n",
            "Training Iteration 1592, Loss: 2.1485097408294678\n",
            "Training Iteration 1593, Loss: 4.109599590301514\n",
            "Training Iteration 1594, Loss: 6.8924760818481445\n",
            "Training Iteration 1595, Loss: 4.859745502471924\n",
            "Training Iteration 1596, Loss: 4.58736515045166\n",
            "Training Iteration 1597, Loss: 3.783048391342163\n",
            "Training Iteration 1598, Loss: 2.820052146911621\n",
            "Training Iteration 1599, Loss: 3.552638530731201\n",
            "Training Iteration 1600, Loss: 4.8375654220581055\n",
            "Training Iteration 1601, Loss: 6.045137405395508\n",
            "Training Iteration 1602, Loss: 2.9053397178649902\n",
            "Training Iteration 1603, Loss: 5.078318119049072\n",
            "Training Iteration 1604, Loss: 3.403841018676758\n",
            "Training Iteration 1605, Loss: 3.1195573806762695\n",
            "Training Iteration 1606, Loss: 3.798915147781372\n",
            "Training Iteration 1607, Loss: 2.517443895339966\n",
            "Training Iteration 1608, Loss: 2.650956869125366\n",
            "Training Iteration 1609, Loss: 4.507594585418701\n",
            "Training Iteration 1610, Loss: 2.64156436920166\n",
            "Training Iteration 1611, Loss: 5.471630096435547\n",
            "Training Iteration 1612, Loss: 4.960333347320557\n",
            "Training Iteration 1613, Loss: 4.589412689208984\n",
            "Training Iteration 1614, Loss: 4.522233963012695\n",
            "Training Iteration 1615, Loss: 3.518892288208008\n",
            "Training Iteration 1616, Loss: 4.696941375732422\n",
            "Training Iteration 1617, Loss: 4.146963596343994\n",
            "Training Iteration 1618, Loss: 5.325061321258545\n",
            "Training Iteration 1619, Loss: 3.346923828125\n",
            "Training Iteration 1620, Loss: 4.787068843841553\n",
            "Training Iteration 1621, Loss: 4.64848518371582\n",
            "Training Iteration 1622, Loss: 4.7358903884887695\n",
            "Training Iteration 1623, Loss: 3.9728894233703613\n",
            "Training Iteration 1624, Loss: 4.017064094543457\n",
            "Training Iteration 1625, Loss: 3.1828558444976807\n",
            "Training Iteration 1626, Loss: 3.326420783996582\n",
            "Training Iteration 1627, Loss: 4.394242286682129\n",
            "Training Iteration 1628, Loss: 3.316962718963623\n",
            "Training Iteration 1629, Loss: 2.942964553833008\n",
            "Training Iteration 1630, Loss: 2.481381416320801\n",
            "Training Iteration 1631, Loss: 1.912524700164795\n",
            "Training Iteration 1632, Loss: 5.079901218414307\n",
            "Training Iteration 1633, Loss: 4.105008125305176\n",
            "Training Iteration 1634, Loss: 4.810573101043701\n",
            "Training Iteration 1635, Loss: 3.5539779663085938\n",
            "Training Iteration 1636, Loss: 2.0871167182922363\n",
            "Training Iteration 1637, Loss: 4.785651683807373\n",
            "Training Iteration 1638, Loss: 5.428149700164795\n",
            "Training Iteration 1639, Loss: 1.9299184083938599\n",
            "Training Iteration 1640, Loss: 2.7926597595214844\n",
            "Training Iteration 1641, Loss: 1.2550464868545532\n",
            "Training Iteration 1642, Loss: 1.5424301624298096\n",
            "Training Iteration 1643, Loss: 2.944873332977295\n",
            "Training Iteration 1644, Loss: 5.5474138259887695\n",
            "Training Iteration 1645, Loss: 4.1932268142700195\n",
            "Training Iteration 1646, Loss: 2.501457929611206\n",
            "Training Iteration 1647, Loss: 4.386476993560791\n",
            "Training Iteration 1648, Loss: 4.917154312133789\n",
            "Training Iteration 1649, Loss: 5.11036491394043\n",
            "Training Iteration 1650, Loss: 2.7790489196777344\n",
            "Training Iteration 1651, Loss: 9.124295234680176\n",
            "Training Iteration 1652, Loss: 7.92537784576416\n",
            "Training Iteration 1653, Loss: 4.376594543457031\n",
            "Training Iteration 1654, Loss: 6.821913719177246\n",
            "Training Iteration 1655, Loss: 2.941225528717041\n",
            "Training Iteration 1656, Loss: 5.212035179138184\n",
            "Training Iteration 1657, Loss: 5.713539123535156\n",
            "Training Iteration 1658, Loss: 3.49013614654541\n",
            "Training Iteration 1659, Loss: 1.219367504119873\n",
            "Training Iteration 1660, Loss: 3.965878963470459\n",
            "Training Iteration 1661, Loss: 4.537998676300049\n",
            "Training Iteration 1662, Loss: 3.802323579788208\n",
            "Training Iteration 1663, Loss: 6.583968162536621\n",
            "Training Iteration 1664, Loss: 4.593580722808838\n",
            "Training Iteration 1665, Loss: 3.931774616241455\n",
            "Training Iteration 1666, Loss: 3.717644453048706\n",
            "Training Iteration 1667, Loss: 4.03763484954834\n",
            "Training Iteration 1668, Loss: 4.313175678253174\n",
            "Training Iteration 1669, Loss: 7.351091384887695\n",
            "Training Iteration 1670, Loss: 3.2097551822662354\n",
            "Training Iteration 1671, Loss: 3.227991819381714\n",
            "Training Iteration 1672, Loss: 3.478224992752075\n",
            "Training Iteration 1673, Loss: 2.8518946170806885\n",
            "Training Iteration 1674, Loss: 5.8356781005859375\n",
            "Training Iteration 1675, Loss: 2.6453983783721924\n",
            "Training Iteration 1676, Loss: 4.4497833251953125\n",
            "Training Iteration 1677, Loss: 2.2110800743103027\n",
            "Training Iteration 1678, Loss: 2.801140785217285\n",
            "Training Iteration 1679, Loss: 3.1004788875579834\n",
            "Training Iteration 1680, Loss: 4.664830684661865\n",
            "Training Iteration 1681, Loss: 6.778229713439941\n",
            "Training Iteration 1682, Loss: 3.338300943374634\n",
            "Training Iteration 1683, Loss: 3.6979353427886963\n",
            "Training Iteration 1684, Loss: 3.343346357345581\n",
            "Training Iteration 1685, Loss: 5.025608062744141\n",
            "Training Iteration 1686, Loss: 4.716922283172607\n",
            "Training Iteration 1687, Loss: 5.001492500305176\n",
            "Training Iteration 1688, Loss: 3.727360725402832\n",
            "Training Iteration 1689, Loss: 6.012181758880615\n",
            "Training Iteration 1690, Loss: 2.460688829421997\n",
            "Training Iteration 1691, Loss: 5.044504165649414\n",
            "Training Iteration 1692, Loss: 6.570008277893066\n",
            "Training Iteration 1693, Loss: 2.754009246826172\n",
            "Training Iteration 1694, Loss: 3.1226272583007812\n",
            "Training Iteration 1695, Loss: 6.517309188842773\n",
            "Training Iteration 1696, Loss: 4.262366771697998\n",
            "Training Iteration 1697, Loss: 3.9256227016448975\n",
            "Training Iteration 1698, Loss: 7.116424560546875\n",
            "Training Iteration 1699, Loss: 6.114347457885742\n",
            "Training Iteration 1700, Loss: 4.647565841674805\n",
            "Training Iteration 1701, Loss: 6.632421493530273\n",
            "Training Iteration 1702, Loss: 4.852988243103027\n",
            "Training Iteration 1703, Loss: 3.8966245651245117\n",
            "Training Iteration 1704, Loss: 3.407715320587158\n",
            "Training Iteration 1705, Loss: 4.1484904289245605\n",
            "Training Iteration 1706, Loss: 6.046163082122803\n",
            "Training Iteration 1707, Loss: 3.7958555221557617\n",
            "Training Iteration 1708, Loss: 3.472069501876831\n",
            "Training Iteration 1709, Loss: 8.202082633972168\n",
            "Training Iteration 1710, Loss: 3.8889803886413574\n",
            "Training Iteration 1711, Loss: 3.030609130859375\n",
            "Training Iteration 1712, Loss: 2.712249279022217\n",
            "Training Iteration 1713, Loss: 4.304068088531494\n",
            "Training Iteration 1714, Loss: 3.637454032897949\n",
            "Training Iteration 1715, Loss: 4.103460311889648\n",
            "Training Iteration 1716, Loss: 4.250536918640137\n",
            "Training Iteration 1717, Loss: 5.0404052734375\n",
            "Training Iteration 1718, Loss: 6.400740623474121\n",
            "Training Iteration 1719, Loss: 3.5811290740966797\n",
            "Training Iteration 1720, Loss: 4.698464870452881\n",
            "Training Iteration 1721, Loss: 4.679931163787842\n",
            "Training Iteration 1722, Loss: 5.320786952972412\n",
            "Training Iteration 1723, Loss: 4.105103969573975\n",
            "Training Iteration 1724, Loss: 3.7103958129882812\n",
            "Training Iteration 1725, Loss: 3.835904121398926\n",
            "Training Iteration 1726, Loss: 3.193110942840576\n",
            "Training Iteration 1727, Loss: 2.670560359954834\n",
            "Training Iteration 1728, Loss: 2.7985105514526367\n",
            "Training Iteration 1729, Loss: 4.044317245483398\n",
            "Training Iteration 1730, Loss: 5.265544891357422\n",
            "Training Iteration 1731, Loss: 5.960003852844238\n",
            "Training Iteration 1732, Loss: 2.6410632133483887\n",
            "Training Iteration 1733, Loss: 5.786247730255127\n",
            "Training Iteration 1734, Loss: 5.076075077056885\n",
            "Training Iteration 1735, Loss: 6.941889762878418\n",
            "Training Iteration 1736, Loss: 4.81352424621582\n",
            "Training Iteration 1737, Loss: 3.650801181793213\n",
            "Training Iteration 1738, Loss: 1.5709457397460938\n",
            "Training Iteration 1739, Loss: 3.4980831146240234\n",
            "Training Iteration 1740, Loss: 5.110190391540527\n",
            "Training Iteration 1741, Loss: 4.132558822631836\n",
            "Training Iteration 1742, Loss: 5.115538597106934\n",
            "Training Iteration 1743, Loss: 3.6276378631591797\n",
            "Training Iteration 1744, Loss: 2.2167115211486816\n",
            "Training Iteration 1745, Loss: 4.282779216766357\n",
            "Training Iteration 1746, Loss: 4.456212997436523\n",
            "Training Iteration 1747, Loss: 7.394686698913574\n",
            "Training Iteration 1748, Loss: 5.39236307144165\n",
            "Training Iteration 1749, Loss: 5.900588035583496\n",
            "Training Iteration 1750, Loss: 7.188110828399658\n",
            "Training Iteration 1751, Loss: 4.802041053771973\n",
            "Training Iteration 1752, Loss: 5.593894004821777\n",
            "Training Iteration 1753, Loss: 4.379162788391113\n",
            "Training Iteration 1754, Loss: 6.784640312194824\n",
            "Training Iteration 1755, Loss: 8.658246040344238\n",
            "Training Iteration 1756, Loss: 5.095654487609863\n",
            "Training Iteration 1757, Loss: 4.104193210601807\n",
            "Training Iteration 1758, Loss: 3.796151638031006\n",
            "Training Iteration 1759, Loss: 1.8689864873886108\n",
            "Training Iteration 1760, Loss: 3.223874568939209\n",
            "Training Iteration 1761, Loss: 3.2334156036376953\n",
            "Training Iteration 1762, Loss: 4.500149250030518\n",
            "Training Iteration 1763, Loss: 4.643332004547119\n",
            "Training Iteration 1764, Loss: 5.699522495269775\n",
            "Training Iteration 1765, Loss: 8.149577140808105\n",
            "Training Iteration 1766, Loss: 4.826689720153809\n",
            "Training Iteration 1767, Loss: 3.5976474285125732\n",
            "Training Iteration 1768, Loss: 4.475616931915283\n",
            "Training Iteration 1769, Loss: 3.3206546306610107\n",
            "Training Iteration 1770, Loss: 6.262543678283691\n",
            "Training Iteration 1771, Loss: 4.659719944000244\n",
            "Training Iteration 1772, Loss: 7.695160388946533\n",
            "Training Iteration 1773, Loss: 11.251337051391602\n",
            "Training Iteration 1774, Loss: 4.995443344116211\n",
            "Training Iteration 1775, Loss: 5.845234394073486\n",
            "Training Iteration 1776, Loss: 5.324765205383301\n",
            "Training Iteration 1777, Loss: 4.604203701019287\n",
            "Training Iteration 1778, Loss: 5.189886569976807\n",
            "Training Iteration 1779, Loss: 5.080867290496826\n",
            "Training Iteration 1780, Loss: 8.176593780517578\n",
            "Training Iteration 1781, Loss: 8.007774353027344\n",
            "Training Iteration 1782, Loss: 7.704003810882568\n",
            "Training Iteration 1783, Loss: 8.243305206298828\n",
            "Training Iteration 1784, Loss: 3.0611071586608887\n",
            "Training Iteration 1785, Loss: 3.4288814067840576\n",
            "Training Iteration 1786, Loss: 5.075351715087891\n",
            "Training Iteration 1787, Loss: 8.122282028198242\n",
            "Training Iteration 1788, Loss: 7.706313133239746\n",
            "Training Iteration 1789, Loss: 7.132920742034912\n",
            "Training Iteration 1790, Loss: 9.188911437988281\n",
            "Training Iteration 1791, Loss: 2.8173348903656006\n",
            "Training Iteration 1792, Loss: 3.5017051696777344\n",
            "Training Iteration 1793, Loss: 5.280041217803955\n",
            "Training Iteration 1794, Loss: 5.92686128616333\n",
            "Training Iteration 1795, Loss: 6.059540748596191\n",
            "Training Iteration 1796, Loss: 4.03142786026001\n",
            "Training Iteration 1797, Loss: 1.8037710189819336\n",
            "Training Iteration 1798, Loss: 3.6340484619140625\n",
            "Training Iteration 1799, Loss: 4.273760795593262\n",
            "Training Iteration 1800, Loss: 3.9733736515045166\n",
            "Training Iteration 1801, Loss: 6.413811683654785\n",
            "Training Iteration 1802, Loss: 6.158963203430176\n",
            "Training Iteration 1803, Loss: 2.7265920639038086\n",
            "Training Iteration 1804, Loss: 5.079220294952393\n",
            "Training Iteration 1805, Loss: 6.730437278747559\n",
            "Training Iteration 1806, Loss: 5.043005466461182\n",
            "Training Iteration 1807, Loss: 6.548688888549805\n",
            "Training Iteration 1808, Loss: 2.8926470279693604\n",
            "Training Iteration 1809, Loss: 3.940769910812378\n",
            "Training Iteration 1810, Loss: 3.3286280632019043\n",
            "Training Iteration 1811, Loss: 8.125659942626953\n",
            "Training Iteration 1812, Loss: 3.3186049461364746\n",
            "Training Iteration 1813, Loss: 3.1414434909820557\n",
            "Training Iteration 1814, Loss: 6.258940696716309\n",
            "Training Iteration 1815, Loss: 4.431272983551025\n",
            "Training Iteration 1816, Loss: 4.106390953063965\n",
            "Training Iteration 1817, Loss: 1.9746397733688354\n",
            "Training Iteration 1818, Loss: 2.3560001850128174\n",
            "Training Iteration 1819, Loss: 4.238560676574707\n",
            "Training Iteration 1820, Loss: 5.2843122482299805\n",
            "Training Iteration 1821, Loss: 5.962270736694336\n",
            "Training Iteration 1822, Loss: 5.888071537017822\n",
            "Training Iteration 1823, Loss: 7.834207534790039\n",
            "Training Iteration 1824, Loss: 3.6567444801330566\n",
            "Training Iteration 1825, Loss: 6.898217678070068\n",
            "Training Iteration 1826, Loss: 4.829434871673584\n",
            "Training Iteration 1827, Loss: 3.400693416595459\n",
            "Training Iteration 1828, Loss: 3.9914743900299072\n",
            "Training Iteration 1829, Loss: 2.865462064743042\n",
            "Training Iteration 1830, Loss: 3.5736935138702393\n",
            "Training Iteration 1831, Loss: 8.257205963134766\n",
            "Training Iteration 1832, Loss: 9.338174819946289\n",
            "Training Iteration 1833, Loss: 3.294405937194824\n",
            "Training Iteration 1834, Loss: 5.427731990814209\n",
            "Training Iteration 1835, Loss: 2.0571560859680176\n",
            "Training Iteration 1836, Loss: 10.608461380004883\n",
            "Training Iteration 1837, Loss: 8.88620662689209\n",
            "Training Iteration 1838, Loss: 4.802478790283203\n",
            "Training Iteration 1839, Loss: 3.273850679397583\n",
            "Training Iteration 1840, Loss: 7.136159896850586\n",
            "Training Iteration 1841, Loss: 3.403531789779663\n",
            "Training Iteration 1842, Loss: 3.7934982776641846\n",
            "Training Iteration 1843, Loss: 5.186552047729492\n",
            "Training Iteration 1844, Loss: 3.077735185623169\n",
            "Training Iteration 1845, Loss: 6.5408430099487305\n",
            "Training Iteration 1846, Loss: 3.6549503803253174\n",
            "Training Iteration 1847, Loss: 2.9284706115722656\n",
            "Training Iteration 1848, Loss: 5.2233662605285645\n",
            "Training Iteration 1849, Loss: 6.194967746734619\n",
            "Training Iteration 1850, Loss: 3.7855396270751953\n",
            "Training Iteration 1851, Loss: 4.7921929359436035\n",
            "Training Iteration 1852, Loss: 5.203444480895996\n",
            "Training Iteration 1853, Loss: 4.971746444702148\n",
            "Training Iteration 1854, Loss: 2.938771963119507\n",
            "Training Iteration 1855, Loss: 4.826315879821777\n",
            "Training Iteration 1856, Loss: 12.564543724060059\n",
            "Training Iteration 1857, Loss: 5.173781394958496\n",
            "Training Iteration 1858, Loss: 4.720502853393555\n",
            "Training Iteration 1859, Loss: 2.7235848903656006\n",
            "Training Iteration 1860, Loss: 4.142210006713867\n",
            "Training Iteration 1861, Loss: 3.28200626373291\n",
            "Training Iteration 1862, Loss: 1.6405954360961914\n",
            "Training Iteration 1863, Loss: 4.905471324920654\n",
            "Training Iteration 1864, Loss: 3.8593268394470215\n",
            "Training Iteration 1865, Loss: 11.532787322998047\n",
            "Training Iteration 1866, Loss: 6.679291248321533\n",
            "Training Iteration 1867, Loss: 10.041603088378906\n",
            "Training Iteration 1868, Loss: 9.920228958129883\n",
            "Training Iteration 1869, Loss: 12.079855918884277\n",
            "Training Iteration 1870, Loss: 7.374764442443848\n",
            "Training Iteration 1871, Loss: 6.175746440887451\n",
            "Training Iteration 1872, Loss: 5.9188408851623535\n",
            "Training Iteration 1873, Loss: 4.227893829345703\n",
            "Training Iteration 1874, Loss: 3.7120890617370605\n",
            "Training Iteration 1875, Loss: 3.2643513679504395\n",
            "Training Iteration 1876, Loss: 4.242088794708252\n",
            "Training Iteration 1877, Loss: 5.06662654876709\n",
            "Training Iteration 1878, Loss: 3.4929792881011963\n",
            "Training Iteration 1879, Loss: 9.823942184448242\n",
            "Training Iteration 1880, Loss: 5.794384479522705\n",
            "Training Iteration 1881, Loss: 5.357325553894043\n",
            "Training Iteration 1882, Loss: 4.023647308349609\n",
            "Training Iteration 1883, Loss: 4.695754051208496\n",
            "Training Iteration 1884, Loss: 6.487844467163086\n",
            "Training Iteration 1885, Loss: 4.594961166381836\n",
            "Training Iteration 1886, Loss: 7.402935028076172\n",
            "Training Iteration 1887, Loss: 4.515904426574707\n",
            "Training Iteration 1888, Loss: 3.7187721729278564\n",
            "Training Iteration 1889, Loss: 5.0223388671875\n",
            "Training Iteration 1890, Loss: 1.1271350383758545\n",
            "Training Iteration 1891, Loss: 3.1936123371124268\n",
            "Training Iteration 1892, Loss: 4.174471855163574\n",
            "Training Iteration 1893, Loss: 1.698906421661377\n",
            "Training Iteration 1894, Loss: 1.4709447622299194\n",
            "Training Iteration 1895, Loss: 1.9735827445983887\n",
            "Training Iteration 1896, Loss: 2.356539249420166\n",
            "Training Iteration 1897, Loss: 4.611827850341797\n",
            "Training Iteration 1898, Loss: 17.360065460205078\n",
            "Training Iteration 1899, Loss: 7.18949031829834\n",
            "Training Iteration 1900, Loss: 5.6644287109375\n",
            "Training Iteration 1901, Loss: 2.676543712615967\n",
            "Training Iteration 1902, Loss: 4.189728260040283\n",
            "Training Iteration 1903, Loss: 6.37687873840332\n",
            "Training Iteration 1904, Loss: 4.052701473236084\n",
            "Training Iteration 1905, Loss: 5.274442672729492\n",
            "Training Iteration 1906, Loss: 3.8203206062316895\n",
            "Training Iteration 1907, Loss: 4.58632755279541\n",
            "Training Iteration 1908, Loss: 4.88784122467041\n",
            "Training Iteration 1909, Loss: 5.594571113586426\n",
            "Training Iteration 1910, Loss: 3.921999454498291\n",
            "Training Iteration 1911, Loss: 1.677662968635559\n",
            "Training Iteration 1912, Loss: 6.515687942504883\n",
            "Training Iteration 1913, Loss: 4.891548156738281\n",
            "Training Iteration 1914, Loss: 3.880871295928955\n",
            "Training Iteration 1915, Loss: 3.2143423557281494\n",
            "Training Iteration 1916, Loss: 4.5771803855896\n",
            "Training Iteration 1917, Loss: 3.21662974357605\n",
            "Training Iteration 1918, Loss: 7.715798854827881\n",
            "Training Iteration 1919, Loss: 9.79531192779541\n",
            "Training Iteration 1920, Loss: 9.62350845336914\n",
            "Training Iteration 1921, Loss: 3.279921293258667\n",
            "Training Iteration 1922, Loss: 4.047440052032471\n",
            "Training Iteration 1923, Loss: 3.3146064281463623\n",
            "Training Iteration 1924, Loss: 7.738879680633545\n",
            "Training Iteration 1925, Loss: 5.612730026245117\n",
            "Training Iteration 1926, Loss: 1.58882737159729\n",
            "Training Iteration 1927, Loss: 6.71854305267334\n",
            "Training Iteration 1928, Loss: 3.633211612701416\n",
            "Training Iteration 1929, Loss: 3.5754263401031494\n",
            "Training Iteration 1930, Loss: 5.776636123657227\n",
            "Training Iteration 1931, Loss: 4.010112762451172\n",
            "Training Iteration 1932, Loss: 4.945648193359375\n",
            "Training Iteration 1933, Loss: 3.082329750061035\n",
            "Training Iteration 1934, Loss: 6.82484769821167\n",
            "Training Iteration 1935, Loss: 2.347198009490967\n",
            "Training Iteration 1936, Loss: 4.518735885620117\n",
            "Training Iteration 1937, Loss: 4.156102180480957\n",
            "Training Iteration 1938, Loss: 2.314131736755371\n",
            "Training Iteration 1939, Loss: 3.608959913253784\n",
            "Training Iteration 1940, Loss: 1.5349549055099487\n",
            "Training Iteration 1941, Loss: 1.6788601875305176\n",
            "Training Iteration 1942, Loss: 5.600530624389648\n",
            "Training Iteration 1943, Loss: 4.86207914352417\n",
            "Training Iteration 1944, Loss: 1.2222801446914673\n",
            "Training Iteration 1945, Loss: 2.2513914108276367\n",
            "Training Iteration 1946, Loss: 5.0306196212768555\n",
            "Training Iteration 1947, Loss: 4.770579814910889\n",
            "Training Iteration 1948, Loss: 2.7461955547332764\n",
            "Training Iteration 1949, Loss: 4.08286714553833\n",
            "Training Iteration 1950, Loss: 4.468666076660156\n",
            "Training Iteration 1951, Loss: 5.205853462219238\n",
            "Training Iteration 1952, Loss: 1.5152573585510254\n",
            "Training Iteration 1953, Loss: 4.004636764526367\n",
            "Training Iteration 1954, Loss: 5.5818939208984375\n",
            "Training Iteration 1955, Loss: 2.492725133895874\n",
            "Training Iteration 1956, Loss: 10.963179588317871\n",
            "Training Iteration 1957, Loss: 2.8259847164154053\n",
            "Training Iteration 1958, Loss: 2.2307732105255127\n",
            "Training Iteration 1959, Loss: 9.089933395385742\n",
            "Training Iteration 1960, Loss: 7.004579544067383\n",
            "Training Iteration 1961, Loss: 2.0394716262817383\n",
            "Training Iteration 1962, Loss: 8.934281349182129\n",
            "Training Iteration 1963, Loss: 4.930100440979004\n",
            "Training Iteration 1964, Loss: 2.8826956748962402\n",
            "Training Iteration 1965, Loss: 3.3491382598876953\n",
            "Training Iteration 1966, Loss: 3.2062606811523438\n",
            "Training Iteration 1967, Loss: 6.586176872253418\n",
            "Training Iteration 1968, Loss: 3.3442792892456055\n",
            "Training Iteration 1969, Loss: 5.0343732833862305\n",
            "Training Iteration 1970, Loss: 4.148358345031738\n",
            "Training Iteration 1971, Loss: 4.190439701080322\n",
            "Training Iteration 1972, Loss: 2.7629778385162354\n",
            "Training Iteration 1973, Loss: 3.018700122833252\n",
            "Training Iteration 1974, Loss: 1.893009066581726\n",
            "Training Iteration 1975, Loss: 3.066251039505005\n",
            "Training Iteration 1976, Loss: 2.6099801063537598\n",
            "Training Iteration 1977, Loss: 5.369002819061279\n",
            "Training Iteration 1978, Loss: 2.467796802520752\n",
            "Training Iteration 1979, Loss: 3.810929775238037\n",
            "Training Iteration 1980, Loss: 4.5955376625061035\n",
            "Training Iteration 1981, Loss: 5.292594909667969\n",
            "Training Iteration 1982, Loss: 3.724064350128174\n",
            "Training Iteration 1983, Loss: 2.8580243587493896\n",
            "Training Iteration 1984, Loss: 8.109352111816406\n",
            "Training Iteration 1985, Loss: 7.681399822235107\n",
            "Training Iteration 1986, Loss: 3.4231598377227783\n",
            "Training Iteration 1987, Loss: 4.452877998352051\n",
            "Training Iteration 1988, Loss: 3.372692108154297\n",
            "Training Iteration 1989, Loss: 7.296963691711426\n",
            "Training Iteration 1990, Loss: 2.5601885318756104\n",
            "Training Iteration 1991, Loss: 2.007784605026245\n",
            "Training Iteration 1992, Loss: 4.108239650726318\n",
            "Training Iteration 1993, Loss: 3.7817373275756836\n",
            "Training Iteration 1994, Loss: 3.7732927799224854\n",
            "Training Iteration 1995, Loss: 3.168128252029419\n",
            "Training Iteration 1996, Loss: 6.355291366577148\n",
            "Training Iteration 1997, Loss: 3.3964169025421143\n",
            "Training Iteration 1998, Loss: 4.748892307281494\n",
            "Training Iteration 1999, Loss: 4.474066734313965\n",
            "Training Iteration 2000, Loss: 5.89422082901001\n",
            "Training Iteration 2001, Loss: 6.482343673706055\n",
            "Training Iteration 2002, Loss: 3.8427205085754395\n",
            "Training Iteration 2003, Loss: 4.634833812713623\n",
            "Training Iteration 2004, Loss: 7.313749313354492\n",
            "Training Iteration 2005, Loss: 8.431983947753906\n",
            "Training Iteration 2006, Loss: 3.7060887813568115\n",
            "Training Iteration 2007, Loss: 6.553440570831299\n",
            "Training Iteration 2008, Loss: 6.700425624847412\n",
            "Training Iteration 2009, Loss: 6.5410919189453125\n",
            "Training Iteration 2010, Loss: 2.843703269958496\n",
            "Training Iteration 2011, Loss: 8.082947731018066\n",
            "Training Iteration 2012, Loss: 5.802407741546631\n",
            "Training Iteration 2013, Loss: 7.599584102630615\n",
            "Training Iteration 2014, Loss: 4.662177085876465\n",
            "Training Iteration 2015, Loss: 4.622376918792725\n",
            "Training Iteration 2016, Loss: 6.341598033905029\n",
            "Training Iteration 2017, Loss: 3.9624834060668945\n",
            "Training Iteration 2018, Loss: 2.0016770362854004\n",
            "Training Iteration 2019, Loss: 6.319430351257324\n",
            "Training Iteration 2020, Loss: 4.818978309631348\n",
            "Training Iteration 2021, Loss: 2.1595332622528076\n",
            "Training Iteration 2022, Loss: 7.054117679595947\n",
            "Training Iteration 2023, Loss: 1.534613013267517\n",
            "Training Iteration 2024, Loss: 10.167571067810059\n",
            "Training Iteration 2025, Loss: 5.655248641967773\n",
            "Training Iteration 2026, Loss: 3.354224920272827\n",
            "Training Iteration 2027, Loss: 7.898426532745361\n",
            "Training Iteration 2028, Loss: 2.440918207168579\n",
            "Training Iteration 2029, Loss: 5.055798053741455\n",
            "Training Iteration 2030, Loss: 6.555630207061768\n",
            "Training Iteration 2031, Loss: 6.973822116851807\n",
            "Training Iteration 2032, Loss: 3.044107675552368\n",
            "Training Iteration 2033, Loss: 5.881743907928467\n",
            "Training Iteration 2034, Loss: 5.163559436798096\n",
            "Training Iteration 2035, Loss: 14.924334526062012\n",
            "Training Iteration 2036, Loss: 2.2136409282684326\n",
            "Training Iteration 2037, Loss: 1.1409326791763306\n",
            "Training Iteration 2038, Loss: 3.9479150772094727\n",
            "Training Iteration 2039, Loss: 3.208479404449463\n",
            "Training Iteration 2040, Loss: 10.103937149047852\n",
            "Training Iteration 2041, Loss: 1.9206323623657227\n",
            "Training Iteration 2042, Loss: 3.814089775085449\n",
            "Training Iteration 2043, Loss: 5.89143705368042\n",
            "Training Iteration 2044, Loss: 7.452678680419922\n",
            "Training Iteration 2045, Loss: 2.3448450565338135\n",
            "Training Iteration 2046, Loss: 6.067878246307373\n",
            "Training Iteration 2047, Loss: 6.034796714782715\n",
            "Training Iteration 2048, Loss: 5.855742931365967\n",
            "Training Iteration 2049, Loss: 3.065115213394165\n",
            "Training Iteration 2050, Loss: 4.278696060180664\n",
            "Training Iteration 2051, Loss: 4.00972843170166\n",
            "Training Iteration 2052, Loss: 3.2949109077453613\n",
            "Training Iteration 2053, Loss: 4.223189353942871\n",
            "Training Iteration 2054, Loss: 4.841461658477783\n",
            "Training Iteration 2055, Loss: 3.9395647048950195\n",
            "Training Iteration 2056, Loss: 5.507959842681885\n",
            "Training Iteration 2057, Loss: 4.942226886749268\n",
            "Training Iteration 2058, Loss: 4.879513740539551\n",
            "Training Iteration 2059, Loss: 4.046630859375\n",
            "Training Iteration 2060, Loss: 3.066462993621826\n",
            "Training Iteration 2061, Loss: 5.321287631988525\n",
            "Training Iteration 2062, Loss: 4.453583240509033\n",
            "Training Iteration 2063, Loss: 5.295468807220459\n",
            "Training Iteration 2064, Loss: 3.5885229110717773\n",
            "Training Iteration 2065, Loss: 5.961128234863281\n",
            "Training Iteration 2066, Loss: 4.8880085945129395\n",
            "Training Iteration 2067, Loss: 3.924175262451172\n",
            "Training Iteration 2068, Loss: 4.806771278381348\n",
            "Training Iteration 2069, Loss: 3.8434860706329346\n",
            "Training Iteration 2070, Loss: 5.788692474365234\n",
            "Training Iteration 2071, Loss: 6.250571250915527\n",
            "Training Iteration 2072, Loss: 4.111879825592041\n",
            "Training Iteration 2073, Loss: 6.529471397399902\n",
            "Training Iteration 2074, Loss: 4.481031894683838\n",
            "Training Iteration 2075, Loss: 4.208506107330322\n",
            "Training Iteration 2076, Loss: 6.5433878898620605\n",
            "Training Iteration 2077, Loss: 4.176364898681641\n",
            "Training Iteration 2078, Loss: 4.487499237060547\n",
            "Training Iteration 2079, Loss: 3.6484804153442383\n",
            "Training Iteration 2080, Loss: 4.0026140213012695\n",
            "Training Iteration 2081, Loss: 1.9235297441482544\n",
            "Training Iteration 2082, Loss: 2.9995834827423096\n",
            "Training Iteration 2083, Loss: 5.648325443267822\n",
            "Training Iteration 2084, Loss: 2.707312822341919\n",
            "Training Iteration 2085, Loss: 2.226536989212036\n",
            "Training Iteration 2086, Loss: 4.269669532775879\n",
            "Training Iteration 2087, Loss: 4.445390224456787\n",
            "Training Iteration 2088, Loss: 5.3644185066223145\n",
            "Training Iteration 2089, Loss: 2.9088003635406494\n",
            "Training Iteration 2090, Loss: 5.649216175079346\n",
            "Training Iteration 2091, Loss: 1.9704010486602783\n",
            "Training Iteration 2092, Loss: 3.9001235961914062\n",
            "Training Iteration 2093, Loss: 2.524960994720459\n",
            "Training Iteration 2094, Loss: 1.633949875831604\n",
            "Training Iteration 2095, Loss: 3.5918824672698975\n",
            "Training Iteration 2096, Loss: 9.572404861450195\n",
            "Training Iteration 2097, Loss: 6.858888626098633\n",
            "Training Iteration 2098, Loss: 3.860827922821045\n",
            "Training Iteration 2099, Loss: 6.667056560516357\n",
            "Training Iteration 2100, Loss: 4.692539215087891\n",
            "Training Iteration 2101, Loss: 6.341176509857178\n",
            "Training Iteration 2102, Loss: 4.303038597106934\n",
            "Training Iteration 2103, Loss: 5.245569229125977\n",
            "Training Iteration 2104, Loss: 3.7372970581054688\n",
            "Training Iteration 2105, Loss: 6.6316022872924805\n",
            "Training Iteration 2106, Loss: 4.284663200378418\n",
            "Training Iteration 2107, Loss: 6.4535603523254395\n",
            "Training Iteration 2108, Loss: 5.852174282073975\n",
            "Training Iteration 2109, Loss: 4.258410930633545\n",
            "Training Iteration 2110, Loss: 8.407759666442871\n",
            "Training Iteration 2111, Loss: 1.6455562114715576\n",
            "Training Iteration 2112, Loss: 4.649323463439941\n",
            "Training Iteration 2113, Loss: 4.330648899078369\n",
            "Training Iteration 2114, Loss: 1.7180171012878418\n",
            "Training Iteration 2115, Loss: 5.4350175857543945\n",
            "Training Iteration 2116, Loss: 5.817475318908691\n",
            "Training Iteration 2117, Loss: 2.7329235076904297\n",
            "Training Iteration 2118, Loss: 6.1465253829956055\n",
            "Training Iteration 2119, Loss: 10.719707489013672\n",
            "Training Iteration 2120, Loss: 11.96604061126709\n",
            "Training Iteration 2121, Loss: 6.231147766113281\n",
            "Training Iteration 2122, Loss: 5.0143327713012695\n",
            "Training Iteration 2123, Loss: 5.5432915687561035\n",
            "Training Iteration 2124, Loss: 4.786065101623535\n",
            "Training Iteration 2125, Loss: 5.150191307067871\n",
            "Training Iteration 2126, Loss: 3.750002145767212\n",
            "Training Iteration 2127, Loss: 5.997617721557617\n",
            "Training Iteration 2128, Loss: 6.139527320861816\n",
            "Training Iteration 2129, Loss: 6.878383636474609\n",
            "Training Iteration 2130, Loss: 2.405519485473633\n",
            "Training Iteration 2131, Loss: 5.589990139007568\n",
            "Training Iteration 2132, Loss: 4.406839370727539\n",
            "Training Iteration 2133, Loss: 5.341466426849365\n",
            "Training Iteration 2134, Loss: 2.451622486114502\n",
            "Training Iteration 2135, Loss: 8.222021102905273\n",
            "Training Iteration 2136, Loss: 3.30381441116333\n",
            "Training Iteration 2137, Loss: 8.656961441040039\n",
            "Training Iteration 2138, Loss: 3.650968313217163\n",
            "Training Iteration 2139, Loss: 3.796781063079834\n",
            "Training Iteration 2140, Loss: 6.252167224884033\n",
            "Training Iteration 2141, Loss: 6.254116058349609\n",
            "Training Iteration 2142, Loss: 7.541897773742676\n",
            "Training Iteration 2143, Loss: 3.2264821529388428\n",
            "Training Iteration 2144, Loss: 2.9537813663482666\n",
            "Training Iteration 2145, Loss: 4.71238899230957\n",
            "Training Iteration 2146, Loss: 5.096399307250977\n",
            "Training Iteration 2147, Loss: 9.674720764160156\n",
            "Training Iteration 2148, Loss: 5.4393463134765625\n",
            "Training Iteration 2149, Loss: 6.123349189758301\n",
            "Training Iteration 2150, Loss: 1.4405012130737305\n",
            "Training Iteration 2151, Loss: 2.6194136142730713\n",
            "Training Iteration 2152, Loss: 5.5000786781311035\n",
            "Training Iteration 2153, Loss: 8.187776565551758\n",
            "Training Iteration 2154, Loss: 4.846663475036621\n",
            "Training Iteration 2155, Loss: 4.326501369476318\n",
            "Training Iteration 2156, Loss: 3.742070198059082\n",
            "Training Iteration 2157, Loss: 4.392597675323486\n",
            "Training Iteration 2158, Loss: 2.9093642234802246\n",
            "Training Iteration 2159, Loss: 3.0925707817077637\n",
            "Training Iteration 2160, Loss: 5.780131816864014\n",
            "Training Iteration 2161, Loss: 2.168736219406128\n",
            "Training Iteration 2162, Loss: 4.081301689147949\n",
            "Training Iteration 2163, Loss: 4.281737804412842\n",
            "Training Iteration 2164, Loss: 3.103975296020508\n",
            "Training Iteration 2165, Loss: 3.8452553749084473\n",
            "Training Iteration 2166, Loss: 2.4793143272399902\n",
            "Training Iteration 2167, Loss: 5.24412202835083\n",
            "Training Iteration 2168, Loss: 4.406642913818359\n",
            "Training Iteration 2169, Loss: 5.195579528808594\n",
            "Training Iteration 2170, Loss: 2.6279823780059814\n",
            "Training Iteration 2171, Loss: 3.9733102321624756\n",
            "Training Iteration 2172, Loss: 4.469935417175293\n",
            "Training Iteration 2173, Loss: 6.502850532531738\n",
            "Training Iteration 2174, Loss: 3.3866782188415527\n",
            "Training Iteration 2175, Loss: 3.129594564437866\n",
            "Training Iteration 2176, Loss: 5.248769760131836\n",
            "Training Iteration 2177, Loss: 6.421022415161133\n",
            "Training Iteration 2178, Loss: 2.9643173217773438\n",
            "Training Iteration 2179, Loss: 3.730072498321533\n",
            "Training Iteration 2180, Loss: 2.2752108573913574\n",
            "Training Iteration 2181, Loss: 3.6501235961914062\n",
            "Training Iteration 2182, Loss: 3.301482677459717\n",
            "Training Iteration 2183, Loss: 4.700139999389648\n",
            "Training Iteration 2184, Loss: 4.751469612121582\n",
            "Training Iteration 2185, Loss: 2.9350790977478027\n",
            "Training Iteration 2186, Loss: 2.498509407043457\n",
            "Training Iteration 2187, Loss: 3.672220468521118\n",
            "Training Iteration 2188, Loss: 3.994683027267456\n",
            "Training Iteration 2189, Loss: 5.168577671051025\n",
            "Training Iteration 2190, Loss: 4.879438877105713\n",
            "Training Iteration 2191, Loss: 5.4725661277771\n",
            "Training Iteration 2192, Loss: 3.4916203022003174\n",
            "Training Iteration 2193, Loss: 4.87241268157959\n",
            "Training Iteration 2194, Loss: 4.8551130294799805\n",
            "Training Iteration 2195, Loss: 3.8702101707458496\n",
            "Training Iteration 2196, Loss: 2.0365853309631348\n",
            "Training Iteration 2197, Loss: 4.273891925811768\n",
            "Training Iteration 2198, Loss: 6.103401184082031\n",
            "Training Iteration 2199, Loss: 5.993304252624512\n",
            "Training Iteration 2200, Loss: 5.099509239196777\n",
            "Training Iteration 2201, Loss: 5.330520153045654\n",
            "Training Iteration 2202, Loss: 5.095423698425293\n",
            "Training Iteration 2203, Loss: 4.2794189453125\n",
            "Training Iteration 2204, Loss: 3.5466222763061523\n",
            "Training Iteration 2205, Loss: 3.038959264755249\n",
            "Training Iteration 2206, Loss: 3.4919373989105225\n",
            "Training Iteration 2207, Loss: 2.782109498977661\n",
            "Training Iteration 2208, Loss: 5.043187618255615\n",
            "Training Iteration 2209, Loss: 5.454851150512695\n",
            "Training Iteration 2210, Loss: 2.347599744796753\n",
            "Training Iteration 2211, Loss: 3.5416016578674316\n",
            "Training Iteration 2212, Loss: 2.7960214614868164\n",
            "Training Iteration 2213, Loss: 3.0371057987213135\n",
            "Training Iteration 2214, Loss: 2.4923839569091797\n",
            "Training Iteration 2215, Loss: 3.954057455062866\n",
            "Training Iteration 2216, Loss: 4.335952281951904\n",
            "Training Iteration 2217, Loss: 5.059024810791016\n",
            "Training Iteration 2218, Loss: 5.214718341827393\n",
            "Training Iteration 2219, Loss: 2.1759841442108154\n",
            "Training Iteration 2220, Loss: 8.650664329528809\n",
            "Training Iteration 2221, Loss: 4.092134475708008\n",
            "Training Iteration 2222, Loss: 4.079101085662842\n",
            "Training Iteration 2223, Loss: 3.4712677001953125\n",
            "Training Iteration 2224, Loss: 3.7445755004882812\n",
            "Training Iteration 2225, Loss: 4.279045581817627\n",
            "Training Iteration 2226, Loss: 5.665398597717285\n",
            "Training Iteration 2227, Loss: 7.678084850311279\n",
            "Training Iteration 2228, Loss: 3.2660491466522217\n",
            "Training Iteration 2229, Loss: 5.32473087310791\n",
            "Training Iteration 2230, Loss: 4.295257568359375\n",
            "Training Iteration 2231, Loss: 4.101170063018799\n",
            "Training Iteration 2232, Loss: 3.9733071327209473\n",
            "Training Iteration 2233, Loss: 4.030241966247559\n",
            "Training Iteration 2234, Loss: 0.8795340657234192\n",
            "Training Iteration 2235, Loss: 3.9310176372528076\n",
            "Training Iteration 2236, Loss: 4.854734420776367\n",
            "Training Iteration 2237, Loss: 7.459078788757324\n",
            "Training Iteration 2238, Loss: 3.1936700344085693\n",
            "Training Iteration 2239, Loss: 2.582641363143921\n",
            "Training Iteration 2240, Loss: 2.6660985946655273\n",
            "Training Iteration 2241, Loss: 3.7266035079956055\n",
            "Training Iteration 2242, Loss: 10.419464111328125\n",
            "Training Iteration 2243, Loss: 7.692232131958008\n",
            "Training Iteration 2244, Loss: 8.070035934448242\n",
            "Training Iteration 2245, Loss: 5.450168132781982\n",
            "Training Iteration 2246, Loss: 2.7951433658599854\n",
            "Training Iteration 2247, Loss: 2.423745632171631\n",
            "Training Iteration 2248, Loss: 4.928208827972412\n",
            "Training Iteration 2249, Loss: 4.421877861022949\n",
            "Training Iteration 2250, Loss: 3.4653491973876953\n",
            "Training Iteration 2251, Loss: 3.459662914276123\n",
            "Training Iteration 2252, Loss: 0.974983274936676\n",
            "Training Iteration 2253, Loss: 7.026254653930664\n",
            "Training Iteration 2254, Loss: 6.371581077575684\n",
            "Training Iteration 2255, Loss: 5.941147327423096\n",
            "Training Iteration 2256, Loss: 6.83535099029541\n",
            "Training Iteration 2257, Loss: 2.9301598072052\n",
            "Training Iteration 2258, Loss: 4.979294776916504\n",
            "Training Iteration 2259, Loss: 3.113905668258667\n",
            "Training Iteration 2260, Loss: 5.320286750793457\n",
            "Training Iteration 2261, Loss: 4.151856899261475\n",
            "Training Iteration 2262, Loss: 4.319087505340576\n",
            "Training Iteration 2263, Loss: 5.416018486022949\n",
            "Training Iteration 2264, Loss: 4.3813796043396\n",
            "Training Iteration 2265, Loss: 4.784475326538086\n",
            "Training Iteration 2266, Loss: 3.1936707496643066\n",
            "Training Iteration 2267, Loss: 3.3444409370422363\n",
            "Training Iteration 2268, Loss: 5.6870880126953125\n",
            "Training Iteration 2269, Loss: 4.594725608825684\n",
            "Training Iteration 2270, Loss: 2.775299310684204\n",
            "Training Iteration 2271, Loss: 4.579601764678955\n",
            "Training Iteration 2272, Loss: 2.677692413330078\n",
            "Training Iteration 2273, Loss: 2.6462371349334717\n",
            "Training Iteration 2274, Loss: 2.7159206867218018\n",
            "Training Iteration 2275, Loss: 4.190567970275879\n",
            "Training Iteration 2276, Loss: 6.564503192901611\n",
            "Training Iteration 2277, Loss: 3.063648223876953\n",
            "Training Iteration 2278, Loss: 3.0894227027893066\n",
            "Training Iteration 2279, Loss: 3.707942485809326\n",
            "Training Iteration 2280, Loss: 3.1654317378997803\n",
            "Training Iteration 2281, Loss: 3.309810161590576\n",
            "Training Iteration 2282, Loss: 4.192619323730469\n",
            "Training Iteration 2283, Loss: 3.2536044120788574\n",
            "Training Iteration 2284, Loss: 3.4301958084106445\n",
            "Training Iteration 2285, Loss: 2.4584920406341553\n",
            "Training Iteration 2286, Loss: 5.8972320556640625\n",
            "Training Iteration 2287, Loss: 4.432967185974121\n",
            "Training Iteration 2288, Loss: 4.558601379394531\n",
            "Training Iteration 2289, Loss: 5.318370819091797\n",
            "Training Iteration 2290, Loss: 4.960141181945801\n",
            "Training Iteration 2291, Loss: 3.506563663482666\n",
            "Training Iteration 2292, Loss: 2.478721857070923\n",
            "Training Iteration 2293, Loss: 5.096724510192871\n",
            "Training Iteration 2294, Loss: 8.428285598754883\n",
            "Training Iteration 2295, Loss: 7.108816623687744\n",
            "Training Iteration 2296, Loss: 4.929695129394531\n",
            "Training Iteration 2297, Loss: 2.309250593185425\n",
            "Training Iteration 2298, Loss: 5.104013919830322\n",
            "Training Iteration 2299, Loss: 4.078577041625977\n",
            "Training Iteration 2300, Loss: 3.9364559650421143\n",
            "Training Iteration 2301, Loss: 4.990052223205566\n",
            "Training Iteration 2302, Loss: 4.675790309906006\n",
            "Training Iteration 2303, Loss: 4.924211025238037\n",
            "Training Iteration 2304, Loss: 6.581155776977539\n",
            "Training Iteration 2305, Loss: 2.5267512798309326\n",
            "Training Iteration 2306, Loss: 5.674261093139648\n",
            "Training Iteration 2307, Loss: 3.9561777114868164\n",
            "Training Iteration 2308, Loss: 3.390915870666504\n",
            "Training Iteration 2309, Loss: 2.3994410037994385\n",
            "Training Iteration 2310, Loss: 6.474693298339844\n",
            "Training Iteration 2311, Loss: 3.4140472412109375\n",
            "Training Iteration 2312, Loss: 4.629608154296875\n",
            "Training Iteration 2313, Loss: 3.2648160457611084\n",
            "Training Iteration 2314, Loss: 3.417285680770874\n",
            "Training Iteration 2315, Loss: 4.243660926818848\n",
            "Training Iteration 2316, Loss: 4.545459270477295\n",
            "Training Iteration 2317, Loss: 2.813185214996338\n",
            "Training Iteration 2318, Loss: 2.6348884105682373\n",
            "Training Iteration 2319, Loss: 2.9478907585144043\n",
            "Training Iteration 2320, Loss: 4.10841178894043\n",
            "Training Iteration 2321, Loss: 11.420768737792969\n",
            "Training Iteration 2322, Loss: 6.712484836578369\n",
            "Training Iteration 2323, Loss: 8.419200897216797\n",
            "Training Iteration 2324, Loss: 6.547496318817139\n",
            "Training Iteration 2325, Loss: 2.2859671115875244\n",
            "Training Iteration 2326, Loss: 3.271937847137451\n",
            "Training Iteration 2327, Loss: 5.023972034454346\n",
            "Training Iteration 2328, Loss: 6.143566131591797\n",
            "Training Iteration 2329, Loss: 4.0377726554870605\n",
            "Training Iteration 2330, Loss: 5.79946756362915\n",
            "Training Iteration 2331, Loss: 3.291748285293579\n",
            "Training Iteration 2332, Loss: 3.887726068496704\n",
            "Training Iteration 2333, Loss: 4.987056732177734\n",
            "Training Iteration 2334, Loss: 2.3695852756500244\n",
            "Training Iteration 2335, Loss: 2.9590649604797363\n",
            "Training Iteration 2336, Loss: 4.495059013366699\n",
            "Training Iteration 2337, Loss: 3.907191038131714\n",
            "Training Iteration 2338, Loss: 7.440145969390869\n",
            "Training Iteration 2339, Loss: 1.5944193601608276\n",
            "Training Iteration 2340, Loss: 5.409368991851807\n",
            "Training Iteration 2341, Loss: 3.748899459838867\n",
            "Training Iteration 2342, Loss: 3.837527275085449\n",
            "Training Iteration 2343, Loss: 6.210271835327148\n",
            "Training Iteration 2344, Loss: 6.534731864929199\n",
            "Training Iteration 2345, Loss: 5.331193923950195\n",
            "Training Iteration 2346, Loss: 4.839366912841797\n",
            "Training Iteration 2347, Loss: 2.3550708293914795\n",
            "Training Iteration 2348, Loss: 5.634522438049316\n",
            "Training Iteration 2349, Loss: 5.434506893157959\n",
            "Training Iteration 2350, Loss: 3.0797476768493652\n",
            "Training Iteration 2351, Loss: 5.157090663909912\n",
            "Training Iteration 2352, Loss: 3.8895225524902344\n",
            "Training Iteration 2353, Loss: 5.9143781661987305\n",
            "Training Iteration 2354, Loss: 2.2725424766540527\n",
            "Training Iteration 2355, Loss: 2.5767693519592285\n",
            "Training Iteration 2356, Loss: 5.350312232971191\n",
            "Training Iteration 2357, Loss: 1.9828578233718872\n",
            "Training Iteration 2358, Loss: 8.970128059387207\n",
            "Training Iteration 2359, Loss: 4.139098167419434\n",
            "Training Iteration 2360, Loss: 0.8850691318511963\n",
            "Training Iteration 2361, Loss: 7.83493185043335\n",
            "Training Iteration 2362, Loss: 3.5517373085021973\n",
            "Training Iteration 2363, Loss: 3.9820871353149414\n",
            "Training Iteration 2364, Loss: 3.3631210327148438\n",
            "Training Iteration 2365, Loss: 4.090975761413574\n",
            "Training Iteration 2366, Loss: 5.970198631286621\n",
            "Training Iteration 2367, Loss: 7.842877388000488\n",
            "Training Iteration 2368, Loss: 7.655882358551025\n",
            "Training Iteration 2369, Loss: 4.72954797744751\n",
            "Training Iteration 2370, Loss: 5.449882507324219\n",
            "Training Iteration 2371, Loss: 4.063943862915039\n",
            "Training Iteration 2372, Loss: 5.49622106552124\n",
            "Training Iteration 2373, Loss: 5.402158737182617\n",
            "Training Iteration 2374, Loss: 5.67824649810791\n",
            "Training Iteration 2375, Loss: 5.074290752410889\n",
            "Training Iteration 2376, Loss: 4.592987537384033\n",
            "Training Iteration 2377, Loss: 3.349976062774658\n",
            "Training Iteration 2378, Loss: 6.920840263366699\n",
            "Training Iteration 2379, Loss: 7.298361778259277\n",
            "Training Iteration 2380, Loss: 3.835219383239746\n",
            "Training Iteration 2381, Loss: 4.453341960906982\n",
            "Training Iteration 2382, Loss: 5.525552749633789\n",
            "Training Iteration 2383, Loss: 5.161112308502197\n",
            "Training Iteration 2384, Loss: 3.6085946559906006\n",
            "Training Iteration 2385, Loss: 7.569511413574219\n",
            "Training Iteration 2386, Loss: 5.030139923095703\n",
            "Training Iteration 2387, Loss: 4.625760078430176\n",
            "Training Iteration 2388, Loss: 4.761622428894043\n",
            "Training Iteration 2389, Loss: 5.581233978271484\n",
            "Training Iteration 2390, Loss: 6.337425231933594\n",
            "Training Iteration 2391, Loss: 3.959456205368042\n",
            "Training Iteration 2392, Loss: 2.6829986572265625\n",
            "Training Iteration 2393, Loss: 6.0115251541137695\n",
            "Training Iteration 2394, Loss: 2.1176962852478027\n",
            "Training Iteration 2395, Loss: 6.547187328338623\n",
            "Training Iteration 2396, Loss: 4.712743282318115\n",
            "Training Iteration 2397, Loss: 4.829437255859375\n",
            "Training Iteration 2398, Loss: 3.4823060035705566\n",
            "Training Iteration 2399, Loss: 3.192513942718506\n",
            "Training Iteration 2400, Loss: 4.847770690917969\n",
            "Training Iteration 2401, Loss: 3.2424445152282715\n",
            "Training Iteration 2402, Loss: 6.133655071258545\n",
            "Training Iteration 2403, Loss: 4.028670310974121\n",
            "Training Iteration 2404, Loss: 3.6888113021850586\n",
            "Training Iteration 2405, Loss: 7.747002601623535\n",
            "Training Iteration 2406, Loss: 3.8105766773223877\n",
            "Training Iteration 2407, Loss: 5.599865436553955\n",
            "Training Iteration 2408, Loss: 3.605729341506958\n",
            "Training Iteration 2409, Loss: 5.384982585906982\n",
            "Training Iteration 2410, Loss: 3.753241777420044\n",
            "Training Iteration 2411, Loss: 6.0460524559021\n",
            "Training Iteration 2412, Loss: 3.075140953063965\n",
            "Training Iteration 2413, Loss: 5.1830573081970215\n",
            "Training Iteration 2414, Loss: 3.687554359436035\n",
            "Training Iteration 2415, Loss: 7.9953083992004395\n",
            "Training Iteration 2416, Loss: 6.622431755065918\n",
            "Training Iteration 2417, Loss: 4.34707498550415\n",
            "Training Iteration 2418, Loss: 8.035422325134277\n",
            "Training Iteration 2419, Loss: 4.43790864944458\n",
            "Training Iteration 2420, Loss: 4.738380432128906\n",
            "Training Iteration 2421, Loss: 4.439746856689453\n",
            "Training Iteration 2422, Loss: 3.218427896499634\n",
            "Training Iteration 2423, Loss: 3.556732416152954\n",
            "Training Iteration 2424, Loss: 1.9438080787658691\n",
            "Training Iteration 2425, Loss: 5.614710807800293\n",
            "Training Iteration 2426, Loss: 6.841936111450195\n",
            "Training Iteration 2427, Loss: 3.6222124099731445\n",
            "Training Iteration 2428, Loss: 3.7894177436828613\n",
            "Training Iteration 2429, Loss: 5.751899719238281\n",
            "Training Iteration 2430, Loss: 4.988857269287109\n",
            "Training Iteration 2431, Loss: 8.870765686035156\n",
            "Training Iteration 2432, Loss: 4.755457878112793\n",
            "Training Iteration 2433, Loss: 8.978909492492676\n",
            "Training Iteration 2434, Loss: 3.4812440872192383\n",
            "Training Iteration 2435, Loss: 4.568499565124512\n",
            "Training Iteration 2436, Loss: 4.753231048583984\n",
            "Training Iteration 2437, Loss: 3.646744728088379\n",
            "Training Iteration 2438, Loss: 3.6717185974121094\n",
            "Training Iteration 2439, Loss: 3.9246714115142822\n",
            "Training Iteration 2440, Loss: 3.5917720794677734\n",
            "Training Iteration 2441, Loss: 4.565994739532471\n",
            "Training Iteration 2442, Loss: 3.8382298946380615\n",
            "Training Iteration 2443, Loss: 4.477281093597412\n",
            "Training Iteration 2444, Loss: 3.904968023300171\n",
            "Training Iteration 2445, Loss: 3.5084376335144043\n",
            "Training Iteration 2446, Loss: 3.468979835510254\n",
            "Training Iteration 2447, Loss: 6.458927154541016\n",
            "Training Iteration 2448, Loss: 3.1125423908233643\n",
            "Training Iteration 2449, Loss: 3.420685291290283\n",
            "Training Iteration 2450, Loss: 5.583906650543213\n",
            "Training Iteration 2451, Loss: 4.207948207855225\n",
            "Training Iteration 2452, Loss: 4.5177321434021\n",
            "Training Iteration 2453, Loss: 3.2618088722229004\n",
            "Training Iteration 2454, Loss: 5.725671768188477\n",
            "Training Iteration 2455, Loss: 5.937749862670898\n",
            "Training Iteration 2456, Loss: 2.4420418739318848\n",
            "Training Iteration 2457, Loss: 2.5216126441955566\n",
            "Training Iteration 2458, Loss: 6.563965797424316\n",
            "Training Iteration 2459, Loss: 2.1501269340515137\n",
            "Training Iteration 2460, Loss: 2.198554515838623\n",
            "Training Iteration 2461, Loss: 5.384079456329346\n",
            "Training Iteration 2462, Loss: 4.943282604217529\n",
            "Training Iteration 2463, Loss: 4.915336608886719\n",
            "Training Iteration 2464, Loss: 5.288364410400391\n",
            "Training Iteration 2465, Loss: 1.9659937620162964\n",
            "Training Iteration 2466, Loss: 3.2681615352630615\n",
            "Training Iteration 2467, Loss: 3.395111083984375\n",
            "Training Iteration 2468, Loss: 2.5453944206237793\n",
            "Training Iteration 2469, Loss: 1.6423661708831787\n",
            "Training Iteration 2470, Loss: 3.0106053352355957\n",
            "Training Iteration 2471, Loss: 6.130382537841797\n",
            "Training Iteration 2472, Loss: 3.6502532958984375\n",
            "Training Iteration 2473, Loss: 2.4170334339141846\n",
            "Training Iteration 2474, Loss: 5.138658046722412\n",
            "Training Iteration 2475, Loss: 5.1586713790893555\n",
            "Training Iteration 2476, Loss: 3.3570215702056885\n",
            "Training Iteration 2477, Loss: 6.050075054168701\n",
            "Training Iteration 2478, Loss: 3.7769534587860107\n",
            "Training Iteration 2479, Loss: 4.772091865539551\n",
            "Training Iteration 2480, Loss: 4.818068981170654\n",
            "Training Iteration 2481, Loss: 2.850621223449707\n",
            "Training Iteration 2482, Loss: 4.7582573890686035\n",
            "Training Iteration 2483, Loss: 6.379127025604248\n",
            "Training Iteration 2484, Loss: 3.88789963722229\n",
            "Training Iteration 2485, Loss: 3.4092535972595215\n",
            "Training Iteration 2486, Loss: 5.124027252197266\n",
            "Training Iteration 2487, Loss: 5.718648910522461\n",
            "Training Iteration 2488, Loss: 2.5813844203948975\n",
            "Training Iteration 2489, Loss: 3.6273200511932373\n",
            "Training Iteration 2490, Loss: 4.321934223175049\n",
            "Training Iteration 2491, Loss: 3.7223408222198486\n",
            "Training Iteration 2492, Loss: 6.341831207275391\n",
            "Training Iteration 2493, Loss: 5.234131336212158\n",
            "Training Iteration 2494, Loss: 3.921802282333374\n",
            "Training Iteration 2495, Loss: 5.792851448059082\n",
            "Training Iteration 2496, Loss: 5.499813079833984\n",
            "Training Iteration 2497, Loss: 3.2309210300445557\n",
            "Training Iteration 2498, Loss: 2.812443733215332\n",
            "Training Iteration 2499, Loss: 4.870112895965576\n",
            "Training Iteration 2500, Loss: 2.2317559719085693\n",
            "Training Iteration 2501, Loss: 2.2348527908325195\n",
            "Training Iteration 2502, Loss: 4.440446853637695\n",
            "Training Iteration 2503, Loss: 6.588869571685791\n",
            "Training Iteration 2504, Loss: 3.5499954223632812\n",
            "Training Iteration 2505, Loss: 9.737082481384277\n",
            "Training Iteration 2506, Loss: 5.89363956451416\n",
            "Training Iteration 2507, Loss: 4.509008884429932\n",
            "Training Iteration 2508, Loss: 4.476362228393555\n",
            "Training Iteration 2509, Loss: 5.210432052612305\n",
            "Training Iteration 2510, Loss: 4.4686431884765625\n",
            "Training Iteration 2511, Loss: 7.103745460510254\n",
            "Training Iteration 2512, Loss: 5.9726243019104\n",
            "Training Iteration 2513, Loss: 3.8234055042266846\n",
            "Training Iteration 2514, Loss: 4.382643699645996\n",
            "Training Iteration 2515, Loss: 4.13590669631958\n",
            "Training Iteration 2516, Loss: 2.4109835624694824\n",
            "Training Iteration 2517, Loss: 2.430067539215088\n",
            "Training Iteration 2518, Loss: 2.9301066398620605\n",
            "Training Iteration 2519, Loss: 4.091039657592773\n",
            "Training Iteration 2520, Loss: 3.450784206390381\n",
            "Training Iteration 2521, Loss: 3.710951089859009\n",
            "Training Iteration 2522, Loss: 4.767423152923584\n",
            "Training Iteration 2523, Loss: 3.5018198490142822\n",
            "Training Iteration 2524, Loss: 5.114110469818115\n",
            "Training Iteration 2525, Loss: 2.652289628982544\n",
            "Training Iteration 2526, Loss: 2.7557458877563477\n",
            "Training Iteration 2527, Loss: 2.7433557510375977\n",
            "Training Iteration 2528, Loss: 5.315466403961182\n",
            "Training Iteration 2529, Loss: 5.931926727294922\n",
            "Training Iteration 2530, Loss: 3.7219223976135254\n",
            "Training Iteration 2531, Loss: 3.002657890319824\n",
            "Training Iteration 2532, Loss: 6.32703161239624\n",
            "Training Iteration 2533, Loss: 5.7231645584106445\n",
            "Training Iteration 2534, Loss: 4.59627103805542\n",
            "Training Iteration 2535, Loss: 3.6035728454589844\n",
            "Training Iteration 2536, Loss: 2.127246379852295\n",
            "Training Iteration 2537, Loss: 3.1659045219421387\n",
            "Training Iteration 2538, Loss: 4.414144039154053\n",
            "Training Iteration 2539, Loss: 3.387545347213745\n",
            "Training Iteration 2540, Loss: 2.842414617538452\n",
            "Training Iteration 2541, Loss: 3.3742337226867676\n",
            "Training Iteration 2542, Loss: 3.285902500152588\n",
            "Training Iteration 2543, Loss: 2.6359574794769287\n",
            "Training Iteration 2544, Loss: 3.729205369949341\n",
            "Training Iteration 2545, Loss: 6.654727935791016\n",
            "Training Iteration 2546, Loss: 5.66094970703125\n",
            "Training Iteration 2547, Loss: 3.2885513305664062\n",
            "Training Iteration 2548, Loss: 4.042464256286621\n",
            "Training Iteration 2549, Loss: 5.688092231750488\n",
            "Training Iteration 2550, Loss: 3.504828453063965\n",
            "Training Iteration 2551, Loss: 4.522799015045166\n",
            "Training Iteration 2552, Loss: 9.454206466674805\n",
            "Training Iteration 2553, Loss: 5.565086364746094\n",
            "Training Iteration 2554, Loss: 9.684450149536133\n",
            "Training Iteration 2555, Loss: 6.651123046875\n",
            "Training Iteration 2556, Loss: 4.664071559906006\n",
            "Training Iteration 2557, Loss: 6.25570011138916\n",
            "Training Iteration 2558, Loss: 3.9507462978363037\n",
            "Training Iteration 2559, Loss: 5.2179341316223145\n",
            "Training Iteration 2560, Loss: 3.219184398651123\n",
            "Training Iteration 2561, Loss: 5.802828311920166\n",
            "Training Iteration 2562, Loss: 4.326023101806641\n",
            "Training Iteration 2563, Loss: 6.42837381362915\n",
            "Training Iteration 2564, Loss: 5.7567033767700195\n",
            "Training Iteration 2565, Loss: 3.752829074859619\n",
            "Training Iteration 2566, Loss: 2.357422113418579\n",
            "Training Iteration 2567, Loss: 3.036228656768799\n",
            "Training Iteration 2568, Loss: 4.623169898986816\n",
            "Training Iteration 2569, Loss: 2.0640547275543213\n",
            "Training Iteration 2570, Loss: 6.025050163269043\n",
            "Training Iteration 2571, Loss: 3.8904049396514893\n",
            "Training Iteration 2572, Loss: 2.0314667224884033\n",
            "Training Iteration 2573, Loss: 7.281238079071045\n",
            "Training Iteration 2574, Loss: 3.2734534740448\n",
            "Training Iteration 2575, Loss: 1.0791230201721191\n",
            "Training Iteration 2576, Loss: 7.650768280029297\n",
            "Training Iteration 2577, Loss: 2.977044105529785\n",
            "Training Iteration 2578, Loss: 5.317194938659668\n",
            "Training Iteration 2579, Loss: 5.342174053192139\n",
            "Training Iteration 2580, Loss: 3.378190040588379\n",
            "Training Iteration 2581, Loss: 5.40714168548584\n",
            "Training Iteration 2582, Loss: 3.2556705474853516\n",
            "Training Iteration 2583, Loss: 2.2965447902679443\n",
            "Training Iteration 2584, Loss: 6.579778671264648\n",
            "Training Iteration 2585, Loss: 4.074394226074219\n",
            "Training Iteration 2586, Loss: 3.472966194152832\n",
            "Training Iteration 2587, Loss: 1.8412129878997803\n",
            "Training Iteration 2588, Loss: 3.9837350845336914\n",
            "Training Iteration 2589, Loss: 5.143313884735107\n",
            "Training Iteration 2590, Loss: 7.7114739418029785\n",
            "Training Iteration 2591, Loss: 1.8757202625274658\n",
            "Training Iteration 2592, Loss: 7.045224666595459\n",
            "Training Iteration 2593, Loss: 5.055189609527588\n",
            "Training Iteration 2594, Loss: 3.103266954421997\n",
            "Training Iteration 2595, Loss: 4.948401927947998\n",
            "Training Iteration 2596, Loss: 3.6358611583709717\n",
            "Training Iteration 2597, Loss: 4.361252784729004\n",
            "Training Iteration 2598, Loss: 3.0757126808166504\n",
            "Training Iteration 2599, Loss: 4.836198806762695\n",
            "Training Iteration 2600, Loss: 3.8582849502563477\n",
            "Training Iteration 2601, Loss: 2.357757806777954\n",
            "Training Iteration 2602, Loss: 5.041438579559326\n",
            "Training Iteration 2603, Loss: 4.317896842956543\n",
            "Training Iteration 2604, Loss: 4.635942459106445\n",
            "Training Iteration 2605, Loss: 5.5312018394470215\n",
            "Training Iteration 2606, Loss: 3.2300326824188232\n",
            "Training Iteration 2607, Loss: 7.255749225616455\n",
            "Training Iteration 2608, Loss: 3.304518461227417\n",
            "Training Iteration 2609, Loss: 1.9857732057571411\n",
            "Training Iteration 2610, Loss: 2.528564214706421\n",
            "Training Iteration 2611, Loss: 5.90336275100708\n",
            "Training Iteration 2612, Loss: 4.973273754119873\n",
            "Training Iteration 2613, Loss: 2.0050244331359863\n",
            "Training Iteration 2614, Loss: 3.3148062229156494\n",
            "Training Iteration 2615, Loss: 3.747767210006714\n",
            "Training Iteration 2616, Loss: 8.083649635314941\n",
            "Training Iteration 2617, Loss: 2.359877109527588\n",
            "Training Iteration 2618, Loss: 4.806145191192627\n",
            "Training Iteration 2619, Loss: 3.6879448890686035\n",
            "Training Iteration 2620, Loss: 4.664274215698242\n",
            "Training Iteration 2621, Loss: 3.8347907066345215\n",
            "Training Iteration 2622, Loss: 2.338701009750366\n",
            "Training Iteration 2623, Loss: 4.151378631591797\n",
            "Training Iteration 2624, Loss: 2.0943210124969482\n",
            "Training Iteration 2625, Loss: 4.939833164215088\n",
            "Training Iteration 2626, Loss: 7.266684055328369\n",
            "Training Iteration 2627, Loss: 3.5899035930633545\n",
            "Training Iteration 2628, Loss: 3.559922218322754\n",
            "Training Iteration 2629, Loss: 4.408090591430664\n",
            "Training Iteration 2630, Loss: 2.494196653366089\n",
            "Training Iteration 2631, Loss: 3.9631736278533936\n",
            "Training Iteration 2632, Loss: 4.413777828216553\n",
            "Training Iteration 2633, Loss: 3.3526387214660645\n",
            "Training Iteration 2634, Loss: 3.1930465698242188\n",
            "Training Iteration 2635, Loss: 3.2127761840820312\n",
            "Training Iteration 2636, Loss: 2.011500597000122\n",
            "Training Iteration 2637, Loss: 2.041623592376709\n",
            "Training Iteration 2638, Loss: 2.467461585998535\n",
            "Training Iteration 2639, Loss: 5.646956920623779\n",
            "Training Iteration 2640, Loss: 4.00766658782959\n",
            "Training Iteration 2641, Loss: 3.0846376419067383\n",
            "Training Iteration 2642, Loss: 3.449892997741699\n",
            "Training Iteration 2643, Loss: 2.9401917457580566\n",
            "Training Iteration 2644, Loss: 5.407040596008301\n",
            "Training Iteration 2645, Loss: 4.38916015625\n",
            "Training Iteration 2646, Loss: 3.162254571914673\n",
            "Training Iteration 2647, Loss: 4.187671184539795\n",
            "Training Iteration 2648, Loss: 5.1425957679748535\n",
            "Training Iteration 2649, Loss: 1.1954761743545532\n",
            "Training Iteration 2650, Loss: 5.140418529510498\n",
            "Training Iteration 2651, Loss: 5.903518199920654\n",
            "Training Iteration 2652, Loss: 5.4626359939575195\n",
            "Training Iteration 2653, Loss: 3.1046106815338135\n",
            "Training Iteration 2654, Loss: 7.522378921508789\n",
            "Training Iteration 2655, Loss: 4.223374366760254\n",
            "Training Iteration 2656, Loss: 5.122489929199219\n",
            "Training Iteration 2657, Loss: 3.99786114692688\n",
            "Training Iteration 2658, Loss: 5.027729511260986\n",
            "Training Iteration 2659, Loss: 4.004667282104492\n",
            "Training Iteration 2660, Loss: 5.698721885681152\n",
            "Training Iteration 2661, Loss: 5.291189193725586\n",
            "Training Iteration 2662, Loss: 5.002629280090332\n",
            "Training Iteration 2663, Loss: 4.094553470611572\n",
            "Training Iteration 2664, Loss: 2.7489676475524902\n",
            "Training Iteration 2665, Loss: 2.7824716567993164\n",
            "Training Iteration 2666, Loss: 3.8722763061523438\n",
            "Training Iteration 2667, Loss: 3.630612373352051\n",
            "Training Iteration 2668, Loss: 2.6635704040527344\n",
            "Training Iteration 2669, Loss: 4.296492099761963\n",
            "Training Iteration 2670, Loss: 3.785576581954956\n",
            "Training Iteration 2671, Loss: 3.4765586853027344\n",
            "Training Iteration 2672, Loss: 5.045666694641113\n",
            "Training Iteration 2673, Loss: 8.393635749816895\n",
            "Training Iteration 2674, Loss: 6.305530548095703\n",
            "Training Iteration 2675, Loss: 3.7674264907836914\n",
            "Training Iteration 2676, Loss: 3.3268826007843018\n",
            "Training Iteration 2677, Loss: 4.554741382598877\n",
            "Training Iteration 2678, Loss: 3.4719250202178955\n",
            "Training Iteration 2679, Loss: 5.688215255737305\n",
            "Training Iteration 2680, Loss: 5.951140880584717\n",
            "Training Iteration 2681, Loss: 9.237593650817871\n",
            "Training Iteration 2682, Loss: 4.605961799621582\n",
            "Training Iteration 2683, Loss: 3.249927282333374\n",
            "Training Iteration 2684, Loss: 5.946000099182129\n",
            "Training Iteration 2685, Loss: 3.8172569274902344\n",
            "Training Iteration 2686, Loss: 2.829228639602661\n",
            "Training Iteration 2687, Loss: 6.093111038208008\n",
            "Training Iteration 2688, Loss: 6.61077880859375\n",
            "Training Iteration 2689, Loss: 3.821713924407959\n",
            "Training Iteration 2690, Loss: 3.313886880874634\n",
            "Training Iteration 2691, Loss: 6.466597080230713\n",
            "Training Iteration 2692, Loss: 7.106975555419922\n",
            "Training Iteration 2693, Loss: 3.877319097518921\n",
            "Training Iteration 2694, Loss: 5.960376262664795\n",
            "Training Iteration 2695, Loss: 3.216020107269287\n",
            "Training Iteration 2696, Loss: 3.8724348545074463\n",
            "Training Iteration 2697, Loss: 4.933493614196777\n",
            "Training Iteration 2698, Loss: 2.4251420497894287\n",
            "Training Iteration 2699, Loss: 3.64174222946167\n",
            "Training Iteration 2700, Loss: 3.430833339691162\n",
            "Training Iteration 2701, Loss: 4.375697612762451\n",
            "Training Iteration 2702, Loss: 3.608163595199585\n",
            "Training Iteration 2703, Loss: 4.705574989318848\n",
            "Training Iteration 2704, Loss: 4.886163711547852\n",
            "Training Iteration 2705, Loss: 3.0447449684143066\n",
            "Training Iteration 2706, Loss: 2.3602514266967773\n",
            "Training Iteration 2707, Loss: 4.61920166015625\n",
            "Training Iteration 2708, Loss: 3.2929961681365967\n",
            "Training Iteration 2709, Loss: 4.4977521896362305\n",
            "Training Iteration 2710, Loss: 3.229722499847412\n",
            "Training Iteration 2711, Loss: 1.328162431716919\n",
            "Training Iteration 2712, Loss: 4.4598493576049805\n",
            "Training Iteration 2713, Loss: 3.5121688842773438\n",
            "Training Iteration 2714, Loss: 4.980734348297119\n",
            "Training Iteration 2715, Loss: 4.469757556915283\n",
            "Training Iteration 2716, Loss: 2.935819149017334\n",
            "Training Iteration 2717, Loss: 5.2442216873168945\n",
            "Training Iteration 2718, Loss: 3.846628427505493\n",
            "Training Iteration 2719, Loss: 3.0545527935028076\n",
            "Training Iteration 2720, Loss: 5.932642936706543\n",
            "Training Iteration 2721, Loss: 5.771634578704834\n",
            "Training Iteration 2722, Loss: 6.104856967926025\n",
            "Training Iteration 2723, Loss: 6.971414089202881\n",
            "Training Iteration 2724, Loss: 5.439357757568359\n",
            "Training Iteration 2725, Loss: 4.344234466552734\n",
            "Training Iteration 2726, Loss: 3.845057487487793\n",
            "Training Iteration 2727, Loss: 5.053126335144043\n",
            "Training Iteration 2728, Loss: 5.640536308288574\n",
            "Training Iteration 2729, Loss: 3.33855938911438\n",
            "Training Iteration 2730, Loss: 4.129863262176514\n",
            "Training Iteration 2731, Loss: 4.335165500640869\n",
            "Training Iteration 2732, Loss: 3.4885590076446533\n",
            "Training Iteration 2733, Loss: 3.8462774753570557\n",
            "Training Iteration 2734, Loss: 4.681016445159912\n",
            "Training Iteration 2735, Loss: 3.369295120239258\n",
            "Training Iteration 2736, Loss: 4.016064643859863\n",
            "Training Iteration 2737, Loss: 3.6693098545074463\n",
            "Training Iteration 2738, Loss: 6.271949291229248\n",
            "Training Iteration 2739, Loss: 4.551899433135986\n",
            "Training Iteration 2740, Loss: 3.057720184326172\n",
            "Training Iteration 2741, Loss: 1.7336175441741943\n",
            "Training Iteration 2742, Loss: 2.5607457160949707\n",
            "Training Iteration 2743, Loss: 1.7877944707870483\n",
            "Training Iteration 2744, Loss: 2.037141799926758\n",
            "Training Iteration 2745, Loss: 4.024105548858643\n",
            "Training Iteration 2746, Loss: 6.378246307373047\n",
            "Training Iteration 2747, Loss: 4.676412582397461\n",
            "Training Iteration 2748, Loss: 5.239119052886963\n",
            "Training Iteration 2749, Loss: 6.2960686683654785\n",
            "Training Iteration 2750, Loss: 5.233904838562012\n",
            "Training Iteration 2751, Loss: 3.1607139110565186\n",
            "Training Iteration 2752, Loss: 7.663188934326172\n",
            "Training Iteration 2753, Loss: 6.3391618728637695\n",
            "Training Iteration 2754, Loss: 3.1818349361419678\n",
            "Training Iteration 2755, Loss: 1.8052517175674438\n",
            "Training Iteration 2756, Loss: 4.810211658477783\n",
            "Training Iteration 2757, Loss: 5.623222351074219\n",
            "Training Iteration 2758, Loss: 4.047689437866211\n",
            "Training Iteration 2759, Loss: 6.154868125915527\n",
            "Training Iteration 2760, Loss: 6.591300964355469\n",
            "Training Iteration 2761, Loss: 3.333706855773926\n",
            "Training Iteration 2762, Loss: 3.8111863136291504\n",
            "Training Iteration 2763, Loss: 3.6558356285095215\n",
            "Training Iteration 2764, Loss: 4.030735969543457\n",
            "Training Iteration 2765, Loss: 4.121913433074951\n",
            "Training Iteration 2766, Loss: 3.366499900817871\n",
            "Training Iteration 2767, Loss: 1.7533762454986572\n",
            "Training Iteration 2768, Loss: 5.876349925994873\n",
            "Training Iteration 2769, Loss: 4.720639228820801\n",
            "Training Iteration 2770, Loss: 2.0421457290649414\n",
            "Training Iteration 2771, Loss: 4.368037700653076\n",
            "Training Iteration 2772, Loss: 3.7927160263061523\n",
            "Training Iteration 2773, Loss: 3.177947998046875\n",
            "Training Iteration 2774, Loss: 5.386564254760742\n",
            "Training Iteration 2775, Loss: 6.807947635650635\n",
            "Training Iteration 2776, Loss: 3.033060073852539\n",
            "Training Iteration 2777, Loss: 6.481264114379883\n",
            "Training Iteration 2778, Loss: 3.071506977081299\n",
            "Training Iteration 2779, Loss: 4.68946647644043\n",
            "Training Iteration 2780, Loss: 3.7128241062164307\n",
            "Training Iteration 2781, Loss: 3.9314229488372803\n",
            "Training Iteration 2782, Loss: 3.463693618774414\n",
            "Training Iteration 2783, Loss: 6.060511589050293\n",
            "Training Iteration 2784, Loss: 4.818598747253418\n",
            "Training Iteration 2785, Loss: 4.535984516143799\n",
            "Training Iteration 2786, Loss: 4.035033702850342\n",
            "Training Iteration 2787, Loss: 4.484572410583496\n",
            "Training Iteration 2788, Loss: 4.2017669677734375\n",
            "Training Iteration 2789, Loss: 6.358137130737305\n",
            "Training Iteration 2790, Loss: 3.748137950897217\n",
            "Training Iteration 2791, Loss: 2.5974173545837402\n",
            "Training Iteration 2792, Loss: 2.9102511405944824\n",
            "Training Iteration 2793, Loss: 3.5333755016326904\n",
            "Training Iteration 2794, Loss: 2.6088385581970215\n",
            "Training Iteration 2795, Loss: 1.745429277420044\n",
            "Training Iteration 2796, Loss: 6.281802177429199\n",
            "Training Iteration 2797, Loss: 2.2968122959136963\n",
            "Training Iteration 2798, Loss: 2.633331775665283\n",
            "Training Iteration 2799, Loss: 1.7075443267822266\n",
            "Training Iteration 2800, Loss: 4.232013702392578\n",
            "Training Iteration 2801, Loss: 5.007699489593506\n",
            "Training Iteration 2802, Loss: 6.663732051849365\n",
            "Training Iteration 2803, Loss: 3.6060807704925537\n",
            "Training Iteration 2804, Loss: 2.923901319503784\n",
            "Training Iteration 2805, Loss: 2.866024971008301\n",
            "Training Iteration 2806, Loss: 3.241083860397339\n",
            "Training Iteration 2807, Loss: 6.279838562011719\n",
            "Training Iteration 2808, Loss: 5.894094467163086\n",
            "Training Iteration 2809, Loss: 2.5903236865997314\n",
            "Training Iteration 2810, Loss: 2.010514974594116\n",
            "Training Iteration 2811, Loss: 5.984490871429443\n",
            "Training Iteration 2812, Loss: 5.467684745788574\n",
            "Training Iteration 2813, Loss: 4.9984893798828125\n",
            "Training Iteration 2814, Loss: 4.629892349243164\n",
            "Training Iteration 2815, Loss: 3.756873369216919\n",
            "Training Iteration 2816, Loss: 6.312478065490723\n",
            "Training Iteration 2817, Loss: 4.804318904876709\n",
            "Training Iteration 2818, Loss: 2.657727003097534\n",
            "Training Iteration 2819, Loss: 6.820732116699219\n",
            "Training Iteration 2820, Loss: 1.496995210647583\n",
            "Training Iteration 2821, Loss: 4.4566850662231445\n",
            "Training Iteration 2822, Loss: 2.719446897506714\n",
            "Training Iteration 2823, Loss: 2.7818546295166016\n",
            "Training Iteration 2824, Loss: 4.774341583251953\n",
            "Training Iteration 2825, Loss: 6.710585594177246\n",
            "Training Iteration 2826, Loss: 5.545823574066162\n",
            "Training Iteration 2827, Loss: 2.8834807872772217\n",
            "Training Iteration 2828, Loss: 7.758241653442383\n",
            "Training Iteration 2829, Loss: 2.6651270389556885\n",
            "Training Iteration 2830, Loss: 3.2358741760253906\n",
            "Training Iteration 2831, Loss: 4.330422401428223\n",
            "Training Iteration 2832, Loss: 5.140691757202148\n",
            "Training Iteration 2833, Loss: 2.59985089302063\n",
            "Training Iteration 2834, Loss: 4.694958209991455\n",
            "Training Iteration 2835, Loss: 8.018912315368652\n",
            "Training Iteration 2836, Loss: 4.117837905883789\n",
            "Training Iteration 2837, Loss: 7.737464904785156\n",
            "Training Iteration 2838, Loss: 8.461358070373535\n",
            "Training Iteration 2839, Loss: 4.362811088562012\n",
            "Training Iteration 2840, Loss: 3.479990005493164\n",
            "Training Iteration 2841, Loss: 3.9792914390563965\n",
            "Training Iteration 2842, Loss: 7.179497241973877\n",
            "Training Iteration 2843, Loss: 7.856077194213867\n",
            "Training Iteration 2844, Loss: 2.362712860107422\n",
            "Training Iteration 2845, Loss: 6.4897637367248535\n",
            "Training Iteration 2846, Loss: 5.334789752960205\n",
            "Training Iteration 2847, Loss: 2.890357494354248\n",
            "Training Iteration 2848, Loss: 2.25791072845459\n",
            "Training Iteration 2849, Loss: 4.29924201965332\n",
            "Training Iteration 2850, Loss: 5.243495941162109\n",
            "Training Iteration 2851, Loss: 4.4665327072143555\n",
            "Training Iteration 2852, Loss: 3.346108913421631\n",
            "Training Iteration 2853, Loss: 4.499370574951172\n",
            "Training Iteration 2854, Loss: 2.6317803859710693\n",
            "Training Iteration 2855, Loss: 4.178164958953857\n",
            "Training Iteration 2856, Loss: 3.1706247329711914\n",
            "Training Iteration 2857, Loss: 4.108236312866211\n",
            "Training Iteration 2858, Loss: 5.120845317840576\n",
            "Training Iteration 2859, Loss: 3.613328456878662\n",
            "Training Iteration 2860, Loss: 6.061356067657471\n",
            "Training Iteration 2861, Loss: 3.591726779937744\n",
            "Training Iteration 2862, Loss: 4.711180210113525\n",
            "Training Iteration 2863, Loss: 8.035958290100098\n",
            "Training Iteration 2864, Loss: 9.11840534210205\n",
            "Training Iteration 2865, Loss: 1.7345112562179565\n",
            "Training Iteration 2866, Loss: 10.186016082763672\n",
            "Training Iteration 2867, Loss: 5.281217575073242\n",
            "Training Iteration 2868, Loss: 2.731621026992798\n",
            "Training Iteration 2869, Loss: 6.672823905944824\n",
            "Training Iteration 2870, Loss: 4.292456150054932\n",
            "Training Iteration 2871, Loss: 4.562671661376953\n",
            "Training Iteration 2872, Loss: 4.4725141525268555\n",
            "Training Iteration 2873, Loss: 4.479955673217773\n",
            "Training Iteration 2874, Loss: 4.844349384307861\n",
            "Training Iteration 2875, Loss: 6.263949871063232\n",
            "Training Iteration 2876, Loss: 4.435690879821777\n",
            "Training Iteration 2877, Loss: 4.734079360961914\n",
            "Training Iteration 2878, Loss: 3.8969602584838867\n",
            "Training Iteration 2879, Loss: 3.546048641204834\n",
            "Training Iteration 2880, Loss: 1.4022818803787231\n",
            "Training Iteration 2881, Loss: 3.7402591705322266\n",
            "Training Iteration 2882, Loss: 4.829536437988281\n",
            "Training Iteration 2883, Loss: 4.825443744659424\n",
            "Training Iteration 2884, Loss: 5.476722717285156\n",
            "Training Iteration 2885, Loss: 3.8731746673583984\n",
            "Training Iteration 2886, Loss: 2.8933653831481934\n",
            "Training Iteration 2887, Loss: 8.245728492736816\n",
            "Training Iteration 2888, Loss: 2.3708372116088867\n",
            "Training Iteration 2889, Loss: 2.916653633117676\n",
            "Training Iteration 2890, Loss: 5.767960548400879\n",
            "Training Iteration 2891, Loss: 3.9078173637390137\n",
            "Training Iteration 2892, Loss: 2.9755539894104004\n",
            "Training Iteration 2893, Loss: 1.8252922296524048\n",
            "Training Iteration 2894, Loss: 3.5134034156799316\n",
            "Training Iteration 2895, Loss: 4.5839924812316895\n",
            "Training Iteration 2896, Loss: 3.4418489933013916\n",
            "Training Iteration 2897, Loss: 6.344778060913086\n",
            "Training Iteration 2898, Loss: 3.4266152381896973\n",
            "Training Iteration 2899, Loss: 3.2412352561950684\n",
            "Training Iteration 2900, Loss: 4.332729339599609\n",
            "Training Iteration 2901, Loss: 5.182039737701416\n",
            "Training Iteration 2902, Loss: 4.570588111877441\n",
            "Training Iteration 2903, Loss: 4.600015163421631\n",
            "Training Iteration 2904, Loss: 2.5574581623077393\n",
            "Training Iteration 2905, Loss: 4.071605682373047\n",
            "Training Iteration 2906, Loss: 2.3389675617218018\n",
            "Training Iteration 2907, Loss: 2.1182425022125244\n",
            "Training Iteration 2908, Loss: 4.6549177169799805\n",
            "Training Iteration 2909, Loss: 3.7661681175231934\n",
            "Training Iteration 2910, Loss: 4.91991662979126\n",
            "Training Iteration 2911, Loss: 4.204957962036133\n",
            "Training Iteration 2912, Loss: 2.971914291381836\n",
            "Training Iteration 2913, Loss: 5.273350715637207\n",
            "Training Iteration 2914, Loss: 9.300152778625488\n",
            "Training Iteration 2915, Loss: 7.671361923217773\n",
            "Training Iteration 2916, Loss: 5.8505024909973145\n",
            "Training Iteration 2917, Loss: 4.588077545166016\n",
            "Training Iteration 2918, Loss: 3.419642925262451\n",
            "Training Iteration 2919, Loss: 7.998408794403076\n",
            "Training Iteration 2920, Loss: 4.28756046295166\n",
            "Training Iteration 2921, Loss: 6.666908264160156\n",
            "Training Iteration 2922, Loss: 3.917741060256958\n",
            "Training Iteration 2923, Loss: 2.9377689361572266\n",
            "Training Iteration 2924, Loss: 4.100412845611572\n",
            "Training Iteration 2925, Loss: 5.008932590484619\n",
            "Training Iteration 2926, Loss: 4.843667984008789\n",
            "Training Iteration 2927, Loss: 2.4726476669311523\n",
            "Training Iteration 2928, Loss: 4.012652397155762\n",
            "Training Iteration 2929, Loss: 7.014479637145996\n",
            "Training Iteration 2930, Loss: 4.018489837646484\n",
            "Training Iteration 2931, Loss: 1.7787481546401978\n",
            "Training Iteration 2932, Loss: 3.7694554328918457\n",
            "Training Iteration 2933, Loss: 2.0648887157440186\n",
            "Training Iteration 2934, Loss: 3.819514751434326\n",
            "Training Iteration 2935, Loss: 2.4730818271636963\n",
            "Training Iteration 2936, Loss: 3.223822832107544\n",
            "Training Iteration 2937, Loss: 9.553049087524414\n",
            "Training Iteration 2938, Loss: 6.363279819488525\n",
            "Training Iteration 2939, Loss: 6.651728630065918\n",
            "Training Iteration 2940, Loss: 3.98323917388916\n",
            "Training Iteration 2941, Loss: 8.007912635803223\n",
            "Training Iteration 2942, Loss: 6.135044097900391\n",
            "Training Iteration 2943, Loss: 3.3138387203216553\n",
            "Training Iteration 2944, Loss: 3.354182004928589\n",
            "Training Iteration 2945, Loss: 7.155722141265869\n",
            "Training Iteration 2946, Loss: 3.891589403152466\n",
            "Training Iteration 2947, Loss: 3.8633534908294678\n",
            "Training Iteration 2948, Loss: 10.84994888305664\n",
            "Training Iteration 2949, Loss: 4.344361305236816\n",
            "Training Iteration 2950, Loss: 7.692514419555664\n",
            "Training Iteration 2951, Loss: 7.249205589294434\n",
            "Training Iteration 2952, Loss: 1.8827083110809326\n",
            "Training Iteration 2953, Loss: 7.418603897094727\n",
            "Training Iteration 2954, Loss: 3.192016839981079\n",
            "Training Iteration 2955, Loss: 4.497677326202393\n",
            "Training Iteration 2956, Loss: 5.232565879821777\n",
            "Training Iteration 2957, Loss: 5.141503810882568\n",
            "Training Iteration 2958, Loss: 6.892665863037109\n",
            "Training Iteration 2959, Loss: 5.243453502655029\n",
            "Training Iteration 2960, Loss: 5.9488420486450195\n",
            "Training Iteration 2961, Loss: 3.9988253116607666\n",
            "Training Iteration 2962, Loss: 3.739046335220337\n",
            "Training Iteration 2963, Loss: 2.7507097721099854\n",
            "Training Iteration 2964, Loss: 5.712193965911865\n",
            "Training Iteration 2965, Loss: 9.852116584777832\n",
            "Training Iteration 2966, Loss: 4.098484992980957\n",
            "Training Iteration 2967, Loss: 6.504202365875244\n",
            "Training Iteration 2968, Loss: 4.4000325202941895\n",
            "Training Iteration 2969, Loss: 4.469869136810303\n",
            "Training Iteration 2970, Loss: 4.590887069702148\n",
            "Training Iteration 2971, Loss: 6.071460247039795\n",
            "Training Iteration 2972, Loss: 3.0126845836639404\n",
            "Training Iteration 2973, Loss: 4.399847984313965\n",
            "Training Iteration 2974, Loss: 2.9392757415771484\n",
            "Training Iteration 2975, Loss: 6.211062908172607\n",
            "Training Iteration 2976, Loss: 1.9247887134552002\n",
            "Training Iteration 2977, Loss: 2.3715720176696777\n",
            "Training Iteration 2978, Loss: 5.249499320983887\n",
            "Training Iteration 2979, Loss: 6.890923023223877\n",
            "Training Iteration 2980, Loss: 3.5079989433288574\n",
            "Training Iteration 2981, Loss: 5.808817386627197\n",
            "Training Iteration 2982, Loss: 6.126449108123779\n",
            "Training Iteration 2983, Loss: 3.122407913208008\n",
            "Training Iteration 2984, Loss: 3.1182150840759277\n",
            "Training Iteration 2985, Loss: 4.285980224609375\n",
            "Training Iteration 2986, Loss: 9.458531379699707\n",
            "Training Iteration 2987, Loss: 3.1454989910125732\n",
            "Training Iteration 2988, Loss: 1.203601598739624\n",
            "Training Iteration 2989, Loss: 3.3647568225860596\n",
            "Training Iteration 2990, Loss: 2.617616891860962\n",
            "Training Iteration 2991, Loss: 5.750441551208496\n",
            "Training Iteration 2992, Loss: 4.780945301055908\n",
            "Training Iteration 2993, Loss: 4.5680413246154785\n",
            "Training Iteration 2994, Loss: 4.118225574493408\n",
            "Training Iteration 2995, Loss: 4.559649467468262\n",
            "Training Iteration 2996, Loss: 7.247649669647217\n",
            "Training Iteration 2997, Loss: 4.3643317222595215\n",
            "Training Iteration 2998, Loss: 3.933915615081787\n",
            "Training Iteration 2999, Loss: 3.9143197536468506\n",
            "Training Iteration 3000, Loss: 6.272800445556641\n",
            "Training Iteration 3001, Loss: 5.3594865798950195\n",
            "Training Iteration 3002, Loss: 1.910400152206421\n",
            "Training Iteration 3003, Loss: 4.734312057495117\n",
            "Training Iteration 3004, Loss: 6.195559024810791\n",
            "Training Iteration 3005, Loss: 5.209125518798828\n",
            "Training Iteration 3006, Loss: 7.877481937408447\n",
            "Training Iteration 3007, Loss: 5.944822311401367\n",
            "Training Iteration 3008, Loss: 3.856875419616699\n",
            "Training Iteration 3009, Loss: 5.202859401702881\n",
            "Training Iteration 3010, Loss: 9.749337196350098\n",
            "Training Iteration 3011, Loss: 4.244450569152832\n",
            "Training Iteration 3012, Loss: 5.271087169647217\n",
            "Training Iteration 3013, Loss: 7.801978588104248\n",
            "Training Iteration 3014, Loss: 3.8186724185943604\n",
            "Training Iteration 3015, Loss: 3.6491103172302246\n",
            "Training Iteration 3016, Loss: 7.182358741760254\n",
            "Training Iteration 3017, Loss: 3.4305989742279053\n",
            "Training Iteration 3018, Loss: 4.096523761749268\n",
            "Training Iteration 3019, Loss: 7.693151950836182\n",
            "Training Iteration 3020, Loss: 6.183175563812256\n",
            "Training Iteration 3021, Loss: 6.375401020050049\n",
            "Training Iteration 3022, Loss: 4.7183122634887695\n",
            "Training Iteration 3023, Loss: 3.6869876384735107\n",
            "Training Iteration 3024, Loss: 3.1947133541107178\n",
            "Training Iteration 3025, Loss: 4.081446647644043\n",
            "Training Iteration 3026, Loss: 5.754834175109863\n",
            "Training Iteration 3027, Loss: 4.160429954528809\n",
            "Training Iteration 3028, Loss: 7.513111114501953\n",
            "Training Iteration 3029, Loss: 9.764077186584473\n",
            "Training Iteration 3030, Loss: 9.0262451171875\n",
            "Training Iteration 3031, Loss: 3.9317126274108887\n",
            "Training Iteration 3032, Loss: 2.9450578689575195\n",
            "Training Iteration 3033, Loss: 4.834704399108887\n",
            "Training Iteration 3034, Loss: 5.250219345092773\n",
            "Training Iteration 3035, Loss: 5.522838115692139\n",
            "Training Iteration 3036, Loss: 7.240945339202881\n",
            "Training Iteration 3037, Loss: 3.194157123565674\n",
            "Training Iteration 3038, Loss: 1.383992314338684\n",
            "Training Iteration 3039, Loss: 7.680995941162109\n",
            "Training Iteration 3040, Loss: 3.537114143371582\n",
            "Training Iteration 3041, Loss: 3.017807722091675\n",
            "Training Iteration 3042, Loss: 4.658511161804199\n",
            "Training Iteration 3043, Loss: 4.449243068695068\n",
            "Training Iteration 3044, Loss: 5.561143398284912\n",
            "Training Iteration 3045, Loss: 2.5717835426330566\n",
            "Training Iteration 3046, Loss: 1.3052359819412231\n",
            "Training Iteration 3047, Loss: 5.939834117889404\n",
            "Training Iteration 3048, Loss: 6.892482280731201\n",
            "Training Iteration 3049, Loss: 7.309314727783203\n",
            "Training Iteration 3050, Loss: 5.875655651092529\n",
            "Training Iteration 3051, Loss: 1.1298284530639648\n",
            "Training Iteration 3052, Loss: 3.4586477279663086\n",
            "Training Iteration 3053, Loss: 5.918069362640381\n",
            "Training Iteration 3054, Loss: 5.648959636688232\n",
            "Training Iteration 3055, Loss: 6.174839973449707\n",
            "Training Iteration 3056, Loss: 6.694305896759033\n",
            "Training Iteration 3057, Loss: 3.0723876953125\n",
            "Training Iteration 3058, Loss: 9.273091316223145\n",
            "Training Iteration 3059, Loss: 3.9160187244415283\n",
            "Training Iteration 3060, Loss: 3.6646690368652344\n",
            "Training Iteration 3061, Loss: 4.6987762451171875\n",
            "Training Iteration 3062, Loss: 2.978447437286377\n",
            "Training Iteration 3063, Loss: 4.989939212799072\n",
            "Training Iteration 3064, Loss: 6.736442565917969\n",
            "Training Iteration 3065, Loss: 10.756016731262207\n",
            "Training Iteration 3066, Loss: 8.747834205627441\n",
            "Training Iteration 3067, Loss: 7.194841384887695\n",
            "Training Iteration 3068, Loss: 6.680765628814697\n",
            "Training Iteration 3069, Loss: 4.046733856201172\n",
            "Training Iteration 3070, Loss: 3.4023661613464355\n",
            "Training Iteration 3071, Loss: 4.78485631942749\n",
            "Training Iteration 3072, Loss: 2.732776641845703\n",
            "Training Iteration 3073, Loss: 2.6774697303771973\n",
            "Training Iteration 3074, Loss: 3.4805192947387695\n",
            "Training Iteration 3075, Loss: 6.0468854904174805\n",
            "Training Iteration 3076, Loss: 3.982302665710449\n",
            "Training Iteration 3077, Loss: 4.0059404373168945\n",
            "Training Iteration 3078, Loss: 5.342831611633301\n",
            "Training Iteration 3079, Loss: 5.429414749145508\n",
            "Training Iteration 3080, Loss: 3.833134412765503\n",
            "Training Iteration 3081, Loss: 5.165439128875732\n",
            "Training Iteration 3082, Loss: 3.284510850906372\n",
            "Training Iteration 3083, Loss: 5.00187873840332\n",
            "Training Iteration 3084, Loss: 2.458676815032959\n",
            "Training Iteration 3085, Loss: 3.0354714393615723\n",
            "Training Iteration 3086, Loss: 5.444397926330566\n",
            "Training Iteration 3087, Loss: 7.294172286987305\n",
            "Training Iteration 3088, Loss: 3.83195424079895\n",
            "Training Iteration 3089, Loss: 4.354597091674805\n",
            "Training Iteration 3090, Loss: 3.271320343017578\n",
            "Training Iteration 3091, Loss: 4.644985675811768\n",
            "Training Iteration 3092, Loss: 3.198615074157715\n",
            "Training Iteration 3093, Loss: 5.865135669708252\n",
            "Training Iteration 3094, Loss: 7.559102535247803\n",
            "Training Iteration 3095, Loss: 5.707925796508789\n",
            "Training Iteration 3096, Loss: 3.9998950958251953\n",
            "Training Iteration 3097, Loss: 3.8279452323913574\n",
            "Training Iteration 3098, Loss: 5.277823448181152\n",
            "Training Iteration 3099, Loss: 4.396726131439209\n",
            "Training Iteration 3100, Loss: 3.6416969299316406\n",
            "Training Iteration 3101, Loss: 2.874957799911499\n",
            "Training Iteration 3102, Loss: 4.446625232696533\n",
            "Training Iteration 3103, Loss: 4.9497528076171875\n",
            "Training Iteration 3104, Loss: 6.751452445983887\n",
            "Training Iteration 3105, Loss: 4.794405460357666\n",
            "Training Iteration 3106, Loss: 3.3924977779388428\n",
            "Training Iteration 3107, Loss: 5.682301998138428\n",
            "Training Iteration 3108, Loss: 5.528324604034424\n",
            "Training Iteration 3109, Loss: 5.256256103515625\n",
            "Training Iteration 3110, Loss: 2.5888001918792725\n",
            "Training Iteration 3111, Loss: 6.603847980499268\n",
            "Training Iteration 3112, Loss: 3.022831916809082\n",
            "Training Iteration 3113, Loss: 8.474411964416504\n",
            "Training Iteration 3114, Loss: 4.589304447174072\n",
            "Training Iteration 3115, Loss: 5.951162338256836\n",
            "Training Iteration 3116, Loss: 4.60330867767334\n",
            "Training Iteration 3117, Loss: 2.2281062602996826\n",
            "Training Iteration 3118, Loss: 4.984292984008789\n",
            "Training Iteration 3119, Loss: 3.9230213165283203\n",
            "Training Iteration 3120, Loss: 4.189187049865723\n",
            "Training Iteration 3121, Loss: 6.979057312011719\n",
            "Training Iteration 3122, Loss: 2.762275218963623\n",
            "Training Iteration 3123, Loss: 5.172520160675049\n",
            "Training Iteration 3124, Loss: 3.712611675262451\n",
            "Training Iteration 3125, Loss: 5.84230375289917\n",
            "Training Iteration 3126, Loss: 4.154809474945068\n",
            "Training Iteration 3127, Loss: 1.4927057027816772\n",
            "Training Iteration 3128, Loss: 2.856234073638916\n",
            "Training Iteration 3129, Loss: 4.596680164337158\n",
            "Training Iteration 3130, Loss: 4.847474575042725\n",
            "Training Iteration 3131, Loss: 5.032913684844971\n",
            "Training Iteration 3132, Loss: 5.465604782104492\n",
            "Training Iteration 3133, Loss: 4.284146308898926\n",
            "Training Iteration 3134, Loss: 6.437150001525879\n",
            "Training Iteration 3135, Loss: 5.75331974029541\n",
            "Training Iteration 3136, Loss: 6.197718143463135\n",
            "Training Iteration 3137, Loss: 3.3739664554595947\n",
            "Training Iteration 3138, Loss: 5.156004428863525\n",
            "Training Iteration 3139, Loss: 3.7820634841918945\n",
            "Training Iteration 3140, Loss: 3.4335432052612305\n",
            "Training Iteration 3141, Loss: 3.1876330375671387\n",
            "Training Iteration 3142, Loss: 3.8597240447998047\n",
            "Training Iteration 3143, Loss: 4.371635437011719\n",
            "Training Iteration 3144, Loss: 8.33287525177002\n",
            "Training Iteration 3145, Loss: 4.597210884094238\n",
            "Training Iteration 3146, Loss: 3.0791711807250977\n",
            "Training Iteration 3147, Loss: 5.576804161071777\n",
            "Training Iteration 3148, Loss: 4.5271735191345215\n",
            "Training Iteration 3149, Loss: 3.667393684387207\n",
            "Training Iteration 3150, Loss: 3.724301815032959\n",
            "Training Iteration 3151, Loss: 4.857778072357178\n",
            "Training Iteration 3152, Loss: 3.990396499633789\n",
            "Training Iteration 3153, Loss: 5.176707744598389\n",
            "Training Iteration 3154, Loss: 4.486492156982422\n",
            "Training Iteration 3155, Loss: 2.99153470993042\n",
            "Training Iteration 3156, Loss: 3.8680477142333984\n",
            "Training Iteration 3157, Loss: 4.415717601776123\n",
            "Training Iteration 3158, Loss: 4.7817559242248535\n",
            "Training Iteration 3159, Loss: 4.7793731689453125\n",
            "Training Iteration 3160, Loss: 5.709981441497803\n",
            "Training Iteration 3161, Loss: 3.255838394165039\n",
            "Training Iteration 3162, Loss: 6.531641960144043\n",
            "Training Iteration 3163, Loss: 2.9818429946899414\n",
            "Training Iteration 3164, Loss: 5.827887535095215\n",
            "Training Iteration 3165, Loss: 4.733036041259766\n",
            "Training Iteration 3166, Loss: 2.229778289794922\n",
            "Training Iteration 3167, Loss: 3.8744518756866455\n",
            "Training Iteration 3168, Loss: 2.133578300476074\n",
            "Training Iteration 3169, Loss: 2.5311338901519775\n",
            "Training Iteration 3170, Loss: 3.3725852966308594\n",
            "Training Iteration 3171, Loss: 5.470630645751953\n",
            "Training Iteration 3172, Loss: 5.76402473449707\n",
            "Training Iteration 3173, Loss: 10.485235214233398\n",
            "Training Iteration 3174, Loss: 1.6329283714294434\n",
            "Training Iteration 3175, Loss: 5.762665748596191\n",
            "Training Iteration 3176, Loss: 13.313252449035645\n",
            "Training Iteration 3177, Loss: 8.699912071228027\n",
            "Training Iteration 3178, Loss: 6.304680824279785\n",
            "Training Iteration 3179, Loss: 7.883917331695557\n",
            "Training Iteration 3180, Loss: 4.411294937133789\n",
            "Training Iteration 3181, Loss: 5.337327003479004\n",
            "Training Iteration 3182, Loss: 3.696472644805908\n",
            "Training Iteration 3183, Loss: 3.9175209999084473\n",
            "Training Iteration 3184, Loss: 4.962980270385742\n",
            "Training Iteration 3185, Loss: 3.5639662742614746\n",
            "Training Iteration 3186, Loss: 6.5126729011535645\n",
            "Training Iteration 3187, Loss: 5.106657028198242\n",
            "Training Iteration 3188, Loss: 1.771796703338623\n",
            "Training Iteration 3189, Loss: 3.8494787216186523\n",
            "Training Iteration 3190, Loss: 7.799962997436523\n",
            "Training Iteration 3191, Loss: 7.016615867614746\n",
            "Training Iteration 3192, Loss: 4.894381046295166\n",
            "Training Iteration 3193, Loss: 2.6641252040863037\n",
            "Training Iteration 3194, Loss: 3.785008192062378\n",
            "Training Iteration 3195, Loss: 4.877308368682861\n",
            "Training Iteration 3196, Loss: 4.459412097930908\n",
            "Training Iteration 3197, Loss: 2.870389699935913\n",
            "Training Iteration 3198, Loss: 2.7420732975006104\n",
            "Training Iteration 3199, Loss: 6.356423854827881\n",
            "Training Iteration 3200, Loss: 6.8783793449401855\n",
            "Training Iteration 3201, Loss: 4.932793140411377\n",
            "Training Iteration 3202, Loss: 2.7823867797851562\n",
            "Training Iteration 3203, Loss: 4.010129928588867\n",
            "Training Iteration 3204, Loss: 3.7137982845306396\n",
            "Training Iteration 3205, Loss: 4.367908000946045\n",
            "Training Iteration 3206, Loss: 4.377399444580078\n",
            "Training Iteration 3207, Loss: 3.5075130462646484\n",
            "Training Iteration 3208, Loss: 4.716687202453613\n",
            "Training Iteration 3209, Loss: 1.6846897602081299\n",
            "Training Iteration 3210, Loss: 4.812979698181152\n",
            "Training Iteration 3211, Loss: 7.4335036277771\n",
            "Training Iteration 3212, Loss: 5.424433708190918\n",
            "Training Iteration 3213, Loss: 3.3323750495910645\n",
            "Training Iteration 3214, Loss: 5.668523788452148\n",
            "Training Iteration 3215, Loss: 4.870183944702148\n",
            "Training Iteration 3216, Loss: 5.833683967590332\n",
            "Training Iteration 3217, Loss: 6.807321548461914\n",
            "Training Iteration 3218, Loss: 4.479462623596191\n",
            "Training Iteration 3219, Loss: 3.9432003498077393\n",
            "Training Iteration 3220, Loss: 2.217787027359009\n",
            "Training Iteration 3221, Loss: 5.360726356506348\n",
            "Training Iteration 3222, Loss: 4.297343730926514\n",
            "Training Iteration 3223, Loss: 4.0633063316345215\n",
            "Training Iteration 3224, Loss: 7.82670259475708\n",
            "Training Iteration 3225, Loss: 5.654099464416504\n",
            "Training Iteration 3226, Loss: 4.671572685241699\n",
            "Training Iteration 3227, Loss: 3.187889575958252\n",
            "Training Iteration 3228, Loss: 4.2839436531066895\n",
            "Training Iteration 3229, Loss: 4.250412464141846\n",
            "Training Iteration 3230, Loss: 2.5283193588256836\n",
            "Training Iteration 3231, Loss: 4.188436985015869\n",
            "Training Iteration 3232, Loss: 4.092769622802734\n",
            "Training Iteration 3233, Loss: 4.352789402008057\n",
            "Training Iteration 3234, Loss: 2.8253698348999023\n",
            "Training Iteration 3235, Loss: 5.269281387329102\n",
            "Training Iteration 3236, Loss: 3.9922337532043457\n",
            "Training Iteration 3237, Loss: 4.889085292816162\n",
            "Training Iteration 3238, Loss: 8.068718910217285\n",
            "Training Iteration 3239, Loss: 2.1239264011383057\n",
            "Training Iteration 3240, Loss: 3.761178970336914\n",
            "Training Iteration 3241, Loss: 5.53365421295166\n",
            "Training Iteration 3242, Loss: 6.629761695861816\n",
            "Training Iteration 3243, Loss: 8.357723236083984\n",
            "Training Iteration 3244, Loss: 4.372132778167725\n",
            "Training Iteration 3245, Loss: 5.302206039428711\n",
            "Training Iteration 3246, Loss: 5.532907962799072\n",
            "Training Iteration 3247, Loss: 3.4319448471069336\n",
            "Training Iteration 3248, Loss: 5.070977210998535\n",
            "Training Iteration 3249, Loss: 5.756263256072998\n",
            "Training Iteration 3250, Loss: 4.983762264251709\n",
            "Training Iteration 3251, Loss: 4.991046905517578\n",
            "Training Iteration 3252, Loss: 2.8163530826568604\n",
            "Training Iteration 3253, Loss: 2.7284905910491943\n",
            "Training Iteration 3254, Loss: 5.153722286224365\n",
            "Training Iteration 3255, Loss: 11.341687202453613\n",
            "Training Iteration 3256, Loss: 3.9404356479644775\n",
            "Training Iteration 3257, Loss: 3.9404854774475098\n",
            "Training Iteration 3258, Loss: 4.356141567230225\n",
            "Training Iteration 3259, Loss: 2.3613874912261963\n",
            "Training Iteration 3260, Loss: 7.321751594543457\n",
            "Training Iteration 3261, Loss: 8.811539649963379\n",
            "Training Iteration 3262, Loss: 7.521453380584717\n",
            "Training Iteration 3263, Loss: 5.553511619567871\n",
            "Training Iteration 3264, Loss: 7.2844343185424805\n",
            "Training Iteration 3265, Loss: 10.980751991271973\n",
            "Training Iteration 3266, Loss: 4.5868239402771\n",
            "Training Iteration 3267, Loss: 4.666093826293945\n",
            "Training Iteration 3268, Loss: 5.7154340744018555\n",
            "Training Iteration 3269, Loss: 6.726159572601318\n",
            "Training Iteration 3270, Loss: 3.914252281188965\n",
            "Training Iteration 3271, Loss: 2.8254919052124023\n",
            "Training Iteration 3272, Loss: 4.037024021148682\n",
            "Training Iteration 3273, Loss: 10.979825019836426\n",
            "Training Iteration 3274, Loss: 3.5765881538391113\n",
            "Training Iteration 3275, Loss: 4.3821892738342285\n",
            "Training Iteration 3276, Loss: 5.7303032875061035\n",
            "Training Iteration 3277, Loss: 4.0867600440979\n",
            "Training Iteration 3278, Loss: 9.37459945678711\n",
            "Training Iteration 3279, Loss: 5.784518718719482\n",
            "Training Iteration 3280, Loss: 2.6280910968780518\n",
            "Training Iteration 3281, Loss: 3.0729894638061523\n",
            "Training Iteration 3282, Loss: 5.7625508308410645\n",
            "Training Iteration 3283, Loss: 6.929255485534668\n",
            "Training Iteration 3284, Loss: 2.9086503982543945\n",
            "Training Iteration 3285, Loss: 5.124970436096191\n",
            "Training Iteration 3286, Loss: 6.424767017364502\n",
            "Training Iteration 3287, Loss: 8.884378433227539\n",
            "Training Iteration 3288, Loss: 2.3858413696289062\n",
            "Training Iteration 3289, Loss: 5.172087669372559\n",
            "Training Iteration 3290, Loss: 4.937023639678955\n",
            "Training Iteration 3291, Loss: 7.678754806518555\n",
            "Training Iteration 3292, Loss: 5.227767467498779\n",
            "Training Iteration 3293, Loss: 4.344770431518555\n",
            "Training Iteration 3294, Loss: 4.379809856414795\n",
            "Training Iteration 3295, Loss: 5.443302154541016\n",
            "Training Iteration 3296, Loss: 5.447572708129883\n",
            "Training Iteration 3297, Loss: 4.126698970794678\n",
            "Training Iteration 3298, Loss: 6.791651725769043\n",
            "Training Iteration 3299, Loss: 3.8326587677001953\n",
            "Training Iteration 3300, Loss: 5.97650146484375\n",
            "Training Iteration 3301, Loss: 3.189302921295166\n",
            "Training Iteration 3302, Loss: 4.251924514770508\n",
            "Training Iteration 3303, Loss: 6.468380451202393\n",
            "Training Iteration 3304, Loss: 2.690264940261841\n",
            "Training Iteration 3305, Loss: 3.762237071990967\n",
            "Training Iteration 3306, Loss: 3.235999822616577\n",
            "Training Iteration 3307, Loss: 4.2423248291015625\n",
            "Training Iteration 3308, Loss: 3.473384380340576\n",
            "Training Iteration 3309, Loss: 5.235342502593994\n",
            "Training Iteration 3310, Loss: 3.8209176063537598\n",
            "Training Iteration 3311, Loss: 3.6606040000915527\n",
            "Training Iteration 3312, Loss: 5.418835639953613\n",
            "Training Iteration 3313, Loss: 6.4939398765563965\n",
            "Training Iteration 3314, Loss: 3.929609775543213\n",
            "Training Iteration 3315, Loss: 5.065781593322754\n",
            "Training Iteration 3316, Loss: 4.725074768066406\n",
            "Training Iteration 3317, Loss: 5.036586761474609\n",
            "Training Iteration 3318, Loss: 3.687763214111328\n",
            "Training Iteration 3319, Loss: 3.9458351135253906\n",
            "Training Iteration 3320, Loss: 3.3087515830993652\n",
            "Training Iteration 3321, Loss: 4.829338550567627\n",
            "Training Iteration 3322, Loss: 4.469431400299072\n",
            "Training Iteration 3323, Loss: 4.578988075256348\n",
            "Training Iteration 3324, Loss: 4.163524150848389\n",
            "Training Iteration 3325, Loss: 3.663041114807129\n",
            "Training Iteration 3326, Loss: 3.905236005783081\n",
            "Training Iteration 3327, Loss: 1.9931082725524902\n",
            "Training Iteration 3328, Loss: 4.127968788146973\n",
            "Training Iteration 3329, Loss: 2.982546329498291\n",
            "Training Iteration 3330, Loss: 3.537914276123047\n",
            "Training Iteration 3331, Loss: 6.11015510559082\n",
            "Training Iteration 3332, Loss: 2.8891305923461914\n",
            "Training Iteration 3333, Loss: 3.4145193099975586\n",
            "Training Iteration 3334, Loss: 4.917725563049316\n",
            "Training Iteration 3335, Loss: 4.8713202476501465\n",
            "Training Iteration 3336, Loss: 1.960141658782959\n",
            "Training Iteration 3337, Loss: 5.475664138793945\n",
            "Training Iteration 3338, Loss: 5.72880220413208\n",
            "Training Iteration 3339, Loss: 6.8336076736450195\n",
            "Training Iteration 3340, Loss: 6.386579990386963\n",
            "Training Iteration 3341, Loss: 5.425706386566162\n",
            "Training Iteration 3342, Loss: 6.270368576049805\n",
            "Training Iteration 3343, Loss: 3.1034975051879883\n",
            "Training Iteration 3344, Loss: 6.312787055969238\n",
            "Training Iteration 3345, Loss: 3.7149465084075928\n",
            "Training Iteration 3346, Loss: 4.151696681976318\n",
            "Training Iteration 3347, Loss: 4.469464302062988\n",
            "Training Iteration 3348, Loss: 5.64206600189209\n",
            "Training Iteration 3349, Loss: 3.1666176319122314\n",
            "Training Iteration 3350, Loss: 8.553705215454102\n",
            "Training Iteration 3351, Loss: 6.673583507537842\n",
            "Training Iteration 3352, Loss: 7.6645002365112305\n",
            "Training Iteration 3353, Loss: 2.3021767139434814\n",
            "Training Iteration 3354, Loss: 1.8283995389938354\n",
            "Training Iteration 3355, Loss: 6.906905651092529\n",
            "Training Iteration 3356, Loss: 6.743411540985107\n",
            "Training Iteration 3357, Loss: 6.558311462402344\n",
            "Training Iteration 3358, Loss: 5.466556072235107\n",
            "Training Iteration 3359, Loss: 6.157408237457275\n",
            "Training Iteration 3360, Loss: 10.644027709960938\n",
            "Training Iteration 3361, Loss: 6.354231834411621\n",
            "Training Iteration 3362, Loss: 5.523942947387695\n",
            "Training Iteration 3363, Loss: 1.6901730298995972\n",
            "Training Iteration 3364, Loss: 7.072512626647949\n",
            "Training Iteration 3365, Loss: 6.652517795562744\n",
            "Training Iteration 3366, Loss: 6.104680061340332\n",
            "Training Iteration 3367, Loss: 7.184683799743652\n",
            "Training Iteration 3368, Loss: 7.97003698348999\n",
            "Training Iteration 3369, Loss: 2.63767147064209\n",
            "Training Iteration 3370, Loss: 4.694478511810303\n",
            "Training Iteration 3371, Loss: 3.9530985355377197\n",
            "Training Iteration 3372, Loss: 7.904275417327881\n",
            "Training Iteration 3373, Loss: 4.618675708770752\n",
            "Training Iteration 3374, Loss: 4.533853530883789\n",
            "Training Iteration 3375, Loss: 8.98554515838623\n",
            "Training Iteration 3376, Loss: 5.642759799957275\n",
            "Training Iteration 3377, Loss: 3.361090898513794\n",
            "Training Iteration 3378, Loss: 5.143265724182129\n",
            "Training Iteration 3379, Loss: 7.495657444000244\n",
            "Training Iteration 3380, Loss: 4.373641490936279\n",
            "Training Iteration 3381, Loss: 4.910479545593262\n",
            "Training Iteration 3382, Loss: 2.2514424324035645\n",
            "Training Iteration 3383, Loss: 3.2356345653533936\n",
            "Training Iteration 3384, Loss: 5.441973686218262\n",
            "Training Iteration 3385, Loss: 3.8922324180603027\n",
            "Training Iteration 3386, Loss: 1.7824431657791138\n",
            "Training Iteration 3387, Loss: 2.3717691898345947\n",
            "Training Iteration 3388, Loss: 4.1804680824279785\n",
            "Training Iteration 3389, Loss: 3.591261863708496\n",
            "Training Iteration 3390, Loss: 2.363522529602051\n",
            "Training Iteration 3391, Loss: 3.2385616302490234\n",
            "Training Iteration 3392, Loss: 3.408994674682617\n",
            "Training Iteration 3393, Loss: 5.469191074371338\n",
            "Training Iteration 3394, Loss: 5.978918552398682\n",
            "Training Iteration 3395, Loss: 4.485010623931885\n",
            "Training Iteration 3396, Loss: 3.3734188079833984\n",
            "Training Iteration 3397, Loss: 4.156230449676514\n",
            "Training Iteration 3398, Loss: 4.894909858703613\n",
            "Training Iteration 3399, Loss: 2.8316309452056885\n",
            "Training Iteration 3400, Loss: 2.7928147315979004\n",
            "Training Iteration 3401, Loss: 2.7661259174346924\n",
            "Training Iteration 3402, Loss: 2.526254415512085\n",
            "Training Iteration 3403, Loss: 6.340580940246582\n",
            "Training Iteration 3404, Loss: 2.871657609939575\n",
            "Training Iteration 3405, Loss: 5.677340030670166\n",
            "Training Iteration 3406, Loss: 5.585413932800293\n",
            "Training Iteration 3407, Loss: 5.4537787437438965\n",
            "Training Iteration 3408, Loss: 1.5746674537658691\n",
            "Training Iteration 3409, Loss: 2.835989236831665\n",
            "Training Iteration 3410, Loss: 2.8495302200317383\n",
            "Training Iteration 3411, Loss: 3.146660327911377\n",
            "Training Iteration 3412, Loss: 2.9558017253875732\n",
            "Training Iteration 3413, Loss: 4.2181620597839355\n",
            "Training Iteration 3414, Loss: 6.513563632965088\n",
            "Training Iteration 3415, Loss: 2.6936497688293457\n",
            "Training Iteration 3416, Loss: 2.925926446914673\n",
            "Training Iteration 3417, Loss: 3.8354883193969727\n",
            "Training Iteration 3418, Loss: 3.416940689086914\n",
            "Training Iteration 3419, Loss: 2.47554874420166\n",
            "Training Iteration 3420, Loss: 4.4404754638671875\n",
            "Training Iteration 3421, Loss: 2.542754888534546\n",
            "Training Iteration 3422, Loss: 3.1747164726257324\n",
            "Training Iteration 3423, Loss: 2.5140843391418457\n",
            "Training Iteration 3424, Loss: 3.3349483013153076\n",
            "Training Iteration 3425, Loss: 2.2076823711395264\n",
            "Training Iteration 3426, Loss: 1.907454490661621\n",
            "Training Iteration 3427, Loss: 0.9536880254745483\n",
            "Training Iteration 3428, Loss: 4.445453643798828\n",
            "Training Iteration 3429, Loss: 6.256021499633789\n",
            "Training Iteration 3430, Loss: 3.15244722366333\n",
            "Training Iteration 3431, Loss: 3.813669204711914\n",
            "Training Iteration 3432, Loss: 7.2183990478515625\n",
            "Training Iteration 3433, Loss: 6.020373821258545\n",
            "Training Iteration 3434, Loss: 4.735898971557617\n",
            "Training Iteration 3435, Loss: 4.136444091796875\n",
            "Training Iteration 3436, Loss: 5.616878032684326\n",
            "Training Iteration 3437, Loss: 4.778733253479004\n",
            "Training Iteration 3438, Loss: 7.524479866027832\n",
            "Training Iteration 3439, Loss: 4.200847625732422\n",
            "Training Iteration 3440, Loss: 5.0945024490356445\n",
            "Training Iteration 3441, Loss: 5.135584831237793\n",
            "Training Iteration 3442, Loss: 4.473299026489258\n",
            "Training Iteration 3443, Loss: 4.074638843536377\n",
            "Training Iteration 3444, Loss: 6.004781723022461\n",
            "Training Iteration 3445, Loss: 7.335438251495361\n",
            "Training Iteration 3446, Loss: 3.240142583847046\n",
            "Training Iteration 3447, Loss: 7.505145072937012\n",
            "Training Iteration 3448, Loss: 5.278561592102051\n",
            "Training Iteration 3449, Loss: 8.021352767944336\n",
            "Training Iteration 3450, Loss: 4.206182956695557\n",
            "Training Iteration 3451, Loss: 5.149628639221191\n",
            "Training Iteration 3452, Loss: 3.593994379043579\n",
            "Training Iteration 3453, Loss: 3.4069271087646484\n",
            "Training Iteration 3454, Loss: 5.097320079803467\n",
            "Training Iteration 3455, Loss: 4.824149131774902\n",
            "Training Iteration 3456, Loss: 4.279948711395264\n",
            "Training Iteration 3457, Loss: 5.15634822845459\n",
            "Training Iteration 3458, Loss: 3.237185478210449\n",
            "Training Iteration 3459, Loss: 3.4521870613098145\n",
            "Training Iteration 3460, Loss: 3.4382121562957764\n",
            "Training Iteration 3461, Loss: 2.843536615371704\n",
            "Training Iteration 3462, Loss: 5.704225540161133\n",
            "Training Iteration 3463, Loss: 3.6123130321502686\n",
            "Training Iteration 3464, Loss: 4.89948844909668\n",
            "Training Iteration 3465, Loss: 3.500108003616333\n",
            "Training Iteration 3466, Loss: 4.7232794761657715\n",
            "Training Iteration 3467, Loss: 4.414244651794434\n",
            "Training Iteration 3468, Loss: 4.6518964767456055\n",
            "Training Iteration 3469, Loss: 1.7972497940063477\n",
            "Training Iteration 3470, Loss: 5.324831962585449\n",
            "Training Iteration 3471, Loss: 4.657535076141357\n",
            "Training Iteration 3472, Loss: 3.3423728942871094\n",
            "Training Iteration 3473, Loss: 3.021301031112671\n",
            "Training Iteration 3474, Loss: 2.658541202545166\n",
            "Training Iteration 3475, Loss: 4.9323530197143555\n",
            "Training Iteration 3476, Loss: 4.711840629577637\n",
            "Training Iteration 3477, Loss: 3.393801212310791\n",
            "Training Iteration 3478, Loss: 5.054843425750732\n",
            "Training Iteration 3479, Loss: 3.3640499114990234\n",
            "Training Iteration 3480, Loss: 2.4207262992858887\n",
            "Training Iteration 3481, Loss: 3.4657599925994873\n",
            "Training Iteration 3482, Loss: 4.0253143310546875\n",
            "Training Iteration 3483, Loss: 4.054596900939941\n",
            "Training Iteration 3484, Loss: 3.6268959045410156\n",
            "Training Iteration 3485, Loss: 3.202345848083496\n",
            "Training Iteration 3486, Loss: 6.872105121612549\n",
            "Training Iteration 3487, Loss: 4.010514736175537\n",
            "Training Iteration 3488, Loss: 2.3615612983703613\n",
            "Training Iteration 3489, Loss: 2.7855920791625977\n",
            "Training Iteration 3490, Loss: 6.6988677978515625\n",
            "Training Iteration 3491, Loss: 4.144617557525635\n",
            "Training Iteration 3492, Loss: 2.5948193073272705\n",
            "Training Iteration 3493, Loss: 2.785451650619507\n",
            "Training Iteration 3494, Loss: 3.9565629959106445\n",
            "Training Iteration 3495, Loss: 3.568671226501465\n",
            "Training Iteration 3496, Loss: 4.6331024169921875\n",
            "Training Iteration 3497, Loss: 3.780872106552124\n",
            "Training Iteration 3498, Loss: 4.916534900665283\n",
            "Training Iteration 3499, Loss: 4.39577054977417\n",
            "Training Iteration 3500, Loss: 5.234580039978027\n",
            "Training Iteration 3501, Loss: 6.8102874755859375\n",
            "Training Iteration 3502, Loss: 3.6558728218078613\n",
            "Training Iteration 3503, Loss: 5.332476615905762\n",
            "Training Iteration 3504, Loss: 3.0723509788513184\n",
            "Training Iteration 3505, Loss: 6.633782386779785\n",
            "Training Iteration 3506, Loss: 4.512789249420166\n",
            "Training Iteration 3507, Loss: 4.941957950592041\n",
            "Training Iteration 3508, Loss: 5.836998462677002\n",
            "Training Iteration 3509, Loss: 5.439517498016357\n",
            "Training Iteration 3510, Loss: 4.6356635093688965\n",
            "Training Iteration 3511, Loss: 4.084143161773682\n",
            "Training Iteration 3512, Loss: 4.175502300262451\n",
            "Training Iteration 3513, Loss: 3.148629665374756\n",
            "Training Iteration 3514, Loss: 3.7762885093688965\n",
            "Training Iteration 3515, Loss: 3.6156744956970215\n",
            "Training Iteration 3516, Loss: 7.411375999450684\n",
            "Training Iteration 3517, Loss: 5.725882530212402\n",
            "Training Iteration 3518, Loss: 5.756956577301025\n",
            "Training Iteration 3519, Loss: 3.6071901321411133\n",
            "Training Iteration 3520, Loss: 2.9932572841644287\n",
            "Training Iteration 3521, Loss: 3.0510261058807373\n",
            "Training Iteration 3522, Loss: 5.043428421020508\n",
            "Training Iteration 3523, Loss: 5.959444522857666\n",
            "Training Iteration 3524, Loss: 4.350832939147949\n",
            "Training Iteration 3525, Loss: 2.8806304931640625\n",
            "Training Iteration 3526, Loss: 3.0070698261260986\n",
            "Training Iteration 3527, Loss: 4.01518440246582\n",
            "Training Iteration 3528, Loss: 4.232958793640137\n",
            "Training Iteration 3529, Loss: 3.4824442863464355\n",
            "Training Iteration 3530, Loss: 4.969427585601807\n",
            "Training Iteration 3531, Loss: 4.0697407722473145\n",
            "Training Iteration 3532, Loss: 7.546271800994873\n",
            "Training Iteration 3533, Loss: 4.8658976554870605\n",
            "Training Iteration 3534, Loss: 3.6718554496765137\n",
            "Training Iteration 3535, Loss: 3.9623665809631348\n",
            "Training Iteration 3536, Loss: 3.0938925743103027\n",
            "Training Iteration 3537, Loss: 4.157901763916016\n",
            "Training Iteration 3538, Loss: 3.9869651794433594\n",
            "Training Iteration 3539, Loss: 3.6821985244750977\n",
            "Training Iteration 3540, Loss: 3.8591299057006836\n",
            "Training Iteration 3541, Loss: 1.8819056749343872\n",
            "Training Iteration 3542, Loss: 5.954654693603516\n",
            "Training Iteration 3543, Loss: 3.7807881832122803\n",
            "Training Iteration 3544, Loss: 5.448842525482178\n",
            "Training Iteration 3545, Loss: 3.9564919471740723\n",
            "Training Iteration 3546, Loss: 6.19695520401001\n",
            "Training Iteration 3547, Loss: 3.3455581665039062\n",
            "Training Iteration 3548, Loss: 5.170063495635986\n",
            "Training Iteration 3549, Loss: 2.6350860595703125\n",
            "Training Iteration 3550, Loss: 2.6083273887634277\n",
            "Training Iteration 3551, Loss: 2.990410804748535\n",
            "Training Iteration 3552, Loss: 6.034139156341553\n",
            "Training Iteration 3553, Loss: 6.106855392456055\n",
            "Training Iteration 3554, Loss: 3.8453636169433594\n",
            "Training Iteration 3555, Loss: 2.4483916759490967\n",
            "Training Iteration 3556, Loss: 4.346297740936279\n",
            "Training Iteration 3557, Loss: 3.155517578125\n",
            "Training Iteration 3558, Loss: 3.803867816925049\n",
            "Training Iteration 3559, Loss: 4.684107780456543\n",
            "Training Iteration 3560, Loss: 3.795501947402954\n",
            "Training Iteration 3561, Loss: 2.841505527496338\n",
            "Training Iteration 3562, Loss: 4.959802627563477\n",
            "Training Iteration 3563, Loss: 3.9387738704681396\n",
            "Training Iteration 3564, Loss: 3.826800584793091\n",
            "Training Iteration 3565, Loss: 5.3861308097839355\n",
            "Training Iteration 3566, Loss: 5.761083602905273\n",
            "Training Iteration 3567, Loss: 6.410059452056885\n",
            "Training Iteration 3568, Loss: 3.452800989151001\n",
            "Training Iteration 3569, Loss: 4.554345607757568\n",
            "Training Iteration 3570, Loss: 3.739393472671509\n",
            "Training Iteration 3571, Loss: 7.751705169677734\n",
            "Training Iteration 3572, Loss: 8.3554048538208\n",
            "Training Iteration 3573, Loss: 4.964990615844727\n",
            "Training Iteration 3574, Loss: 7.466508865356445\n",
            "Training Iteration 3575, Loss: 3.8896656036376953\n",
            "Training Iteration 3576, Loss: 4.918291091918945\n",
            "Training Iteration 3577, Loss: 2.6575653553009033\n",
            "Training Iteration 3578, Loss: 4.069289207458496\n",
            "Training Iteration 3579, Loss: 5.284884452819824\n",
            "Training Iteration 3580, Loss: 6.473348140716553\n",
            "Training Iteration 3581, Loss: 5.079418182373047\n",
            "Training Iteration 3582, Loss: 9.564387321472168\n",
            "Training Iteration 3583, Loss: 4.9623212814331055\n",
            "Training Iteration 3584, Loss: 9.072787284851074\n",
            "Training Iteration 3585, Loss: 5.448642253875732\n",
            "Training Iteration 3586, Loss: 8.298062324523926\n",
            "Training Iteration 3587, Loss: 7.925872802734375\n",
            "Training Iteration 3588, Loss: 4.569602966308594\n",
            "Training Iteration 3589, Loss: 5.067744731903076\n",
            "Training Iteration 3590, Loss: 4.0081787109375\n",
            "Training Iteration 3591, Loss: 4.044523239135742\n",
            "Training Iteration 3592, Loss: 6.178247451782227\n",
            "Training Iteration 3593, Loss: 5.208800792694092\n",
            "Training Iteration 3594, Loss: 5.386834621429443\n",
            "Training Iteration 3595, Loss: 5.929960250854492\n",
            "Training Iteration 3596, Loss: 4.943501949310303\n",
            "Training Iteration 3597, Loss: 3.102342128753662\n",
            "Training Iteration 3598, Loss: 4.580501079559326\n",
            "Training Iteration 3599, Loss: 4.312028884887695\n",
            "Training Iteration 3600, Loss: 3.7591469287872314\n",
            "Training Iteration 3601, Loss: 5.33014440536499\n",
            "Training Iteration 3602, Loss: 3.913041591644287\n",
            "Training Iteration 3603, Loss: 4.995751857757568\n",
            "Training Iteration 3604, Loss: 5.405310153961182\n",
            "Training Iteration 3605, Loss: 2.9474503993988037\n",
            "Training Iteration 3606, Loss: 4.4747819900512695\n",
            "Training Iteration 3607, Loss: 7.715418815612793\n",
            "Training Iteration 3608, Loss: 6.244154453277588\n",
            "Training Iteration 3609, Loss: 5.571185111999512\n",
            "Training Iteration 3610, Loss: 1.2534736394882202\n",
            "Training Iteration 3611, Loss: 5.29266881942749\n",
            "Training Iteration 3612, Loss: 5.738846302032471\n",
            "Training Iteration 3613, Loss: 5.294524669647217\n",
            "Training Iteration 3614, Loss: 6.571719169616699\n",
            "Training Iteration 3615, Loss: 4.489076614379883\n",
            "Training Iteration 3616, Loss: 6.521450042724609\n",
            "Training Iteration 3617, Loss: 1.5238118171691895\n",
            "Training Iteration 3618, Loss: 5.478753089904785\n",
            "Training Iteration 3619, Loss: 2.1517674922943115\n",
            "Training Iteration 3620, Loss: 6.433391571044922\n",
            "Training Iteration 3621, Loss: 4.170639991760254\n",
            "Training Iteration 3622, Loss: 4.6748247146606445\n",
            "Training Iteration 3623, Loss: 5.660177707672119\n",
            "Training Iteration 3624, Loss: 1.9605880975723267\n",
            "Training Iteration 3625, Loss: 5.654191017150879\n",
            "Training Iteration 3626, Loss: 2.986175060272217\n",
            "Training Iteration 3627, Loss: 4.904684066772461\n",
            "Training Iteration 3628, Loss: 3.971510171890259\n",
            "Training Iteration 3629, Loss: 3.579010009765625\n",
            "Training Iteration 3630, Loss: 6.7394208908081055\n",
            "Training Iteration 3631, Loss: 5.04868745803833\n",
            "Training Iteration 3632, Loss: 6.459533214569092\n",
            "Training Iteration 3633, Loss: 4.559700012207031\n",
            "Training Iteration 3634, Loss: 3.3453986644744873\n",
            "Training Iteration 3635, Loss: 7.150828838348389\n",
            "Training Iteration 3636, Loss: 10.692575454711914\n",
            "Training Iteration 3637, Loss: 5.647209644317627\n",
            "Training Iteration 3638, Loss: 3.3928701877593994\n",
            "Training Iteration 3639, Loss: 2.326566696166992\n",
            "Training Iteration 3640, Loss: 4.715246677398682\n",
            "Training Iteration 3641, Loss: 5.909385681152344\n",
            "Training Iteration 3642, Loss: 3.28770112991333\n",
            "Training Iteration 3643, Loss: 3.753892660140991\n",
            "Training Iteration 3644, Loss: 4.665838241577148\n",
            "Training Iteration 3645, Loss: 4.426623344421387\n",
            "Training Iteration 3646, Loss: 2.8250365257263184\n",
            "Training Iteration 3647, Loss: 1.2101092338562012\n",
            "Training Iteration 3648, Loss: 7.428467750549316\n",
            "Training Iteration 3649, Loss: 4.78609561920166\n",
            "Training Iteration 3650, Loss: 7.975405693054199\n",
            "Training Iteration 3651, Loss: 6.770740032196045\n",
            "Training Iteration 3652, Loss: 5.292187213897705\n",
            "Training Iteration 3653, Loss: 4.413728713989258\n",
            "Training Iteration 3654, Loss: 5.634422302246094\n",
            "Training Iteration 3655, Loss: 5.319406509399414\n",
            "Training Iteration 3656, Loss: 4.952404499053955\n",
            "Training Iteration 3657, Loss: 6.488276958465576\n",
            "Training Iteration 3658, Loss: 4.135387420654297\n",
            "Training Iteration 3659, Loss: 3.643007755279541\n",
            "Training Iteration 3660, Loss: 5.130971908569336\n",
            "Training Iteration 3661, Loss: 3.9144577980041504\n",
            "Training Iteration 3662, Loss: 2.699676036834717\n",
            "Training Iteration 3663, Loss: 2.9395008087158203\n",
            "Training Iteration 3664, Loss: 5.078141689300537\n",
            "Training Iteration 3665, Loss: 5.005209922790527\n",
            "Training Iteration 3666, Loss: 4.787795543670654\n",
            "Training Iteration 3667, Loss: 3.3685693740844727\n",
            "Training Iteration 3668, Loss: 3.252915143966675\n",
            "Training Iteration 3669, Loss: 3.599160671234131\n",
            "Training Iteration 3670, Loss: 4.268446445465088\n",
            "Training Iteration 3671, Loss: 9.049628257751465\n",
            "Training Iteration 3672, Loss: 6.329329967498779\n",
            "Training Iteration 3673, Loss: 5.7585015296936035\n",
            "Training Iteration 3674, Loss: 4.064793586730957\n",
            "Training Iteration 3675, Loss: 1.632981300354004\n",
            "Training Iteration 3676, Loss: 4.569997787475586\n",
            "Training Iteration 3677, Loss: 5.5243239402771\n",
            "Training Iteration 3678, Loss: 4.146061420440674\n",
            "Training Iteration 3679, Loss: 5.480228424072266\n",
            "Training Iteration 3680, Loss: 2.080976963043213\n",
            "Training Iteration 3681, Loss: 6.538143157958984\n",
            "Training Iteration 3682, Loss: 3.9470489025115967\n",
            "Training Iteration 3683, Loss: 2.692025899887085\n",
            "Training Iteration 3684, Loss: 1.840600609779358\n",
            "Training Iteration 3685, Loss: 1.6448315382003784\n",
            "Training Iteration 3686, Loss: 3.4427406787872314\n",
            "Training Iteration 3687, Loss: 5.5986199378967285\n",
            "Training Iteration 3688, Loss: 3.764376163482666\n",
            "Training Iteration 3689, Loss: 4.432003498077393\n",
            "Training Iteration 3690, Loss: 8.255227088928223\n",
            "Training Iteration 3691, Loss: 1.487220287322998\n",
            "Training Iteration 3692, Loss: 6.190394878387451\n",
            "Training Iteration 3693, Loss: 3.4245166778564453\n",
            "Training Iteration 3694, Loss: 4.84924840927124\n",
            "Training Iteration 3695, Loss: 5.7271647453308105\n",
            "Training Iteration 3696, Loss: 4.667516708374023\n",
            "Training Iteration 3697, Loss: 4.877789497375488\n",
            "Training Iteration 3698, Loss: 3.934439182281494\n",
            "Training Iteration 3699, Loss: 5.178490161895752\n",
            "Training Iteration 3700, Loss: 4.830597400665283\n",
            "Training Iteration 3701, Loss: 4.619197368621826\n",
            "Training Iteration 3702, Loss: 4.444653511047363\n",
            "Training Iteration 3703, Loss: 4.857503414154053\n",
            "Training Iteration 3704, Loss: 4.504486083984375\n",
            "Training Iteration 3705, Loss: 4.114238739013672\n",
            "Training Iteration 3706, Loss: 3.299487590789795\n",
            "Training Iteration 3707, Loss: 3.932586431503296\n",
            "Training Iteration 3708, Loss: 3.4268147945404053\n",
            "Training Iteration 3709, Loss: 4.751946449279785\n",
            "Training Iteration 3710, Loss: 5.8467912673950195\n",
            "Training Iteration 3711, Loss: 2.9818222522735596\n",
            "Training Iteration 3712, Loss: 1.5805974006652832\n",
            "Training Iteration 3713, Loss: 2.69468355178833\n",
            "Training Iteration 3714, Loss: 1.521841049194336\n",
            "Training Iteration 3715, Loss: 4.069973468780518\n",
            "Training Iteration 3716, Loss: 5.323792934417725\n",
            "Training Iteration 3717, Loss: 3.6262528896331787\n",
            "Training Iteration 3718, Loss: 2.486680507659912\n",
            "Training Iteration 3719, Loss: 4.286255359649658\n",
            "Training Iteration 3720, Loss: 5.138359069824219\n",
            "Training Iteration 3721, Loss: 5.926686763763428\n",
            "Training Iteration 3722, Loss: 4.017749309539795\n",
            "Training Iteration 3723, Loss: 3.5967230796813965\n",
            "Training Iteration 3724, Loss: 4.737497329711914\n",
            "Training Iteration 3725, Loss: 4.015628814697266\n",
            "Training Iteration 3726, Loss: 3.0176873207092285\n",
            "Training Iteration 3727, Loss: 5.113443851470947\n",
            "Training Iteration 3728, Loss: 2.784487724304199\n",
            "Training Iteration 3729, Loss: 3.4528896808624268\n",
            "Training Iteration 3730, Loss: 6.129312038421631\n",
            "Training Iteration 3731, Loss: 6.064006328582764\n",
            "Training Iteration 3732, Loss: 5.044776916503906\n",
            "Training Iteration 3733, Loss: 3.7273714542388916\n",
            "Training Iteration 3734, Loss: 4.237862586975098\n",
            "Training Iteration 3735, Loss: 3.118603229522705\n",
            "Training Iteration 3736, Loss: 5.647423267364502\n",
            "Training Iteration 3737, Loss: 3.875657320022583\n",
            "Training Iteration 3738, Loss: 3.4589664936065674\n",
            "Training Iteration 3739, Loss: 4.297811508178711\n",
            "Training Iteration 3740, Loss: 5.520864009857178\n",
            "Training Iteration 3741, Loss: 5.969013690948486\n",
            "Training Iteration 3742, Loss: 4.4568400382995605\n",
            "Training Iteration 3743, Loss: 3.146923303604126\n",
            "Training Iteration 3744, Loss: 6.070644855499268\n",
            "Training Iteration 3745, Loss: 6.822192192077637\n",
            "Training Iteration 3746, Loss: 4.309547424316406\n",
            "Training Iteration 3747, Loss: 4.403285980224609\n",
            "Training Iteration 3748, Loss: 5.108390808105469\n",
            "Training Iteration 3749, Loss: 4.267432689666748\n",
            "Training Iteration 3750, Loss: 5.529706001281738\n",
            "Training Iteration 3751, Loss: 5.713318824768066\n",
            "Training Iteration 3752, Loss: 5.596489429473877\n",
            "Training Iteration 3753, Loss: 4.769472122192383\n",
            "Training Iteration 3754, Loss: 5.397921085357666\n",
            "Training Iteration 3755, Loss: 4.668488025665283\n",
            "Training Iteration 3756, Loss: 4.015261650085449\n",
            "Training Iteration 3757, Loss: 5.077106475830078\n",
            "Training Iteration 3758, Loss: 2.9092445373535156\n",
            "Training Iteration 3759, Loss: 5.068160533905029\n",
            "Training Iteration 3760, Loss: 4.253572940826416\n",
            "Training Iteration 3761, Loss: 2.691530704498291\n",
            "Training Iteration 3762, Loss: 2.9461135864257812\n",
            "Training Iteration 3763, Loss: 4.761998176574707\n",
            "Training Iteration 3764, Loss: 1.8355774879455566\n",
            "Training Iteration 3765, Loss: 4.4750285148620605\n",
            "Training Iteration 3766, Loss: 2.592283248901367\n",
            "Training Iteration 3767, Loss: 4.27041482925415\n",
            "Training Iteration 3768, Loss: 2.062086343765259\n",
            "Training Iteration 3769, Loss: 4.612157821655273\n",
            "Training Iteration 3770, Loss: 4.227487087249756\n",
            "Training Iteration 3771, Loss: 2.772756338119507\n",
            "Training Iteration 3772, Loss: 3.8763246536254883\n",
            "Training Iteration 3773, Loss: 3.515340805053711\n",
            "Training Iteration 3774, Loss: 4.057208061218262\n",
            "Training Iteration 3775, Loss: 1.8986929655075073\n",
            "Training Iteration 3776, Loss: 3.2735722064971924\n",
            "Training Iteration 3777, Loss: 3.9593420028686523\n",
            "Training Iteration 3778, Loss: 4.039677143096924\n",
            "Training Iteration 3779, Loss: 2.939608573913574\n",
            "Training Iteration 3780, Loss: 3.2463698387145996\n",
            "Training Iteration 3781, Loss: 2.978403091430664\n",
            "Training Iteration 3782, Loss: 2.0545408725738525\n",
            "Training Iteration 3783, Loss: 4.0494303703308105\n",
            "Training Iteration 3784, Loss: 4.064332962036133\n",
            "Training Iteration 3785, Loss: 7.01967191696167\n",
            "Training Iteration 3786, Loss: 4.769745826721191\n",
            "Training Iteration 3787, Loss: 4.285036563873291\n",
            "Training Iteration 3788, Loss: 4.169683933258057\n",
            "Training Iteration 3789, Loss: 3.365111827850342\n",
            "Training Iteration 3790, Loss: 7.020092010498047\n",
            "Training Iteration 3791, Loss: 3.6180193424224854\n",
            "Training Iteration 3792, Loss: 4.533099174499512\n",
            "Training Iteration 3793, Loss: 4.749900817871094\n",
            "Training Iteration 3794, Loss: 3.2993175983428955\n",
            "Training Iteration 3795, Loss: 2.6060118675231934\n",
            "Training Iteration 3796, Loss: 7.742472171783447\n",
            "Training Iteration 3797, Loss: 3.0823957920074463\n",
            "Training Iteration 3798, Loss: 5.111813068389893\n",
            "Training Iteration 3799, Loss: 6.429806709289551\n",
            "Training Iteration 3800, Loss: 3.21993350982666\n",
            "Training Iteration 3801, Loss: 2.761280059814453\n",
            "Training Iteration 3802, Loss: 2.954709529876709\n",
            "Training Iteration 3803, Loss: 5.295645713806152\n",
            "Training Iteration 3804, Loss: 3.5490753650665283\n",
            "Training Iteration 3805, Loss: 3.02754807472229\n",
            "Training Iteration 3806, Loss: 2.4223408699035645\n",
            "Training Iteration 3807, Loss: 3.0938684940338135\n",
            "Training Iteration 3808, Loss: 4.402367115020752\n",
            "Training Iteration 3809, Loss: 2.151261806488037\n",
            "Training Iteration 3810, Loss: 4.001825332641602\n",
            "Training Iteration 3811, Loss: 5.973598003387451\n",
            "Training Iteration 3812, Loss: 6.029482841491699\n",
            "Training Iteration 3813, Loss: 3.3664472103118896\n",
            "Training Iteration 3814, Loss: 4.848304748535156\n",
            "Training Iteration 3815, Loss: 5.418169021606445\n",
            "Training Iteration 3816, Loss: 4.142277717590332\n",
            "Training Iteration 3817, Loss: 5.955732345581055\n",
            "Training Iteration 3818, Loss: 4.906470775604248\n",
            "Training Iteration 3819, Loss: 3.295546293258667\n",
            "Training Iteration 3820, Loss: 3.8935868740081787\n",
            "Training Iteration 3821, Loss: 6.429351806640625\n",
            "Training Iteration 3822, Loss: 7.663473129272461\n",
            "Training Iteration 3823, Loss: 5.043739318847656\n",
            "Training Iteration 3824, Loss: 3.8817501068115234\n",
            "Training Iteration 3825, Loss: 4.672302722930908\n",
            "Training Iteration 3826, Loss: 4.091876983642578\n",
            "Training Iteration 3827, Loss: 4.210433483123779\n",
            "Training Iteration 3828, Loss: 3.228882312774658\n",
            "Training Iteration 3829, Loss: 2.488349437713623\n",
            "Training Iteration 3830, Loss: 3.1669106483459473\n",
            "Training Iteration 3831, Loss: 3.6795732975006104\n",
            "Training Iteration 3832, Loss: 6.983846187591553\n",
            "Training Iteration 3833, Loss: 3.2461609840393066\n",
            "Training Iteration 3834, Loss: 2.9927077293395996\n",
            "Training Iteration 3835, Loss: 3.582374334335327\n",
            "Training Iteration 3836, Loss: 8.727684020996094\n",
            "Training Iteration 3837, Loss: 4.643714427947998\n",
            "Training Iteration 3838, Loss: 4.932311534881592\n",
            "Training Iteration 3839, Loss: 6.915334224700928\n",
            "Training Iteration 3840, Loss: 5.4227423667907715\n",
            "Training Iteration 3841, Loss: 3.895216941833496\n",
            "Training Iteration 3842, Loss: 3.561305522918701\n",
            "Training Iteration 3843, Loss: 4.916618347167969\n",
            "Training Iteration 3844, Loss: 2.5181326866149902\n",
            "Training Iteration 3845, Loss: 5.621931552886963\n",
            "Training Iteration 3846, Loss: 4.324748992919922\n",
            "Training Iteration 3847, Loss: 2.9654808044433594\n",
            "Training Iteration 3848, Loss: 2.8879194259643555\n",
            "Training Iteration 3849, Loss: 4.658767223358154\n",
            "Training Iteration 3850, Loss: 5.308417797088623\n",
            "Training Iteration 3851, Loss: 3.2382917404174805\n",
            "Training Iteration 3852, Loss: 5.222051620483398\n",
            "Training Iteration 3853, Loss: 3.1788346767425537\n",
            "Training Iteration 3854, Loss: 4.663535118103027\n",
            "Training Iteration 3855, Loss: 4.201590061187744\n",
            "Training Iteration 3856, Loss: 4.801704406738281\n",
            "Training Iteration 3857, Loss: 3.530200242996216\n",
            "Training Iteration 3858, Loss: 4.022487640380859\n",
            "Training Iteration 3859, Loss: 1.7976897954940796\n",
            "Training Iteration 3860, Loss: 5.085525035858154\n",
            "Training Iteration 3861, Loss: 7.185638427734375\n",
            "Training Iteration 3862, Loss: 5.706485271453857\n",
            "Training Iteration 3863, Loss: 3.4534659385681152\n",
            "Training Iteration 3864, Loss: 3.6727523803710938\n",
            "Training Iteration 3865, Loss: 2.758333206176758\n",
            "Training Iteration 3866, Loss: 7.139585018157959\n",
            "Training Iteration 3867, Loss: 2.1053695678710938\n",
            "Training Iteration 3868, Loss: 3.3917269706726074\n",
            "Training Iteration 3869, Loss: 6.614541530609131\n",
            "Training Iteration 3870, Loss: 4.743785381317139\n",
            "Training Iteration 3871, Loss: 4.097775459289551\n",
            "Training Iteration 3872, Loss: 6.566493034362793\n",
            "Training Iteration 3873, Loss: 5.31524658203125\n",
            "Training Iteration 3874, Loss: 5.543770790100098\n",
            "Training Iteration 3875, Loss: 4.7191267013549805\n",
            "Training Iteration 3876, Loss: 5.831630229949951\n",
            "Training Iteration 3877, Loss: 3.9518232345581055\n",
            "Training Iteration 3878, Loss: 4.598601818084717\n",
            "Training Iteration 3879, Loss: 3.0898683071136475\n",
            "Training Iteration 3880, Loss: 6.155082702636719\n",
            "Training Iteration 3881, Loss: 3.348067283630371\n",
            "Training Iteration 3882, Loss: 1.6686913967132568\n",
            "Training Iteration 3883, Loss: 7.429445743560791\n",
            "Training Iteration 3884, Loss: 6.220476150512695\n",
            "Training Iteration 3885, Loss: 4.414186954498291\n",
            "Training Iteration 3886, Loss: 13.114883422851562\n",
            "Training Iteration 3887, Loss: 8.17391300201416\n",
            "Training Iteration 3888, Loss: 7.3334431648254395\n",
            "Training Iteration 3889, Loss: 6.6957831382751465\n",
            "Training Iteration 3890, Loss: 6.091801643371582\n",
            "Training Iteration 3891, Loss: 3.355051279067993\n",
            "Training Iteration 3892, Loss: 4.057206153869629\n",
            "Training Iteration 3893, Loss: 5.832903861999512\n",
            "Training Iteration 3894, Loss: 8.307552337646484\n",
            "Training Iteration 3895, Loss: 4.465496063232422\n",
            "Training Iteration 3896, Loss: 5.297327041625977\n",
            "Training Iteration 3897, Loss: 2.749433994293213\n",
            "Training Iteration 3898, Loss: 6.602141380310059\n",
            "Training Iteration 3899, Loss: 4.684301853179932\n",
            "Training Iteration 3900, Loss: 3.3931009769439697\n",
            "Training Iteration 3901, Loss: 5.084945201873779\n",
            "Training Iteration 3902, Loss: 4.839480400085449\n",
            "Training Iteration 3903, Loss: 4.736319065093994\n",
            "Training Iteration 3904, Loss: 3.3978841304779053\n",
            "Training Iteration 3905, Loss: 3.3979299068450928\n",
            "Training Iteration 3906, Loss: 3.7831380367279053\n",
            "Training Iteration 3907, Loss: 3.695141315460205\n",
            "Training Iteration 3908, Loss: 5.37418270111084\n",
            "Training Iteration 3909, Loss: 7.129239559173584\n",
            "Training Iteration 3910, Loss: 7.632632255554199\n",
            "Training Iteration 3911, Loss: 3.6511082649230957\n",
            "Training Iteration 3912, Loss: 7.433440685272217\n",
            "Training Iteration 3913, Loss: 4.581244945526123\n",
            "Training Iteration 3914, Loss: 2.2373130321502686\n",
            "Training Iteration 3915, Loss: 4.275106430053711\n",
            "Training Iteration 3916, Loss: 3.643954038619995\n",
            "Training Iteration 3917, Loss: 3.9636831283569336\n",
            "Training Iteration 3918, Loss: 7.382768630981445\n",
            "Training Iteration 3919, Loss: 6.649656772613525\n",
            "Training Iteration 3920, Loss: 2.1339211463928223\n",
            "Training Iteration 3921, Loss: 5.510101318359375\n",
            "Training Iteration 3922, Loss: 8.297582626342773\n",
            "Training Iteration 3923, Loss: 7.524171352386475\n",
            "Training Iteration 3924, Loss: 5.0249481201171875\n",
            "Training Iteration 3925, Loss: 5.494627952575684\n",
            "Training Iteration 3926, Loss: 6.45127534866333\n",
            "Training Iteration 3927, Loss: 3.530468463897705\n",
            "Training Iteration 3928, Loss: 6.261465549468994\n",
            "Training Iteration 3929, Loss: 3.7405431270599365\n",
            "Training Iteration 3930, Loss: 6.665251731872559\n",
            "Training Iteration 3931, Loss: 2.9688189029693604\n",
            "Training Iteration 3932, Loss: 4.597854137420654\n",
            "Training Iteration 3933, Loss: 4.767996788024902\n",
            "Training Iteration 3934, Loss: 4.808239936828613\n",
            "Training Iteration 3935, Loss: 3.2747981548309326\n",
            "Training Iteration 3936, Loss: 2.551384687423706\n",
            "Training Iteration 3937, Loss: 4.889863967895508\n",
            "Training Iteration 3938, Loss: 5.725849151611328\n",
            "Training Iteration 3939, Loss: 4.017751216888428\n",
            "Training Iteration 3940, Loss: 9.543763160705566\n",
            "Training Iteration 3941, Loss: 2.183135747909546\n",
            "Training Iteration 3942, Loss: 3.774200201034546\n",
            "Training Iteration 3943, Loss: 5.380159854888916\n",
            "Training Iteration 3944, Loss: 5.456202030181885\n",
            "Training Iteration 3945, Loss: 4.154818534851074\n",
            "Training Iteration 3946, Loss: 3.604079246520996\n",
            "Training Iteration 3947, Loss: 3.623894214630127\n",
            "Training Iteration 3948, Loss: 5.066793441772461\n",
            "Training Iteration 3949, Loss: 4.20759391784668\n",
            "Training Iteration 3950, Loss: 3.314211130142212\n",
            "Training Iteration 3951, Loss: 4.211232662200928\n",
            "Training Iteration 3952, Loss: 3.7692339420318604\n",
            "Training Iteration 3953, Loss: 3.271181106567383\n",
            "Training Iteration 3954, Loss: 3.04502272605896\n",
            "Training Iteration 3955, Loss: 4.459358215332031\n",
            "Training Iteration 3956, Loss: 7.528989791870117\n",
            "Training Iteration 3957, Loss: 7.286426544189453\n",
            "Training Iteration 3958, Loss: 2.904496192932129\n",
            "Training Iteration 3959, Loss: 4.799071788787842\n",
            "Training Iteration 3960, Loss: 6.442102909088135\n",
            "Training Iteration 3961, Loss: 3.3149640560150146\n",
            "Training Iteration 3962, Loss: 3.821310520172119\n",
            "Training Iteration 3963, Loss: 4.640414714813232\n",
            "Training Iteration 3964, Loss: 5.001022815704346\n",
            "Training Iteration 3965, Loss: 7.262618541717529\n",
            "Training Iteration 3966, Loss: 2.7720913887023926\n",
            "Training Iteration 3967, Loss: 3.8314085006713867\n",
            "Training Iteration 3968, Loss: 3.3580129146575928\n",
            "Training Iteration 3969, Loss: 3.105018377304077\n",
            "Training Iteration 3970, Loss: 8.32908821105957\n",
            "Training Iteration 3971, Loss: 2.6196279525756836\n",
            "Training Iteration 3972, Loss: 4.079589366912842\n",
            "Training Iteration 3973, Loss: 4.836686611175537\n",
            "Training Iteration 3974, Loss: 4.414977550506592\n",
            "Training Iteration 3975, Loss: 5.515313625335693\n",
            "Training Iteration 3976, Loss: 3.6897950172424316\n",
            "Training Iteration 3977, Loss: 4.194095611572266\n",
            "Training Iteration 3978, Loss: 9.324846267700195\n",
            "Training Iteration 3979, Loss: 7.080982685089111\n",
            "Training Iteration 3980, Loss: 5.264098644256592\n",
            "Training Iteration 3981, Loss: 6.8315653800964355\n",
            "Training Iteration 3982, Loss: 5.135953426361084\n",
            "Training Iteration 3983, Loss: 1.8387476205825806\n",
            "Training Iteration 3984, Loss: 5.280275344848633\n",
            "Training Iteration 3985, Loss: 1.9469516277313232\n",
            "Training Iteration 3986, Loss: 5.24455451965332\n",
            "Training Iteration 3987, Loss: 5.095248222351074\n",
            "Training Iteration 3988, Loss: 2.477459669113159\n",
            "Training Iteration 3989, Loss: 7.033167839050293\n",
            "Training Iteration 3990, Loss: 1.9673312902450562\n",
            "Training Iteration 3991, Loss: 4.088912010192871\n",
            "Training Iteration 3992, Loss: 3.537787675857544\n",
            "Training Iteration 3993, Loss: 4.202864646911621\n",
            "Training Iteration 3994, Loss: 5.931313514709473\n",
            "Training Iteration 3995, Loss: 3.5459792613983154\n",
            "Training Iteration 3996, Loss: 5.184839248657227\n",
            "Training Iteration 3997, Loss: 6.654094219207764\n",
            "Training Iteration 3998, Loss: 4.7865495681762695\n",
            "Training Iteration 3999, Loss: 3.9550294876098633\n",
            "Training Iteration 4000, Loss: 5.464153289794922\n",
            "Training Iteration 4001, Loss: 6.060800552368164\n",
            "Training Iteration 4002, Loss: 2.0042622089385986\n",
            "Training Iteration 4003, Loss: 3.8824315071105957\n",
            "Training Iteration 4004, Loss: 5.096031665802002\n",
            "Training Iteration 4005, Loss: 4.654212951660156\n",
            "Training Iteration 4006, Loss: 8.510464668273926\n",
            "Training Iteration 4007, Loss: 3.8095901012420654\n",
            "Training Iteration 4008, Loss: 4.983806610107422\n",
            "Training Iteration 4009, Loss: 4.030524253845215\n",
            "Training Iteration 4010, Loss: 5.974050998687744\n",
            "Training Iteration 4011, Loss: 6.249969482421875\n",
            "Training Iteration 4012, Loss: 3.582019805908203\n",
            "Training Iteration 4013, Loss: 3.6314191818237305\n",
            "Training Iteration 4014, Loss: 6.4534502029418945\n",
            "Training Iteration 4015, Loss: 6.312380790710449\n",
            "Training Iteration 4016, Loss: 7.251997470855713\n",
            "Training Iteration 4017, Loss: 2.919250726699829\n",
            "Training Iteration 4018, Loss: 5.362031936645508\n",
            "Training Iteration 4019, Loss: 5.06834077835083\n",
            "Training Iteration 4020, Loss: 3.633533239364624\n",
            "Training Iteration 4021, Loss: 5.325057029724121\n",
            "Training Iteration 4022, Loss: 4.114920616149902\n",
            "Training Iteration 4023, Loss: 4.73145055770874\n",
            "Training Iteration 4024, Loss: 4.2471232414245605\n",
            "Training Iteration 4025, Loss: 5.772097587585449\n",
            "Training Iteration 4026, Loss: 4.03517484664917\n",
            "Training Iteration 4027, Loss: 4.093277931213379\n",
            "Training Iteration 4028, Loss: 4.233906269073486\n",
            "Training Iteration 4029, Loss: 3.176161289215088\n",
            "Training Iteration 4030, Loss: 5.864578723907471\n",
            "Training Iteration 4031, Loss: 5.258545875549316\n",
            "Training Iteration 4032, Loss: 8.766268730163574\n",
            "Training Iteration 4033, Loss: 8.57373046875\n",
            "Training Iteration 4034, Loss: 3.4963014125823975\n",
            "Training Iteration 4035, Loss: 3.46274471282959\n",
            "Training Iteration 4036, Loss: 6.039848804473877\n",
            "Training Iteration 4037, Loss: 4.347731590270996\n",
            "Training Iteration 4038, Loss: 4.5437421798706055\n",
            "Training Iteration 4039, Loss: 5.266660213470459\n",
            "Training Iteration 4040, Loss: 6.903343677520752\n",
            "Training Iteration 4041, Loss: 4.863241195678711\n",
            "Training Iteration 4042, Loss: 4.928643226623535\n",
            "Training Iteration 4043, Loss: 4.142526626586914\n",
            "Training Iteration 4044, Loss: 3.9799444675445557\n",
            "Training Iteration 4045, Loss: 4.814671516418457\n",
            "Training Iteration 4046, Loss: 7.947821140289307\n",
            "Training Iteration 4047, Loss: 5.309787273406982\n",
            "Training Iteration 4048, Loss: 8.220385551452637\n",
            "Training Iteration 4049, Loss: 12.544867515563965\n",
            "Training Iteration 4050, Loss: 6.182486534118652\n",
            "Training Iteration 4051, Loss: 2.2442240715026855\n",
            "Training Iteration 4052, Loss: 2.8605616092681885\n",
            "Training Iteration 4053, Loss: 4.334870338439941\n",
            "Training Iteration 4054, Loss: 7.74951696395874\n",
            "Training Iteration 4055, Loss: 4.329109191894531\n",
            "Training Iteration 4056, Loss: 4.476928234100342\n",
            "Training Iteration 4057, Loss: 5.574646949768066\n",
            "Training Iteration 4058, Loss: 4.3179450035095215\n",
            "Training Iteration 4059, Loss: 2.44071626663208\n",
            "Training Iteration 4060, Loss: 1.6664825677871704\n",
            "Training Iteration 4061, Loss: 4.820469856262207\n",
            "Training Iteration 4062, Loss: 3.2008090019226074\n",
            "Training Iteration 4063, Loss: 8.93116569519043\n",
            "Training Iteration 4064, Loss: 2.354520320892334\n",
            "Training Iteration 4065, Loss: 4.014867305755615\n",
            "Training Iteration 4066, Loss: 9.8508882522583\n",
            "Training Iteration 4067, Loss: 8.187390327453613\n",
            "Training Iteration 4068, Loss: 4.885339260101318\n",
            "Training Iteration 4069, Loss: 5.363116264343262\n",
            "Training Iteration 4070, Loss: 4.0373101234436035\n",
            "Training Iteration 4071, Loss: 8.404854774475098\n",
            "Training Iteration 4072, Loss: 4.284192085266113\n",
            "Training Iteration 4073, Loss: 7.22268009185791\n",
            "Training Iteration 4074, Loss: 2.9292123317718506\n",
            "Training Iteration 4075, Loss: 4.169712066650391\n",
            "Training Iteration 4076, Loss: 2.5552632808685303\n",
            "Training Iteration 4077, Loss: 6.276823043823242\n",
            "Training Iteration 4078, Loss: 1.2058812379837036\n",
            "Training Iteration 4079, Loss: 6.196366786956787\n",
            "Training Iteration 4080, Loss: 9.99395751953125\n",
            "Training Iteration 4081, Loss: 7.650272846221924\n",
            "Training Iteration 4082, Loss: 9.612545013427734\n",
            "Training Iteration 4083, Loss: 5.509780406951904\n",
            "Training Iteration 4084, Loss: 5.374603271484375\n",
            "Training Iteration 4085, Loss: 5.516871452331543\n",
            "Training Iteration 4086, Loss: 1.1746699810028076\n",
            "Training Iteration 4087, Loss: 4.689056396484375\n",
            "Training Iteration 4088, Loss: 7.9367523193359375\n",
            "Training Iteration 4089, Loss: 6.719241142272949\n",
            "Training Iteration 4090, Loss: 5.017385005950928\n",
            "Training Iteration 4091, Loss: 4.996343612670898\n",
            "Training Iteration 4092, Loss: 5.582808494567871\n",
            "Training Iteration 4093, Loss: 4.940519332885742\n",
            "Training Iteration 4094, Loss: 5.947088241577148\n",
            "Training Iteration 4095, Loss: 6.0710225105285645\n",
            "Training Iteration 4096, Loss: 5.333872318267822\n",
            "Training Iteration 4097, Loss: 7.58160924911499\n",
            "Training Iteration 4098, Loss: 5.723150253295898\n",
            "Training Iteration 4099, Loss: 4.198509216308594\n",
            "Training Iteration 4100, Loss: 4.213379383087158\n",
            "Training Iteration 4101, Loss: 2.1196177005767822\n",
            "Training Iteration 4102, Loss: 5.9145684242248535\n",
            "Training Iteration 4103, Loss: 5.325870513916016\n",
            "Training Iteration 4104, Loss: 9.401378631591797\n",
            "Training Iteration 4105, Loss: 4.007726192474365\n",
            "Training Iteration 4106, Loss: 3.601123332977295\n",
            "Training Iteration 4107, Loss: 2.06032395362854\n",
            "Training Iteration 4108, Loss: 5.189794063568115\n",
            "Training Iteration 4109, Loss: 4.841438293457031\n",
            "Training Iteration 4110, Loss: 4.677098274230957\n",
            "Training Iteration 4111, Loss: 5.234806060791016\n",
            "Training Iteration 4112, Loss: 4.296548843383789\n",
            "Training Iteration 4113, Loss: 3.6192703247070312\n",
            "Training Iteration 4114, Loss: 2.9271037578582764\n",
            "Training Iteration 4115, Loss: 7.295034885406494\n",
            "Training Iteration 4116, Loss: 7.220953464508057\n",
            "Training Iteration 4117, Loss: 7.45237922668457\n",
            "Training Iteration 4118, Loss: 5.694522857666016\n",
            "Training Iteration 4119, Loss: 3.8081839084625244\n",
            "Training Iteration 4120, Loss: 6.1565260887146\n",
            "Training Iteration 4121, Loss: 5.586674690246582\n",
            "Training Iteration 4122, Loss: 6.181032657623291\n",
            "Training Iteration 4123, Loss: 6.341320037841797\n",
            "Training Iteration 4124, Loss: 2.9470086097717285\n",
            "Training Iteration 4125, Loss: 6.671503067016602\n",
            "Training Iteration 4126, Loss: 4.60360050201416\n",
            "Training Iteration 4127, Loss: 4.752023696899414\n",
            "Training Iteration 4128, Loss: 6.031640529632568\n",
            "Training Iteration 4129, Loss: 2.5966227054595947\n",
            "Training Iteration 4130, Loss: 3.234549045562744\n",
            "Training Iteration 4131, Loss: 5.627559185028076\n",
            "Training Iteration 4132, Loss: 7.939716815948486\n",
            "Training Iteration 4133, Loss: 4.006782054901123\n",
            "Training Iteration 4134, Loss: 3.982292652130127\n",
            "Training Iteration 4135, Loss: 5.17832612991333\n",
            "Training Iteration 4136, Loss: 2.439784049987793\n",
            "Training Iteration 4137, Loss: 5.7877349853515625\n",
            "Training Iteration 4138, Loss: 5.274008750915527\n",
            "Training Iteration 4139, Loss: 5.443946838378906\n",
            "Training Iteration 4140, Loss: 4.860050201416016\n",
            "Training Iteration 4141, Loss: 2.8545565605163574\n",
            "Training Iteration 4142, Loss: 3.225735664367676\n",
            "Training Iteration 4143, Loss: 5.283083438873291\n",
            "Training Iteration 4144, Loss: 6.251211166381836\n",
            "Training Iteration 4145, Loss: 4.038374900817871\n",
            "Training Iteration 4146, Loss: 6.3814697265625\n",
            "Training Iteration 4147, Loss: 4.71951150894165\n",
            "Training Iteration 4148, Loss: 3.354203462600708\n",
            "Training Iteration 4149, Loss: 3.6574501991271973\n",
            "Training Iteration 4150, Loss: 5.339674949645996\n",
            "Training Iteration 4151, Loss: 6.859204292297363\n",
            "Training Iteration 4152, Loss: 6.7769389152526855\n",
            "Training Iteration 4153, Loss: 3.261751174926758\n",
            "Training Iteration 4154, Loss: 3.597865581512451\n",
            "Training Iteration 4155, Loss: 5.820418357849121\n",
            "Training Iteration 4156, Loss: 3.5536959171295166\n",
            "Training Iteration 4157, Loss: 5.419124126434326\n",
            "Training Iteration 4158, Loss: 2.889193058013916\n",
            "Training Iteration 4159, Loss: 7.572666645050049\n",
            "Training Iteration 4160, Loss: 1.6574652194976807\n",
            "Training Iteration 4161, Loss: 3.6092119216918945\n",
            "Training Iteration 4162, Loss: 3.1928787231445312\n",
            "Training Iteration 4163, Loss: 1.0175198316574097\n",
            "Training Iteration 4164, Loss: 3.0795347690582275\n",
            "Training Iteration 4165, Loss: 0.8944350481033325\n",
            "Training Iteration 4166, Loss: 1.5022313594818115\n",
            "Training Iteration 4167, Loss: 4.98056697845459\n",
            "Training Iteration 4168, Loss: 7.375507354736328\n",
            "Training Iteration 4169, Loss: 7.841098785400391\n",
            "Training Iteration 4170, Loss: 6.58609676361084\n",
            "Training Iteration 4171, Loss: 4.436157703399658\n",
            "Training Iteration 4172, Loss: 8.218781471252441\n",
            "Training Iteration 4173, Loss: 5.719209671020508\n",
            "Training Iteration 4174, Loss: 6.446817398071289\n",
            "Training Iteration 4175, Loss: 4.290372371673584\n",
            "Training Iteration 4176, Loss: 2.4181621074676514\n",
            "Training Iteration 4177, Loss: 5.925096035003662\n",
            "Training Iteration 4178, Loss: 6.675537109375\n",
            "Training Iteration 4179, Loss: 4.49325704574585\n",
            "Training Iteration 4180, Loss: 4.137593746185303\n",
            "Training Iteration 4181, Loss: 5.671989917755127\n",
            "Training Iteration 4182, Loss: 3.3725428581237793\n",
            "Training Iteration 4183, Loss: 3.872401475906372\n",
            "Training Iteration 4184, Loss: 5.611386299133301\n",
            "Training Iteration 4185, Loss: 4.074680328369141\n",
            "Training Iteration 4186, Loss: 4.6302900314331055\n",
            "Training Iteration 4187, Loss: 4.779539585113525\n",
            "Training Iteration 4188, Loss: 6.183628082275391\n",
            "Training Iteration 4189, Loss: 2.8894827365875244\n",
            "Training Iteration 4190, Loss: 5.785422325134277\n",
            "Training Iteration 4191, Loss: 2.8760428428649902\n",
            "Training Iteration 4192, Loss: 4.620703220367432\n",
            "Training Iteration 4193, Loss: 5.537789344787598\n",
            "Training Iteration 4194, Loss: 3.8628549575805664\n",
            "Training Iteration 4195, Loss: 4.1511549949646\n",
            "Training Iteration 4196, Loss: 3.3192079067230225\n",
            "Training Iteration 4197, Loss: 6.298440456390381\n",
            "Training Iteration 4198, Loss: 4.066582202911377\n",
            "Training Iteration 4199, Loss: 1.941157341003418\n",
            "Training Iteration 4200, Loss: 5.811794757843018\n",
            "Training Iteration 4201, Loss: 4.84112548828125\n",
            "Training Iteration 4202, Loss: 3.361585855484009\n",
            "Training Iteration 4203, Loss: 8.203771591186523\n",
            "Training Iteration 4204, Loss: 4.068916320800781\n",
            "Training Iteration 4205, Loss: 4.549437046051025\n",
            "Training Iteration 4206, Loss: 3.9012017250061035\n",
            "Training Iteration 4207, Loss: 4.546443462371826\n",
            "Training Iteration 4208, Loss: 3.709291934967041\n",
            "Training Iteration 4209, Loss: 4.5846357345581055\n",
            "Training Iteration 4210, Loss: 2.920485734939575\n",
            "Training Iteration 4211, Loss: 5.079310894012451\n",
            "Training Iteration 4212, Loss: 3.3401763439178467\n",
            "Training Iteration 4213, Loss: 5.896670818328857\n",
            "Training Iteration 4214, Loss: 4.262794494628906\n",
            "Training Iteration 4215, Loss: 3.31770396232605\n",
            "Training Iteration 4216, Loss: 2.7573463916778564\n",
            "Training Iteration 4217, Loss: 6.695315837860107\n",
            "Training Iteration 4218, Loss: 6.655997276306152\n",
            "Training Iteration 4219, Loss: 1.9644702672958374\n",
            "Training Iteration 4220, Loss: 6.6564154624938965\n",
            "Training Iteration 4221, Loss: 8.019594192504883\n",
            "Training Iteration 4222, Loss: 4.378292083740234\n",
            "Training Iteration 4223, Loss: 3.2794737815856934\n",
            "Training Iteration 4224, Loss: 1.7119861841201782\n",
            "Training Iteration 4225, Loss: 6.063131809234619\n",
            "Training Iteration 4226, Loss: 1.745083212852478\n",
            "Training Iteration 4227, Loss: 3.7993621826171875\n",
            "Training Iteration 4228, Loss: 3.7785279750823975\n",
            "Training Iteration 4229, Loss: 6.045131683349609\n",
            "Training Iteration 4230, Loss: 2.894852638244629\n",
            "Training Iteration 4231, Loss: 4.4657182693481445\n",
            "Training Iteration 4232, Loss: 7.415427207946777\n",
            "Training Iteration 4233, Loss: 7.47889518737793\n",
            "Training Iteration 4234, Loss: 4.26894474029541\n",
            "Training Iteration 4235, Loss: 5.224930763244629\n",
            "Training Iteration 4236, Loss: 4.628644943237305\n",
            "Training Iteration 4237, Loss: 4.930798053741455\n",
            "Training Iteration 4238, Loss: 3.1141133308410645\n",
            "Training Iteration 4239, Loss: 2.0123350620269775\n",
            "Training Iteration 4240, Loss: 4.075990676879883\n",
            "Training Iteration 4241, Loss: 3.43707537651062\n",
            "Training Iteration 4242, Loss: 1.470983624458313\n",
            "Training Iteration 4243, Loss: 5.6582207679748535\n",
            "Training Iteration 4244, Loss: 3.372249126434326\n",
            "Training Iteration 4245, Loss: 2.3236920833587646\n",
            "Training Iteration 4246, Loss: 5.094229221343994\n",
            "Training Iteration 4247, Loss: 3.1517295837402344\n",
            "Training Iteration 4248, Loss: 3.3696627616882324\n",
            "Training Iteration 4249, Loss: 8.980793952941895\n",
            "Training Iteration 4250, Loss: 3.620218515396118\n",
            "Training Iteration 4251, Loss: 3.8456227779388428\n",
            "Training Iteration 4252, Loss: 9.479331970214844\n",
            "Training Iteration 4253, Loss: 9.126649856567383\n",
            "Training Iteration 4254, Loss: 5.894302845001221\n",
            "Training Iteration 4255, Loss: 4.548786640167236\n",
            "Training Iteration 4256, Loss: 6.235239505767822\n",
            "Training Iteration 4257, Loss: 4.283010959625244\n",
            "Training Iteration 4258, Loss: 3.213840961456299\n",
            "Training Iteration 4259, Loss: 4.899113178253174\n",
            "Training Iteration 4260, Loss: 5.787150859832764\n",
            "Training Iteration 4261, Loss: 2.108924627304077\n",
            "Training Iteration 4262, Loss: 3.3157012462615967\n",
            "Training Iteration 4263, Loss: 2.8305182456970215\n",
            "Training Iteration 4264, Loss: 4.591680526733398\n",
            "Training Iteration 4265, Loss: 9.750591278076172\n",
            "Training Iteration 4266, Loss: 2.33553409576416\n",
            "Training Iteration 4267, Loss: 2.976090908050537\n",
            "Training Iteration 4268, Loss: 6.955389976501465\n",
            "Training Iteration 4269, Loss: 3.3179290294647217\n",
            "Training Iteration 4270, Loss: 5.210700988769531\n",
            "Training Iteration 4271, Loss: 6.371907711029053\n",
            "Training Iteration 4272, Loss: 3.8004472255706787\n",
            "Training Iteration 4273, Loss: 3.9528489112854004\n",
            "Training Iteration 4274, Loss: 3.422311544418335\n",
            "Training Iteration 4275, Loss: 5.421358108520508\n",
            "Training Iteration 4276, Loss: 5.937193870544434\n",
            "Training Iteration 4277, Loss: 4.109004974365234\n",
            "Training Iteration 4278, Loss: 5.679373741149902\n",
            "Training Iteration 4279, Loss: 4.449017524719238\n",
            "Training Iteration 4280, Loss: 3.521141290664673\n",
            "Training Iteration 4281, Loss: 4.99700403213501\n",
            "Training Iteration 4282, Loss: 5.262015342712402\n",
            "Training Iteration 4283, Loss: 5.0150556564331055\n",
            "Training Iteration 4284, Loss: 3.3898227214813232\n",
            "Training Iteration 4285, Loss: 4.491706848144531\n",
            "Training Iteration 4286, Loss: 5.806411266326904\n",
            "Training Iteration 4287, Loss: 2.86259388923645\n",
            "Training Iteration 4288, Loss: 3.056522846221924\n",
            "Training Iteration 4289, Loss: 4.06071662902832\n",
            "Training Iteration 4290, Loss: 3.7966763973236084\n",
            "Training Iteration 4291, Loss: 8.565960884094238\n",
            "Training Iteration 4292, Loss: 2.060765266418457\n",
            "Training Iteration 4293, Loss: 3.4636030197143555\n",
            "Training Iteration 4294, Loss: 2.6849241256713867\n",
            "Training Iteration 4295, Loss: 3.3399057388305664\n",
            "Training Iteration 4296, Loss: 7.164588928222656\n",
            "Training Iteration 4297, Loss: 4.976805210113525\n",
            "Training Iteration 4298, Loss: 4.248317718505859\n",
            "Training Iteration 4299, Loss: 7.873870372772217\n",
            "Training Iteration 4300, Loss: 3.5694243907928467\n",
            "Training Iteration 4301, Loss: 2.9808197021484375\n",
            "Training Iteration 4302, Loss: 4.303654670715332\n",
            "Training Iteration 4303, Loss: 5.294021129608154\n",
            "Training Iteration 4304, Loss: 9.074711799621582\n",
            "Training Iteration 4305, Loss: 3.611755847930908\n",
            "Training Iteration 4306, Loss: 7.440145492553711\n",
            "Training Iteration 4307, Loss: 6.103747844696045\n",
            "Training Iteration 4308, Loss: 6.144031047821045\n",
            "Training Iteration 4309, Loss: 9.76821517944336\n",
            "Training Iteration 4310, Loss: 10.27660083770752\n",
            "Training Iteration 4311, Loss: 11.538887023925781\n",
            "Training Iteration 4312, Loss: 10.652725219726562\n",
            "Training Iteration 4313, Loss: 8.695513725280762\n",
            "Training Iteration 4314, Loss: 6.116137504577637\n",
            "Training Iteration 4315, Loss: 9.280515670776367\n",
            "Training Iteration 4316, Loss: 2.6483054161071777\n",
            "Training Iteration 4317, Loss: 5.006701469421387\n",
            "Training Iteration 4318, Loss: 5.296553611755371\n",
            "Training Iteration 4319, Loss: 4.825634956359863\n",
            "Training Iteration 4320, Loss: 2.140997886657715\n",
            "Training Iteration 4321, Loss: 7.049935817718506\n",
            "Training Iteration 4322, Loss: 4.598174095153809\n",
            "Training Iteration 4323, Loss: 6.4399495124816895\n",
            "Training Iteration 4324, Loss: 1.858999490737915\n",
            "Training Iteration 4325, Loss: 4.749035358428955\n",
            "Training Iteration 4326, Loss: 5.4488444328308105\n",
            "Training Iteration 4327, Loss: 4.908483028411865\n",
            "Training Iteration 4328, Loss: 5.285583972930908\n",
            "Training Iteration 4329, Loss: 8.5545015335083\n",
            "Training Iteration 4330, Loss: 6.015785217285156\n",
            "Training Iteration 4331, Loss: 9.433023452758789\n",
            "Training Iteration 4332, Loss: 2.845233678817749\n",
            "Training Iteration 4333, Loss: 4.973219394683838\n",
            "Training Iteration 4334, Loss: 2.204893112182617\n",
            "Training Iteration 4335, Loss: 4.123683452606201\n",
            "Training Iteration 4336, Loss: 3.993553876876831\n",
            "Training Iteration 4337, Loss: 8.755163192749023\n",
            "Training Iteration 4338, Loss: 7.560952663421631\n",
            "Training Iteration 4339, Loss: 4.675244331359863\n",
            "Training Iteration 4340, Loss: 6.223521709442139\n",
            "Training Iteration 4341, Loss: 5.340109825134277\n",
            "Training Iteration 4342, Loss: 3.2820968627929688\n",
            "Training Iteration 4343, Loss: 3.3034512996673584\n",
            "Training Iteration 4344, Loss: 8.970643997192383\n",
            "Training Iteration 4345, Loss: 9.306642532348633\n",
            "Training Iteration 4346, Loss: 2.7411320209503174\n",
            "Training Iteration 4347, Loss: 5.150118827819824\n",
            "Training Iteration 4348, Loss: 5.179327964782715\n",
            "Training Iteration 4349, Loss: 4.05470609664917\n",
            "Training Iteration 4350, Loss: 4.827833652496338\n",
            "Training Iteration 4351, Loss: 4.305637836456299\n",
            "Training Iteration 4352, Loss: 4.424847602844238\n",
            "Training Iteration 4353, Loss: 2.8012280464172363\n",
            "Training Iteration 4354, Loss: 4.317729949951172\n",
            "Training Iteration 4355, Loss: 2.0654349327087402\n",
            "Training Iteration 4356, Loss: 3.4520797729492188\n",
            "Training Iteration 4357, Loss: 3.1224279403686523\n",
            "Training Iteration 4358, Loss: 1.2340177297592163\n",
            "Training Iteration 4359, Loss: 6.718037128448486\n",
            "Training Iteration 4360, Loss: 4.673724174499512\n",
            "Training Iteration 4361, Loss: 3.584993362426758\n",
            "Training Iteration 4362, Loss: 6.852173805236816\n",
            "Training Iteration 4363, Loss: 3.1331515312194824\n",
            "Training Iteration 4364, Loss: 4.469018459320068\n",
            "Training Iteration 4365, Loss: 2.9700331687927246\n",
            "Training Iteration 4366, Loss: 6.28646183013916\n",
            "Training Iteration 4367, Loss: 6.629870891571045\n",
            "Training Iteration 4368, Loss: 9.258891105651855\n",
            "Training Iteration 4369, Loss: 4.638830661773682\n",
            "Training Iteration 4370, Loss: 5.846640110015869\n",
            "Training Iteration 4371, Loss: 5.261277198791504\n",
            "Training Iteration 4372, Loss: 3.9127607345581055\n",
            "Training Iteration 4373, Loss: 3.147270679473877\n",
            "Training Iteration 4374, Loss: 3.766371726989746\n",
            "Training Iteration 4375, Loss: 5.380754470825195\n",
            "Training Iteration 4376, Loss: 2.6976962089538574\n",
            "Training Iteration 4377, Loss: 5.2535271644592285\n",
            "Training Iteration 4378, Loss: 2.461862564086914\n",
            "Training Iteration 4379, Loss: 6.697146415710449\n",
            "Training Iteration 4380, Loss: 5.314493179321289\n",
            "Training Iteration 4381, Loss: 5.667416095733643\n",
            "Training Iteration 4382, Loss: 3.6096746921539307\n",
            "Training Iteration 4383, Loss: 3.329738140106201\n",
            "Training Iteration 4384, Loss: 2.844454526901245\n",
            "Training Iteration 4385, Loss: 2.5419721603393555\n",
            "Training Iteration 4386, Loss: 10.202914237976074\n",
            "Training Iteration 4387, Loss: 3.4714252948760986\n",
            "Training Iteration 4388, Loss: 3.6284663677215576\n",
            "Training Iteration 4389, Loss: 4.422706604003906\n",
            "Training Iteration 4390, Loss: 3.8284974098205566\n",
            "Training Iteration 4391, Loss: 4.134170055389404\n",
            "Training Iteration 4392, Loss: 6.201761245727539\n",
            "Training Iteration 4393, Loss: 6.091525077819824\n",
            "Training Iteration 4394, Loss: 6.468251705169678\n",
            "Training Iteration 4395, Loss: 4.588830471038818\n",
            "Training Iteration 4396, Loss: 5.943728923797607\n",
            "Training Iteration 4397, Loss: 1.381827712059021\n",
            "Training Iteration 4398, Loss: 4.701010704040527\n",
            "Training Iteration 4399, Loss: 5.580277919769287\n",
            "Training Iteration 4400, Loss: 3.848393440246582\n",
            "Training Iteration 4401, Loss: 5.0384297370910645\n",
            "Training Iteration 4402, Loss: 3.7722105979919434\n",
            "Training Iteration 4403, Loss: 2.8168091773986816\n",
            "Training Iteration 4404, Loss: 2.651090145111084\n",
            "Training Iteration 4405, Loss: 5.262338161468506\n",
            "Training Iteration 4406, Loss: 5.8380022048950195\n",
            "Training Iteration 4407, Loss: 7.197305679321289\n",
            "Training Iteration 4408, Loss: 6.321171760559082\n",
            "Training Iteration 4409, Loss: 3.578341484069824\n",
            "Training Iteration 4410, Loss: 4.925688743591309\n",
            "Training Iteration 4411, Loss: 3.1855225563049316\n",
            "Training Iteration 4412, Loss: 6.553990840911865\n",
            "Training Iteration 4413, Loss: 7.774367809295654\n",
            "Training Iteration 4414, Loss: 7.364290714263916\n",
            "Training Iteration 4415, Loss: 5.709769248962402\n",
            "Training Iteration 4416, Loss: 3.625694990158081\n",
            "Training Iteration 4417, Loss: 4.543057441711426\n",
            "Training Iteration 4418, Loss: 3.444556713104248\n",
            "Training Iteration 4419, Loss: 5.461938381195068\n",
            "Training Iteration 4420, Loss: 4.28963565826416\n",
            "Training Iteration 4421, Loss: 4.022108554840088\n",
            "Training Iteration 4422, Loss: 8.220507621765137\n",
            "Training Iteration 4423, Loss: 2.467628002166748\n",
            "Training Iteration 4424, Loss: 2.797684907913208\n",
            "Training Iteration 4425, Loss: 3.5627546310424805\n",
            "Training Iteration 4426, Loss: 4.969386100769043\n",
            "Training Iteration 4427, Loss: 3.963146686553955\n",
            "Training Iteration 4428, Loss: 3.623353958129883\n",
            "Training Iteration 4429, Loss: 3.6999034881591797\n",
            "Training Iteration 4430, Loss: 5.613525390625\n",
            "Training Iteration 4431, Loss: 2.5193064212799072\n",
            "Training Iteration 4432, Loss: 4.74515962600708\n",
            "Training Iteration 4433, Loss: 4.0395379066467285\n",
            "Training Iteration 4434, Loss: 3.4537193775177\n",
            "Training Iteration 4435, Loss: 5.163300037384033\n",
            "Training Iteration 4436, Loss: 4.1618266105651855\n",
            "Training Iteration 4437, Loss: 5.431975841522217\n",
            "Training Iteration 4438, Loss: 3.7643415927886963\n",
            "Training Iteration 4439, Loss: 1.6089774370193481\n",
            "Training Iteration 4440, Loss: 4.573246479034424\n",
            "Training Iteration 4441, Loss: 4.057530879974365\n",
            "Training Iteration 4442, Loss: 6.574080944061279\n",
            "Training Iteration 4443, Loss: 4.968737602233887\n",
            "Training Iteration 4444, Loss: 3.2305455207824707\n",
            "Training Iteration 4445, Loss: 4.266469955444336\n",
            "Training Iteration 4446, Loss: 2.4609038829803467\n",
            "Training Iteration 4447, Loss: 6.48306131362915\n",
            "Training Iteration 4448, Loss: 5.095523357391357\n",
            "Training Iteration 4449, Loss: 5.4964165687561035\n",
            "Training Iteration 4450, Loss: 3.4960837364196777\n",
            "Training Iteration 4451, Loss: 3.3765833377838135\n",
            "Training Iteration 4452, Loss: 4.060606956481934\n",
            "Training Iteration 4453, Loss: 5.133309841156006\n",
            "Training Iteration 4454, Loss: 3.6827847957611084\n",
            "Training Iteration 4455, Loss: 2.553340196609497\n",
            "Training Iteration 4456, Loss: 4.100993633270264\n",
            "Training Iteration 4457, Loss: 4.179678916931152\n",
            "Training Iteration 4458, Loss: 5.247435092926025\n",
            "Training Iteration 4459, Loss: 4.2672343254089355\n",
            "Training Iteration 4460, Loss: 3.8867738246917725\n",
            "Training Iteration 4461, Loss: 3.3194844722747803\n",
            "Training Iteration 4462, Loss: 4.502709865570068\n",
            "Training Iteration 4463, Loss: 5.864570140838623\n",
            "Training Iteration 4464, Loss: 3.8326005935668945\n",
            "Training Iteration 4465, Loss: 3.317955732345581\n",
            "Training Iteration 4466, Loss: 2.925877571105957\n",
            "Training Iteration 4467, Loss: 4.086878299713135\n",
            "Training Iteration 4468, Loss: 6.609104156494141\n",
            "Training Iteration 4469, Loss: 3.6492197513580322\n",
            "Training Iteration 4470, Loss: 3.95944881439209\n",
            "Training Iteration 4471, Loss: 5.7131452560424805\n",
            "Training Iteration 4472, Loss: 5.8659892082214355\n",
            "Training Iteration 4473, Loss: 4.068814277648926\n",
            "Training Iteration 4474, Loss: 3.786104440689087\n",
            "Training Iteration 4475, Loss: 3.430556297302246\n",
            "Training Iteration 4476, Loss: 5.1919426918029785\n",
            "Training Iteration 4477, Loss: 5.278428554534912\n",
            "Training Iteration 4478, Loss: 2.9168355464935303\n",
            "Training Iteration 4479, Loss: 3.4773361682891846\n",
            "Training Iteration 4480, Loss: 2.824084997177124\n",
            "Training Iteration 4481, Loss: 6.400830268859863\n",
            "Training Iteration 4482, Loss: 5.1351637840271\n",
            "Training Iteration 4483, Loss: 5.293934345245361\n",
            "Training Iteration 4484, Loss: 2.9705677032470703\n",
            "Training Iteration 4485, Loss: 4.045241832733154\n",
            "Training Iteration 4486, Loss: 6.568648815155029\n",
            "Training Iteration 4487, Loss: 6.874130725860596\n",
            "Training Iteration 4488, Loss: 4.656998157501221\n",
            "Training Iteration 4489, Loss: 4.034414291381836\n",
            "Training Iteration 4490, Loss: 3.37532901763916\n",
            "Training Iteration 4491, Loss: 4.461495399475098\n",
            "Training Iteration 4492, Loss: 3.6608057022094727\n",
            "Training Iteration 4493, Loss: 4.29498815536499\n",
            "Training Iteration 4494, Loss: 3.2046594619750977\n",
            "Training Iteration 4495, Loss: 5.03486442565918\n",
            "Training Iteration 4496, Loss: 6.938713073730469\n",
            "Training Iteration 4497, Loss: 7.60509729385376\n",
            "Training Iteration 4498, Loss: 3.715911626815796\n",
            "Training Iteration 4499, Loss: 4.669244766235352\n",
            "Training Iteration 4500, Loss: 6.461943626403809\n",
            "Training Iteration 4501, Loss: 3.031149387359619\n",
            "Training Iteration 4502, Loss: 5.543858051300049\n",
            "Training Iteration 4503, Loss: 8.096074104309082\n",
            "Training Iteration 4504, Loss: 4.989699363708496\n",
            "Training Iteration 4505, Loss: 3.3250129222869873\n",
            "Training Iteration 4506, Loss: 5.135247707366943\n",
            "Training Iteration 4507, Loss: 1.9833409786224365\n",
            "Training Iteration 4508, Loss: 2.0442492961883545\n",
            "Training Iteration 4509, Loss: 4.6282243728637695\n",
            "Training Iteration 4510, Loss: 4.447545051574707\n",
            "Training Iteration 4511, Loss: 2.006239414215088\n",
            "Training Iteration 4512, Loss: 5.411552429199219\n",
            "Training Iteration 4513, Loss: 3.2982161045074463\n",
            "Training Iteration 4514, Loss: 1.1859296560287476\n",
            "Training Iteration 4515, Loss: 4.204120635986328\n",
            "Training Iteration 4516, Loss: 5.147439479827881\n",
            "Training Iteration 4517, Loss: 4.80363655090332\n",
            "Training Iteration 4518, Loss: 7.0598063468933105\n",
            "Training Iteration 4519, Loss: 3.8972432613372803\n",
            "Training Iteration 4520, Loss: 3.4067294597625732\n",
            "Training Iteration 4521, Loss: 6.051050662994385\n",
            "Training Iteration 4522, Loss: 3.3496932983398438\n",
            "Training Iteration 4523, Loss: 4.371315002441406\n",
            "Training Iteration 4524, Loss: 4.219659805297852\n",
            "Training Iteration 4525, Loss: 3.528134822845459\n",
            "Training Iteration 4526, Loss: 4.2757568359375\n",
            "Training Iteration 4527, Loss: 3.090893030166626\n",
            "Training Iteration 4528, Loss: 3.5988707542419434\n",
            "Training Iteration 4529, Loss: 4.110823154449463\n",
            "Training Iteration 4530, Loss: 2.499274969100952\n",
            "Training Iteration 4531, Loss: 2.974228858947754\n",
            "Training Iteration 4532, Loss: 3.223644733428955\n",
            "Training Iteration 4533, Loss: 7.049005508422852\n",
            "Training Iteration 4534, Loss: 7.47771692276001\n",
            "Training Iteration 4535, Loss: 3.481562614440918\n",
            "Training Iteration 4536, Loss: 5.059800148010254\n",
            "Training Iteration 4537, Loss: 4.455048561096191\n",
            "Training Iteration 4538, Loss: 3.6195685863494873\n",
            "Training Iteration 4539, Loss: 3.0567502975463867\n",
            "Training Iteration 4540, Loss: 7.664551734924316\n",
            "Training Iteration 4541, Loss: 1.954646348953247\n",
            "Training Iteration 4542, Loss: 9.528116226196289\n",
            "Training Iteration 4543, Loss: 4.357378005981445\n",
            "Training Iteration 4544, Loss: 1.8502769470214844\n",
            "Training Iteration 4545, Loss: 7.587618350982666\n",
            "Training Iteration 4546, Loss: 5.461544990539551\n",
            "Training Iteration 4547, Loss: 6.385715007781982\n",
            "Training Iteration 4548, Loss: 4.771182060241699\n",
            "Training Iteration 4549, Loss: 5.714897155761719\n",
            "Training Iteration 4550, Loss: 5.917619705200195\n",
            "Training Iteration 4551, Loss: 10.385516166687012\n",
            "Training Iteration 4552, Loss: 7.089020252227783\n",
            "Training Iteration 4553, Loss: 6.284590721130371\n",
            "Training Iteration 4554, Loss: 5.501330375671387\n",
            "Training Iteration 4555, Loss: 4.893473148345947\n",
            "Training Iteration 4556, Loss: 3.5789613723754883\n",
            "Training Iteration 4557, Loss: 3.048657178878784\n",
            "Training Iteration 4558, Loss: 3.995032787322998\n",
            "Training Iteration 4559, Loss: 2.7629895210266113\n",
            "Training Iteration 4560, Loss: 2.908102035522461\n",
            "Training Iteration 4561, Loss: 4.16350793838501\n",
            "Training Iteration 4562, Loss: 9.405088424682617\n",
            "Training Iteration 4563, Loss: 3.4180121421813965\n",
            "Training Iteration 4564, Loss: 4.4424357414245605\n",
            "Training Iteration 4565, Loss: 3.630286931991577\n",
            "Training Iteration 4566, Loss: 5.262087345123291\n",
            "Training Iteration 4567, Loss: 6.049062252044678\n",
            "Training Iteration 4568, Loss: 5.921330451965332\n",
            "Training Iteration 4569, Loss: 3.4043164253234863\n",
            "Training Iteration 4570, Loss: 1.0438450574874878\n",
            "Training Iteration 4571, Loss: 6.193905353546143\n",
            "Training Iteration 4572, Loss: 3.5139148235321045\n",
            "Training Iteration 4573, Loss: 2.6994822025299072\n",
            "Training Iteration 4574, Loss: 13.531895637512207\n",
            "Training Iteration 4575, Loss: 3.819915771484375\n",
            "Training Iteration 4576, Loss: 11.056225776672363\n",
            "Training Iteration 4577, Loss: 4.879678726196289\n",
            "Training Iteration 4578, Loss: 3.192765474319458\n",
            "Training Iteration 4579, Loss: 5.652503490447998\n",
            "Training Iteration 4580, Loss: 6.960262775421143\n",
            "Training Iteration 4581, Loss: 3.9466001987457275\n",
            "Training Iteration 4582, Loss: 3.496439218521118\n",
            "Training Iteration 4583, Loss: 2.5676448345184326\n",
            "Training Iteration 4584, Loss: 6.267065525054932\n",
            "Training Iteration 4585, Loss: 4.512328624725342\n",
            "Training Iteration 4586, Loss: 6.30478572845459\n",
            "Training Iteration 4587, Loss: 4.862372398376465\n",
            "Training Iteration 4588, Loss: 3.1007916927337646\n",
            "Training Iteration 4589, Loss: 2.153595447540283\n",
            "Training Iteration 4590, Loss: 2.572343587875366\n",
            "Training Iteration 4591, Loss: 2.973283529281616\n",
            "Training Iteration 4592, Loss: 2.2392685413360596\n",
            "Training Iteration 4593, Loss: 4.2391252517700195\n",
            "Training Iteration 4594, Loss: 4.011299133300781\n",
            "Training Iteration 4595, Loss: 1.4307376146316528\n",
            "Training Iteration 4596, Loss: 3.771348714828491\n",
            "Training Iteration 4597, Loss: 2.7510039806365967\n",
            "Training Iteration 4598, Loss: 2.433866024017334\n",
            "Training Iteration 4599, Loss: 2.851794481277466\n",
            "Training Iteration 4600, Loss: 8.826438903808594\n",
            "Training Iteration 4601, Loss: 5.235815525054932\n",
            "Training Iteration 4602, Loss: 4.432827472686768\n",
            "Training Iteration 4603, Loss: 4.319180011749268\n",
            "Training Iteration 4604, Loss: 4.04586935043335\n",
            "Training Iteration 4605, Loss: 2.1094727516174316\n",
            "Training Iteration 4606, Loss: 2.4731998443603516\n",
            "Training Iteration 4607, Loss: 2.01865291595459\n",
            "Training Iteration 4608, Loss: 3.58601450920105\n",
            "Training Iteration 4609, Loss: 6.146298885345459\n",
            "Training Iteration 4610, Loss: 3.0769498348236084\n",
            "Training Iteration 4611, Loss: 3.3690061569213867\n",
            "Training Iteration 4612, Loss: 3.431884527206421\n",
            "Training Iteration 4613, Loss: 5.655998229980469\n",
            "Training Iteration 4614, Loss: 8.190422058105469\n",
            "Training Iteration 4615, Loss: 3.028428792953491\n",
            "Training Iteration 4616, Loss: 4.824316501617432\n",
            "Training Iteration 4617, Loss: 3.5974273681640625\n",
            "Training Iteration 4618, Loss: 4.788630962371826\n",
            "Training Iteration 4619, Loss: 6.851683616638184\n",
            "Training Iteration 4620, Loss: 4.3328633308410645\n",
            "Training Iteration 4621, Loss: 3.0530896186828613\n",
            "Training Iteration 4622, Loss: 4.461086750030518\n",
            "Training Iteration 4623, Loss: 4.057858467102051\n",
            "Training Iteration 4624, Loss: 3.843320608139038\n",
            "Training Iteration 4625, Loss: 2.2008233070373535\n",
            "Training Iteration 4626, Loss: 5.35008430480957\n",
            "Training Iteration 4627, Loss: 3.254152774810791\n",
            "Training Iteration 4628, Loss: 2.775054693222046\n",
            "Training Iteration 4629, Loss: 3.9423398971557617\n",
            "Training Iteration 4630, Loss: 4.889357089996338\n",
            "Training Iteration 4631, Loss: 3.025430679321289\n",
            "Training Iteration 4632, Loss: 6.3199968338012695\n",
            "Training Iteration 4633, Loss: 5.816852569580078\n",
            "Training Iteration 4634, Loss: 2.8183741569519043\n",
            "Training Iteration 4635, Loss: 1.6516252756118774\n",
            "Training Iteration 4636, Loss: 2.729166269302368\n",
            "Training Iteration 4637, Loss: 5.271071910858154\n",
            "Training Iteration 4638, Loss: 8.431337356567383\n",
            "Training Iteration 4639, Loss: 5.837992191314697\n",
            "Training Iteration 4640, Loss: 3.3064870834350586\n",
            "Training Iteration 4641, Loss: 4.73256778717041\n",
            "Training Iteration 4642, Loss: 5.186839580535889\n",
            "Training Iteration 4643, Loss: 3.4335498809814453\n",
            "Training Iteration 4644, Loss: 4.443883895874023\n",
            "Training Iteration 4645, Loss: 4.983510494232178\n",
            "Training Iteration 4646, Loss: 5.318910121917725\n",
            "Training Iteration 4647, Loss: 7.2637553215026855\n",
            "Training Iteration 4648, Loss: 4.14109992980957\n",
            "Training Iteration 4649, Loss: 2.2010977268218994\n",
            "Training Iteration 4650, Loss: 7.555904865264893\n",
            "Training Iteration 4651, Loss: 4.6118316650390625\n",
            "Training Iteration 4652, Loss: 8.031834602355957\n",
            "Training Iteration 4653, Loss: 4.8567328453063965\n",
            "Training Iteration 4654, Loss: 4.326813697814941\n",
            "Training Iteration 4655, Loss: 3.9769179821014404\n",
            "Training Iteration 4656, Loss: 2.1837539672851562\n",
            "Training Iteration 4657, Loss: 4.558435916900635\n",
            "Training Iteration 4658, Loss: 3.3311004638671875\n",
            "Training Iteration 4659, Loss: 5.277899265289307\n",
            "Training Iteration 4660, Loss: 4.776878356933594\n",
            "Training Iteration 4661, Loss: 5.855255603790283\n",
            "Training Iteration 4662, Loss: 7.043933391571045\n",
            "Training Iteration 4663, Loss: 4.182828903198242\n",
            "Training Iteration 4664, Loss: 2.614820957183838\n",
            "Training Iteration 4665, Loss: 4.2663493156433105\n",
            "Training Iteration 4666, Loss: 6.138935565948486\n",
            "Training Iteration 4667, Loss: 4.48998498916626\n",
            "Training Iteration 4668, Loss: 3.383938789367676\n",
            "Training Iteration 4669, Loss: 5.257360935211182\n",
            "Training Iteration 4670, Loss: 4.31875467300415\n",
            "Training Iteration 4671, Loss: 1.7107781171798706\n",
            "Training Iteration 4672, Loss: 3.1933062076568604\n",
            "Training Iteration 4673, Loss: 3.7565019130706787\n",
            "Training Iteration 4674, Loss: 5.2849907875061035\n",
            "Training Iteration 4675, Loss: 8.728019714355469\n",
            "Training Iteration 4676, Loss: 3.7801854610443115\n",
            "Training Iteration 4677, Loss: 5.309871196746826\n",
            "Training Iteration 4678, Loss: 8.707276344299316\n",
            "Training Iteration 4679, Loss: 5.4595136642456055\n",
            "Training Iteration 4680, Loss: 2.54610276222229\n",
            "Training Iteration 4681, Loss: 4.068950176239014\n",
            "Training Iteration 4682, Loss: 4.113626003265381\n",
            "Training Iteration 4683, Loss: 7.785642147064209\n",
            "Training Iteration 4684, Loss: 4.820682048797607\n",
            "Training Iteration 4685, Loss: 4.584066390991211\n",
            "Training Iteration 4686, Loss: 8.85879135131836\n",
            "Training Iteration 4687, Loss: 4.72922945022583\n",
            "Training Iteration 4688, Loss: 2.8906450271606445\n",
            "Training Iteration 4689, Loss: 4.741707801818848\n",
            "Training Iteration 4690, Loss: 3.8534536361694336\n",
            "Training Iteration 4691, Loss: 2.853036642074585\n",
            "Training Iteration 4692, Loss: 4.111090660095215\n",
            "Training Iteration 4693, Loss: 4.703245162963867\n",
            "Training Iteration 4694, Loss: 4.471881866455078\n",
            "Training Iteration 4695, Loss: 2.3941264152526855\n",
            "Training Iteration 4696, Loss: 5.648122787475586\n",
            "Training Iteration 4697, Loss: 6.789339542388916\n",
            "Training Iteration 4698, Loss: 5.046399116516113\n",
            "Training Iteration 4699, Loss: 4.643092155456543\n",
            "Training Iteration 4700, Loss: 2.508028030395508\n",
            "Training Iteration 4701, Loss: 2.898646354675293\n",
            "Training Iteration 4702, Loss: 5.8623247146606445\n",
            "Training Iteration 4703, Loss: 7.314209938049316\n",
            "Training Iteration 4704, Loss: 5.148581504821777\n",
            "Training Iteration 4705, Loss: 5.671845436096191\n",
            "Training Iteration 4706, Loss: 3.693718671798706\n",
            "Training Iteration 4707, Loss: 6.393237113952637\n",
            "Training Iteration 4708, Loss: 3.4510538578033447\n",
            "Training Iteration 4709, Loss: 1.637175440788269\n",
            "Training Iteration 4710, Loss: 3.7019317150115967\n",
            "Training Iteration 4711, Loss: 5.310708522796631\n",
            "Training Iteration 4712, Loss: 3.4401750564575195\n",
            "Training Iteration 4713, Loss: 4.32149600982666\n",
            "Training Iteration 4714, Loss: 6.204439163208008\n",
            "Training Iteration 4715, Loss: 1.905869483947754\n",
            "Training Iteration 4716, Loss: 4.692691802978516\n",
            "Training Iteration 4717, Loss: 3.3784079551696777\n",
            "Training Iteration 4718, Loss: 2.951470375061035\n",
            "Training Iteration 4719, Loss: 3.83073091506958\n",
            "Training Iteration 4720, Loss: 4.841340065002441\n",
            "Training Iteration 4721, Loss: 5.467815399169922\n",
            "Training Iteration 4722, Loss: 2.075348377227783\n",
            "Training Iteration 4723, Loss: 6.290722846984863\n",
            "Training Iteration 4724, Loss: 5.378255844116211\n",
            "Training Iteration 4725, Loss: 4.207738399505615\n",
            "Training Iteration 4726, Loss: 4.518296718597412\n",
            "Training Iteration 4727, Loss: 4.297647476196289\n",
            "Training Iteration 4728, Loss: 5.599961280822754\n",
            "Training Iteration 4729, Loss: 3.99286150932312\n",
            "Training Iteration 4730, Loss: 7.975722312927246\n",
            "Training Iteration 4731, Loss: 8.105120658874512\n",
            "Training Iteration 4732, Loss: 3.3784050941467285\n",
            "Training Iteration 4733, Loss: 2.3788437843322754\n",
            "Training Iteration 4734, Loss: 2.308976173400879\n",
            "Training Iteration 4735, Loss: 2.6387224197387695\n",
            "Training Iteration 4736, Loss: 2.6653530597686768\n",
            "Training Iteration 4737, Loss: 9.158629417419434\n",
            "Training Iteration 4738, Loss: 5.1107940673828125\n",
            "Training Iteration 4739, Loss: 5.9886932373046875\n",
            "Training Iteration 4740, Loss: 2.4737253189086914\n",
            "Training Iteration 4741, Loss: 3.377397298812866\n",
            "Training Iteration 4742, Loss: 4.908193588256836\n",
            "Training Iteration 4743, Loss: 3.6488125324249268\n",
            "Training Iteration 4744, Loss: 3.781045913696289\n",
            "Training Iteration 4745, Loss: 2.362539768218994\n",
            "Training Iteration 4746, Loss: 4.866856098175049\n",
            "Training Iteration 4747, Loss: 4.447108268737793\n",
            "Training Iteration 4748, Loss: 6.714430332183838\n",
            "Training Iteration 4749, Loss: 4.328679084777832\n",
            "Training Iteration 4750, Loss: 3.6321029663085938\n",
            "Training Iteration 4751, Loss: 6.735507965087891\n",
            "Training Iteration 4752, Loss: 6.428674221038818\n",
            "Training Iteration 4753, Loss: 6.053752899169922\n",
            "Training Iteration 4754, Loss: 2.99691104888916\n",
            "Training Iteration 4755, Loss: 1.4511761665344238\n",
            "Training Iteration 4756, Loss: 6.233865737915039\n",
            "Training Iteration 4757, Loss: 7.125887870788574\n",
            "Training Iteration 4758, Loss: 6.922341346740723\n",
            "Training Iteration 4759, Loss: 8.28371524810791\n",
            "Training Iteration 4760, Loss: 5.250600814819336\n",
            "Training Iteration 4761, Loss: 6.763510227203369\n",
            "Training Iteration 4762, Loss: 8.349536895751953\n",
            "Training Iteration 4763, Loss: 8.459431648254395\n",
            "Training Iteration 4764, Loss: 9.97986888885498\n",
            "Training Iteration 4765, Loss: 9.534805297851562\n",
            "Training Iteration 4766, Loss: 3.8154854774475098\n",
            "Training Iteration 4767, Loss: 2.7742576599121094\n",
            "Training Iteration 4768, Loss: 6.061464309692383\n",
            "Training Iteration 4769, Loss: 8.406383514404297\n",
            "Training Iteration 4770, Loss: 3.0088114738464355\n",
            "Training Iteration 4771, Loss: 3.8204479217529297\n",
            "Training Iteration 4772, Loss: 3.548661947250366\n",
            "Training Iteration 4773, Loss: 3.254114866256714\n",
            "Training Iteration 4774, Loss: 3.24074387550354\n",
            "Training Iteration 4775, Loss: 5.38426399230957\n",
            "Training Iteration 4776, Loss: 4.555587291717529\n",
            "Training Iteration 4777, Loss: 5.587177276611328\n",
            "Training Iteration 4778, Loss: 7.510148048400879\n",
            "Training Iteration 4779, Loss: 4.725703239440918\n",
            "Training Iteration 4780, Loss: 4.041182994842529\n",
            "Training Iteration 4781, Loss: 4.473586559295654\n",
            "Training Iteration 4782, Loss: 3.149994134902954\n",
            "Training Iteration 4783, Loss: 5.30919885635376\n",
            "Training Iteration 4784, Loss: 5.13539981842041\n",
            "Training Iteration 4785, Loss: 9.680266380310059\n",
            "Training Iteration 4786, Loss: 9.499608993530273\n",
            "Training Iteration 4787, Loss: 3.543889284133911\n",
            "Training Iteration 4788, Loss: 3.562002658843994\n",
            "Training Iteration 4789, Loss: 5.632214546203613\n",
            "Training Iteration 4790, Loss: 7.7342023849487305\n",
            "Training Iteration 4791, Loss: 4.719498634338379\n",
            "Training Iteration 4792, Loss: 9.761573791503906\n",
            "Training Iteration 4793, Loss: 7.580231666564941\n",
            "Training Iteration 4794, Loss: 7.737366199493408\n",
            "Training Iteration 4795, Loss: 7.811465740203857\n",
            "Training Iteration 4796, Loss: 4.327507495880127\n",
            "Training Iteration 4797, Loss: 2.7133445739746094\n",
            "Training Iteration 4798, Loss: 4.142609596252441\n",
            "Training Iteration 4799, Loss: 4.642970085144043\n",
            "Training Iteration 4800, Loss: 2.6288235187530518\n",
            "Training Iteration 4801, Loss: 4.095180034637451\n",
            "Training Iteration 4802, Loss: 4.893039226531982\n",
            "Training Iteration 4803, Loss: 4.101790428161621\n",
            "Training Iteration 4804, Loss: 8.41306209564209\n",
            "Training Iteration 4805, Loss: 5.593364715576172\n",
            "Training Iteration 4806, Loss: 4.199059963226318\n",
            "Training Iteration 4807, Loss: 5.826750755310059\n",
            "Training Iteration 4808, Loss: 4.457070350646973\n",
            "Training Iteration 4809, Loss: 1.6269031763076782\n",
            "Training Iteration 4810, Loss: 3.4390640258789062\n",
            "Training Iteration 4811, Loss: 4.202764987945557\n",
            "Training Iteration 4812, Loss: 4.622077465057373\n",
            "Training Iteration 4813, Loss: 4.460958957672119\n",
            "Training Iteration 4814, Loss: 5.842168807983398\n",
            "Training Iteration 4815, Loss: 4.725641250610352\n",
            "Training Iteration 4816, Loss: 5.727079391479492\n",
            "Training Iteration 4817, Loss: 2.62251353263855\n",
            "Training Iteration 4818, Loss: 3.1852915287017822\n",
            "Training Iteration 4819, Loss: 6.352466583251953\n",
            "Training Iteration 4820, Loss: 4.117903709411621\n",
            "Training Iteration 4821, Loss: 4.508859634399414\n",
            "Training Iteration 4822, Loss: 4.118466377258301\n",
            "Training Iteration 4823, Loss: 6.499712944030762\n",
            "Training Iteration 4824, Loss: 4.292283058166504\n",
            "Training Iteration 4825, Loss: 7.253271102905273\n",
            "Training Iteration 4826, Loss: 2.4287145137786865\n",
            "Training Iteration 4827, Loss: 3.7094128131866455\n",
            "Training Iteration 4828, Loss: 5.647193431854248\n",
            "Training Iteration 4829, Loss: 9.26602554321289\n",
            "Training Iteration 4830, Loss: 2.3052916526794434\n",
            "Training Iteration 4831, Loss: 4.509273529052734\n",
            "Training Iteration 4832, Loss: 8.739968299865723\n",
            "Training Iteration 4833, Loss: 4.28688383102417\n",
            "Training Iteration 4834, Loss: 6.456872940063477\n",
            "Training Iteration 4835, Loss: 6.23313045501709\n",
            "Training Iteration 4836, Loss: 4.848932266235352\n",
            "Training Iteration 4837, Loss: 4.983383655548096\n",
            "Training Iteration 4838, Loss: 3.0482003688812256\n",
            "Training Iteration 4839, Loss: 3.4429967403411865\n",
            "Training Iteration 4840, Loss: 6.627168655395508\n",
            "Training Iteration 4841, Loss: 3.6428933143615723\n",
            "Training Iteration 4842, Loss: 2.956528425216675\n",
            "Training Iteration 4843, Loss: 2.701049566268921\n",
            "Training Iteration 4844, Loss: 5.118934154510498\n",
            "Training Iteration 4845, Loss: 3.4488232135772705\n",
            "Training Iteration 4846, Loss: 5.064155578613281\n",
            "Training Iteration 4847, Loss: 4.817854881286621\n",
            "Training Iteration 4848, Loss: 3.9226267337799072\n",
            "Training Iteration 4849, Loss: 4.392332553863525\n",
            "Training Iteration 4850, Loss: 5.655581951141357\n",
            "Training Iteration 4851, Loss: 5.028066635131836\n",
            "Training Iteration 4852, Loss: 3.9817919731140137\n",
            "Training Iteration 4853, Loss: 4.169233322143555\n",
            "Training Iteration 4854, Loss: 4.352616786956787\n",
            "Training Iteration 4855, Loss: 3.101149320602417\n",
            "Training Iteration 4856, Loss: 4.844215393066406\n",
            "Training Iteration 4857, Loss: 6.768226623535156\n",
            "Training Iteration 4858, Loss: 3.2832188606262207\n",
            "Training Iteration 4859, Loss: 3.757422924041748\n",
            "Training Iteration 4860, Loss: 6.548362731933594\n",
            "Training Iteration 4861, Loss: 11.835758209228516\n",
            "Training Iteration 4862, Loss: 3.978275775909424\n",
            "Training Iteration 4863, Loss: 4.437992095947266\n",
            "Training Iteration 4864, Loss: 3.492652416229248\n",
            "Training Iteration 4865, Loss: 3.045278310775757\n",
            "Training Iteration 4866, Loss: 4.531648635864258\n",
            "Training Iteration 4867, Loss: 4.2188568115234375\n",
            "Training Iteration 4868, Loss: 4.09108304977417\n",
            "Training Iteration 4869, Loss: 5.326999187469482\n",
            "Training Iteration 4870, Loss: 3.2246201038360596\n",
            "Training Iteration 4871, Loss: 6.930351257324219\n",
            "Training Iteration 4872, Loss: 4.0251874923706055\n",
            "Training Iteration 4873, Loss: 5.5465288162231445\n",
            "Training Iteration 4874, Loss: 3.1922504901885986\n",
            "Training Iteration 4875, Loss: 5.0814595222473145\n",
            "Training Iteration 4876, Loss: 3.306704044342041\n",
            "Training Iteration 4877, Loss: 4.328497886657715\n",
            "Training Iteration 4878, Loss: 5.5399699211120605\n",
            "Training Iteration 4879, Loss: 2.978769063949585\n",
            "Training Iteration 4880, Loss: 5.781038761138916\n",
            "Training Iteration 4881, Loss: 2.97519850730896\n",
            "Training Iteration 4882, Loss: 4.095191955566406\n",
            "Training Iteration 4883, Loss: 3.15788197517395\n",
            "Training Iteration 4884, Loss: 5.7034382820129395\n",
            "Training Iteration 4885, Loss: 5.674524784088135\n",
            "Training Iteration 4886, Loss: 2.6861839294433594\n",
            "Training Iteration 4887, Loss: 3.674830198287964\n",
            "Training Iteration 4888, Loss: 5.133399963378906\n",
            "Training Iteration 4889, Loss: 2.781482219696045\n",
            "Training Iteration 4890, Loss: 6.355524063110352\n",
            "Training Iteration 4891, Loss: 5.497791767120361\n",
            "Training Iteration 4892, Loss: 5.248543739318848\n",
            "Training Iteration 4893, Loss: 2.582261800765991\n",
            "Training Iteration 4894, Loss: 4.092735767364502\n",
            "Training Iteration 4895, Loss: 1.1912436485290527\n",
            "Training Iteration 4896, Loss: 7.1218485832214355\n",
            "Training Iteration 4897, Loss: 3.3610169887542725\n",
            "Training Iteration 4898, Loss: 6.300582408905029\n",
            "Training Iteration 4899, Loss: 4.541703701019287\n",
            "Training Iteration 4900, Loss: 3.089614152908325\n",
            "Training Iteration 4901, Loss: 2.550654172897339\n",
            "Training Iteration 4902, Loss: 5.384432792663574\n",
            "Training Iteration 4903, Loss: 7.260369300842285\n",
            "Training Iteration 4904, Loss: 4.2864179611206055\n",
            "Training Iteration 4905, Loss: 3.3461294174194336\n",
            "Training Iteration 4906, Loss: 4.040156841278076\n",
            "Training Iteration 4907, Loss: 4.032633304595947\n",
            "Training Iteration 4908, Loss: 6.290458679199219\n",
            "Training Iteration 4909, Loss: 6.27203369140625\n",
            "Training Iteration 4910, Loss: 4.015752792358398\n",
            "Training Iteration 4911, Loss: 3.6710782051086426\n",
            "Training Iteration 4912, Loss: 5.651189804077148\n",
            "Training Iteration 4913, Loss: 4.463951587677002\n",
            "Training Iteration 4914, Loss: 7.038710117340088\n",
            "Training Iteration 4915, Loss: 3.6981215476989746\n",
            "Training Iteration 4916, Loss: 2.5148673057556152\n",
            "Training Iteration 4917, Loss: 2.1678993701934814\n",
            "Training Iteration 4918, Loss: 3.175905466079712\n",
            "Training Iteration 4919, Loss: 3.617624044418335\n",
            "Training Iteration 4920, Loss: 3.385807991027832\n",
            "Training Iteration 4921, Loss: 6.244815826416016\n",
            "Training Iteration 4922, Loss: 4.396663188934326\n",
            "Training Iteration 4923, Loss: 8.246179580688477\n",
            "Training Iteration 4924, Loss: 6.75408935546875\n",
            "Training Iteration 4925, Loss: 4.069228649139404\n",
            "Training Iteration 4926, Loss: 6.589881420135498\n",
            "Training Iteration 4927, Loss: 7.4229302406311035\n",
            "Training Iteration 4928, Loss: 4.132725715637207\n",
            "Training Iteration 4929, Loss: 4.0202412605285645\n",
            "Training Iteration 4930, Loss: 4.835084438323975\n",
            "Training Iteration 4931, Loss: 2.6897075176239014\n",
            "Training Iteration 4932, Loss: 4.023767471313477\n",
            "Training Iteration 4933, Loss: 6.0862627029418945\n",
            "Training Iteration 4934, Loss: 6.159487724304199\n",
            "Training Iteration 4935, Loss: 4.116018772125244\n",
            "Training Iteration 4936, Loss: 5.830643653869629\n",
            "Training Iteration 4937, Loss: 7.080381393432617\n",
            "Training Iteration 4938, Loss: 8.86532974243164\n",
            "Training Iteration 4939, Loss: 4.987415313720703\n",
            "Training Iteration 4940, Loss: 4.044991970062256\n",
            "Training Iteration 4941, Loss: 1.8057427406311035\n",
            "Training Iteration 4942, Loss: 6.561371803283691\n",
            "Training Iteration 4943, Loss: 2.137436866760254\n",
            "Training Iteration 4944, Loss: 4.64023494720459\n",
            "Training Iteration 4945, Loss: 1.7569161653518677\n",
            "Training Iteration 4946, Loss: 4.00110387802124\n",
            "Training Iteration 4947, Loss: 5.5273942947387695\n",
            "Training Iteration 4948, Loss: 3.8809654712677\n",
            "Training Iteration 4949, Loss: 5.669984340667725\n",
            "Training Iteration 4950, Loss: 12.143646240234375\n",
            "Training Iteration 4951, Loss: 5.568844795227051\n",
            "Training Iteration 4952, Loss: 3.081357479095459\n",
            "Training Iteration 4953, Loss: 3.277334690093994\n",
            "Training Iteration 4954, Loss: 4.2345428466796875\n",
            "Training Iteration 4955, Loss: 3.2490038871765137\n",
            "Training Iteration 4956, Loss: 7.858017921447754\n",
            "Training Iteration 4957, Loss: 3.209587574005127\n",
            "Training Iteration 4958, Loss: 0.8746395707130432\n",
            "Training Iteration 4959, Loss: 6.982970237731934\n",
            "Training Iteration 4960, Loss: 3.640409469604492\n",
            "Training Iteration 4961, Loss: 4.654003143310547\n",
            "Training Iteration 4962, Loss: 4.368224620819092\n",
            "Training Iteration 4963, Loss: 7.126376152038574\n",
            "Training Iteration 4964, Loss: 4.907196044921875\n",
            "Training Iteration 4965, Loss: 2.6837940216064453\n",
            "Training Iteration 4966, Loss: 2.6598377227783203\n",
            "Training Iteration 4967, Loss: 4.675445556640625\n",
            "Training Iteration 4968, Loss: 3.7319588661193848\n",
            "Training Iteration 4969, Loss: 6.639810085296631\n",
            "Training Iteration 4970, Loss: 2.4086859226226807\n",
            "Training Iteration 4971, Loss: 3.653109073638916\n",
            "Training Iteration 4972, Loss: 2.9691481590270996\n",
            "Training Iteration 4973, Loss: 4.442978858947754\n",
            "Training Iteration 4974, Loss: 1.9180047512054443\n",
            "Training Iteration 4975, Loss: 4.8044891357421875\n",
            "Training Iteration 4976, Loss: 4.893219470977783\n",
            "Training Iteration 4977, Loss: 5.139920234680176\n",
            "Training Iteration 4978, Loss: 3.036370038986206\n",
            "Training Iteration 4979, Loss: 2.2834274768829346\n",
            "Training Iteration 4980, Loss: 1.107004165649414\n",
            "Training Iteration 4981, Loss: 2.323934555053711\n",
            "Training Iteration 4982, Loss: 4.542384147644043\n",
            "Training Iteration 4983, Loss: 4.224976539611816\n",
            "Training Iteration 4984, Loss: 3.7514665126800537\n",
            "Training Iteration 4985, Loss: 4.584296226501465\n",
            "Training Iteration 4986, Loss: 4.422797679901123\n",
            "Training Iteration 4987, Loss: 6.451945781707764\n",
            "Training Iteration 4988, Loss: 3.5294528007507324\n",
            "Training Iteration 4989, Loss: 2.8503074645996094\n",
            "Training Iteration 4990, Loss: 5.019127368927002\n",
            "Training Iteration 4991, Loss: 6.446396350860596\n",
            "Training Iteration 4992, Loss: 4.9073896408081055\n",
            "Training Iteration 4993, Loss: 4.407436370849609\n",
            "Training Iteration 4994, Loss: 3.642969846725464\n",
            "Training Iteration 4995, Loss: 2.3638877868652344\n",
            "Training Iteration 4996, Loss: 3.4503085613250732\n",
            "Training Iteration 4997, Loss: 2.4724977016448975\n",
            "Training Iteration 4998, Loss: 0.9578502178192139\n",
            "Training Iteration 4999, Loss: 4.3253350257873535\n",
            "Training Iteration 5000, Loss: 3.7108166217803955\n",
            "Training Iteration 5001, Loss: 2.5509231090545654\n",
            "Training Iteration 5002, Loss: 5.248784065246582\n",
            "Training Iteration 5003, Loss: 4.980292320251465\n",
            "Training Iteration 5004, Loss: 7.381929397583008\n",
            "Training Iteration 5005, Loss: 4.405204772949219\n",
            "Training Iteration 5006, Loss: 4.8108038902282715\n",
            "Training Iteration 5007, Loss: 2.190491199493408\n",
            "Training Iteration 5008, Loss: 4.295431137084961\n",
            "Training Iteration 5009, Loss: 2.5648696422576904\n",
            "Training Iteration 5010, Loss: 9.428544044494629\n",
            "Training Iteration 5011, Loss: 4.625430107116699\n",
            "Training Iteration 5012, Loss: 3.979184150695801\n",
            "Training Iteration 5013, Loss: 3.7668848037719727\n",
            "Training Iteration 5014, Loss: 5.444791793823242\n",
            "Training Iteration 5015, Loss: 1.909095287322998\n",
            "Training Iteration 5016, Loss: 3.9164488315582275\n",
            "Training Iteration 5017, Loss: 3.8701488971710205\n",
            "Training Iteration 5018, Loss: 2.8163928985595703\n",
            "Training Iteration 5019, Loss: 5.985565185546875\n",
            "Training Iteration 5020, Loss: 1.794253945350647\n",
            "Training Iteration 5021, Loss: 5.515077114105225\n",
            "Training Iteration 5022, Loss: 3.801875352859497\n",
            "Training Iteration 5023, Loss: 3.715970516204834\n",
            "Training Iteration 5024, Loss: 5.817451477050781\n",
            "Training Iteration 5025, Loss: 5.668590545654297\n",
            "Training Iteration 5026, Loss: 4.828601360321045\n",
            "Training Iteration 5027, Loss: 6.157432556152344\n",
            "Training Iteration 5028, Loss: 3.400087833404541\n",
            "Training Iteration 5029, Loss: 4.418656826019287\n",
            "Training Iteration 5030, Loss: 4.139099597930908\n",
            "Training Iteration 5031, Loss: 5.65415620803833\n",
            "Training Iteration 5032, Loss: 4.205913066864014\n",
            "Training Iteration 5033, Loss: 5.1395063400268555\n",
            "Training Iteration 5034, Loss: 2.5825133323669434\n",
            "Training Iteration 5035, Loss: 2.7735838890075684\n",
            "Training Iteration 5036, Loss: 5.036715507507324\n",
            "Training Iteration 5037, Loss: 3.7511250972747803\n",
            "Training Iteration 5038, Loss: 2.4525508880615234\n",
            "Training Iteration 5039, Loss: 4.215705871582031\n",
            "Training Iteration 5040, Loss: 6.4260663986206055\n",
            "Training Iteration 5041, Loss: 5.441863536834717\n",
            "Training Iteration 5042, Loss: 5.852121353149414\n",
            "Training Iteration 5043, Loss: 4.510709285736084\n",
            "Training Iteration 5044, Loss: 5.144979953765869\n",
            "Training Iteration 5045, Loss: 4.403803825378418\n",
            "Training Iteration 5046, Loss: 4.512826442718506\n",
            "Training Iteration 5047, Loss: 4.373465538024902\n",
            "Training Iteration 5048, Loss: 5.954174995422363\n",
            "Training Iteration 5049, Loss: 7.872808933258057\n",
            "Training Iteration 5050, Loss: 1.9414963722229004\n",
            "Training Iteration 5051, Loss: 4.9290642738342285\n",
            "Training Iteration 5052, Loss: 3.985308885574341\n",
            "Training Iteration 5053, Loss: 5.779862880706787\n",
            "Training Iteration 5054, Loss: 4.421107292175293\n",
            "Training Iteration 5055, Loss: 4.632758140563965\n",
            "Training Iteration 5056, Loss: 4.656243801116943\n",
            "Training Iteration 5057, Loss: 8.261610984802246\n",
            "Training Iteration 5058, Loss: 6.362802505493164\n",
            "Training Iteration 5059, Loss: 5.801551342010498\n",
            "Training Iteration 5060, Loss: 2.6425986289978027\n",
            "Training Iteration 5061, Loss: 9.297553062438965\n",
            "Training Iteration 5062, Loss: 9.12030029296875\n",
            "Training Iteration 5063, Loss: 4.479534149169922\n",
            "Training Iteration 5064, Loss: 11.284539222717285\n",
            "Training Iteration 5065, Loss: 3.599485397338867\n",
            "Training Iteration 5066, Loss: 7.930893898010254\n",
            "Training Iteration 5067, Loss: 2.148916244506836\n",
            "Training Iteration 5068, Loss: 3.091407060623169\n",
            "Training Iteration 5069, Loss: 7.036201477050781\n",
            "Training Iteration 5070, Loss: 3.0061299800872803\n",
            "Training Iteration 5071, Loss: 7.818788528442383\n",
            "Training Iteration 5072, Loss: 9.573240280151367\n",
            "Training Iteration 5073, Loss: 6.8599371910095215\n",
            "Training Iteration 5074, Loss: 2.8109655380249023\n",
            "Training Iteration 5075, Loss: 4.172061443328857\n",
            "Training Iteration 5076, Loss: 3.154423952102661\n",
            "Training Iteration 5077, Loss: 4.036075592041016\n",
            "Training Iteration 5078, Loss: 4.8615546226501465\n",
            "Training Iteration 5079, Loss: 3.9182369709014893\n",
            "Training Iteration 5080, Loss: 5.210634708404541\n",
            "Training Iteration 5081, Loss: 3.5329601764678955\n",
            "Training Iteration 5082, Loss: 4.184750080108643\n",
            "Training Iteration 5083, Loss: 5.543579578399658\n",
            "Training Iteration 5084, Loss: 2.0098557472229004\n",
            "Training Iteration 5085, Loss: 4.180822372436523\n",
            "Training Iteration 5086, Loss: 5.229432582855225\n",
            "Training Iteration 5087, Loss: 4.2173542976379395\n",
            "Training Iteration 5088, Loss: 3.322728395462036\n",
            "Training Iteration 5089, Loss: 3.857447624206543\n",
            "Training Iteration 5090, Loss: 5.2344746589660645\n",
            "Training Iteration 5091, Loss: 3.6928844451904297\n",
            "Training Iteration 5092, Loss: 6.227819919586182\n",
            "Training Iteration 5093, Loss: 3.316861152648926\n",
            "Training Iteration 5094, Loss: 5.6861252784729\n",
            "Training Iteration 5095, Loss: 4.483619689941406\n",
            "Training Iteration 5096, Loss: 7.685769557952881\n",
            "Training Iteration 5097, Loss: 4.214030742645264\n",
            "Training Iteration 5098, Loss: 6.485281944274902\n",
            "Training Iteration 5099, Loss: 2.777967929840088\n",
            "Training Iteration 5100, Loss: 5.160826683044434\n",
            "Training Iteration 5101, Loss: 4.674499034881592\n",
            "Training Iteration 5102, Loss: 3.340928077697754\n",
            "Training Iteration 5103, Loss: 5.6072211265563965\n",
            "Training Iteration 5104, Loss: 4.330846309661865\n",
            "Training Iteration 5105, Loss: 4.646854400634766\n",
            "Training Iteration 5106, Loss: 6.788963794708252\n",
            "Training Iteration 5107, Loss: 2.161273956298828\n",
            "Training Iteration 5108, Loss: 6.000209331512451\n",
            "Training Iteration 5109, Loss: 3.8859641551971436\n",
            "Training Iteration 5110, Loss: 4.602923393249512\n",
            "Training Iteration 5111, Loss: 3.608008861541748\n",
            "Training Iteration 5112, Loss: 4.074568748474121\n",
            "Training Iteration 5113, Loss: 4.423405647277832\n",
            "Training Iteration 5114, Loss: 4.357831001281738\n",
            "Training Iteration 5115, Loss: 3.628655195236206\n",
            "Training Iteration 5116, Loss: 4.786933422088623\n",
            "Training Iteration 5117, Loss: 2.9666991233825684\n",
            "Training Iteration 5118, Loss: 3.2706594467163086\n",
            "Training Iteration 5119, Loss: 1.841007947921753\n",
            "Training Iteration 5120, Loss: 5.047453880310059\n",
            "Training Iteration 5121, Loss: 4.564868450164795\n",
            "Training Iteration 5122, Loss: 5.903600692749023\n",
            "Training Iteration 5123, Loss: 6.9942522048950195\n",
            "Training Iteration 5124, Loss: 3.473550796508789\n",
            "Training Iteration 5125, Loss: 5.154341697692871\n",
            "Training Iteration 5126, Loss: 4.168483734130859\n",
            "Training Iteration 5127, Loss: 3.9496891498565674\n",
            "Training Iteration 5128, Loss: 4.456146717071533\n",
            "Training Iteration 5129, Loss: 2.4006824493408203\n",
            "Training Iteration 5130, Loss: 4.838332176208496\n",
            "Training Iteration 5131, Loss: 6.154680252075195\n",
            "Training Iteration 5132, Loss: 3.954624652862549\n",
            "Training Iteration 5133, Loss: 4.9302520751953125\n",
            "Training Iteration 5134, Loss: 5.8894453048706055\n",
            "Training Iteration 5135, Loss: 5.800675392150879\n",
            "Training Iteration 5136, Loss: 3.377199172973633\n",
            "Training Iteration 5137, Loss: 2.569669008255005\n",
            "Training Iteration 5138, Loss: 4.8228936195373535\n",
            "Training Iteration 5139, Loss: 4.767815589904785\n",
            "Training Iteration 5140, Loss: 3.151648998260498\n",
            "Training Iteration 5141, Loss: 6.2119975090026855\n",
            "Training Iteration 5142, Loss: 4.170473098754883\n",
            "Training Iteration 5143, Loss: 6.295454978942871\n",
            "Training Iteration 5144, Loss: 3.2577455043792725\n",
            "Training Iteration 5145, Loss: 8.734049797058105\n",
            "Training Iteration 5146, Loss: 3.2313973903656006\n",
            "Training Iteration 5147, Loss: 3.8462820053100586\n",
            "Training Iteration 5148, Loss: 7.2435197830200195\n",
            "Training Iteration 5149, Loss: 5.910229682922363\n",
            "Training Iteration 5150, Loss: 3.413466453552246\n",
            "Training Iteration 5151, Loss: 3.8107943534851074\n",
            "Training Iteration 5152, Loss: 3.594730854034424\n",
            "Training Iteration 5153, Loss: 6.3631272315979\n",
            "Training Iteration 5154, Loss: 4.685898780822754\n",
            "Training Iteration 5155, Loss: 6.353775978088379\n",
            "Training Iteration 5156, Loss: 2.581927537918091\n",
            "Training Iteration 5157, Loss: 3.5584096908569336\n",
            "Training Iteration 5158, Loss: 3.776050090789795\n",
            "Training Iteration 5159, Loss: 4.028940200805664\n",
            "Training Iteration 5160, Loss: 3.2859549522399902\n",
            "Training Iteration 5161, Loss: 3.156083822250366\n",
            "Training Iteration 5162, Loss: 3.277249336242676\n",
            "Training Iteration 5163, Loss: 3.6778364181518555\n",
            "Training Iteration 5164, Loss: 5.383246421813965\n",
            "Training Iteration 5165, Loss: 4.321645736694336\n",
            "Training Iteration 5166, Loss: 4.673737525939941\n",
            "Training Iteration 5167, Loss: 5.275495529174805\n",
            "Training Iteration 5168, Loss: 4.1157636642456055\n",
            "Training Iteration 5169, Loss: 5.77052640914917\n",
            "Training Iteration 5170, Loss: 3.607022523880005\n",
            "Training Iteration 5171, Loss: 5.104844570159912\n",
            "Training Iteration 5172, Loss: 1.8190456628799438\n",
            "Training Iteration 5173, Loss: 1.7309099435806274\n",
            "Training Iteration 5174, Loss: 3.4269986152648926\n",
            "Training Iteration 5175, Loss: 4.658711910247803\n",
            "Training Iteration 5176, Loss: 3.782912015914917\n",
            "Training Iteration 5177, Loss: 4.09822940826416\n",
            "Training Iteration 5178, Loss: 1.2479283809661865\n",
            "Training Iteration 5179, Loss: 4.14120626449585\n",
            "Training Iteration 5180, Loss: 4.888192176818848\n",
            "Training Iteration 5181, Loss: 7.356110572814941\n",
            "Training Iteration 5182, Loss: 5.701318264007568\n",
            "Training Iteration 5183, Loss: 4.9900312423706055\n",
            "Training Iteration 5184, Loss: 4.776910305023193\n",
            "Training Iteration 5185, Loss: 3.2170708179473877\n",
            "Training Iteration 5186, Loss: 4.030694484710693\n",
            "Training Iteration 5187, Loss: 4.195915222167969\n",
            "Training Iteration 5188, Loss: 6.463902473449707\n",
            "Training Iteration 5189, Loss: 5.86248254776001\n",
            "Training Iteration 5190, Loss: 6.680028915405273\n",
            "Training Iteration 5191, Loss: 2.3343706130981445\n",
            "Training Iteration 5192, Loss: 7.154201030731201\n",
            "Training Iteration 5193, Loss: 3.72497296333313\n",
            "Training Iteration 5194, Loss: 6.5419416427612305\n",
            "Training Iteration 5195, Loss: 2.6209092140197754\n",
            "Training Iteration 5196, Loss: 7.588266372680664\n",
            "Training Iteration 5197, Loss: 4.533318519592285\n",
            "Training Iteration 5198, Loss: 7.252531051635742\n",
            "Training Iteration 5199, Loss: 2.973914384841919\n",
            "Training Iteration 5200, Loss: 4.34631872177124\n",
            "Training Iteration 5201, Loss: 4.18040657043457\n",
            "Training Iteration 5202, Loss: 4.739592552185059\n",
            "Training Iteration 5203, Loss: 0.6972994804382324\n",
            "Training Iteration 5204, Loss: 1.2101975679397583\n",
            "Training Iteration 5205, Loss: 3.5237064361572266\n",
            "Training Iteration 5206, Loss: 5.822411060333252\n",
            "Training Iteration 5207, Loss: 6.285722732543945\n",
            "Training Iteration 5208, Loss: 4.805299282073975\n",
            "Training Iteration 5209, Loss: 3.7312145233154297\n",
            "Training Iteration 5210, Loss: 4.309810638427734\n",
            "Training Iteration 5211, Loss: 5.4566731452941895\n",
            "Training Iteration 5212, Loss: 4.105062484741211\n",
            "Training Iteration 5213, Loss: 4.416741847991943\n",
            "Training Iteration 5214, Loss: 8.40931224822998\n",
            "Training Iteration 5215, Loss: 4.093597888946533\n",
            "Training Iteration 5216, Loss: 1.9775556325912476\n",
            "Training Iteration 5217, Loss: 1.2594358921051025\n",
            "Training Iteration 5218, Loss: 3.668931722640991\n",
            "Training Iteration 5219, Loss: 2.8740551471710205\n",
            "Training Iteration 5220, Loss: 2.6395349502563477\n",
            "Training Iteration 5221, Loss: 2.7303380966186523\n",
            "Training Iteration 5222, Loss: 2.7777554988861084\n",
            "Training Iteration 5223, Loss: 7.645823001861572\n",
            "Training Iteration 5224, Loss: 3.302978277206421\n",
            "Training Iteration 5225, Loss: 4.838407039642334\n",
            "Training Iteration 5226, Loss: 2.6742825508117676\n",
            "Training Iteration 5227, Loss: 5.528179168701172\n",
            "Training Iteration 5228, Loss: 8.328141212463379\n",
            "Training Iteration 5229, Loss: 2.796842098236084\n",
            "Training Iteration 5230, Loss: 4.243661403656006\n",
            "Training Iteration 5231, Loss: 3.4906809329986572\n",
            "Training Iteration 5232, Loss: 4.918168067932129\n",
            "Training Iteration 5233, Loss: 6.317576885223389\n",
            "Training Iteration 5234, Loss: 6.42707633972168\n",
            "Training Iteration 5235, Loss: 6.985339641571045\n",
            "Training Iteration 5236, Loss: 5.014159679412842\n",
            "Training Iteration 5237, Loss: 4.168562889099121\n",
            "Training Iteration 5238, Loss: 5.029789924621582\n",
            "Training Iteration 5239, Loss: 2.8864240646362305\n",
            "Training Iteration 5240, Loss: 8.213949203491211\n",
            "Training Iteration 5241, Loss: 3.8200900554656982\n",
            "Training Iteration 5242, Loss: 2.9628801345825195\n",
            "Training Iteration 5243, Loss: 4.615664005279541\n",
            "Training Iteration 5244, Loss: 3.811584711074829\n",
            "Training Iteration 5245, Loss: 3.701444149017334\n",
            "Training Iteration 5246, Loss: 3.752087116241455\n",
            "Training Iteration 5247, Loss: 6.90681791305542\n",
            "Training Iteration 5248, Loss: 3.9871010780334473\n",
            "Training Iteration 5249, Loss: 5.920248508453369\n",
            "Training Iteration 5250, Loss: 6.770672798156738\n",
            "Training Iteration 5251, Loss: 5.1650848388671875\n",
            "Training Iteration 5252, Loss: 4.391485691070557\n",
            "Training Iteration 5253, Loss: 8.279836654663086\n",
            "Training Iteration 5254, Loss: 7.884960174560547\n",
            "Training Iteration 5255, Loss: 6.134047031402588\n",
            "Training Iteration 5256, Loss: 8.448229789733887\n",
            "Training Iteration 5257, Loss: 9.142663955688477\n",
            "Training Iteration 5258, Loss: 4.70208740234375\n",
            "Training Iteration 5259, Loss: 6.3027448654174805\n",
            "Training Iteration 5260, Loss: 6.483009338378906\n",
            "Training Iteration 5261, Loss: 5.095080852508545\n",
            "Training Iteration 5262, Loss: 1.8906813859939575\n",
            "Training Iteration 5263, Loss: 3.235191822052002\n",
            "Training Iteration 5264, Loss: 4.009703636169434\n",
            "Training Iteration 5265, Loss: 7.2380876541137695\n",
            "Training Iteration 5266, Loss: 5.144547939300537\n",
            "Training Iteration 5267, Loss: 5.1437859535217285\n",
            "Training Iteration 5268, Loss: 2.0823757648468018\n",
            "Training Iteration 5269, Loss: 3.0902621746063232\n",
            "Training Iteration 5270, Loss: 8.081989288330078\n",
            "Training Iteration 5271, Loss: 5.04226016998291\n",
            "Training Iteration 5272, Loss: 5.069641590118408\n",
            "Training Iteration 5273, Loss: 3.38985538482666\n",
            "Training Iteration 5274, Loss: 4.3425750732421875\n",
            "Training Iteration 5275, Loss: 3.13132905960083\n",
            "Training Iteration 5276, Loss: 5.454612731933594\n",
            "Training Iteration 5277, Loss: 2.9601430892944336\n",
            "Training Iteration 5278, Loss: 3.618439197540283\n",
            "Training Iteration 5279, Loss: 5.599699974060059\n",
            "Training Iteration 5280, Loss: 4.315350532531738\n",
            "Training Iteration 5281, Loss: 2.5635340213775635\n",
            "Training Iteration 5282, Loss: 3.8036441802978516\n",
            "Training Iteration 5283, Loss: 5.938574314117432\n",
            "Training Iteration 5284, Loss: 3.8506624698638916\n",
            "Training Iteration 5285, Loss: 5.645168781280518\n",
            "Training Iteration 5286, Loss: 6.194134712219238\n",
            "Training Iteration 5287, Loss: 2.298948287963867\n",
            "Training Iteration 5288, Loss: 4.176555156707764\n",
            "Training Iteration 5289, Loss: 4.903049468994141\n",
            "Training Iteration 5290, Loss: 5.3779802322387695\n",
            "Training Iteration 5291, Loss: 5.163282871246338\n",
            "Training Iteration 5292, Loss: 2.043600082397461\n",
            "Training Iteration 5293, Loss: 3.705888509750366\n",
            "Training Iteration 5294, Loss: 6.418297290802002\n",
            "Training Iteration 5295, Loss: 3.1470789909362793\n",
            "Training Iteration 5296, Loss: 3.622103691101074\n",
            "Training Iteration 5297, Loss: 4.647915363311768\n",
            "Training Iteration 5298, Loss: 6.773988723754883\n",
            "Training Iteration 5299, Loss: 0.9980251789093018\n",
            "Training Iteration 5300, Loss: 4.456055164337158\n",
            "Training Iteration 5301, Loss: 1.971398115158081\n",
            "Training Iteration 5302, Loss: 2.448566436767578\n",
            "Training Iteration 5303, Loss: 5.791332244873047\n",
            "Training Iteration 5304, Loss: 5.038542747497559\n",
            "Training Iteration 5305, Loss: 7.14346981048584\n",
            "Training Iteration 5306, Loss: 4.854859352111816\n",
            "Training Iteration 5307, Loss: 5.010796546936035\n",
            "Training Iteration 5308, Loss: 3.1494388580322266\n",
            "Training Iteration 5309, Loss: 2.7725467681884766\n",
            "Training Iteration 5310, Loss: 2.369922637939453\n",
            "Training Iteration 5311, Loss: 3.727238416671753\n",
            "Training Iteration 5312, Loss: 3.344618082046509\n",
            "Training Iteration 5313, Loss: 6.344852924346924\n",
            "Training Iteration 5314, Loss: 4.336297035217285\n",
            "Training Iteration 5315, Loss: 3.589090585708618\n",
            "Training Iteration 5316, Loss: 3.675774097442627\n",
            "Training Iteration 5317, Loss: 5.679360866546631\n",
            "Training Iteration 5318, Loss: 4.042330741882324\n",
            "Training Iteration 5319, Loss: 1.775957703590393\n",
            "Training Iteration 5320, Loss: 3.348316192626953\n",
            "Training Iteration 5321, Loss: 4.411285877227783\n",
            "Training Iteration 5322, Loss: 3.585829734802246\n",
            "Training Iteration 5323, Loss: 2.9575538635253906\n",
            "Training Iteration 5324, Loss: 4.739687442779541\n",
            "Training Iteration 5325, Loss: 2.4678845405578613\n",
            "Training Iteration 5326, Loss: 3.597130298614502\n",
            "Training Iteration 5327, Loss: 2.5226969718933105\n",
            "Training Iteration 5328, Loss: 4.631485462188721\n",
            "Training Iteration 5329, Loss: 2.782130002975464\n",
            "Training Iteration 5330, Loss: 2.886489152908325\n",
            "Training Iteration 5331, Loss: 4.2194108963012695\n",
            "Training Iteration 5332, Loss: 2.2206614017486572\n",
            "Training Iteration 5333, Loss: 2.755168914794922\n",
            "Training Iteration 5334, Loss: 3.2635021209716797\n",
            "Training Iteration 5335, Loss: 4.191758155822754\n",
            "Training Iteration 5336, Loss: 2.7039902210235596\n",
            "Training Iteration 5337, Loss: 5.323741436004639\n",
            "Training Iteration 5338, Loss: 7.76231050491333\n",
            "Training Iteration 5339, Loss: 3.166409730911255\n",
            "Training Iteration 5340, Loss: 7.007252216339111\n",
            "Training Iteration 5341, Loss: 3.809727668762207\n",
            "Training Iteration 5342, Loss: 6.105327129364014\n",
            "Training Iteration 5343, Loss: 3.8851280212402344\n",
            "Training Iteration 5344, Loss: 4.637260913848877\n",
            "Training Iteration 5345, Loss: 4.294826030731201\n",
            "Training Iteration 5346, Loss: 4.28485107421875\n",
            "Training Iteration 5347, Loss: 3.065669059753418\n",
            "Training Iteration 5348, Loss: 2.240999221801758\n",
            "Training Iteration 5349, Loss: 5.814889907836914\n",
            "Training Iteration 5350, Loss: 2.6630654335021973\n",
            "Training Iteration 5351, Loss: 6.177825450897217\n",
            "Training Iteration 5352, Loss: 3.908576011657715\n",
            "Training Iteration 5353, Loss: 5.773741245269775\n",
            "Training Iteration 5354, Loss: 7.684074878692627\n",
            "Training Iteration 5355, Loss: 5.559901237487793\n",
            "Training Iteration 5356, Loss: 3.551597833633423\n",
            "Training Iteration 5357, Loss: 3.796721935272217\n",
            "Training Iteration 5358, Loss: 3.807623863220215\n",
            "Training Iteration 5359, Loss: 4.8282341957092285\n",
            "Training Iteration 5360, Loss: 3.344998836517334\n",
            "Training Iteration 5361, Loss: 6.962470531463623\n",
            "Training Iteration 5362, Loss: 6.676037788391113\n",
            "Training Iteration 5363, Loss: 2.9047610759735107\n",
            "Training Iteration 5364, Loss: 4.986693859100342\n",
            "Training Iteration 5365, Loss: 2.3868722915649414\n",
            "Training Iteration 5366, Loss: 2.2520620822906494\n",
            "Training Iteration 5367, Loss: 7.742464542388916\n",
            "Training Iteration 5368, Loss: 1.581665277481079\n",
            "Training Iteration 5369, Loss: 5.722020149230957\n",
            "Training Iteration 5370, Loss: 4.0883307456970215\n",
            "Training Iteration 5371, Loss: 3.237427234649658\n",
            "Training Iteration 5372, Loss: 2.1328485012054443\n",
            "Training Iteration 5373, Loss: 7.197356700897217\n",
            "Training Iteration 5374, Loss: 4.035417556762695\n",
            "Training Iteration 5375, Loss: 5.225984573364258\n",
            "Training Iteration 5376, Loss: 5.758086681365967\n",
            "Training Iteration 5377, Loss: 6.68456506729126\n",
            "Training Iteration 5378, Loss: 12.990987777709961\n",
            "Training Iteration 5379, Loss: 3.6204025745391846\n",
            "Training Iteration 5380, Loss: 6.834770679473877\n",
            "Training Iteration 5381, Loss: 4.967091083526611\n",
            "Training Iteration 5382, Loss: 3.350733518600464\n",
            "Training Iteration 5383, Loss: 4.295295238494873\n",
            "Training Iteration 5384, Loss: 3.4590959548950195\n",
            "Training Iteration 5385, Loss: 3.0876028537750244\n",
            "Training Iteration 5386, Loss: 5.277768135070801\n",
            "Training Iteration 5387, Loss: 3.716989517211914\n",
            "Training Iteration 5388, Loss: 3.3874759674072266\n",
            "Training Iteration 5389, Loss: 2.42680025100708\n",
            "Training Iteration 5390, Loss: 7.8106608390808105\n",
            "Training Iteration 5391, Loss: 5.640859603881836\n",
            "Training Iteration 5392, Loss: 7.54954719543457\n",
            "Training Iteration 5393, Loss: 3.51767635345459\n",
            "Training Iteration 5394, Loss: 1.7837990522384644\n",
            "Training Iteration 5395, Loss: 5.746884346008301\n",
            "Training Iteration 5396, Loss: 5.369994163513184\n",
            "Training Iteration 5397, Loss: 1.7432838678359985\n",
            "Training Iteration 5398, Loss: 5.909399509429932\n",
            "Training Iteration 5399, Loss: 5.821077823638916\n",
            "Training Iteration 5400, Loss: 7.131181716918945\n",
            "Training Iteration 5401, Loss: 5.544345378875732\n",
            "Training Iteration 5402, Loss: 3.9098854064941406\n",
            "Training Iteration 5403, Loss: 4.739748954772949\n",
            "Training Iteration 5404, Loss: 4.890017509460449\n",
            "Training Iteration 5405, Loss: 3.967520236968994\n",
            "Training Iteration 5406, Loss: 6.3009538650512695\n",
            "Training Iteration 5407, Loss: 3.9980833530426025\n",
            "Training Iteration 5408, Loss: 3.1635351181030273\n",
            "Training Iteration 5409, Loss: 6.316091537475586\n",
            "Training Iteration 5410, Loss: 4.272016525268555\n",
            "Training Iteration 5411, Loss: 2.951004981994629\n",
            "Training Iteration 5412, Loss: 5.3702168464660645\n",
            "Training Iteration 5413, Loss: 3.7548835277557373\n",
            "Training Iteration 5414, Loss: 6.726186275482178\n",
            "Training Iteration 5415, Loss: 5.917782306671143\n",
            "Training Iteration 5416, Loss: 5.7309980392456055\n",
            "Training Iteration 5417, Loss: 6.289600372314453\n",
            "Training Iteration 5418, Loss: 2.2248334884643555\n",
            "Training Iteration 5419, Loss: 3.1768834590911865\n",
            "Training Iteration 5420, Loss: 2.722321033477783\n",
            "Training Iteration 5421, Loss: 5.5519609451293945\n",
            "Training Iteration 5422, Loss: 4.452939987182617\n",
            "Training Iteration 5423, Loss: 1.7439148426055908\n",
            "Training Iteration 5424, Loss: 2.9756267070770264\n",
            "Training Iteration 5425, Loss: 5.516293525695801\n",
            "Training Iteration 5426, Loss: 1.5520744323730469\n",
            "Training Iteration 5427, Loss: 4.733089923858643\n",
            "Training Iteration 5428, Loss: 3.363523483276367\n",
            "Training Iteration 5429, Loss: 8.354755401611328\n",
            "Training Iteration 5430, Loss: 4.642521858215332\n",
            "Training Iteration 5431, Loss: 3.3522300720214844\n",
            "Training Iteration 5432, Loss: 3.7310972213745117\n",
            "Training Iteration 5433, Loss: 4.936295986175537\n",
            "Training Iteration 5434, Loss: 3.9251937866210938\n",
            "Training Iteration 5435, Loss: 4.644161224365234\n",
            "Training Iteration 5436, Loss: 1.729483962059021\n",
            "Training Iteration 5437, Loss: 2.0500540733337402\n",
            "Training Iteration 5438, Loss: 5.022182941436768\n",
            "Training Iteration 5439, Loss: 3.113853931427002\n",
            "Training Iteration 5440, Loss: 5.723740577697754\n",
            "Training Iteration 5441, Loss: 0.9062328338623047\n",
            "Training Iteration 5442, Loss: 3.50858736038208\n",
            "Training Iteration 5443, Loss: 3.46724271774292\n",
            "Training Iteration 5444, Loss: 4.197137832641602\n",
            "Training Iteration 5445, Loss: 2.3319242000579834\n",
            "Training Iteration 5446, Loss: 3.266345977783203\n",
            "Training Iteration 5447, Loss: 3.4374446868896484\n",
            "Training Iteration 5448, Loss: 3.6533265113830566\n",
            "Training Iteration 5449, Loss: 4.900094985961914\n",
            "Training Iteration 5450, Loss: 4.293093204498291\n",
            "Training Iteration 5451, Loss: 4.691055774688721\n",
            "Training Iteration 5452, Loss: 2.804460287094116\n",
            "Training Iteration 5453, Loss: 4.844995498657227\n",
            "Training Iteration 5454, Loss: 5.267199516296387\n",
            "Training Iteration 5455, Loss: 3.3270821571350098\n",
            "Training Iteration 5456, Loss: 3.833559274673462\n",
            "Training Iteration 5457, Loss: 3.176330327987671\n",
            "Training Iteration 5458, Loss: 3.6208460330963135\n",
            "Training Iteration 5459, Loss: 4.373793601989746\n",
            "Training Iteration 5460, Loss: 6.062533378601074\n",
            "Training Iteration 5461, Loss: 4.671303749084473\n",
            "Training Iteration 5462, Loss: 4.8532843589782715\n",
            "Training Iteration 5463, Loss: 4.97032356262207\n",
            "Training Iteration 5464, Loss: 4.501242637634277\n",
            "Training Iteration 5465, Loss: 4.208823204040527\n",
            "Training Iteration 5466, Loss: 5.214923858642578\n",
            "Training Iteration 5467, Loss: 6.193544387817383\n",
            "Training Iteration 5468, Loss: 4.403278827667236\n",
            "Training Iteration 5469, Loss: 4.088497161865234\n",
            "Training Iteration 5470, Loss: 5.493061542510986\n",
            "Training Iteration 5471, Loss: 3.5761985778808594\n",
            "Training Iteration 5472, Loss: 3.784452438354492\n",
            "Training Iteration 5473, Loss: 5.924945831298828\n",
            "Training Iteration 5474, Loss: 3.4466848373413086\n",
            "Training Iteration 5475, Loss: 8.678190231323242\n",
            "Training Iteration 5476, Loss: 3.8651022911071777\n",
            "Training Iteration 5477, Loss: 5.1429266929626465\n",
            "Training Iteration 5478, Loss: 4.753330230712891\n",
            "Training Iteration 5479, Loss: 3.8115758895874023\n",
            "Training Iteration 5480, Loss: 5.791254997253418\n",
            "Training Iteration 5481, Loss: 1.4638680219650269\n",
            "Training Iteration 5482, Loss: 4.160445213317871\n",
            "Training Iteration 5483, Loss: 4.4833855628967285\n",
            "Training Iteration 5484, Loss: 4.370364189147949\n",
            "Training Iteration 5485, Loss: 3.5832698345184326\n",
            "Training Iteration 5486, Loss: 2.5219366550445557\n",
            "Training Iteration 5487, Loss: 6.733095169067383\n",
            "Training Iteration 5488, Loss: 3.769613742828369\n",
            "Training Iteration 5489, Loss: 6.1838507652282715\n",
            "Training Iteration 5490, Loss: 2.4891374111175537\n",
            "Training Iteration 5491, Loss: 5.492194652557373\n",
            "Training Iteration 5492, Loss: 6.930027008056641\n",
            "Training Iteration 5493, Loss: 3.0208866596221924\n",
            "Training Iteration 5494, Loss: 3.7185592651367188\n",
            "Training Iteration 5495, Loss: 4.580206871032715\n",
            "Training Iteration 5496, Loss: 3.375033378601074\n",
            "Training Iteration 5497, Loss: 3.79455304145813\n",
            "Training Iteration 5498, Loss: 2.7734203338623047\n",
            "Training Iteration 5499, Loss: 3.3648316860198975\n",
            "Training Iteration 5500, Loss: 3.862490177154541\n",
            "Training Iteration 5501, Loss: 4.406494617462158\n",
            "Training Iteration 5502, Loss: 6.731603622436523\n",
            "Training Iteration 5503, Loss: 3.616027593612671\n",
            "Training Iteration 5504, Loss: 3.0575530529022217\n",
            "Training Iteration 5505, Loss: 3.609625816345215\n",
            "Training Iteration 5506, Loss: 4.546431541442871\n",
            "Training Iteration 5507, Loss: 6.962576389312744\n",
            "Training Iteration 5508, Loss: 1.535660982131958\n",
            "Training Iteration 5509, Loss: 3.6924009323120117\n",
            "Training Iteration 5510, Loss: 3.6931028366088867\n",
            "Training Iteration 5511, Loss: 2.0922861099243164\n",
            "Training Iteration 5512, Loss: 2.936291217803955\n",
            "Training Iteration 5513, Loss: 5.023352146148682\n",
            "Training Iteration 5514, Loss: 4.101547718048096\n",
            "Training Iteration 5515, Loss: 3.5142786502838135\n",
            "Training Iteration 5516, Loss: 5.172769069671631\n",
            "Training Iteration 5517, Loss: 1.8894685506820679\n",
            "Training Iteration 5518, Loss: 2.777055025100708\n",
            "Training Iteration 5519, Loss: 4.300256729125977\n",
            "Training Iteration 5520, Loss: 6.637704372406006\n",
            "Training Iteration 5521, Loss: 5.158027648925781\n",
            "Training Iteration 5522, Loss: 3.399049758911133\n",
            "Training Iteration 5523, Loss: 5.515941619873047\n",
            "Training Iteration 5524, Loss: 2.468454122543335\n",
            "Training Iteration 5525, Loss: 5.173163890838623\n",
            "Training Iteration 5526, Loss: 3.5247962474823\n",
            "Training Iteration 5527, Loss: 3.2601139545440674\n",
            "Training Iteration 5528, Loss: 3.9403631687164307\n",
            "Training Iteration 5529, Loss: 3.716140031814575\n",
            "Training Iteration 5530, Loss: 5.203003883361816\n",
            "Training Iteration 5531, Loss: 4.770686626434326\n",
            "Training Iteration 5532, Loss: 5.450324058532715\n",
            "Training Iteration 5533, Loss: 3.0661168098449707\n",
            "Training Iteration 5534, Loss: 4.278480529785156\n",
            "Training Iteration 5535, Loss: 3.787801742553711\n",
            "Training Iteration 5536, Loss: 6.76102876663208\n",
            "Training Iteration 5537, Loss: 3.9054362773895264\n",
            "Training Iteration 5538, Loss: 4.024115562438965\n",
            "Training Iteration 5539, Loss: 3.328111410140991\n",
            "Training Iteration 5540, Loss: 5.105812072753906\n",
            "Training Iteration 5541, Loss: 5.465699672698975\n",
            "Training Iteration 5542, Loss: 4.23861026763916\n",
            "Training Iteration 5543, Loss: 7.822455883026123\n",
            "Training Iteration 5544, Loss: 6.219644069671631\n",
            "Training Iteration 5545, Loss: 3.494255781173706\n",
            "Training Iteration 5546, Loss: 3.7194557189941406\n",
            "Training Iteration 5547, Loss: 4.353682994842529\n",
            "Training Iteration 5548, Loss: 4.6611833572387695\n",
            "Training Iteration 5549, Loss: 5.981461048126221\n",
            "Training Iteration 5550, Loss: 6.705069541931152\n",
            "Training Iteration 5551, Loss: 4.950522422790527\n",
            "Training Iteration 5552, Loss: 5.4976301193237305\n",
            "Training Iteration 5553, Loss: 5.265422821044922\n",
            "Training Iteration 5554, Loss: 5.039170742034912\n",
            "Training Iteration 5555, Loss: 4.572237491607666\n",
            "Training Iteration 5556, Loss: 4.996883869171143\n",
            "Training Iteration 5557, Loss: 3.9050683975219727\n",
            "Training Iteration 5558, Loss: 3.411888599395752\n",
            "Training Iteration 5559, Loss: 7.918351650238037\n",
            "Training Iteration 5560, Loss: 6.231064319610596\n",
            "Training Iteration 5561, Loss: 5.243905067443848\n",
            "Training Iteration 5562, Loss: 3.8180742263793945\n",
            "Training Iteration 5563, Loss: 5.743523597717285\n",
            "Training Iteration 5564, Loss: 4.826870441436768\n",
            "Training Iteration 5565, Loss: 5.7283430099487305\n",
            "Training Iteration 5566, Loss: 3.1778719425201416\n",
            "Training Iteration 5567, Loss: 5.530519962310791\n",
            "Training Iteration 5568, Loss: 3.6178789138793945\n",
            "Training Iteration 5569, Loss: 4.1199822425842285\n",
            "Training Iteration 5570, Loss: 8.30302906036377\n",
            "Training Iteration 5571, Loss: 6.160383224487305\n",
            "Training Iteration 5572, Loss: 4.570580005645752\n",
            "Training Iteration 5573, Loss: 1.9072529077529907\n",
            "Training Iteration 5574, Loss: 9.206570625305176\n",
            "Training Iteration 5575, Loss: 4.7220869064331055\n",
            "Training Iteration 5576, Loss: 12.86628246307373\n",
            "Training Iteration 5577, Loss: 3.770845413208008\n",
            "Training Iteration 5578, Loss: 2.301374912261963\n",
            "Training Iteration 5579, Loss: 3.345921516418457\n",
            "Training Iteration 5580, Loss: 2.9979586601257324\n",
            "Training Iteration 5581, Loss: 6.778104782104492\n",
            "Training Iteration 5582, Loss: 4.6438493728637695\n",
            "Training Iteration 5583, Loss: 3.7100958824157715\n",
            "Training Iteration 5584, Loss: 6.423646450042725\n",
            "Training Iteration 5585, Loss: 6.340502738952637\n",
            "Training Iteration 5586, Loss: 7.840028762817383\n",
            "Training Iteration 5587, Loss: 4.70357084274292\n",
            "Training Iteration 5588, Loss: 5.864021301269531\n",
            "Training Iteration 5589, Loss: 4.9561543464660645\n",
            "Training Iteration 5590, Loss: 4.7751874923706055\n",
            "Training Iteration 5591, Loss: 3.8302793502807617\n",
            "Training Iteration 5592, Loss: 5.5476202964782715\n",
            "Training Iteration 5593, Loss: 7.71602725982666\n",
            "Training Iteration 5594, Loss: 4.759678840637207\n",
            "Training Iteration 5595, Loss: 6.041674613952637\n",
            "Training Iteration 5596, Loss: 4.816731929779053\n",
            "Training Iteration 5597, Loss: 2.714705467224121\n",
            "Training Iteration 5598, Loss: 4.2843756675720215\n",
            "Training Iteration 5599, Loss: 8.47890853881836\n",
            "Training Iteration 5600, Loss: 9.756978988647461\n",
            "Training Iteration 5601, Loss: 5.01814603805542\n",
            "Training Iteration 5602, Loss: 6.694324493408203\n",
            "Training Iteration 5603, Loss: 3.194199562072754\n",
            "Training Iteration 5604, Loss: 5.266650676727295\n",
            "Training Iteration 5605, Loss: 2.877413511276245\n",
            "Training Iteration 5606, Loss: 5.0984930992126465\n",
            "Training Iteration 5607, Loss: 7.653253078460693\n",
            "Training Iteration 5608, Loss: 4.4386725425720215\n",
            "Training Iteration 5609, Loss: 2.2987632751464844\n",
            "Training Iteration 5610, Loss: 5.356388092041016\n",
            "Training Iteration 5611, Loss: 7.976661682128906\n",
            "Training Iteration 5612, Loss: 8.019287109375\n",
            "Training Iteration 5613, Loss: 6.296186923980713\n",
            "Training Iteration 5614, Loss: 4.049679756164551\n",
            "Training Iteration 5615, Loss: 2.3267641067504883\n",
            "Training Iteration 5616, Loss: 4.306270599365234\n",
            "Training Iteration 5617, Loss: 9.779006004333496\n",
            "Training Iteration 5618, Loss: 12.070535659790039\n",
            "Training Iteration 5619, Loss: 7.200191020965576\n",
            "Training Iteration 5620, Loss: 6.268707752227783\n",
            "Training Iteration 5621, Loss: 3.1633448600769043\n",
            "Training Iteration 5622, Loss: 11.914054870605469\n",
            "Training Iteration 5623, Loss: 6.063121795654297\n",
            "Training Iteration 5624, Loss: 8.074079513549805\n",
            "Training Iteration 5625, Loss: 2.7661075592041016\n",
            "Training Iteration 5626, Loss: 4.256053924560547\n",
            "Training Iteration 5627, Loss: 6.943642616271973\n",
            "Training Iteration 5628, Loss: 5.0376996994018555\n",
            "Training Iteration 5629, Loss: 5.215541839599609\n",
            "Training Iteration 5630, Loss: 10.534868240356445\n",
            "Training Iteration 5631, Loss: 1.9007689952850342\n",
            "Training Iteration 5632, Loss: 5.253856658935547\n",
            "Training Iteration 5633, Loss: 3.29221248626709\n",
            "Training Iteration 5634, Loss: 4.5631794929504395\n",
            "Training Iteration 5635, Loss: 6.83464241027832\n",
            "Training Iteration 5636, Loss: 6.720200538635254\n",
            "Training Iteration 5637, Loss: 4.984961986541748\n",
            "Training Iteration 5638, Loss: 9.217537879943848\n",
            "Training Iteration 5639, Loss: 4.59562349319458\n",
            "Training Iteration 5640, Loss: 6.361630439758301\n",
            "Training Iteration 5641, Loss: 8.08267879486084\n",
            "Training Iteration 5642, Loss: 2.6425254344940186\n",
            "Training Iteration 5643, Loss: 1.683802843093872\n",
            "Training Iteration 5644, Loss: 2.8674514293670654\n",
            "Training Iteration 5645, Loss: 4.626541614532471\n",
            "Training Iteration 5646, Loss: 4.351811408996582\n",
            "Training Iteration 5647, Loss: 3.3595621585845947\n",
            "Training Iteration 5648, Loss: 3.6207830905914307\n",
            "Training Iteration 5649, Loss: 3.7049524784088135\n",
            "Training Iteration 5650, Loss: 4.0142822265625\n",
            "Training Iteration 5651, Loss: 4.105053424835205\n",
            "Training Iteration 5652, Loss: 2.846065044403076\n",
            "Training Iteration 5653, Loss: 4.28573751449585\n",
            "Training Iteration 5654, Loss: 5.22648286819458\n",
            "Training Iteration 5655, Loss: 5.3376569747924805\n",
            "Training Iteration 5656, Loss: 6.715804100036621\n",
            "Training Iteration 5657, Loss: 4.028286933898926\n",
            "Training Iteration 5658, Loss: 5.436559677124023\n",
            "Training Iteration 5659, Loss: 4.627351760864258\n",
            "Training Iteration 5660, Loss: 5.098910331726074\n",
            "Training Iteration 5661, Loss: 5.705259323120117\n",
            "Training Iteration 5662, Loss: 2.5242011547088623\n",
            "Training Iteration 5663, Loss: 5.999600410461426\n",
            "Training Iteration 5664, Loss: 4.531789302825928\n",
            "Training Iteration 5665, Loss: 3.8766679763793945\n",
            "Training Iteration 5666, Loss: 5.476935386657715\n",
            "Training Iteration 5667, Loss: 3.199186325073242\n",
            "Training Iteration 5668, Loss: 7.417702674865723\n",
            "Training Iteration 5669, Loss: 2.6805641651153564\n",
            "Training Iteration 5670, Loss: 3.5372915267944336\n",
            "Training Iteration 5671, Loss: 4.307208061218262\n",
            "Training Iteration 5672, Loss: 3.838385820388794\n",
            "Training Iteration 5673, Loss: 6.833820343017578\n",
            "Training Iteration 5674, Loss: 5.643719673156738\n",
            "Training Iteration 5675, Loss: 4.361751079559326\n",
            "Training Iteration 5676, Loss: 4.9741950035095215\n",
            "Training Iteration 5677, Loss: 7.888163089752197\n",
            "Training Iteration 5678, Loss: 3.7267072200775146\n",
            "Training Iteration 5679, Loss: 2.9200599193573\n",
            "Training Iteration 5680, Loss: 3.21463680267334\n",
            "Training Iteration 5681, Loss: 4.362064361572266\n",
            "Training Iteration 5682, Loss: 2.683444023132324\n",
            "Training Iteration 5683, Loss: 2.479722261428833\n",
            "Training Iteration 5684, Loss: 6.689562797546387\n",
            "Training Iteration 5685, Loss: 3.46842098236084\n",
            "Training Iteration 5686, Loss: 3.8173136711120605\n",
            "Training Iteration 5687, Loss: 4.984099864959717\n",
            "Training Iteration 5688, Loss: 8.560383796691895\n",
            "Training Iteration 5689, Loss: 7.191522121429443\n",
            "Training Iteration 5690, Loss: 7.021298885345459\n",
            "Training Iteration 5691, Loss: 7.238535404205322\n",
            "Training Iteration 5692, Loss: 5.402271270751953\n",
            "Training Iteration 5693, Loss: 5.597800254821777\n",
            "Training Iteration 5694, Loss: 3.489776849746704\n",
            "Training Iteration 5695, Loss: 6.363390922546387\n",
            "Training Iteration 5696, Loss: 6.385359764099121\n",
            "Training Iteration 5697, Loss: 6.302210330963135\n",
            "Training Iteration 5698, Loss: 5.403672695159912\n",
            "Training Iteration 5699, Loss: 2.882847547531128\n",
            "Training Iteration 5700, Loss: 5.282825469970703\n",
            "Training Iteration 5701, Loss: 8.366326332092285\n",
            "Training Iteration 5702, Loss: 2.593531370162964\n",
            "Training Iteration 5703, Loss: 6.592113971710205\n",
            "Training Iteration 5704, Loss: 5.3494696617126465\n",
            "Training Iteration 5705, Loss: 4.629591464996338\n",
            "Training Iteration 5706, Loss: 3.1250946521759033\n",
            "Training Iteration 5707, Loss: 3.1753482818603516\n",
            "Training Iteration 5708, Loss: 6.207474231719971\n",
            "Training Iteration 5709, Loss: 2.8300280570983887\n",
            "Training Iteration 5710, Loss: 4.592975616455078\n",
            "Training Iteration 5711, Loss: 3.380927085876465\n",
            "Training Iteration 5712, Loss: 5.0947723388671875\n",
            "Training Iteration 5713, Loss: 5.237547397613525\n",
            "Training Iteration 5714, Loss: 2.5516297817230225\n",
            "Training Iteration 5715, Loss: 5.779444217681885\n",
            "Training Iteration 5716, Loss: 3.7807583808898926\n",
            "Training Iteration 5717, Loss: 5.487300872802734\n",
            "Training Iteration 5718, Loss: 8.427529335021973\n",
            "Training Iteration 5719, Loss: 6.419942855834961\n",
            "Training Iteration 5720, Loss: 4.200597763061523\n",
            "Training Iteration 5721, Loss: 2.8040122985839844\n",
            "Training Iteration 5722, Loss: 6.708752632141113\n",
            "Training Iteration 5723, Loss: 6.650991916656494\n",
            "Training Iteration 5724, Loss: 3.197183609008789\n",
            "Training Iteration 5725, Loss: 8.187348365783691\n",
            "Training Iteration 5726, Loss: 7.605492115020752\n",
            "Training Iteration 5727, Loss: 4.082975387573242\n",
            "Training Iteration 5728, Loss: 1.2764947414398193\n",
            "Training Iteration 5729, Loss: 2.6124038696289062\n",
            "Training Iteration 5730, Loss: 7.174211025238037\n",
            "Training Iteration 5731, Loss: 3.8512825965881348\n",
            "Training Iteration 5732, Loss: 4.963903427124023\n",
            "Training Iteration 5733, Loss: 2.7407803535461426\n",
            "Training Iteration 5734, Loss: 6.136455535888672\n",
            "Training Iteration 5735, Loss: 3.5715348720550537\n",
            "Training Iteration 5736, Loss: 4.327093601226807\n",
            "Training Iteration 5737, Loss: 2.5125553607940674\n",
            "Training Iteration 5738, Loss: 6.82304048538208\n",
            "Training Iteration 5739, Loss: 2.782517671585083\n",
            "Training Iteration 5740, Loss: 3.787855625152588\n",
            "Training Iteration 5741, Loss: 3.2928624153137207\n",
            "Training Iteration 5742, Loss: 5.3463897705078125\n",
            "Training Iteration 5743, Loss: 2.8214242458343506\n",
            "Training Iteration 5744, Loss: 7.052425384521484\n",
            "Training Iteration 5745, Loss: 4.1021318435668945\n",
            "Training Iteration 5746, Loss: 1.310593843460083\n",
            "Training Iteration 5747, Loss: 4.313642501831055\n",
            "Training Iteration 5748, Loss: 3.356050729751587\n",
            "Training Iteration 5749, Loss: 4.286162853240967\n",
            "Training Iteration 5750, Loss: 5.16007137298584\n",
            "Training Iteration 5751, Loss: 5.18143367767334\n",
            "Training Iteration 5752, Loss: 5.632274150848389\n",
            "Training Iteration 5753, Loss: 5.8872575759887695\n",
            "Training Iteration 5754, Loss: 5.027235984802246\n",
            "Training Iteration 5755, Loss: 4.661404132843018\n",
            "Training Iteration 5756, Loss: 4.576587200164795\n",
            "Training Iteration 5757, Loss: 5.620622634887695\n",
            "Training Iteration 5758, Loss: 4.205989360809326\n",
            "Training Iteration 5759, Loss: 3.987823963165283\n",
            "Training Iteration 5760, Loss: 7.38276481628418\n",
            "Training Iteration 5761, Loss: 3.812394618988037\n",
            "Training Iteration 5762, Loss: 6.232158660888672\n",
            "Training Iteration 5763, Loss: 5.816799163818359\n",
            "Training Iteration 5764, Loss: 2.540225028991699\n",
            "Training Iteration 5765, Loss: 3.1573054790496826\n",
            "Training Iteration 5766, Loss: 5.024252414703369\n",
            "Training Iteration 5767, Loss: 8.510367393493652\n",
            "Training Iteration 5768, Loss: 6.700137615203857\n",
            "Training Iteration 5769, Loss: 6.963727951049805\n",
            "Training Iteration 5770, Loss: 2.652958393096924\n",
            "Training Iteration 5771, Loss: 4.884016990661621\n",
            "Training Iteration 5772, Loss: 2.8778135776519775\n",
            "Training Iteration 5773, Loss: 5.864386558532715\n",
            "Training Iteration 5774, Loss: 3.7764499187469482\n",
            "Training Iteration 5775, Loss: 1.1738409996032715\n",
            "Training Iteration 5776, Loss: 4.107302665710449\n",
            "Training Iteration 5777, Loss: 3.9312262535095215\n",
            "Training Iteration 5778, Loss: 3.944056510925293\n",
            "Training Iteration 5779, Loss: 4.477376937866211\n",
            "Training Iteration 5780, Loss: 3.0749127864837646\n",
            "Training Iteration 5781, Loss: 6.038725852966309\n",
            "Training Iteration 5782, Loss: 4.088330268859863\n",
            "Training Iteration 5783, Loss: 3.8889477252960205\n",
            "Training Iteration 5784, Loss: 3.549856424331665\n",
            "Training Iteration 5785, Loss: 4.43939208984375\n",
            "Training Iteration 5786, Loss: 2.281968116760254\n",
            "Training Iteration 5787, Loss: 6.588223934173584\n",
            "Training Iteration 5788, Loss: 5.156802654266357\n",
            "Training Iteration 5789, Loss: 6.102608680725098\n",
            "Training Iteration 5790, Loss: 4.835704803466797\n",
            "Training Iteration 5791, Loss: 6.1738505363464355\n",
            "Training Iteration 5792, Loss: 1.9426685571670532\n",
            "Training Iteration 5793, Loss: 4.696112155914307\n",
            "Training Iteration 5794, Loss: 4.529236316680908\n",
            "Training Iteration 5795, Loss: 4.984865188598633\n",
            "Training Iteration 5796, Loss: 4.500476360321045\n",
            "Training Iteration 5797, Loss: 5.684223175048828\n",
            "Training Iteration 5798, Loss: 5.018253326416016\n",
            "Training Iteration 5799, Loss: 6.409170150756836\n",
            "Training Iteration 5800, Loss: 3.500761032104492\n",
            "Training Iteration 5801, Loss: 4.390080451965332\n",
            "Training Iteration 5802, Loss: 4.1320343017578125\n",
            "Training Iteration 5803, Loss: 5.890153884887695\n",
            "Training Iteration 5804, Loss: 4.918365001678467\n",
            "Training Iteration 5805, Loss: 8.234904289245605\n",
            "Training Iteration 5806, Loss: 4.044722557067871\n",
            "Training Iteration 5807, Loss: 2.2809934616088867\n",
            "Training Iteration 5808, Loss: 2.736790180206299\n",
            "Training Iteration 5809, Loss: 5.543107032775879\n",
            "Training Iteration 5810, Loss: 4.917223930358887\n",
            "Training Iteration 5811, Loss: 5.389062881469727\n",
            "Training Iteration 5812, Loss: 4.677337646484375\n",
            "Training Iteration 5813, Loss: 5.196139812469482\n",
            "Training Iteration 5814, Loss: 3.3311116695404053\n",
            "Training Iteration 5815, Loss: 2.024832248687744\n",
            "Training Iteration 5816, Loss: 2.2368886470794678\n",
            "Training Iteration 5817, Loss: 2.226682662963867\n",
            "Training Iteration 5818, Loss: 2.408454656600952\n",
            "Training Iteration 5819, Loss: 3.416548252105713\n",
            "Training Iteration 5820, Loss: 3.671755313873291\n",
            "Training Iteration 5821, Loss: 5.473356246948242\n",
            "Training Iteration 5822, Loss: 3.7743945121765137\n",
            "Training Iteration 5823, Loss: 3.203946828842163\n",
            "Training Iteration 5824, Loss: 4.419274806976318\n",
            "Training Iteration 5825, Loss: 4.195459365844727\n",
            "Training Iteration 5826, Loss: 4.949624061584473\n",
            "Training Iteration 5827, Loss: 4.523299217224121\n",
            "Training Iteration 5828, Loss: 3.5019593238830566\n",
            "Training Iteration 5829, Loss: 3.391848087310791\n",
            "Training Iteration 5830, Loss: 8.104024887084961\n",
            "Training Iteration 5831, Loss: 2.8948886394500732\n",
            "Training Iteration 5832, Loss: 2.923588275909424\n",
            "Training Iteration 5833, Loss: 6.540250778198242\n",
            "Training Iteration 5834, Loss: 3.5857865810394287\n",
            "Training Iteration 5835, Loss: 6.6366496086120605\n",
            "Training Iteration 5836, Loss: 6.244457721710205\n",
            "Training Iteration 5837, Loss: 4.410910129547119\n",
            "Training Iteration 5838, Loss: 5.13941764831543\n",
            "Training Iteration 5839, Loss: 6.502479553222656\n",
            "Training Iteration 5840, Loss: 4.224224090576172\n",
            "Training Iteration 5841, Loss: 4.873299598693848\n",
            "Training Iteration 5842, Loss: 3.490593910217285\n",
            "Training Iteration 5843, Loss: 3.8286635875701904\n",
            "Training Iteration 5844, Loss: 2.8447954654693604\n",
            "Training Iteration 5845, Loss: 8.085221290588379\n",
            "Training Iteration 5846, Loss: 3.299834728240967\n",
            "Training Iteration 5847, Loss: 3.596735715866089\n",
            "Training Iteration 5848, Loss: 2.2956011295318604\n",
            "Training Iteration 5849, Loss: 4.220954895019531\n",
            "Training Iteration 5850, Loss: 8.955950736999512\n",
            "Training Iteration 5851, Loss: 5.7676472663879395\n",
            "Training Iteration 5852, Loss: 6.170685768127441\n",
            "Training Iteration 5853, Loss: 5.450089931488037\n",
            "Training Iteration 5854, Loss: 3.7925291061401367\n",
            "Training Iteration 5855, Loss: 4.984296798706055\n",
            "Training Iteration 5856, Loss: 4.3562493324279785\n",
            "Training Iteration 5857, Loss: 3.7451093196868896\n",
            "Training Iteration 5858, Loss: 3.5171196460723877\n",
            "Training Iteration 5859, Loss: 5.2175726890563965\n",
            "Training Iteration 5860, Loss: 10.097021102905273\n",
            "Training Iteration 5861, Loss: 4.644535064697266\n",
            "Training Iteration 5862, Loss: 5.872629165649414\n",
            "Training Iteration 5863, Loss: 2.315558671951294\n",
            "Training Iteration 5864, Loss: 4.936677932739258\n",
            "Training Iteration 5865, Loss: 4.762889862060547\n",
            "Training Iteration 5866, Loss: 8.73768424987793\n",
            "Training Iteration 5867, Loss: 3.412092924118042\n",
            "Training Iteration 5868, Loss: 4.283679962158203\n",
            "Training Iteration 5869, Loss: 8.890083312988281\n",
            "Training Iteration 5870, Loss: 3.7382149696350098\n",
            "Training Iteration 5871, Loss: 3.366765260696411\n",
            "Training Iteration 5872, Loss: 2.2535624504089355\n",
            "Training Iteration 5873, Loss: 2.0793285369873047\n",
            "Training Iteration 5874, Loss: 4.8030805587768555\n",
            "Training Iteration 5875, Loss: 2.8279261589050293\n",
            "Training Iteration 5876, Loss: 3.321667194366455\n",
            "Training Iteration 5877, Loss: 4.314465522766113\n",
            "Training Iteration 5878, Loss: 4.008798122406006\n",
            "Training Iteration 5879, Loss: 4.739688873291016\n",
            "Training Iteration 5880, Loss: 2.8989858627319336\n",
            "Training Iteration 5881, Loss: 3.940415859222412\n",
            "Training Iteration 5882, Loss: 2.925536870956421\n",
            "Training Iteration 5883, Loss: 4.2296552658081055\n",
            "Training Iteration 5884, Loss: 8.602432250976562\n",
            "Training Iteration 5885, Loss: 5.124663352966309\n",
            "Training Iteration 5886, Loss: 2.417112112045288\n",
            "Training Iteration 5887, Loss: 5.261092185974121\n",
            "Training Iteration 5888, Loss: 3.508068084716797\n",
            "Training Iteration 5889, Loss: 2.309504747390747\n",
            "Training Iteration 5890, Loss: 2.734060764312744\n",
            "Training Iteration 5891, Loss: 1.870635747909546\n",
            "Training Iteration 5892, Loss: 4.175494194030762\n",
            "Training Iteration 5893, Loss: 3.0800974369049072\n",
            "Training Iteration 5894, Loss: 8.106575012207031\n",
            "Training Iteration 5895, Loss: 3.3626046180725098\n",
            "Training Iteration 5896, Loss: 6.526598930358887\n",
            "Training Iteration 5897, Loss: 3.5161986351013184\n",
            "Training Iteration 5898, Loss: 3.137887716293335\n",
            "Training Iteration 5899, Loss: 14.643324851989746\n",
            "Training Iteration 5900, Loss: 6.171414375305176\n",
            "Training Iteration 5901, Loss: 2.658228874206543\n",
            "Training Iteration 5902, Loss: 1.2395271062850952\n",
            "Training Iteration 5903, Loss: 2.6723384857177734\n",
            "Training Iteration 5904, Loss: 4.030055046081543\n",
            "Training Iteration 5905, Loss: 3.8246986865997314\n",
            "Training Iteration 5906, Loss: 3.810140609741211\n",
            "Training Iteration 5907, Loss: 4.358348846435547\n",
            "Training Iteration 5908, Loss: 5.136944770812988\n",
            "Training Iteration 5909, Loss: 3.441185712814331\n",
            "Training Iteration 5910, Loss: 7.325255393981934\n",
            "Training Iteration 5911, Loss: 4.765619277954102\n",
            "Training Iteration 5912, Loss: 2.676610231399536\n",
            "Training Iteration 5913, Loss: 2.530989646911621\n",
            "Training Iteration 5914, Loss: 4.557806491851807\n",
            "Training Iteration 5915, Loss: 4.10131311416626\n",
            "Training Iteration 5916, Loss: 4.558632850646973\n",
            "Training Iteration 5917, Loss: 3.9730072021484375\n",
            "Training Iteration 5918, Loss: 8.302456855773926\n",
            "Training Iteration 5919, Loss: 4.281850337982178\n",
            "Training Iteration 5920, Loss: 3.2617149353027344\n",
            "Training Iteration 5921, Loss: 4.34323787689209\n",
            "Training Iteration 5922, Loss: 5.054633617401123\n",
            "Training Iteration 5923, Loss: 5.116611003875732\n",
            "Training Iteration 5924, Loss: 4.937900066375732\n",
            "Training Iteration 5925, Loss: 6.003443717956543\n",
            "Training Iteration 5926, Loss: 4.31337833404541\n",
            "Training Iteration 5927, Loss: 5.703319549560547\n",
            "Training Iteration 5928, Loss: 1.042902946472168\n",
            "Training Iteration 5929, Loss: 4.980306148529053\n",
            "Training Iteration 5930, Loss: 4.201989650726318\n",
            "Training Iteration 5931, Loss: 6.9539384841918945\n",
            "Training Iteration 5932, Loss: 5.451541423797607\n",
            "Training Iteration 5933, Loss: 9.209478378295898\n",
            "Training Iteration 5934, Loss: 4.039007186889648\n",
            "Training Iteration 5935, Loss: 5.263925552368164\n",
            "Training Iteration 5936, Loss: 6.793558120727539\n",
            "Training Iteration 5937, Loss: 4.2135820388793945\n",
            "Training Iteration 5938, Loss: 4.251789569854736\n",
            "Training Iteration 5939, Loss: 3.9568839073181152\n",
            "Training Iteration 5940, Loss: 4.491893768310547\n",
            "Training Iteration 5941, Loss: 4.6287665367126465\n",
            "Training Iteration 5942, Loss: 9.845319747924805\n",
            "Training Iteration 5943, Loss: 3.532083034515381\n",
            "Training Iteration 5944, Loss: 6.0360798835754395\n",
            "Training Iteration 5945, Loss: 8.787895202636719\n",
            "Training Iteration 5946, Loss: 3.1049468517303467\n",
            "Training Iteration 5947, Loss: 4.031355857849121\n",
            "Training Iteration 5948, Loss: 2.9067227840423584\n",
            "Training Iteration 5949, Loss: 9.346217155456543\n",
            "Training Iteration 5950, Loss: 7.151131629943848\n",
            "Training Iteration 5951, Loss: 3.6421918869018555\n",
            "Training Iteration 5952, Loss: 4.208912372589111\n",
            "Training Iteration 5953, Loss: 4.898911476135254\n",
            "Training Iteration 5954, Loss: 5.365396499633789\n",
            "Training Iteration 5955, Loss: 4.5314741134643555\n",
            "Training Iteration 5956, Loss: 4.899633884429932\n",
            "Training Iteration 5957, Loss: 6.487504959106445\n",
            "Training Iteration 5958, Loss: 5.214729309082031\n",
            "Training Iteration 5959, Loss: 3.9537925720214844\n",
            "Training Iteration 5960, Loss: 2.3731918334960938\n",
            "Training Iteration 5961, Loss: 4.623844146728516\n",
            "Training Iteration 5962, Loss: 2.979095935821533\n",
            "Training Iteration 5963, Loss: 4.21373987197876\n",
            "Training Iteration 5964, Loss: 5.173402786254883\n",
            "Training Iteration 5965, Loss: 6.390531539916992\n",
            "Training Iteration 5966, Loss: 9.324164390563965\n",
            "Training Iteration 5967, Loss: 4.550995349884033\n",
            "Training Iteration 5968, Loss: 6.02734899520874\n",
            "Training Iteration 5969, Loss: 5.518865585327148\n",
            "Training Iteration 5970, Loss: 3.0032904148101807\n",
            "Training Iteration 5971, Loss: 5.882246017456055\n",
            "Training Iteration 5972, Loss: 6.52617073059082\n",
            "Training Iteration 5973, Loss: 6.9725165367126465\n",
            "Training Iteration 5974, Loss: 4.960947513580322\n",
            "Training Iteration 5975, Loss: 7.061191082000732\n",
            "Training Iteration 5976, Loss: 1.8290959596633911\n",
            "Training Iteration 5977, Loss: 4.251162528991699\n",
            "Training Iteration 5978, Loss: 2.680736541748047\n",
            "Training Iteration 5979, Loss: 3.766719102859497\n",
            "Training Iteration 5980, Loss: 4.156265735626221\n",
            "Training Iteration 5981, Loss: 4.036941051483154\n",
            "Training Iteration 5982, Loss: 4.076897144317627\n",
            "Training Iteration 5983, Loss: 2.7460074424743652\n",
            "Training Iteration 5984, Loss: 3.276573896408081\n",
            "Training Iteration 5985, Loss: 8.09731674194336\n",
            "Training Iteration 5986, Loss: 7.330685138702393\n",
            "Training Iteration 5987, Loss: 3.7481420040130615\n",
            "Training Iteration 5988, Loss: 3.786675453186035\n",
            "Training Iteration 5989, Loss: 1.0359312295913696\n",
            "Training Iteration 5990, Loss: 11.82351016998291\n",
            "Training Iteration 5991, Loss: 6.435521125793457\n",
            "Training Iteration 5992, Loss: 11.527456283569336\n",
            "Training Iteration 5993, Loss: 7.554052829742432\n",
            "Training Iteration 5994, Loss: 3.7088496685028076\n",
            "Training Iteration 5995, Loss: 1.8540687561035156\n",
            "Training Iteration 5996, Loss: 3.5283899307250977\n",
            "Training Iteration 5997, Loss: 3.3583691120147705\n",
            "Training Iteration 5998, Loss: 5.700667381286621\n",
            "Training Iteration 5999, Loss: 2.44240140914917\n",
            "Training Iteration 6000, Loss: 4.327975749969482\n",
            "Training Iteration 6001, Loss: 4.934704303741455\n",
            "Training Iteration 6002, Loss: 5.235161304473877\n",
            "Training Iteration 6003, Loss: 6.579886436462402\n",
            "Training Iteration 6004, Loss: 2.423151969909668\n",
            "Training Iteration 6005, Loss: 4.882761001586914\n",
            "Training Iteration 6006, Loss: 4.35783052444458\n",
            "Training Iteration 6007, Loss: 5.39142370223999\n",
            "Training Iteration 6008, Loss: 7.205116271972656\n",
            "Training Iteration 6009, Loss: 3.596342086791992\n",
            "Training Iteration 6010, Loss: 3.394791603088379\n",
            "Training Iteration 6011, Loss: 3.2440171241760254\n",
            "Training Iteration 6012, Loss: 6.381731986999512\n",
            "Training Iteration 6013, Loss: 5.1570210456848145\n",
            "Training Iteration 6014, Loss: 4.751058101654053\n",
            "Training Iteration 6015, Loss: 5.082297325134277\n",
            "Training Iteration 6016, Loss: 4.339123249053955\n",
            "Training Iteration 6017, Loss: 4.690299987792969\n",
            "Training Iteration 6018, Loss: 5.255544662475586\n",
            "Training Iteration 6019, Loss: 5.707327842712402\n",
            "Training Iteration 6020, Loss: 4.492213249206543\n",
            "Training Iteration 6021, Loss: 5.340189456939697\n",
            "Training Iteration 6022, Loss: 4.780396938323975\n",
            "Training Iteration 6023, Loss: 3.431161880493164\n",
            "Training Iteration 6024, Loss: 6.589452743530273\n",
            "Training Iteration 6025, Loss: 11.875326156616211\n",
            "Training Iteration 6026, Loss: 6.017856597900391\n",
            "Training Iteration 6027, Loss: 3.2614827156066895\n",
            "Training Iteration 6028, Loss: 6.557805061340332\n",
            "Training Iteration 6029, Loss: 3.910900115966797\n",
            "Training Iteration 6030, Loss: 7.366354942321777\n",
            "Training Iteration 6031, Loss: 3.1378231048583984\n",
            "Training Iteration 6032, Loss: 3.680096387863159\n",
            "Training Iteration 6033, Loss: 4.667976379394531\n",
            "Training Iteration 6034, Loss: 2.8092994689941406\n",
            "Training Iteration 6035, Loss: 2.7205684185028076\n",
            "Training Iteration 6036, Loss: 3.523305892944336\n",
            "Training Iteration 6037, Loss: 5.305273532867432\n",
            "Training Iteration 6038, Loss: 4.018885135650635\n",
            "Training Iteration 6039, Loss: 2.0823371410369873\n",
            "Training Iteration 6040, Loss: 4.5968828201293945\n",
            "Training Iteration 6041, Loss: 3.1555824279785156\n",
            "Training Iteration 6042, Loss: 3.931260824203491\n",
            "Training Iteration 6043, Loss: 4.194497108459473\n",
            "Training Iteration 6044, Loss: 4.45083475112915\n",
            "Training Iteration 6045, Loss: 4.3604960441589355\n",
            "Training Iteration 6046, Loss: 3.4922704696655273\n",
            "Training Iteration 6047, Loss: 3.7873916625976562\n",
            "Training Iteration 6048, Loss: 10.600194931030273\n",
            "Training Iteration 6049, Loss: 3.781003713607788\n",
            "Training Iteration 6050, Loss: 6.447988986968994\n",
            "Training Iteration 6051, Loss: 2.5441296100616455\n",
            "Training Iteration 6052, Loss: 2.4771854877471924\n",
            "Training Iteration 6053, Loss: 2.287576198577881\n",
            "Training Iteration 6054, Loss: 1.7408437728881836\n",
            "Training Iteration 6055, Loss: 4.370080471038818\n",
            "Training Iteration 6056, Loss: 2.4262356758117676\n",
            "Training Iteration 6057, Loss: 3.7123525142669678\n",
            "Training Iteration 6058, Loss: 5.691339492797852\n",
            "Training Iteration 6059, Loss: 3.8076751232147217\n",
            "Training Iteration 6060, Loss: 4.453982353210449\n",
            "Training Iteration 6061, Loss: 4.598578453063965\n",
            "Training Iteration 6062, Loss: 2.3343920707702637\n",
            "Training Iteration 6063, Loss: 7.268840789794922\n",
            "Training Iteration 6064, Loss: 3.133850574493408\n",
            "Training Iteration 6065, Loss: 3.981780767440796\n",
            "Training Iteration 6066, Loss: 3.1917905807495117\n",
            "Training Iteration 6067, Loss: 5.150403022766113\n",
            "Training Iteration 6068, Loss: 6.613236427307129\n",
            "Training Iteration 6069, Loss: 4.554832458496094\n",
            "Training Iteration 6070, Loss: 2.511836290359497\n",
            "Training Iteration 6071, Loss: 5.849767208099365\n",
            "Training Iteration 6072, Loss: 5.178333759307861\n",
            "Training Iteration 6073, Loss: 7.460460186004639\n",
            "Training Iteration 6074, Loss: 3.3332314491271973\n",
            "Training Iteration 6075, Loss: 5.142701148986816\n",
            "Training Iteration 6076, Loss: 2.8015329837799072\n",
            "Training Iteration 6077, Loss: 2.1125895977020264\n",
            "Training Iteration 6078, Loss: 6.160021781921387\n",
            "Training Iteration 6079, Loss: 4.820585250854492\n",
            "Training Iteration 6080, Loss: 2.506486177444458\n",
            "Training Iteration 6081, Loss: 4.949679851531982\n",
            "Training Iteration 6082, Loss: 4.596404075622559\n",
            "Training Iteration 6083, Loss: 9.804784774780273\n",
            "Training Iteration 6084, Loss: 6.319646835327148\n",
            "Training Iteration 6085, Loss: 3.625190258026123\n",
            "Training Iteration 6086, Loss: 5.611384868621826\n",
            "Training Iteration 6087, Loss: 3.719923496246338\n",
            "Training Iteration 6088, Loss: 4.892477035522461\n",
            "Training Iteration 6089, Loss: 5.43376350402832\n",
            "Training Iteration 6090, Loss: 3.5370519161224365\n",
            "Training Iteration 6091, Loss: 5.7125701904296875\n",
            "Training Iteration 6092, Loss: 6.395659446716309\n",
            "Training Iteration 6093, Loss: 6.029309272766113\n",
            "Training Iteration 6094, Loss: 4.967710494995117\n",
            "Training Iteration 6095, Loss: 5.628581523895264\n",
            "Training Iteration 6096, Loss: 6.891776084899902\n",
            "Training Iteration 6097, Loss: 4.926313877105713\n",
            "Training Iteration 6098, Loss: 3.506563186645508\n",
            "Training Iteration 6099, Loss: 7.566355228424072\n",
            "Training Iteration 6100, Loss: 5.123856544494629\n",
            "Training Iteration 6101, Loss: 6.816140174865723\n",
            "Training Iteration 6102, Loss: 6.924222469329834\n",
            "Training Iteration 6103, Loss: 4.220886707305908\n",
            "Training Iteration 6104, Loss: 3.309455633163452\n",
            "Training Iteration 6105, Loss: 5.457925319671631\n",
            "Training Iteration 6106, Loss: 4.0055670738220215\n",
            "Training Iteration 6107, Loss: 6.266081809997559\n",
            "Training Iteration 6108, Loss: 6.861573219299316\n",
            "Training Iteration 6109, Loss: 4.353564739227295\n",
            "Training Iteration 6110, Loss: 3.5687808990478516\n",
            "Training Iteration 6111, Loss: 2.440988540649414\n",
            "Training Iteration 6112, Loss: 2.607178211212158\n",
            "Training Iteration 6113, Loss: 2.8265233039855957\n",
            "Training Iteration 6114, Loss: 7.557584762573242\n",
            "Training Iteration 6115, Loss: 4.929530620574951\n",
            "Training Iteration 6116, Loss: 5.211183547973633\n",
            "Training Iteration 6117, Loss: 5.386234283447266\n",
            "Training Iteration 6118, Loss: 4.417000770568848\n",
            "Training Iteration 6119, Loss: 5.272396087646484\n",
            "Training Iteration 6120, Loss: 3.089233160018921\n",
            "Training Iteration 6121, Loss: 5.783919334411621\n",
            "Training Iteration 6122, Loss: 2.049853563308716\n",
            "Training Iteration 6123, Loss: 6.0686750411987305\n",
            "Training Iteration 6124, Loss: 6.601957321166992\n",
            "Training Iteration 6125, Loss: 3.3586599826812744\n",
            "Training Iteration 6126, Loss: 2.7100143432617188\n",
            "Training Iteration 6127, Loss: 2.22725510597229\n",
            "Training Iteration 6128, Loss: 9.151762008666992\n",
            "Training Iteration 6129, Loss: 5.715685844421387\n",
            "Training Iteration 6130, Loss: 2.651585817337036\n",
            "Training Iteration 6131, Loss: 2.6508445739746094\n",
            "Training Iteration 6132, Loss: 7.297480583190918\n",
            "Training Iteration 6133, Loss: 3.6496386528015137\n",
            "Training Iteration 6134, Loss: 5.021326541900635\n",
            "Training Iteration 6135, Loss: 7.580918788909912\n",
            "Training Iteration 6136, Loss: 5.66844367980957\n",
            "Training Iteration 6137, Loss: 4.644852161407471\n",
            "Training Iteration 6138, Loss: 6.441314697265625\n",
            "Training Iteration 6139, Loss: 2.789423704147339\n",
            "Training Iteration 6140, Loss: 4.410158157348633\n",
            "Training Iteration 6141, Loss: 8.96916675567627\n",
            "Training Iteration 6142, Loss: 5.486267566680908\n",
            "Training Iteration 6143, Loss: 6.08307409286499\n",
            "Training Iteration 6144, Loss: 6.363781929016113\n",
            "Training Iteration 6145, Loss: 3.2300045490264893\n",
            "Training Iteration 6146, Loss: 2.4283461570739746\n",
            "Training Iteration 6147, Loss: 2.9694511890411377\n",
            "Training Iteration 6148, Loss: 2.2507121562957764\n",
            "Training Iteration 6149, Loss: 5.729926109313965\n",
            "Training Iteration 6150, Loss: 4.703886985778809\n",
            "Training Iteration 6151, Loss: 4.364445209503174\n",
            "Training Iteration 6152, Loss: 2.136291742324829\n",
            "Training Iteration 6153, Loss: 5.126409530639648\n",
            "Training Iteration 6154, Loss: 6.274571418762207\n",
            "Training Iteration 6155, Loss: 1.240140438079834\n",
            "Training Iteration 6156, Loss: 2.977588176727295\n",
            "Training Iteration 6157, Loss: 1.7228362560272217\n",
            "Training Iteration 6158, Loss: 1.0466033220291138\n",
            "Training Iteration 6159, Loss: 2.8743648529052734\n",
            "Training Iteration 6160, Loss: 7.681993007659912\n",
            "Training Iteration 6161, Loss: 2.996248245239258\n",
            "Training Iteration 6162, Loss: 6.871956825256348\n",
            "Training Iteration 6163, Loss: 3.096064567565918\n",
            "Training Iteration 6164, Loss: 2.2761895656585693\n",
            "Training Iteration 6165, Loss: 5.842901229858398\n",
            "Training Iteration 6166, Loss: 4.9738593101501465\n",
            "Training Iteration 6167, Loss: 5.242931365966797\n",
            "Training Iteration 6168, Loss: 3.086488723754883\n",
            "Training Iteration 6169, Loss: 1.5574778318405151\n",
            "Training Iteration 6170, Loss: 4.903823375701904\n",
            "Training Iteration 6171, Loss: 3.6081478595733643\n",
            "Training Iteration 6172, Loss: 3.344669818878174\n",
            "Training Iteration 6173, Loss: 3.8998711109161377\n",
            "Training Iteration 6174, Loss: 6.820520877838135\n",
            "Training Iteration 6175, Loss: 1.963739275932312\n",
            "Training Iteration 6176, Loss: 5.825555801391602\n",
            "Training Iteration 6177, Loss: 5.630795478820801\n",
            "Training Iteration 6178, Loss: 3.8158576488494873\n",
            "Training Iteration 6179, Loss: 4.4962687492370605\n",
            "Training Iteration 6180, Loss: 4.992057800292969\n",
            "Training Iteration 6181, Loss: 1.970489501953125\n",
            "Training Iteration 6182, Loss: 12.824586868286133\n",
            "Training Iteration 6183, Loss: 4.037687301635742\n",
            "Training Iteration 6184, Loss: 2.87174654006958\n",
            "Training Iteration 6185, Loss: 5.445000648498535\n",
            "Training Iteration 6186, Loss: 3.2804768085479736\n",
            "Training Iteration 6187, Loss: 5.293968200683594\n",
            "Training Iteration 6188, Loss: 3.791785478591919\n",
            "Training Iteration 6189, Loss: 2.638514518737793\n",
            "Training Iteration 6190, Loss: 9.112235069274902\n",
            "Training Iteration 6191, Loss: 8.162736892700195\n",
            "Training Iteration 6192, Loss: 3.949047327041626\n",
            "Training Iteration 6193, Loss: 3.1402440071105957\n",
            "Training Iteration 6194, Loss: 3.3044919967651367\n",
            "Training Iteration 6195, Loss: 2.270965337753296\n",
            "Training Iteration 6196, Loss: 3.181588649749756\n",
            "Training Iteration 6197, Loss: 4.53607177734375\n",
            "Training Iteration 6198, Loss: 7.313924312591553\n",
            "Training Iteration 6199, Loss: 5.568620681762695\n",
            "Training Iteration 6200, Loss: 4.888034343719482\n",
            "Training Iteration 6201, Loss: 4.369691848754883\n",
            "Training Iteration 6202, Loss: 2.662594795227051\n",
            "Training Iteration 6203, Loss: 5.316671848297119\n",
            "Training Iteration 6204, Loss: 6.8211822509765625\n",
            "Training Iteration 6205, Loss: 4.008056163787842\n",
            "Training Iteration 6206, Loss: 2.7308621406555176\n",
            "Training Iteration 6207, Loss: 2.488309383392334\n",
            "Training Iteration 6208, Loss: 2.5958023071289062\n",
            "Training Iteration 6209, Loss: 1.926131010055542\n",
            "Training Iteration 6210, Loss: 5.341969013214111\n",
            "Training Iteration 6211, Loss: 3.4054641723632812\n",
            "Training Iteration 6212, Loss: 6.590322017669678\n",
            "Training Iteration 6213, Loss: 2.664591073989868\n",
            "Training Iteration 6214, Loss: 1.3321353197097778\n",
            "Training Iteration 6215, Loss: 3.1181020736694336\n",
            "Training Iteration 6216, Loss: 3.1469693183898926\n",
            "Training Iteration 6217, Loss: 6.552677631378174\n",
            "Training Iteration 6218, Loss: 4.546648025512695\n",
            "Training Iteration 6219, Loss: 4.819536209106445\n",
            "Training Iteration 6220, Loss: 5.327097415924072\n",
            "Training Iteration 6221, Loss: 3.679323434829712\n",
            "Training Iteration 6222, Loss: 4.090663433074951\n",
            "Training Iteration 6223, Loss: 3.8115077018737793\n",
            "Training Iteration 6224, Loss: 2.9122445583343506\n",
            "Training Iteration 6225, Loss: 4.449075222015381\n",
            "Training Iteration 6226, Loss: 5.391702175140381\n",
            "Training Iteration 6227, Loss: 6.906972408294678\n",
            "Training Iteration 6228, Loss: 5.505577087402344\n",
            "Training Iteration 6229, Loss: 6.910668849945068\n",
            "Training Iteration 6230, Loss: 8.552413940429688\n",
            "Training Iteration 6231, Loss: 3.1487340927124023\n",
            "Training Iteration 6232, Loss: 2.89668607711792\n",
            "Training Iteration 6233, Loss: 3.3699588775634766\n",
            "Training Iteration 6234, Loss: 5.103272914886475\n",
            "Training Iteration 6235, Loss: 3.3032898902893066\n",
            "Training Iteration 6236, Loss: 4.445558071136475\n",
            "Training Iteration 6237, Loss: 3.7228708267211914\n",
            "Training Iteration 6238, Loss: 2.697251796722412\n",
            "Training Iteration 6239, Loss: 4.433933258056641\n",
            "Training Iteration 6240, Loss: 7.62118673324585\n",
            "Training Iteration 6241, Loss: 4.4633989334106445\n",
            "Training Iteration 6242, Loss: 5.438957691192627\n",
            "Training Iteration 6243, Loss: 4.406739234924316\n",
            "Training Iteration 6244, Loss: 4.615540504455566\n",
            "Training Iteration 6245, Loss: 5.738590717315674\n",
            "Training Iteration 6246, Loss: 7.1256608963012695\n",
            "Training Iteration 6247, Loss: 6.918635368347168\n",
            "Training Iteration 6248, Loss: 2.9214398860931396\n",
            "Training Iteration 6249, Loss: 3.490976572036743\n",
            "Training Iteration 6250, Loss: 4.228975296020508\n",
            "Training Iteration 6251, Loss: 4.664083480834961\n",
            "Training Iteration 6252, Loss: 5.479663848876953\n",
            "Training Iteration 6253, Loss: 2.951275587081909\n",
            "Training Iteration 6254, Loss: 5.030229568481445\n",
            "Training Iteration 6255, Loss: 6.390374183654785\n",
            "Training Iteration 6256, Loss: 1.5357682704925537\n",
            "Training Iteration 6257, Loss: 4.803354263305664\n",
            "Training Iteration 6258, Loss: 5.88863468170166\n",
            "Training Iteration 6259, Loss: 3.0969319343566895\n",
            "Training Iteration 6260, Loss: 3.9490742683410645\n",
            "Training Iteration 6261, Loss: 4.87899923324585\n",
            "Training Iteration 6262, Loss: 3.46614146232605\n",
            "Training Iteration 6263, Loss: 4.513218879699707\n",
            "Training Iteration 6264, Loss: 2.8911144733428955\n",
            "Training Iteration 6265, Loss: 6.374207496643066\n",
            "Training Iteration 6266, Loss: 4.093049049377441\n",
            "Training Iteration 6267, Loss: 5.842593193054199\n",
            "Training Iteration 6268, Loss: 3.8801159858703613\n",
            "Training Iteration 6269, Loss: 3.6570088863372803\n",
            "Training Iteration 6270, Loss: 8.000650405883789\n",
            "Training Iteration 6271, Loss: 4.85638952255249\n",
            "Training Iteration 6272, Loss: 6.275689125061035\n",
            "Training Iteration 6273, Loss: 3.3423876762390137\n",
            "Training Iteration 6274, Loss: 3.736640691757202\n",
            "Training Iteration 6275, Loss: 6.068060398101807\n",
            "Training Iteration 6276, Loss: 6.48117208480835\n",
            "Training Iteration 6277, Loss: 4.481163501739502\n",
            "Training Iteration 6278, Loss: 3.698988676071167\n",
            "Training Iteration 6279, Loss: 4.6087493896484375\n",
            "Training Iteration 6280, Loss: 5.576489448547363\n",
            "Training Iteration 6281, Loss: 5.73396110534668\n",
            "Training Iteration 6282, Loss: 3.077934980392456\n",
            "Training Iteration 6283, Loss: 5.210416793823242\n",
            "Training Iteration 6284, Loss: 4.877096652984619\n",
            "Training Iteration 6285, Loss: 6.252586841583252\n",
            "Training Iteration 6286, Loss: 2.8096203804016113\n",
            "Training Iteration 6287, Loss: 4.618382453918457\n",
            "Training Iteration 6288, Loss: 6.021475315093994\n",
            "Training Iteration 6289, Loss: 9.096001625061035\n",
            "Training Iteration 6290, Loss: 8.373184204101562\n",
            "Training Iteration 6291, Loss: 3.879379987716675\n",
            "Training Iteration 6292, Loss: 5.0479912757873535\n",
            "Training Iteration 6293, Loss: 6.378436088562012\n",
            "Training Iteration 6294, Loss: 2.7097702026367188\n",
            "Training Iteration 6295, Loss: 5.058818817138672\n",
            "Training Iteration 6296, Loss: 3.8019652366638184\n",
            "Training Iteration 6297, Loss: 3.536233425140381\n",
            "Training Iteration 6298, Loss: 4.93348503112793\n",
            "Training Iteration 6299, Loss: 6.760929107666016\n",
            "Training Iteration 6300, Loss: 3.8815829753875732\n",
            "Training Iteration 6301, Loss: 1.736842155456543\n",
            "Training Iteration 6302, Loss: 4.26400899887085\n",
            "Training Iteration 6303, Loss: 6.073145866394043\n",
            "Training Iteration 6304, Loss: 12.440475463867188\n",
            "Training Iteration 6305, Loss: 8.656606674194336\n",
            "Training Iteration 6306, Loss: 11.299829483032227\n",
            "Training Iteration 6307, Loss: 4.608822822570801\n",
            "Training Iteration 6308, Loss: 11.438570022583008\n",
            "Training Iteration 6309, Loss: 5.897891044616699\n",
            "Training Iteration 6310, Loss: 7.395668029785156\n",
            "Training Iteration 6311, Loss: 3.578657627105713\n",
            "Training Iteration 6312, Loss: 5.846236228942871\n",
            "Training Iteration 6313, Loss: 2.715263605117798\n",
            "Training Iteration 6314, Loss: 3.6773219108581543\n",
            "Training Iteration 6315, Loss: 3.0037858486175537\n",
            "Training Iteration 6316, Loss: 3.846299409866333\n",
            "Training Iteration 6317, Loss: 4.820784091949463\n",
            "Training Iteration 6318, Loss: 5.48417329788208\n",
            "Training Iteration 6319, Loss: 3.5183024406433105\n",
            "Training Iteration 6320, Loss: 4.146631717681885\n",
            "Training Iteration 6321, Loss: 3.248434066772461\n",
            "Training Iteration 6322, Loss: 3.65836763381958\n",
            "Training Iteration 6323, Loss: 6.52076530456543\n",
            "Training Iteration 6324, Loss: 5.308069229125977\n",
            "Training Iteration 6325, Loss: 5.833418846130371\n",
            "Training Iteration 6326, Loss: 7.511641979217529\n",
            "Training Iteration 6327, Loss: 4.3365888595581055\n",
            "Training Iteration 6328, Loss: 1.9863582849502563\n",
            "Training Iteration 6329, Loss: 4.11616325378418\n",
            "Training Iteration 6330, Loss: 4.707772731781006\n",
            "Training Iteration 6331, Loss: 7.708685398101807\n",
            "Training Iteration 6332, Loss: 3.587674617767334\n",
            "Training Iteration 6333, Loss: 3.2288427352905273\n",
            "Training Iteration 6334, Loss: 6.693001747131348\n",
            "Training Iteration 6335, Loss: 7.308383464813232\n",
            "Training Iteration 6336, Loss: 3.8129210472106934\n",
            "Training Iteration 6337, Loss: 7.22054386138916\n",
            "Training Iteration 6338, Loss: 3.7875075340270996\n",
            "Training Iteration 6339, Loss: 2.5875730514526367\n",
            "Training Iteration 6340, Loss: 4.461838245391846\n",
            "Training Iteration 6341, Loss: 3.0671322345733643\n",
            "Training Iteration 6342, Loss: 5.991663455963135\n",
            "Training Iteration 6343, Loss: 4.829275608062744\n",
            "Training Iteration 6344, Loss: 4.339035511016846\n",
            "Training Iteration 6345, Loss: 2.535606622695923\n",
            "Training Iteration 6346, Loss: 5.135844707489014\n",
            "Training Iteration 6347, Loss: 3.018256664276123\n",
            "Training Iteration 6348, Loss: 4.89044713973999\n",
            "Training Iteration 6349, Loss: 6.41585636138916\n",
            "Training Iteration 6350, Loss: 5.161920547485352\n",
            "Training Iteration 6351, Loss: 1.709951639175415\n",
            "Training Iteration 6352, Loss: 3.403834104537964\n",
            "Training Iteration 6353, Loss: 5.154613018035889\n",
            "Training Iteration 6354, Loss: 4.347199440002441\n",
            "Training Iteration 6355, Loss: 5.832298755645752\n",
            "Training Iteration 6356, Loss: 3.6531035900115967\n",
            "Training Iteration 6357, Loss: 2.423811435699463\n",
            "Training Iteration 6358, Loss: 3.28639554977417\n",
            "Training Iteration 6359, Loss: 5.937860012054443\n",
            "Training Iteration 6360, Loss: 1.9008930921554565\n",
            "Training Iteration 6361, Loss: 4.967947006225586\n",
            "Training Iteration 6362, Loss: 3.7569286823272705\n",
            "Training Iteration 6363, Loss: 4.681356430053711\n",
            "Training Iteration 6364, Loss: 2.965017795562744\n",
            "Training Iteration 6365, Loss: 4.243844509124756\n",
            "Training Iteration 6366, Loss: 2.547420024871826\n",
            "Training Iteration 6367, Loss: 6.275970458984375\n",
            "Training Iteration 6368, Loss: 3.49373722076416\n",
            "Training Iteration 6369, Loss: 4.620846748352051\n",
            "Training Iteration 6370, Loss: 5.609203815460205\n",
            "Training Iteration 6371, Loss: 1.5361244678497314\n",
            "Training Iteration 6372, Loss: 5.80903434753418\n",
            "Training Iteration 6373, Loss: 2.9481966495513916\n",
            "Training Iteration 6374, Loss: 3.7677361965179443\n",
            "Training Iteration 6375, Loss: 3.6306419372558594\n",
            "Training Iteration 6376, Loss: 4.859312057495117\n",
            "Training Iteration 6377, Loss: 2.006925582885742\n",
            "Training Iteration 6378, Loss: 3.4036710262298584\n",
            "Training Iteration 6379, Loss: 4.060179710388184\n",
            "Training Iteration 6380, Loss: 3.682598114013672\n",
            "Training Iteration 6381, Loss: 3.903428077697754\n",
            "Training Iteration 6382, Loss: 3.428190231323242\n",
            "Training Iteration 6383, Loss: 3.1587514877319336\n",
            "Training Iteration 6384, Loss: 3.6965808868408203\n",
            "Training Iteration 6385, Loss: 4.547981262207031\n",
            "Training Iteration 6386, Loss: 4.1766252517700195\n",
            "Training Iteration 6387, Loss: 2.956879138946533\n",
            "Training Iteration 6388, Loss: 2.9827258586883545\n",
            "Training Iteration 6389, Loss: 6.857574462890625\n",
            "Training Iteration 6390, Loss: 6.889308452606201\n",
            "Training Iteration 6391, Loss: 5.32934045791626\n",
            "Training Iteration 6392, Loss: 7.115663051605225\n",
            "Training Iteration 6393, Loss: 5.51148796081543\n",
            "Training Iteration 6394, Loss: 5.539123058319092\n",
            "Training Iteration 6395, Loss: 5.113008499145508\n",
            "Training Iteration 6396, Loss: 5.115360260009766\n",
            "Training Iteration 6397, Loss: 4.535984039306641\n",
            "Training Iteration 6398, Loss: 5.262012004852295\n",
            "Training Iteration 6399, Loss: 3.501215696334839\n",
            "Training Iteration 6400, Loss: 6.182958126068115\n",
            "Training Iteration 6401, Loss: 3.1753807067871094\n",
            "Training Iteration 6402, Loss: 2.130925416946411\n",
            "Training Iteration 6403, Loss: 5.339103698730469\n",
            "Training Iteration 6404, Loss: 3.3793578147888184\n",
            "Training Iteration 6405, Loss: 6.0171003341674805\n",
            "Training Iteration 6406, Loss: 5.851478099822998\n",
            "Training Iteration 6407, Loss: 4.209528923034668\n",
            "Training Iteration 6408, Loss: 3.3243236541748047\n",
            "Training Iteration 6409, Loss: 2.496439218521118\n",
            "Training Iteration 6410, Loss: 3.2217094898223877\n",
            "Training Iteration 6411, Loss: 4.330343723297119\n",
            "Training Iteration 6412, Loss: 4.722299098968506\n",
            "Training Iteration 6413, Loss: 6.063165187835693\n",
            "Training Iteration 6414, Loss: 5.123615264892578\n",
            "Training Iteration 6415, Loss: 4.498294830322266\n",
            "Training Iteration 6416, Loss: 5.014527797698975\n",
            "Training Iteration 6417, Loss: 3.8179125785827637\n",
            "Training Iteration 6418, Loss: 4.957170009613037\n",
            "Training Iteration 6419, Loss: 3.6103525161743164\n",
            "Training Iteration 6420, Loss: 3.5573606491088867\n",
            "Training Iteration 6421, Loss: 6.908705234527588\n",
            "Training Iteration 6422, Loss: 4.919505596160889\n",
            "Training Iteration 6423, Loss: 3.003171443939209\n",
            "Training Iteration 6424, Loss: 5.600302696228027\n",
            "Training Iteration 6425, Loss: 2.7895421981811523\n",
            "Training Iteration 6426, Loss: 3.103994846343994\n",
            "Training Iteration 6427, Loss: 7.477988243103027\n",
            "Training Iteration 6428, Loss: 4.321155548095703\n",
            "Training Iteration 6429, Loss: 4.460782051086426\n",
            "Training Iteration 6430, Loss: 5.565056800842285\n",
            "Training Iteration 6431, Loss: 4.092462539672852\n",
            "Training Iteration 6432, Loss: 6.661707401275635\n",
            "Training Iteration 6433, Loss: 1.971472978591919\n",
            "Training Iteration 6434, Loss: 4.615322589874268\n",
            "Training Iteration 6435, Loss: 6.265169143676758\n",
            "Training Iteration 6436, Loss: 4.13254976272583\n",
            "Training Iteration 6437, Loss: 6.592713832855225\n",
            "Training Iteration 6438, Loss: 3.234154224395752\n",
            "Training Iteration 6439, Loss: 5.167468070983887\n",
            "Training Iteration 6440, Loss: 6.458258628845215\n",
            "Training Iteration 6441, Loss: 6.874966621398926\n",
            "Training Iteration 6442, Loss: 4.178072452545166\n",
            "Training Iteration 6443, Loss: 5.606558799743652\n",
            "Training Iteration 6444, Loss: 5.594088077545166\n",
            "Training Iteration 6445, Loss: 5.009374141693115\n",
            "Training Iteration 6446, Loss: 4.263190746307373\n",
            "Training Iteration 6447, Loss: 4.009584903717041\n",
            "Training Iteration 6448, Loss: 6.123198509216309\n",
            "Training Iteration 6449, Loss: 4.564523696899414\n",
            "Training Iteration 6450, Loss: 4.05715274810791\n",
            "Training Iteration 6451, Loss: 1.790156364440918\n",
            "Training Iteration 6452, Loss: 4.508237838745117\n",
            "Training Iteration 6453, Loss: 4.828167915344238\n",
            "Training Iteration 6454, Loss: 2.697821617126465\n",
            "Training Iteration 6455, Loss: 6.173263072967529\n",
            "Training Iteration 6456, Loss: 6.580755233764648\n",
            "Training Iteration 6457, Loss: 5.1774091720581055\n",
            "Training Iteration 6458, Loss: 2.4550440311431885\n",
            "Training Iteration 6459, Loss: 5.799651145935059\n",
            "Training Iteration 6460, Loss: 3.9012818336486816\n",
            "Training Iteration 6461, Loss: 4.641385555267334\n",
            "Training Iteration 6462, Loss: 6.421338081359863\n",
            "Training Iteration 6463, Loss: 3.6586594581604004\n",
            "Training Iteration 6464, Loss: 4.536686420440674\n",
            "Training Iteration 6465, Loss: 5.724057197570801\n",
            "Training Iteration 6466, Loss: 3.693568229675293\n",
            "Training Iteration 6467, Loss: 6.786979675292969\n",
            "Training Iteration 6468, Loss: 5.2407708168029785\n",
            "Training Iteration 6469, Loss: 2.0355145931243896\n",
            "Training Iteration 6470, Loss: 1.646512746810913\n",
            "Training Iteration 6471, Loss: 6.247197151184082\n",
            "Training Iteration 6472, Loss: 8.200102806091309\n",
            "Training Iteration 6473, Loss: 6.75467586517334\n",
            "Training Iteration 6474, Loss: 4.613837242126465\n",
            "Training Iteration 6475, Loss: 3.323525905609131\n",
            "Training Iteration 6476, Loss: 3.8887035846710205\n",
            "Training Iteration 6477, Loss: 6.1190385818481445\n",
            "Training Iteration 6478, Loss: 6.27044153213501\n",
            "Training Iteration 6479, Loss: 2.902346134185791\n",
            "Training Iteration 6480, Loss: 3.3652267456054688\n",
            "Training Iteration 6481, Loss: 5.248394966125488\n",
            "Training Iteration 6482, Loss: 4.605086803436279\n",
            "Training Iteration 6483, Loss: 2.359243869781494\n",
            "Training Iteration 6484, Loss: 6.463187217712402\n",
            "Training Iteration 6485, Loss: 3.3214523792266846\n",
            "Training Iteration 6486, Loss: 8.438530921936035\n",
            "Training Iteration 6487, Loss: 3.33300518989563\n",
            "Training Iteration 6488, Loss: 6.677882671356201\n",
            "Training Iteration 6489, Loss: 4.252380847930908\n",
            "Training Iteration 6490, Loss: 3.7993438243865967\n",
            "Training Iteration 6491, Loss: 4.570426940917969\n",
            "Training Iteration 6492, Loss: 8.895432472229004\n",
            "Training Iteration 6493, Loss: 5.251232624053955\n",
            "Training Iteration 6494, Loss: 5.4490861892700195\n",
            "Training Iteration 6495, Loss: 4.6772284507751465\n",
            "Training Iteration 6496, Loss: 2.0690128803253174\n",
            "Training Iteration 6497, Loss: 2.542184352874756\n",
            "Training Iteration 6498, Loss: 6.14571475982666\n",
            "Training Iteration 6499, Loss: 4.45680046081543\n",
            "Training Iteration 6500, Loss: 3.5631351470947266\n",
            "Training Iteration 6501, Loss: 2.4991836547851562\n",
            "Training Iteration 6502, Loss: 4.7094292640686035\n",
            "Training Iteration 6503, Loss: 5.94663143157959\n",
            "Training Iteration 6504, Loss: 4.580635070800781\n",
            "Training Iteration 6505, Loss: 4.062440872192383\n",
            "Training Iteration 6506, Loss: 5.6786627769470215\n",
            "Training Iteration 6507, Loss: 4.899977684020996\n",
            "Training Iteration 6508, Loss: 6.527163028717041\n",
            "Training Iteration 6509, Loss: 2.125606060028076\n",
            "Training Iteration 6510, Loss: 2.3714122772216797\n",
            "Training Iteration 6511, Loss: 2.5596277713775635\n",
            "Training Iteration 6512, Loss: 4.612032890319824\n",
            "Training Iteration 6513, Loss: 4.481649875640869\n",
            "Training Iteration 6514, Loss: 3.5356147289276123\n",
            "Training Iteration 6515, Loss: 3.9226651191711426\n",
            "Training Iteration 6516, Loss: 4.951008319854736\n",
            "Training Iteration 6517, Loss: 4.28685998916626\n",
            "Training Iteration 6518, Loss: 4.587904930114746\n",
            "Training Iteration 6519, Loss: 3.653672218322754\n",
            "Training Iteration 6520, Loss: 4.086332321166992\n",
            "Training Iteration 6521, Loss: 5.516729354858398\n",
            "Training Iteration 6522, Loss: 5.9086503982543945\n",
            "Training Iteration 6523, Loss: 2.5085928440093994\n",
            "Training Iteration 6524, Loss: 2.698890209197998\n",
            "Training Iteration 6525, Loss: 2.4406332969665527\n",
            "Training Iteration 6526, Loss: 6.983120918273926\n",
            "Training Iteration 6527, Loss: 7.5217509269714355\n",
            "Training Iteration 6528, Loss: 3.7654523849487305\n",
            "Training Iteration 6529, Loss: 3.9412429332733154\n",
            "Training Iteration 6530, Loss: 5.36349630355835\n",
            "Training Iteration 6531, Loss: 4.902855396270752\n",
            "Training Iteration 6532, Loss: 4.080976963043213\n",
            "Training Iteration 6533, Loss: 5.196354866027832\n",
            "Training Iteration 6534, Loss: 4.731897354125977\n",
            "Training Iteration 6535, Loss: 3.5246975421905518\n",
            "Training Iteration 6536, Loss: 1.6445010900497437\n",
            "Training Iteration 6537, Loss: 3.4895951747894287\n",
            "Training Iteration 6538, Loss: 8.240738868713379\n",
            "Training Iteration 6539, Loss: 3.098233222961426\n",
            "Training Iteration 6540, Loss: 4.34215784072876\n",
            "Training Iteration 6541, Loss: 3.947981595993042\n",
            "Training Iteration 6542, Loss: 1.1931395530700684\n",
            "Training Iteration 6543, Loss: 5.065525531768799\n",
            "Training Iteration 6544, Loss: 5.4214396476745605\n",
            "Training Iteration 6545, Loss: 3.591550827026367\n",
            "Training Iteration 6546, Loss: 4.017759799957275\n",
            "Training Iteration 6547, Loss: 5.138904094696045\n",
            "Training Iteration 6548, Loss: 2.6577353477478027\n",
            "Training Iteration 6549, Loss: 2.72578763961792\n",
            "Training Iteration 6550, Loss: 4.755064010620117\n",
            "Training Iteration 6551, Loss: 1.3592944145202637\n",
            "Training Iteration 6552, Loss: 3.7003955841064453\n",
            "Training Iteration 6553, Loss: 4.770967483520508\n",
            "Training Iteration 6554, Loss: 2.1647884845733643\n",
            "tensor([[9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        ...,\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05]])\n",
            "Training loss for epcoh 9: 3.329205726364018\n",
            "Training accuracy for epoch 9: 0.30667989165681153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        ...,\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05],\n",
            "        [9.2335e-01, 5.0644e-02, 2.7527e-03, 2.1914e-02, 1.3184e-03, 2.0004e-05]])\n",
            "Validation loss for epcoh 9: 3.32775485051922\n",
            "Test accuracy for epoch 9: 0.3103685053788052\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch No: 10:   0%|          | 0/6553 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "460f0129a25c4af6a5aaed5b6c17d4a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Iteration 1, Loss: 4.770269393920898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Iteration 1564, Loss: 3.7350282669067383\n",
            "Training Iteration 1565, Loss: 4.820809841156006\n",
            "Training Iteration 1566, Loss: 3.1551878452301025\n",
            "Training Iteration 1567, Loss: 3.234713315963745\n",
            "Training Iteration 1568, Loss: 4.789275169372559\n",
            "Training Iteration 1569, Loss: 4.314996242523193\n",
            "Training Iteration 1570, Loss: 7.6658616065979\n",
            "Training Iteration 1571, Loss: 3.4667515754699707\n",
            "Training Iteration 1572, Loss: 5.563244819641113\n",
            "Training Iteration 1573, Loss: 2.4524025917053223\n",
            "Training Iteration 1574, Loss: 8.049346923828125\n",
            "Training Iteration 1575, Loss: 3.8093323707580566\n",
            "Training Iteration 1576, Loss: 6.28852653503418\n",
            "Training Iteration 1577, Loss: 3.010226249694824\n",
            "Training Iteration 1578, Loss: 3.834747552871704\n",
            "Training Iteration 1579, Loss: 2.357412815093994\n",
            "Training Iteration 1580, Loss: 6.966704845428467\n",
            "Training Iteration 1581, Loss: 5.9711809158325195\n",
            "Training Iteration 1582, Loss: 2.700460910797119\n",
            "Training Iteration 1583, Loss: 5.784205436706543\n",
            "Training Iteration 1584, Loss: 4.021980285644531\n",
            "Training Iteration 1585, Loss: 2.818240165710449\n",
            "Training Iteration 1586, Loss: 5.985912322998047\n",
            "Training Iteration 1587, Loss: 5.60227632522583\n",
            "Training Iteration 1588, Loss: 4.804200172424316\n",
            "Training Iteration 1589, Loss: 2.1633195877075195\n",
            "Training Iteration 1590, Loss: 2.8900020122528076\n",
            "Training Iteration 1591, Loss: 4.8219499588012695\n",
            "Training Iteration 1592, Loss: 3.216890811920166\n",
            "Training Iteration 1593, Loss: 1.8232901096343994\n",
            "Training Iteration 1594, Loss: 2.7436156272888184\n",
            "Training Iteration 1595, Loss: 6.615899562835693\n",
            "Training Iteration 1596, Loss: 3.9288153648376465\n",
            "Training Iteration 1597, Loss: 5.367750644683838\n",
            "Training Iteration 1598, Loss: 3.4249181747436523\n",
            "Training Iteration 1599, Loss: 2.9749014377593994\n",
            "Training Iteration 1600, Loss: 4.530370712280273\n",
            "Training Iteration 1601, Loss: 6.259657859802246\n",
            "Training Iteration 1602, Loss: 4.798628330230713\n",
            "Training Iteration 1603, Loss: 5.499270439147949\n",
            "Training Iteration 1604, Loss: 2.598588705062866\n",
            "Training Iteration 1605, Loss: 1.4065699577331543\n",
            "Training Iteration 1606, Loss: 5.189568519592285\n",
            "Training Iteration 1607, Loss: 4.715941905975342\n",
            "Training Iteration 1608, Loss: 3.626290798187256\n",
            "Training Iteration 1609, Loss: 8.027351379394531\n",
            "Training Iteration 1610, Loss: 1.9647767543792725\n",
            "Training Iteration 1611, Loss: 5.27398157119751\n",
            "Training Iteration 1612, Loss: 1.2925446033477783\n",
            "Training Iteration 1613, Loss: 4.307790756225586\n",
            "Training Iteration 1614, Loss: 3.3996777534484863\n",
            "Training Iteration 1615, Loss: 4.500151634216309\n",
            "Training Iteration 1616, Loss: 4.299840450286865\n",
            "Training Iteration 1617, Loss: 4.7599406242370605\n",
            "Training Iteration 1618, Loss: 4.454433441162109\n",
            "Training Iteration 1619, Loss: 13.15230655670166\n",
            "Training Iteration 1620, Loss: 6.9544878005981445\n",
            "Training Iteration 1621, Loss: 6.535541534423828\n",
            "Training Iteration 1622, Loss: 4.924971580505371\n",
            "Training Iteration 1623, Loss: 1.859038233757019\n",
            "Training Iteration 1624, Loss: 7.3356547355651855\n",
            "Training Iteration 1625, Loss: 4.075186729431152\n",
            "Training Iteration 1626, Loss: 2.8463873863220215\n",
            "Training Iteration 1627, Loss: 5.126410484313965\n",
            "Training Iteration 1628, Loss: 6.352738857269287\n",
            "Training Iteration 1629, Loss: 3.8215198516845703\n",
            "Training Iteration 1630, Loss: 3.5317020416259766\n",
            "Training Iteration 1631, Loss: 2.070211410522461\n",
            "Training Iteration 1632, Loss: 3.990049362182617\n",
            "Training Iteration 1633, Loss: 3.3508195877075195\n",
            "Training Iteration 1634, Loss: 4.154173851013184\n",
            "Training Iteration 1635, Loss: 3.499332904815674\n",
            "Training Iteration 1636, Loss: 4.233489990234375\n",
            "Training Iteration 1637, Loss: 2.11210298538208\n",
            "Training Iteration 1638, Loss: 4.1595139503479\n",
            "Training Iteration 1639, Loss: 4.278550148010254\n",
            "Training Iteration 1640, Loss: 7.849249362945557\n",
            "Training Iteration 1641, Loss: 2.1025521755218506\n",
            "Training Iteration 1642, Loss: 1.8202310800552368\n",
            "Training Iteration 1643, Loss: 7.613837718963623\n",
            "Training Iteration 1644, Loss: 5.782275199890137\n",
            "Training Iteration 1645, Loss: 4.147892951965332\n",
            "Training Iteration 1646, Loss: 2.9065306186676025\n",
            "Training Iteration 1647, Loss: 3.940718173980713\n",
            "Training Iteration 1648, Loss: 3.169405221939087\n",
            "Training Iteration 1649, Loss: 4.897945404052734\n",
            "Training Iteration 1650, Loss: 2.701402187347412\n",
            "Training Iteration 1651, Loss: 4.229432582855225\n",
            "Training Iteration 1652, Loss: 5.72487735748291\n",
            "Training Iteration 1653, Loss: 2.0327773094177246\n",
            "Training Iteration 1654, Loss: 4.586006164550781\n",
            "Training Iteration 1655, Loss: 5.31629753112793\n",
            "Training Iteration 1656, Loss: 4.936357021331787\n",
            "Training Iteration 1657, Loss: 6.208408355712891\n",
            "Training Iteration 1658, Loss: 3.5724780559539795\n",
            "Training Iteration 1659, Loss: 6.468509674072266\n",
            "Training Iteration 1660, Loss: 3.877336025238037\n",
            "Training Iteration 1661, Loss: 2.7603213787078857\n",
            "Training Iteration 1662, Loss: 7.340976238250732\n",
            "Training Iteration 1663, Loss: 7.74724006652832\n",
            "Training Iteration 1664, Loss: 4.092604637145996\n",
            "Training Iteration 1665, Loss: 2.7524678707122803\n",
            "Training Iteration 1666, Loss: 2.9460601806640625\n",
            "Training Iteration 1667, Loss: 7.013484477996826\n",
            "Training Iteration 1668, Loss: 3.91176700592041\n",
            "Training Iteration 1669, Loss: 6.904853820800781\n",
            "Training Iteration 1670, Loss: 3.6712112426757812\n",
            "Training Iteration 1671, Loss: 4.736947536468506\n",
            "Training Iteration 1672, Loss: 7.626298904418945\n",
            "Training Iteration 1673, Loss: 2.9497358798980713\n",
            "Training Iteration 1674, Loss: 5.456670761108398\n",
            "Training Iteration 1675, Loss: 6.205257415771484\n",
            "Training Iteration 1676, Loss: 6.229794025421143\n",
            "Training Iteration 1677, Loss: 4.13314962387085\n",
            "Training Iteration 1678, Loss: 3.409282684326172\n",
            "Training Iteration 1679, Loss: 3.517194986343384\n",
            "Training Iteration 1680, Loss: 3.1515588760375977\n",
            "Training Iteration 1681, Loss: 3.3653740882873535\n",
            "Training Iteration 1682, Loss: 4.6237640380859375\n",
            "Training Iteration 1683, Loss: 3.15497088432312\n",
            "Training Iteration 1684, Loss: 3.14874529838562\n",
            "Training Iteration 1685, Loss: 6.568938255310059\n",
            "Training Iteration 1686, Loss: 2.255274772644043\n",
            "Training Iteration 1687, Loss: 3.874176025390625\n",
            "Training Iteration 1688, Loss: 3.386134147644043\n",
            "Training Iteration 1689, Loss: 1.5982351303100586\n",
            "Training Iteration 1690, Loss: 4.560656547546387\n",
            "Training Iteration 1691, Loss: 6.163235187530518\n",
            "Training Iteration 1692, Loss: 3.224240303039551\n",
            "Training Iteration 1693, Loss: 2.950286388397217\n",
            "Training Iteration 1694, Loss: 5.433801174163818\n",
            "Training Iteration 1695, Loss: 4.683332920074463\n",
            "Training Iteration 1696, Loss: 4.269506454467773\n",
            "Training Iteration 1697, Loss: 3.996598243713379\n",
            "Training Iteration 1698, Loss: 3.598113775253296\n",
            "Training Iteration 1699, Loss: 2.985475540161133\n",
            "Training Iteration 1700, Loss: 4.269984722137451\n",
            "Training Iteration 1701, Loss: 4.335931777954102\n",
            "Training Iteration 1702, Loss: 6.0453410148620605\n",
            "Training Iteration 1703, Loss: 3.3865463733673096\n",
            "Training Iteration 1704, Loss: 4.44453763961792\n",
            "Training Iteration 1705, Loss: 5.832522392272949\n",
            "Training Iteration 1706, Loss: 4.807467937469482\n",
            "Training Iteration 1707, Loss: 3.3440418243408203\n",
            "Training Iteration 1708, Loss: 1.8703640699386597\n",
            "Training Iteration 1709, Loss: 2.6501588821411133\n",
            "Training Iteration 1710, Loss: 7.465768814086914\n",
            "Training Iteration 1711, Loss: 3.51816987991333\n",
            "Training Iteration 1712, Loss: 3.3713135719299316\n",
            "Training Iteration 1713, Loss: 5.61348819732666\n",
            "Training Iteration 1714, Loss: 3.725405693054199\n",
            "Training Iteration 1715, Loss: 5.518855571746826\n",
            "Training Iteration 1716, Loss: 4.151438236236572\n",
            "Training Iteration 1717, Loss: 3.152881622314453\n",
            "Training Iteration 1718, Loss: 6.28209114074707\n",
            "Training Iteration 1719, Loss: 2.259561538696289\n",
            "Training Iteration 1720, Loss: 5.746010780334473\n",
            "Training Iteration 1721, Loss: 3.150087356567383\n",
            "Training Iteration 1722, Loss: 4.873467445373535\n",
            "Training Iteration 1723, Loss: 6.516503810882568\n",
            "Training Iteration 1724, Loss: 5.356195449829102\n",
            "Training Iteration 1725, Loss: 3.87032151222229\n",
            "Training Iteration 1726, Loss: 5.629730224609375\n",
            "Training Iteration 1727, Loss: 6.619332313537598\n",
            "Training Iteration 1728, Loss: 5.0819172859191895\n",
            "Training Iteration 1729, Loss: 2.902503490447998\n",
            "Training Iteration 1730, Loss: 6.041647911071777\n",
            "Training Iteration 1731, Loss: 4.056921005249023\n",
            "Training Iteration 1732, Loss: 5.227590560913086\n",
            "Training Iteration 1733, Loss: 4.369100570678711\n",
            "Training Iteration 1734, Loss: 8.831954956054688\n",
            "Training Iteration 1735, Loss: 11.541807174682617\n",
            "Training Iteration 1736, Loss: 4.783175468444824\n",
            "Training Iteration 1737, Loss: 3.512529134750366\n",
            "Training Iteration 1738, Loss: 0.9684151411056519\n",
            "Training Iteration 1739, Loss: 4.242349624633789\n",
            "Training Iteration 1740, Loss: 5.059329032897949\n",
            "Training Iteration 1741, Loss: 1.9079298973083496\n",
            "Training Iteration 1742, Loss: 4.360125541687012\n",
            "Training Iteration 1743, Loss: 7.60502290725708\n",
            "Training Iteration 1744, Loss: 6.15035343170166\n",
            "Training Iteration 1745, Loss: 6.852681636810303\n",
            "Training Iteration 1746, Loss: 5.40302848815918\n",
            "Training Iteration 1747, Loss: 5.870767116546631\n",
            "Training Iteration 1748, Loss: 5.157854080200195\n",
            "Training Iteration 1749, Loss: 3.2094075679779053\n",
            "Training Iteration 1750, Loss: 5.38096284866333\n",
            "Training Iteration 1751, Loss: 6.191611289978027\n",
            "Training Iteration 1752, Loss: 2.7970540523529053\n",
            "Training Iteration 1753, Loss: 7.127479076385498\n",
            "Training Iteration 1754, Loss: 6.392023086547852\n",
            "Training Iteration 1755, Loss: 4.7885870933532715\n",
            "Training Iteration 1756, Loss: 5.742223262786865\n",
            "Training Iteration 1757, Loss: 5.360245227813721\n",
            "Training Iteration 1758, Loss: 4.140941619873047\n",
            "Training Iteration 1759, Loss: 3.8200442790985107\n",
            "Training Iteration 1760, Loss: 5.928104400634766\n",
            "Training Iteration 1761, Loss: 9.945941925048828\n",
            "Training Iteration 1762, Loss: 6.058838844299316\n",
            "Training Iteration 1763, Loss: 4.5510478019714355\n",
            "Training Iteration 1764, Loss: 5.05341911315918\n",
            "Training Iteration 1765, Loss: 3.2372727394104004\n",
            "Training Iteration 1766, Loss: 3.4002857208251953\n",
            "Training Iteration 1767, Loss: 3.021505832672119\n",
            "Training Iteration 1768, Loss: 7.342768669128418\n",
            "Training Iteration 1769, Loss: 6.975700378417969\n",
            "Training Iteration 1770, Loss: 6.7355804443359375\n",
            "Training Iteration 1771, Loss: 3.985431671142578\n",
            "Training Iteration 1772, Loss: 5.6177978515625\n",
            "Training Iteration 1773, Loss: 5.090573310852051\n",
            "Training Iteration 1774, Loss: 8.17947006225586\n",
            "Training Iteration 1775, Loss: 8.065978050231934\n",
            "Training Iteration 1776, Loss: 5.945583343505859\n",
            "Training Iteration 1777, Loss: 4.621370315551758\n",
            "Training Iteration 1778, Loss: 7.216909885406494\n",
            "Training Iteration 1779, Loss: 10.49610424041748\n",
            "Training Iteration 1780, Loss: 10.586150169372559\n",
            "Training Iteration 1781, Loss: 9.255902290344238\n",
            "Training Iteration 1782, Loss: 3.183690071105957\n",
            "Training Iteration 1783, Loss: 3.5206239223480225\n",
            "Training Iteration 1784, Loss: 5.613718032836914\n",
            "Training Iteration 1785, Loss: 5.047658443450928\n",
            "Training Iteration 1786, Loss: 3.353513240814209\n",
            "Training Iteration 1787, Loss: 6.5455098152160645\n",
            "Training Iteration 1788, Loss: 9.586931228637695\n",
            "Training Iteration 1789, Loss: 8.097718238830566\n",
            "Training Iteration 1790, Loss: 1.3259013891220093\n",
            "Training Iteration 1791, Loss: 4.822408676147461\n",
            "Training Iteration 1792, Loss: 2.702824592590332\n",
            "Training Iteration 1793, Loss: 2.534153938293457\n",
            "Training Iteration 1794, Loss: 3.9471633434295654\n",
            "Training Iteration 1795, Loss: 4.460111141204834\n",
            "Training Iteration 1796, Loss: 1.7297883033752441\n",
            "Training Iteration 1797, Loss: 2.679966449737549\n",
            "Training Iteration 1798, Loss: 2.2755777835845947\n",
            "Training Iteration 1799, Loss: 3.1785404682159424\n",
            "Training Iteration 1800, Loss: 4.9074578285217285\n",
            "Training Iteration 1801, Loss: 1.0513533353805542\n",
            "Training Iteration 1802, Loss: 7.189702987670898\n",
            "Training Iteration 1803, Loss: 4.860702037811279\n",
            "Training Iteration 1804, Loss: 2.1552233695983887\n",
            "Training Iteration 1805, Loss: 3.0485999584198\n",
            "Training Iteration 1806, Loss: 4.422639846801758\n",
            "Training Iteration 1807, Loss: 3.440250873565674\n",
            "Training Iteration 1808, Loss: 5.5746870040893555\n",
            "Training Iteration 1809, Loss: 2.7877490520477295\n",
            "Training Iteration 1810, Loss: 5.122769355773926\n",
            "Training Iteration 1811, Loss: 2.045464038848877\n",
            "Training Iteration 1812, Loss: 3.27793025970459\n",
            "Training Iteration 1813, Loss: 5.230257987976074\n",
            "Training Iteration 1814, Loss: 4.6450371742248535\n",
            "Training Iteration 1815, Loss: 4.638667583465576\n",
            "Training Iteration 1816, Loss: 4.982732772827148\n",
            "Training Iteration 1817, Loss: 5.039806842803955\n",
            "Training Iteration 1818, Loss: 5.097716808319092\n",
            "Training Iteration 1819, Loss: 2.057591676712036\n",
            "Training Iteration 1820, Loss: 4.87623929977417\n",
            "Training Iteration 1821, Loss: 8.979460716247559\n",
            "Training Iteration 1822, Loss: 2.9664769172668457\n",
            "Training Iteration 1823, Loss: 4.8916215896606445\n",
            "Training Iteration 1824, Loss: 2.62463116645813\n",
            "Training Iteration 1825, Loss: 5.233431339263916\n",
            "Training Iteration 1826, Loss: 4.899256706237793\n",
            "Training Iteration 1827, Loss: 5.820713520050049\n",
            "Training Iteration 1828, Loss: 6.685781002044678\n",
            "Training Iteration 1829, Loss: 3.4521498680114746\n",
            "Training Iteration 1830, Loss: 6.045013427734375\n",
            "Training Iteration 1831, Loss: 2.799062490463257\n",
            "Training Iteration 1832, Loss: 5.074551105499268\n",
            "Training Iteration 1833, Loss: 6.636740684509277\n",
            "Training Iteration 1834, Loss: 5.552988052368164\n",
            "Training Iteration 1835, Loss: 5.951541423797607\n",
            "Training Iteration 1836, Loss: 5.738690376281738\n",
            "Training Iteration 1837, Loss: 8.2930326461792\n",
            "Training Iteration 1838, Loss: 3.126286029815674\n",
            "Training Iteration 1839, Loss: 2.7094058990478516\n",
            "Training Iteration 1840, Loss: 7.360153675079346\n",
            "Training Iteration 1841, Loss: 4.216686725616455\n",
            "Training Iteration 1842, Loss: 4.427295207977295\n",
            "Training Iteration 1843, Loss: 4.187247276306152\n",
            "Training Iteration 1844, Loss: 2.4365932941436768\n",
            "Training Iteration 1845, Loss: 4.379846096038818\n",
            "Training Iteration 1846, Loss: 5.670489311218262\n",
            "Training Iteration 1847, Loss: 2.622462272644043\n",
            "Training Iteration 1848, Loss: 4.730236530303955\n",
            "Training Iteration 1849, Loss: 3.700500011444092\n",
            "Training Iteration 1850, Loss: 5.488940715789795\n",
            "Training Iteration 1851, Loss: 7.176327228546143\n",
            "Training Iteration 1852, Loss: 5.027127742767334\n",
            "Training Iteration 1853, Loss: 4.098941326141357\n",
            "Training Iteration 1854, Loss: 2.7302165031433105\n",
            "Training Iteration 1855, Loss: 5.911134719848633\n",
            "Training Iteration 1856, Loss: 2.567258834838867\n",
            "Training Iteration 1857, Loss: 4.485171794891357\n",
            "Training Iteration 1858, Loss: 4.9413275718688965\n",
            "Training Iteration 1859, Loss: 3.819838523864746\n",
            "Training Iteration 1860, Loss: 8.73241901397705\n",
            "Training Iteration 1861, Loss: 6.600986957550049\n",
            "Training Iteration 1862, Loss: 3.4204044342041016\n",
            "Training Iteration 1863, Loss: 3.985748529434204\n",
            "Training Iteration 1864, Loss: 3.475149154663086\n",
            "Training Iteration 1865, Loss: 2.7961244583129883\n",
            "Training Iteration 1866, Loss: 3.4656810760498047\n",
            "Training Iteration 1867, Loss: 6.268345355987549\n",
            "Training Iteration 1868, Loss: 5.184579849243164\n",
            "Training Iteration 1869, Loss: 6.985436916351318\n",
            "Training Iteration 1870, Loss: 5.019803524017334\n",
            "Training Iteration 1871, Loss: 6.508638381958008\n",
            "Training Iteration 1872, Loss: 3.241042137145996\n",
            "Training Iteration 1873, Loss: 5.086483478546143\n",
            "Training Iteration 1874, Loss: 3.741987943649292\n",
            "Training Iteration 1875, Loss: 6.018450736999512\n",
            "Training Iteration 1876, Loss: 6.046205997467041\n",
            "Training Iteration 1877, Loss: 5.420801162719727\n",
            "Training Iteration 1878, Loss: 4.783876895904541\n",
            "Training Iteration 1879, Loss: 3.7038557529449463\n",
            "Training Iteration 1880, Loss: 2.974043130874634\n",
            "Training Iteration 1881, Loss: 3.845008134841919\n",
            "Training Iteration 1882, Loss: 5.114446640014648\n",
            "Training Iteration 1883, Loss: 5.251399993896484\n",
            "Training Iteration 1884, Loss: 4.778487205505371\n",
            "Training Iteration 1885, Loss: 2.2632744312286377\n",
            "Training Iteration 1886, Loss: 4.959395408630371\n",
            "Training Iteration 1887, Loss: 5.038357257843018\n",
            "Training Iteration 1888, Loss: 1.4898815155029297\n",
            "Training Iteration 1889, Loss: 3.8360095024108887\n",
            "Training Iteration 1890, Loss: 5.3478169441223145\n",
            "Training Iteration 1891, Loss: 4.260426998138428\n",
            "Training Iteration 1892, Loss: 3.529172420501709\n",
            "Training Iteration 1893, Loss: 4.591789245605469\n",
            "Training Iteration 1894, Loss: 5.850247859954834\n",
            "Training Iteration 1895, Loss: 4.775697231292725\n",
            "Training Iteration 1896, Loss: 6.442449569702148\n",
            "Training Iteration 1897, Loss: 6.292877674102783\n",
            "Training Iteration 1898, Loss: 2.9081430435180664\n",
            "Training Iteration 1899, Loss: 5.264711380004883\n",
            "Training Iteration 1900, Loss: 6.600152969360352\n",
            "Training Iteration 1901, Loss: 3.085137367248535\n",
            "Training Iteration 1902, Loss: 3.466581344604492\n",
            "Training Iteration 1903, Loss: 4.553950309753418\n",
            "Training Iteration 1904, Loss: 4.756940841674805\n",
            "Training Iteration 1905, Loss: 2.6789002418518066\n",
            "Training Iteration 1906, Loss: 7.159007549285889\n",
            "Training Iteration 1907, Loss: 6.896849155426025\n",
            "Training Iteration 1908, Loss: 4.362324237823486\n",
            "Training Iteration 1909, Loss: 3.89321231842041\n",
            "Training Iteration 1910, Loss: 5.9788665771484375\n",
            "Training Iteration 1911, Loss: 5.400882720947266\n",
            "Training Iteration 1912, Loss: 8.63971996307373\n",
            "Training Iteration 1913, Loss: 4.110194206237793\n",
            "Training Iteration 1914, Loss: 1.8186858892440796\n",
            "Training Iteration 1915, Loss: 3.7071120738983154\n",
            "Training Iteration 1916, Loss: 8.801752090454102\n",
            "Training Iteration 1917, Loss: 3.443516731262207\n",
            "Training Iteration 1918, Loss: 3.3117635250091553\n",
            "Training Iteration 1919, Loss: 3.7325997352600098\n",
            "Training Iteration 1920, Loss: 3.8505892753601074\n",
            "Training Iteration 1921, Loss: 4.0382399559021\n",
            "Training Iteration 1922, Loss: 3.2091023921966553\n",
            "Training Iteration 1923, Loss: 5.841359615325928\n",
            "Training Iteration 1924, Loss: 3.519057512283325\n",
            "Training Iteration 1925, Loss: 1.4900295734405518\n",
            "Training Iteration 1926, Loss: 3.71899151802063\n",
            "Training Iteration 1927, Loss: 5.173733234405518\n",
            "Training Iteration 1928, Loss: 5.765995502471924\n",
            "Training Iteration 1929, Loss: 6.593827247619629\n",
            "Training Iteration 1930, Loss: 2.1536242961883545\n",
            "Training Iteration 1931, Loss: 3.4753546714782715\n",
            "Training Iteration 1932, Loss: 5.4895148277282715\n",
            "Training Iteration 1933, Loss: 8.833924293518066\n",
            "Training Iteration 1934, Loss: 4.361145496368408\n",
            "Training Iteration 1935, Loss: 8.364258766174316\n",
            "Training Iteration 1936, Loss: 6.1795830726623535\n",
            "Training Iteration 1937, Loss: 1.8514670133590698\n",
            "Training Iteration 1938, Loss: 5.7170820236206055\n",
            "Training Iteration 1939, Loss: 1.444339632987976\n",
            "Training Iteration 1940, Loss: 6.151157379150391\n",
            "Training Iteration 1941, Loss: 5.127842903137207\n",
            "Training Iteration 1942, Loss: 5.636590003967285\n",
            "Training Iteration 1943, Loss: 4.665072441101074\n",
            "Training Iteration 1944, Loss: 3.0591282844543457\n",
            "Training Iteration 1945, Loss: 3.3408522605895996\n",
            "Training Iteration 1946, Loss: 2.3574180603027344\n",
            "Training Iteration 1947, Loss: 3.787210464477539\n",
            "Training Iteration 1948, Loss: 4.897446632385254\n",
            "Training Iteration 1949, Loss: 3.8595550060272217\n",
            "Training Iteration 1950, Loss: 5.16646671295166\n",
            "Training Iteration 1951, Loss: 3.639327049255371\n",
            "Training Iteration 1952, Loss: 7.553267002105713\n",
            "Training Iteration 1953, Loss: 3.0302133560180664\n",
            "Training Iteration 1954, Loss: 4.5792107582092285\n",
            "Training Iteration 1955, Loss: 5.311829566955566\n",
            "Training Iteration 1956, Loss: 2.615177869796753\n",
            "Training Iteration 1957, Loss: 3.3705978393554688\n",
            "Training Iteration 1958, Loss: 4.686553001403809\n",
            "Training Iteration 1959, Loss: 3.3077337741851807\n",
            "Training Iteration 1960, Loss: 3.767075538635254\n",
            "Training Iteration 1961, Loss: 4.358564853668213\n",
            "Training Iteration 1962, Loss: 3.8633832931518555\n",
            "Training Iteration 1963, Loss: 2.5386226177215576\n",
            "Training Iteration 1964, Loss: 5.860534191131592\n",
            "Training Iteration 1965, Loss: 3.806346893310547\n",
            "Training Iteration 1966, Loss: 5.175915241241455\n",
            "Training Iteration 1967, Loss: 4.499138355255127\n",
            "Training Iteration 1968, Loss: 4.292220115661621\n",
            "Training Iteration 1969, Loss: 5.900831699371338\n",
            "Training Iteration 1970, Loss: 3.984281063079834\n",
            "Training Iteration 1971, Loss: 4.335207939147949\n",
            "Training Iteration 1972, Loss: 1.7505788803100586\n",
            "Training Iteration 1973, Loss: 3.4543936252593994\n",
            "Training Iteration 1974, Loss: 5.602884292602539\n",
            "Training Iteration 1975, Loss: 6.622923374176025\n",
            "Training Iteration 1976, Loss: 2.659982204437256\n",
            "Training Iteration 1977, Loss: 2.2872066497802734\n",
            "Training Iteration 1978, Loss: 5.220984935760498\n",
            "Training Iteration 1979, Loss: 5.30681848526001\n",
            "Training Iteration 1980, Loss: 4.13454532623291\n",
            "Training Iteration 1981, Loss: 2.5466997623443604\n",
            "Training Iteration 1982, Loss: 1.751583456993103\n",
            "Training Iteration 1983, Loss: 7.587532997131348\n",
            "Training Iteration 1984, Loss: 7.419063091278076\n",
            "Training Iteration 1985, Loss: 3.5607664585113525\n",
            "Training Iteration 1986, Loss: 3.9290759563446045\n",
            "Training Iteration 1987, Loss: 3.0158467292785645\n",
            "Training Iteration 1988, Loss: 4.523871421813965\n",
            "Training Iteration 1989, Loss: 5.224216938018799\n",
            "Training Iteration 1990, Loss: 3.4149482250213623\n",
            "Training Iteration 1991, Loss: 7.080707550048828\n",
            "Training Iteration 1992, Loss: 2.0093541145324707\n",
            "Training Iteration 1993, Loss: 2.7225232124328613\n",
            "Training Iteration 1994, Loss: 6.436561584472656\n",
            "Training Iteration 1995, Loss: 5.633962154388428\n",
            "Training Iteration 1996, Loss: 2.690164804458618\n",
            "Training Iteration 1997, Loss: 4.05230712890625\n",
            "Training Iteration 1998, Loss: 3.918463706970215\n",
            "Training Iteration 1999, Loss: 3.6677937507629395\n",
            "Training Iteration 2000, Loss: 3.9033114910125732\n",
            "Training Iteration 2001, Loss: 2.230440616607666\n",
            "Training Iteration 2002, Loss: 2.868994951248169\n",
            "Training Iteration 2003, Loss: 8.952211380004883\n",
            "Training Iteration 2004, Loss: 8.370676040649414\n",
            "Training Iteration 2005, Loss: 6.201283931732178\n",
            "Training Iteration 2006, Loss: 4.157463073730469\n",
            "Training Iteration 2007, Loss: 4.377575397491455\n",
            "Training Iteration 2008, Loss: 5.848105430603027\n",
            "Training Iteration 2009, Loss: 4.752832412719727\n",
            "Training Iteration 2010, Loss: 5.116026401519775\n",
            "Training Iteration 2011, Loss: 5.513670444488525\n",
            "Training Iteration 2012, Loss: 6.297794818878174\n",
            "Training Iteration 2013, Loss: 0.7804484963417053\n",
            "Training Iteration 2014, Loss: 2.8562328815460205\n",
            "Training Iteration 2015, Loss: 2.748880624771118\n",
            "Training Iteration 2016, Loss: 2.939243793487549\n",
            "Training Iteration 2017, Loss: 2.3080461025238037\n",
            "Training Iteration 2018, Loss: 6.273576259613037\n",
            "Training Iteration 2019, Loss: 4.593470096588135\n",
            "Training Iteration 2020, Loss: 6.571865558624268\n",
            "Training Iteration 2021, Loss: 2.7740421295166016\n",
            "Training Iteration 2022, Loss: 4.492946624755859\n",
            "Training Iteration 2023, Loss: 3.930522918701172\n",
            "Training Iteration 2024, Loss: 3.3193106651306152\n",
            "Training Iteration 2025, Loss: 4.083021640777588\n",
            "Training Iteration 2026, Loss: 2.1457467079162598\n",
            "Training Iteration 2027, Loss: 2.911353349685669\n",
            "Training Iteration 2028, Loss: 1.9158406257629395\n",
            "Training Iteration 2029, Loss: 4.777819633483887\n",
            "Training Iteration 2030, Loss: 2.2346036434173584\n",
            "Training Iteration 2031, Loss: 4.720410346984863\n",
            "Training Iteration 2032, Loss: 3.3296680450439453\n",
            "Training Iteration 2033, Loss: 1.9447457790374756\n",
            "Training Iteration 2034, Loss: 5.026297092437744\n",
            "Training Iteration 2035, Loss: 2.574040174484253\n",
            "Training Iteration 2036, Loss: 3.2469847202301025\n",
            "Training Iteration 2037, Loss: 2.366884708404541\n",
            "Training Iteration 2038, Loss: 5.846127033233643\n",
            "Training Iteration 2039, Loss: 6.385501861572266\n",
            "Training Iteration 2040, Loss: 6.590177536010742\n",
            "Training Iteration 2041, Loss: 5.352166175842285\n",
            "Training Iteration 2042, Loss: 2.6205050945281982\n",
            "Training Iteration 2043, Loss: 3.0278730392456055\n",
            "Training Iteration 2044, Loss: 9.256287574768066\n",
            "Training Iteration 2045, Loss: 5.504308700561523\n",
            "Training Iteration 2046, Loss: 5.95860481262207\n",
            "Training Iteration 2047, Loss: 3.6426172256469727\n",
            "Training Iteration 2048, Loss: 6.312972068786621\n",
            "Training Iteration 2049, Loss: 2.2641942501068115\n",
            "Training Iteration 2050, Loss: 5.240056037902832\n",
            "Training Iteration 2051, Loss: 4.340243339538574\n",
            "Training Iteration 2052, Loss: 5.1501994132995605\n",
            "Training Iteration 2053, Loss: 6.464663982391357\n",
            "Training Iteration 2054, Loss: 4.3764328956604\n",
            "Training Iteration 2055, Loss: 3.1008191108703613\n",
            "Training Iteration 2056, Loss: 4.533839225769043\n",
            "Training Iteration 2057, Loss: 1.7493419647216797\n",
            "Training Iteration 2058, Loss: 4.319390773773193\n",
            "Training Iteration 2059, Loss: 4.976538181304932\n",
            "Training Iteration 2060, Loss: 3.263385772705078\n",
            "Training Iteration 2061, Loss: 4.260890960693359\n",
            "Training Iteration 2062, Loss: 10.464336395263672\n",
            "Training Iteration 2063, Loss: 2.490051746368408\n",
            "Training Iteration 2064, Loss: 2.6496598720550537\n",
            "Training Iteration 2065, Loss: 4.45388126373291\n",
            "Training Iteration 2066, Loss: 5.321741580963135\n",
            "Training Iteration 2067, Loss: 4.969232082366943\n",
            "Training Iteration 2068, Loss: 5.024240016937256\n",
            "Training Iteration 2069, Loss: 4.355814456939697\n",
            "Training Iteration 2070, Loss: 3.0556654930114746\n",
            "Training Iteration 2071, Loss: 4.236845970153809\n",
            "Training Iteration 2072, Loss: 5.917120456695557\n",
            "Training Iteration 2073, Loss: 6.636275291442871\n",
            "Training Iteration 2074, Loss: 3.7900400161743164\n",
            "Training Iteration 2075, Loss: 4.01420783996582\n",
            "Training Iteration 2076, Loss: 5.477354049682617\n",
            "Training Iteration 2077, Loss: 2.984396457672119\n",
            "Training Iteration 2078, Loss: 5.0177483558654785\n",
            "Training Iteration 2079, Loss: 2.1999754905700684\n",
            "Training Iteration 2080, Loss: 6.281348705291748\n",
            "Training Iteration 2081, Loss: 6.44784688949585\n",
            "Training Iteration 2082, Loss: 5.722092628479004\n",
            "Training Iteration 2083, Loss: 5.555024147033691\n",
            "Training Iteration 2084, Loss: 8.380258560180664\n",
            "Training Iteration 2085, Loss: 7.403817653656006\n",
            "Training Iteration 2086, Loss: 2.880399703979492\n",
            "Training Iteration 2087, Loss: 5.370957851409912\n",
            "Training Iteration 2088, Loss: 3.9822497367858887\n",
            "Training Iteration 2089, Loss: 4.092508792877197\n",
            "Training Iteration 2090, Loss: 9.032011985778809\n",
            "Training Iteration 2091, Loss: 9.353206634521484\n",
            "Training Iteration 2092, Loss: 7.400052070617676\n",
            "Training Iteration 2093, Loss: 4.5022993087768555\n",
            "Training Iteration 2094, Loss: 6.89402961730957\n",
            "Training Iteration 2095, Loss: 3.497640371322632\n",
            "Training Iteration 2096, Loss: 4.033356189727783\n",
            "Training Iteration 2097, Loss: 4.882676601409912\n",
            "Training Iteration 2098, Loss: 5.853728294372559\n",
            "Training Iteration 2099, Loss: 5.876635551452637\n",
            "Training Iteration 2100, Loss: 3.1334164142608643\n",
            "Training Iteration 2101, Loss: 2.8136374950408936\n",
            "Training Iteration 2102, Loss: 6.671593189239502\n",
            "Training Iteration 2103, Loss: 3.317676544189453\n",
            "Training Iteration 2104, Loss: 5.286320686340332\n",
            "Training Iteration 2105, Loss: 4.84192419052124\n",
            "Training Iteration 2106, Loss: 5.913813591003418\n",
            "Training Iteration 2107, Loss: 6.736415863037109\n",
            "Training Iteration 2108, Loss: 7.088263988494873\n",
            "Training Iteration 2109, Loss: 7.349043846130371\n",
            "Training Iteration 2110, Loss: 5.9105682373046875\n",
            "Training Iteration 2111, Loss: 6.253819942474365\n",
            "Training Iteration 2112, Loss: 5.486627101898193\n",
            "Training Iteration 2113, Loss: 3.433647632598877\n",
            "Training Iteration 2114, Loss: 4.279944896697998\n",
            "Training Iteration 2115, Loss: 3.4432637691497803\n",
            "Training Iteration 2116, Loss: 4.697398662567139\n",
            "Training Iteration 2117, Loss: 5.612354278564453\n",
            "Training Iteration 2118, Loss: 6.1386566162109375\n",
            "Training Iteration 2119, Loss: 2.1002204418182373\n",
            "Training Iteration 2120, Loss: 5.707104682922363\n",
            "Training Iteration 2121, Loss: 3.5549628734588623\n",
            "Training Iteration 2122, Loss: 5.7694830894470215\n",
            "Training Iteration 2123, Loss: 6.341485023498535\n",
            "Training Iteration 2124, Loss: 3.596829414367676\n",
            "Training Iteration 2125, Loss: 6.358836650848389\n",
            "Training Iteration 2126, Loss: 5.9860992431640625\n",
            "Training Iteration 2127, Loss: 5.605423927307129\n",
            "Training Iteration 2128, Loss: 5.179047584533691\n",
            "Training Iteration 2129, Loss: 6.233367919921875\n",
            "Training Iteration 2130, Loss: 1.7996082305908203\n",
            "Training Iteration 2131, Loss: 4.0728936195373535\n",
            "Training Iteration 2132, Loss: 6.834871768951416\n",
            "Training Iteration 2133, Loss: 3.5604605674743652\n",
            "Training Iteration 2134, Loss: 4.497931003570557\n",
            "Training Iteration 2135, Loss: 5.3708415031433105\n",
            "Training Iteration 2136, Loss: 5.802957057952881\n",
            "Training Iteration 2137, Loss: 6.404873847961426\n",
            "Training Iteration 2138, Loss: 2.903972625732422\n",
            "Training Iteration 2139, Loss: 6.070899963378906\n",
            "Training Iteration 2140, Loss: 4.892240047454834\n",
            "Training Iteration 2141, Loss: 4.469898700714111\n",
            "Training Iteration 2142, Loss: 3.833278179168701\n",
            "Training Iteration 2143, Loss: 5.210060119628906\n",
            "Training Iteration 2144, Loss: 11.233843803405762\n",
            "Training Iteration 2145, Loss: 6.198175430297852\n",
            "Training Iteration 2146, Loss: 4.3301239013671875\n",
            "Training Iteration 2147, Loss: 7.10275936126709\n",
            "Training Iteration 2148, Loss: 9.868221282958984\n",
            "Training Iteration 2149, Loss: 5.286903381347656\n",
            "Training Iteration 2150, Loss: 4.824650764465332\n",
            "Training Iteration 2151, Loss: 2.2984306812286377\n",
            "Training Iteration 2152, Loss: 3.434760093688965\n",
            "Training Iteration 2153, Loss: 5.941434383392334\n",
            "Training Iteration 2154, Loss: 4.9450554847717285\n",
            "Training Iteration 2155, Loss: 6.438920021057129\n",
            "Training Iteration 2156, Loss: 3.803471088409424\n",
            "Training Iteration 2157, Loss: 3.605039596557617\n",
            "Training Iteration 2158, Loss: 3.8398091793060303\n",
            "Training Iteration 2159, Loss: 2.7820286750793457\n",
            "Training Iteration 2160, Loss: 6.136285305023193\n",
            "Training Iteration 2161, Loss: 6.431304454803467\n",
            "Training Iteration 2162, Loss: 4.257933139801025\n",
            "Training Iteration 2163, Loss: 5.890204429626465\n",
            "Training Iteration 2164, Loss: 9.723573684692383\n",
            "Training Iteration 2165, Loss: 5.725671291351318\n",
            "Training Iteration 2166, Loss: 5.812541484832764\n",
            "Training Iteration 2167, Loss: 4.777371883392334\n",
            "Training Iteration 2168, Loss: 4.456528186798096\n",
            "Training Iteration 2169, Loss: 4.176335334777832\n",
            "Training Iteration 2170, Loss: 6.886444568634033\n",
            "Training Iteration 2171, Loss: 5.46975040435791\n",
            "Training Iteration 2172, Loss: 1.5933724641799927\n",
            "Training Iteration 2173, Loss: 2.4663054943084717\n",
            "Training Iteration 2174, Loss: 2.3758280277252197\n",
            "Training Iteration 2175, Loss: 2.579718828201294\n",
            "Training Iteration 2176, Loss: 6.7842254638671875\n",
            "Training Iteration 2177, Loss: 3.5301051139831543\n",
            "Training Iteration 2178, Loss: 4.635864734649658\n",
            "Training Iteration 2179, Loss: 5.124640464782715\n",
            "Training Iteration 2180, Loss: 3.5465567111968994\n",
            "Training Iteration 2181, Loss: 4.15656042098999\n",
            "Training Iteration 2182, Loss: 4.32136869430542\n",
            "Training Iteration 2183, Loss: 1.4956510066986084\n",
            "Training Iteration 2184, Loss: 2.579322576522827\n",
            "Training Iteration 2185, Loss: 6.211165428161621\n",
            "Training Iteration 2186, Loss: 6.345952033996582\n",
            "Training Iteration 2187, Loss: 4.267762660980225\n",
            "Training Iteration 2188, Loss: 5.35113525390625\n",
            "Training Iteration 2189, Loss: 7.728299140930176\n",
            "Training Iteration 2190, Loss: 4.666749954223633\n",
            "Training Iteration 2191, Loss: 4.462653160095215\n",
            "Training Iteration 2192, Loss: 7.8026838302612305\n",
            "Training Iteration 2193, Loss: 6.129145622253418\n",
            "Training Iteration 2194, Loss: 5.395108699798584\n",
            "Training Iteration 2195, Loss: 4.981869220733643\n",
            "Training Iteration 2196, Loss: 2.359971046447754\n",
            "Training Iteration 2197, Loss: 6.563441753387451\n",
            "Training Iteration 2198, Loss: 3.2555668354034424\n",
            "Training Iteration 2199, Loss: 4.864533424377441\n",
            "Training Iteration 2200, Loss: 3.1277618408203125\n",
            "Training Iteration 2201, Loss: 5.563848495483398\n",
            "Training Iteration 2202, Loss: 5.3862152099609375\n",
            "Training Iteration 2203, Loss: 5.09218692779541\n",
            "Training Iteration 2204, Loss: 5.74312162399292\n",
            "Training Iteration 2205, Loss: 7.271583557128906\n",
            "Training Iteration 2206, Loss: 3.1267178058624268\n",
            "Training Iteration 2207, Loss: 7.923025131225586\n",
            "Training Iteration 2208, Loss: 3.1230673789978027\n",
            "Training Iteration 2209, Loss: 5.0341315269470215\n",
            "Training Iteration 2210, Loss: 1.9197907447814941\n",
            "Training Iteration 2211, Loss: 3.7558467388153076\n",
            "Training Iteration 2212, Loss: 7.773819446563721\n",
            "Training Iteration 2213, Loss: 4.27720832824707\n",
            "Training Iteration 2214, Loss: 3.4016153812408447\n",
            "Training Iteration 2215, Loss: 5.694775581359863\n",
            "Training Iteration 2216, Loss: 2.6862854957580566\n",
            "Training Iteration 2217, Loss: 5.784571647644043\n",
            "Training Iteration 2218, Loss: 6.77622127532959\n",
            "Training Iteration 2219, Loss: 5.784616470336914\n",
            "Training Iteration 2220, Loss: 2.8842978477478027\n",
            "Training Iteration 2221, Loss: 5.561335563659668\n",
            "Training Iteration 2222, Loss: 4.9072041511535645\n",
            "Training Iteration 2223, Loss: 4.446056365966797\n",
            "Training Iteration 2224, Loss: 1.7358611822128296\n",
            "Training Iteration 2225, Loss: 8.594013214111328\n",
            "Training Iteration 2226, Loss: 5.533190727233887\n",
            "Training Iteration 2227, Loss: 3.645658016204834\n",
            "Training Iteration 2228, Loss: 6.696457862854004\n",
            "Training Iteration 2229, Loss: 3.5909082889556885\n",
            "Training Iteration 2230, Loss: 2.974492311477661\n",
            "Training Iteration 2231, Loss: 4.977401256561279\n",
            "Training Iteration 2232, Loss: 1.8950097560882568\n",
            "Training Iteration 2233, Loss: 5.785521030426025\n",
            "Training Iteration 2234, Loss: 7.296778202056885\n",
            "Training Iteration 2235, Loss: 2.854729413986206\n",
            "Training Iteration 2236, Loss: 7.006862640380859\n",
            "Training Iteration 2237, Loss: 6.05742073059082\n",
            "Training Iteration 2238, Loss: 6.188198089599609\n",
            "Training Iteration 2239, Loss: 2.4952592849731445\n",
            "Training Iteration 2240, Loss: 4.563055515289307\n",
            "Training Iteration 2241, Loss: 3.533968925476074\n",
            "Training Iteration 2242, Loss: 3.7668745517730713\n",
            "Training Iteration 2243, Loss: 4.021575927734375\n",
            "Training Iteration 2244, Loss: 5.291822910308838\n",
            "Training Iteration 2245, Loss: 3.8431453704833984\n",
            "Training Iteration 2246, Loss: 2.997512102127075\n",
            "Training Iteration 2247, Loss: 4.983264446258545\n",
            "Training Iteration 2248, Loss: 5.688037872314453\n",
            "Training Iteration 2249, Loss: 4.783731460571289\n",
            "Training Iteration 2250, Loss: 4.948902606964111\n",
            "Training Iteration 2251, Loss: 4.16856575012207\n",
            "Training Iteration 2252, Loss: 3.903214454650879\n",
            "Training Iteration 2253, Loss: 4.935955047607422\n",
            "Training Iteration 2254, Loss: 2.9980006217956543\n",
            "Training Iteration 2255, Loss: 4.38563346862793\n",
            "Training Iteration 2256, Loss: 3.443641185760498\n",
            "Training Iteration 2257, Loss: 3.824054002761841\n",
            "Training Iteration 2258, Loss: 6.414559841156006\n",
            "Training Iteration 2259, Loss: 3.266779899597168\n",
            "Training Iteration 2260, Loss: 1.170432686805725\n",
            "Training Iteration 2261, Loss: 4.953320503234863\n",
            "Training Iteration 2262, Loss: 3.8998348712921143\n",
            "Training Iteration 2263, Loss: 4.90916109085083\n",
            "Training Iteration 2264, Loss: 6.8566575050354\n",
            "Training Iteration 2265, Loss: 4.550714015960693\n",
            "Training Iteration 2266, Loss: 5.308100700378418\n",
            "Training Iteration 2267, Loss: 5.247554302215576\n",
            "Training Iteration 2268, Loss: 4.930845260620117\n",
            "Training Iteration 2269, Loss: 4.648387432098389\n",
            "Training Iteration 2270, Loss: 2.2240219116210938\n",
            "Training Iteration 2271, Loss: 5.484610557556152\n",
            "Training Iteration 2272, Loss: 5.763343334197998\n",
            "Training Iteration 2273, Loss: 5.7826032638549805\n",
            "Training Iteration 2274, Loss: 4.555070877075195\n",
            "Training Iteration 2275, Loss: 4.927677154541016\n",
            "Training Iteration 2276, Loss: 5.470486640930176\n",
            "Training Iteration 2277, Loss: 6.420963287353516\n",
            "Training Iteration 2278, Loss: 8.087892532348633\n",
            "Training Iteration 2279, Loss: 4.482789516448975\n",
            "Training Iteration 2280, Loss: 5.087366580963135\n",
            "Training Iteration 2281, Loss: 3.278226613998413\n",
            "Training Iteration 2282, Loss: 4.330951690673828\n",
            "Training Iteration 2283, Loss: 5.622663497924805\n",
            "Training Iteration 2284, Loss: 5.829538822174072\n",
            "Training Iteration 2285, Loss: 5.540351390838623\n",
            "Training Iteration 2286, Loss: 3.078876495361328\n",
            "Training Iteration 2287, Loss: 3.4731202125549316\n",
            "Training Iteration 2288, Loss: 5.291547775268555\n",
            "Training Iteration 2289, Loss: 2.2081689834594727\n",
            "Training Iteration 2290, Loss: 6.082879066467285\n",
            "Training Iteration 2291, Loss: 4.492727756500244\n",
            "Training Iteration 2292, Loss: 2.721832036972046\n",
            "Training Iteration 2293, Loss: 2.3130199909210205\n",
            "Training Iteration 2294, Loss: 3.500267744064331\n",
            "Training Iteration 2295, Loss: 7.014458179473877\n",
            "Training Iteration 2296, Loss: 5.030799388885498\n",
            "Training Iteration 2297, Loss: 4.928365707397461\n",
            "Training Iteration 2298, Loss: 4.425882816314697\n",
            "Training Iteration 2299, Loss: 5.469631195068359\n",
            "Training Iteration 2300, Loss: 1.8272894620895386\n",
            "Training Iteration 2301, Loss: 7.8610148429870605\n",
            "Training Iteration 2302, Loss: 4.800851821899414\n",
            "Training Iteration 2303, Loss: 5.889533042907715\n",
            "Training Iteration 2304, Loss: 3.6554808616638184\n",
            "Training Iteration 2305, Loss: 5.553842544555664\n",
            "Training Iteration 2306, Loss: 2.619818925857544\n",
            "Training Iteration 2307, Loss: 1.8490711450576782\n",
            "Training Iteration 2308, Loss: 6.676817417144775\n",
            "Training Iteration 2309, Loss: 6.436186790466309\n",
            "Training Iteration 2310, Loss: 12.360391616821289\n",
            "Training Iteration 2311, Loss: 8.772512435913086\n",
            "Training Iteration 2312, Loss: 3.5274431705474854\n",
            "Training Iteration 2313, Loss: 3.913332939147949\n",
            "Training Iteration 2314, Loss: 4.9405436515808105\n",
            "Training Iteration 2315, Loss: 3.969919443130493\n",
            "Training Iteration 2316, Loss: 4.217576026916504\n",
            "Training Iteration 2317, Loss: 3.364372968673706\n",
            "Training Iteration 2318, Loss: 5.592837333679199\n",
            "Training Iteration 2319, Loss: 5.891040325164795\n",
            "Training Iteration 2320, Loss: 2.575437307357788\n",
            "Training Iteration 2321, Loss: 6.840460777282715\n",
            "Training Iteration 2322, Loss: 3.437620162963867\n",
            "Training Iteration 2323, Loss: 3.875584363937378\n",
            "Training Iteration 2324, Loss: 5.4476118087768555\n",
            "Training Iteration 2325, Loss: 3.239494800567627\n",
            "Training Iteration 2326, Loss: 4.707333087921143\n",
            "Training Iteration 2327, Loss: 4.583409786224365\n",
            "Training Iteration 2328, Loss: 1.937458872795105\n",
            "Training Iteration 2329, Loss: 2.90142560005188\n",
            "Training Iteration 2330, Loss: 3.047813892364502\n",
            "Training Iteration 2331, Loss: 3.738753080368042\n",
            "Training Iteration 2332, Loss: 5.949367523193359\n",
            "Training Iteration 2333, Loss: 5.617378234863281\n",
            "Training Iteration 2334, Loss: 1.5252665281295776\n",
            "Training Iteration 2335, Loss: 4.813467025756836\n",
            "Training Iteration 2336, Loss: 6.462564945220947\n",
            "Training Iteration 2337, Loss: 4.084586143493652\n",
            "Training Iteration 2338, Loss: 4.002245903015137\n",
            "Training Iteration 2339, Loss: 4.450282096862793\n",
            "Training Iteration 2340, Loss: 4.273464202880859\n",
            "Training Iteration 2341, Loss: 5.359798908233643\n",
            "Training Iteration 2342, Loss: 3.4710869789123535\n",
            "Training Iteration 2343, Loss: 5.950349807739258\n",
            "Training Iteration 2344, Loss: 4.0012640953063965\n",
            "Training Iteration 2345, Loss: 3.7182705402374268\n",
            "Training Iteration 2346, Loss: 4.669981002807617\n",
            "Training Iteration 2347, Loss: 2.2871761322021484\n",
            "Training Iteration 2348, Loss: 1.280624270439148\n",
            "Training Iteration 2349, Loss: 3.4012722969055176\n",
            "Training Iteration 2350, Loss: 4.514891147613525\n",
            "Training Iteration 2351, Loss: 3.9190897941589355\n",
            "Training Iteration 2352, Loss: 7.026852607727051\n",
            "Training Iteration 2353, Loss: 3.259723663330078\n",
            "Training Iteration 2354, Loss: 4.521076202392578\n",
            "Training Iteration 2355, Loss: 5.714519500732422\n",
            "Training Iteration 2356, Loss: 4.919449329376221\n",
            "Training Iteration 2357, Loss: 1.1252816915512085\n",
            "Training Iteration 2358, Loss: 9.732898712158203\n",
            "Training Iteration 2359, Loss: 3.3541359901428223\n",
            "Training Iteration 2360, Loss: 5.733791351318359\n",
            "Training Iteration 2361, Loss: 4.304266452789307\n",
            "Training Iteration 2362, Loss: 5.1482648849487305\n",
            "Training Iteration 2363, Loss: 2.9676008224487305\n",
            "Training Iteration 2364, Loss: 3.1399829387664795\n",
            "Training Iteration 2365, Loss: 6.427194118499756\n",
            "Training Iteration 2366, Loss: 6.713293075561523\n",
            "Training Iteration 2367, Loss: 5.548155784606934\n",
            "Training Iteration 2368, Loss: 3.957094669342041\n",
            "Training Iteration 2369, Loss: 3.373833417892456\n",
            "Training Iteration 2370, Loss: 2.3358497619628906\n",
            "Training Iteration 2371, Loss: 5.8012237548828125\n",
            "Training Iteration 2372, Loss: 7.39390754699707\n",
            "Training Iteration 2373, Loss: 5.625401020050049\n",
            "Training Iteration 2374, Loss: 4.480118751525879\n",
            "Training Iteration 2375, Loss: 3.6578283309936523\n",
            "Training Iteration 2376, Loss: 3.0082147121429443\n",
            "Training Iteration 2377, Loss: 3.0200624465942383\n",
            "Training Iteration 2378, Loss: 1.930477499961853\n",
            "Training Iteration 2379, Loss: 5.272909641265869\n",
            "Training Iteration 2380, Loss: 2.2505974769592285\n",
            "Training Iteration 2381, Loss: 2.4289438724517822\n",
            "Training Iteration 2382, Loss: 1.8867558240890503\n",
            "Training Iteration 2383, Loss: 4.174154281616211\n",
            "Training Iteration 2384, Loss: 4.1446003913879395\n",
            "Training Iteration 2385, Loss: 5.091098785400391\n",
            "Training Iteration 2386, Loss: 4.8103227615356445\n",
            "Training Iteration 2387, Loss: 1.5672729015350342\n",
            "Training Iteration 2388, Loss: 2.755858898162842\n",
            "Training Iteration 2389, Loss: 5.261312484741211\n",
            "Training Iteration 2390, Loss: 2.16473126411438\n",
            "Training Iteration 2391, Loss: 3.082972764968872\n",
            "Training Iteration 2392, Loss: 1.9317810535430908\n",
            "Training Iteration 2393, Loss: 5.0743727684021\n",
            "Training Iteration 2394, Loss: 5.195467948913574\n",
            "Training Iteration 2395, Loss: 1.8886162042617798\n",
            "Training Iteration 2396, Loss: 10.390335083007812\n",
            "Training Iteration 2397, Loss: 2.5171549320220947\n",
            "Training Iteration 2398, Loss: 4.471210479736328\n",
            "Training Iteration 2399, Loss: 3.529221296310425\n",
            "Training Iteration 2400, Loss: 3.32060170173645\n",
            "Training Iteration 2401, Loss: 3.8312220573425293\n",
            "Training Iteration 2402, Loss: 6.405511379241943\n",
            "Training Iteration 2403, Loss: 4.401500225067139\n",
            "Training Iteration 2404, Loss: 4.062353134155273\n",
            "Training Iteration 2405, Loss: 2.5290610790252686\n",
            "Training Iteration 2406, Loss: 4.050656795501709\n",
            "Training Iteration 2407, Loss: 4.968097686767578\n",
            "Training Iteration 2408, Loss: 4.50645637512207\n",
            "Training Iteration 2409, Loss: 4.523819446563721\n",
            "Training Iteration 2410, Loss: 5.181385517120361\n",
            "Training Iteration 2411, Loss: 5.962945938110352\n",
            "Training Iteration 2412, Loss: 3.628361940383911\n",
            "Training Iteration 2413, Loss: 2.8397834300994873\n",
            "Training Iteration 2414, Loss: 3.204558849334717\n",
            "Training Iteration 2415, Loss: 4.179542064666748\n",
            "Training Iteration 2416, Loss: 4.9737725257873535\n",
            "Training Iteration 2417, Loss: 3.6090009212493896\n",
            "Training Iteration 2418, Loss: 1.4045099020004272\n",
            "Training Iteration 2419, Loss: 4.404432773590088\n",
            "Training Iteration 2420, Loss: 5.857236385345459\n",
            "Training Iteration 2421, Loss: 3.254783868789673\n",
            "Training Iteration 2422, Loss: 5.779080867767334\n",
            "Training Iteration 2423, Loss: 4.624224662780762\n",
            "Training Iteration 2424, Loss: 7.7055864334106445\n",
            "Training Iteration 2425, Loss: 6.3292317390441895\n",
            "Training Iteration 2426, Loss: 1.235929250717163\n",
            "Training Iteration 2427, Loss: 3.7984676361083984\n",
            "Training Iteration 2428, Loss: 9.677818298339844\n",
            "Training Iteration 2429, Loss: 4.831160068511963\n",
            "Training Iteration 2430, Loss: 7.861245155334473\n",
            "Training Iteration 2431, Loss: 3.2333431243896484\n",
            "Training Iteration 2432, Loss: 4.919191837310791\n",
            "Training Iteration 2433, Loss: 3.935955286026001\n",
            "Training Iteration 2434, Loss: 4.428062915802002\n",
            "Training Iteration 2435, Loss: 5.924017906188965\n",
            "Training Iteration 2436, Loss: 6.144214630126953\n",
            "Training Iteration 2437, Loss: 4.511037349700928\n",
            "Training Iteration 2438, Loss: 4.58599853515625\n",
            "Training Iteration 2439, Loss: 5.078304767608643\n",
            "Training Iteration 2440, Loss: 4.931074619293213\n",
            "Training Iteration 2441, Loss: 2.675920009613037\n",
            "Training Iteration 2442, Loss: 6.470746040344238\n",
            "Training Iteration 2443, Loss: 3.6865580081939697\n",
            "Training Iteration 2444, Loss: 6.19537353515625\n",
            "Training Iteration 2445, Loss: 4.771979331970215\n",
            "Training Iteration 2446, Loss: 4.586686134338379\n",
            "Training Iteration 2447, Loss: 4.429152488708496\n",
            "Training Iteration 2448, Loss: 3.9059977531433105\n",
            "Training Iteration 2449, Loss: 3.247450351715088\n",
            "Training Iteration 2450, Loss: 5.75114631652832\n",
            "Training Iteration 2451, Loss: 3.298063039779663\n",
            "Training Iteration 2452, Loss: 4.15800666809082\n",
            "Training Iteration 2453, Loss: 3.3191044330596924\n",
            "Training Iteration 2454, Loss: 4.858262062072754\n",
            "Training Iteration 2455, Loss: 4.442422866821289\n",
            "Training Iteration 2456, Loss: 5.870014667510986\n",
            "Training Iteration 2457, Loss: 3.9502861499786377\n",
            "Training Iteration 2458, Loss: 3.526163339614868\n",
            "Training Iteration 2459, Loss: 3.62498140335083\n",
            "Training Iteration 2460, Loss: 7.982571125030518\n",
            "Training Iteration 2461, Loss: 3.4707553386688232\n",
            "Training Iteration 2462, Loss: 6.909435749053955\n",
            "Training Iteration 2463, Loss: 6.6537652015686035\n",
            "Training Iteration 2464, Loss: 3.8029322624206543\n",
            "Training Iteration 2465, Loss: 4.237998962402344\n",
            "Training Iteration 2466, Loss: 2.7670555114746094\n",
            "Training Iteration 2467, Loss: 4.461305141448975\n",
            "Training Iteration 2468, Loss: 2.9501380920410156\n",
            "Training Iteration 2469, Loss: 3.193246603012085\n",
            "Training Iteration 2470, Loss: 6.35548734664917\n",
            "Training Iteration 2471, Loss: 3.0587317943573\n",
            "Training Iteration 2472, Loss: 8.148516654968262\n",
            "Training Iteration 2473, Loss: 2.2644569873809814\n",
            "Training Iteration 2474, Loss: 4.657590866088867\n",
            "Training Iteration 2475, Loss: 3.2276763916015625\n",
            "Training Iteration 2476, Loss: 4.97385835647583\n",
            "Training Iteration 2477, Loss: 4.203919887542725\n",
            "Training Iteration 2478, Loss: 6.821895122528076\n",
            "Training Iteration 2479, Loss: 6.910515308380127\n",
            "Training Iteration 2480, Loss: 4.6403584480285645\n",
            "Training Iteration 2481, Loss: 4.904116153717041\n",
            "Training Iteration 2482, Loss: 4.115503311157227\n",
            "Training Iteration 2483, Loss: 3.1682662963867188\n",
            "Training Iteration 2484, Loss: 6.671949863433838\n",
            "Training Iteration 2485, Loss: 5.6835408210754395\n",
            "Training Iteration 2486, Loss: 2.251704216003418\n",
            "Training Iteration 2487, Loss: 4.386903285980225\n",
            "Training Iteration 2488, Loss: 2.572964668273926\n",
            "Training Iteration 2489, Loss: 4.045142650604248\n",
            "Training Iteration 2490, Loss: 3.8699746131896973\n",
            "Training Iteration 2491, Loss: 3.024568796157837\n",
            "Training Iteration 2492, Loss: 2.8522324562072754\n",
            "Training Iteration 2493, Loss: 8.532403945922852\n",
            "Training Iteration 2494, Loss: 5.558162212371826\n",
            "Training Iteration 2495, Loss: 3.977715015411377\n",
            "Training Iteration 2496, Loss: 7.482748508453369\n",
            "Training Iteration 2497, Loss: 8.69735050201416\n",
            "Training Iteration 2498, Loss: 6.166958332061768\n",
            "Training Iteration 2499, Loss: 2.9884519577026367\n",
            "Training Iteration 2500, Loss: 8.473305702209473\n",
            "Training Iteration 2501, Loss: 7.037055015563965\n",
            "Training Iteration 2502, Loss: 4.462129592895508\n",
            "Training Iteration 2503, Loss: 3.6758246421813965\n",
            "Training Iteration 2504, Loss: 9.149352073669434\n",
            "Training Iteration 2505, Loss: 10.428458213806152\n",
            "Training Iteration 2506, Loss: 6.914069175720215\n",
            "Training Iteration 2507, Loss: 9.026007652282715\n",
            "Training Iteration 2508, Loss: 4.376448631286621\n",
            "Training Iteration 2509, Loss: 7.622158527374268\n",
            "Training Iteration 2510, Loss: 3.443239212036133\n",
            "Training Iteration 2511, Loss: 4.3323974609375\n",
            "Training Iteration 2512, Loss: 4.102420806884766\n",
            "Training Iteration 2513, Loss: 4.813692569732666\n",
            "Training Iteration 2514, Loss: 8.226606369018555\n",
            "Training Iteration 2515, Loss: 4.832335472106934\n",
            "Training Iteration 2516, Loss: 6.541198253631592\n",
            "Training Iteration 2517, Loss: 7.522019386291504\n",
            "Training Iteration 2518, Loss: 4.404916286468506\n",
            "Training Iteration 2519, Loss: 2.5389084815979004\n",
            "Training Iteration 2520, Loss: 5.0804972648620605\n",
            "Training Iteration 2521, Loss: 5.277812957763672\n",
            "Training Iteration 2522, Loss: 4.403237342834473\n",
            "Training Iteration 2523, Loss: 3.8730907440185547\n",
            "Training Iteration 2524, Loss: 4.892629146575928\n",
            "Training Iteration 2525, Loss: 9.044411659240723\n",
            "Training Iteration 2526, Loss: 2.81634783744812\n",
            "Training Iteration 2527, Loss: 7.155364513397217\n",
            "Training Iteration 2528, Loss: 4.5861382484436035\n",
            "Training Iteration 2529, Loss: 3.0428051948547363\n",
            "Training Iteration 2530, Loss: 7.137109756469727\n",
            "Training Iteration 2531, Loss: 5.23301887512207\n",
            "Training Iteration 2532, Loss: 3.7524325847625732\n",
            "Training Iteration 2533, Loss: 5.930683612823486\n",
            "Training Iteration 2534, Loss: 3.813272714614868\n",
            "Training Iteration 2535, Loss: 5.601640701293945\n",
            "Training Iteration 2536, Loss: 1.716052532196045\n",
            "Training Iteration 2537, Loss: 7.688633441925049\n",
            "Training Iteration 2538, Loss: 2.632099151611328\n",
            "Training Iteration 2539, Loss: 2.877260684967041\n",
            "Training Iteration 2540, Loss: 4.3532538414001465\n",
            "Training Iteration 2541, Loss: 5.096921920776367\n",
            "Training Iteration 2542, Loss: 3.4140424728393555\n",
            "Training Iteration 2543, Loss: 3.8763551712036133\n",
            "Training Iteration 2544, Loss: 4.224176406860352\n",
            "Training Iteration 2545, Loss: 5.228311061859131\n",
            "Training Iteration 2546, Loss: 4.628300189971924\n",
            "Training Iteration 2547, Loss: 3.8020153045654297\n",
            "Training Iteration 2548, Loss: 1.546271800994873\n",
            "Training Iteration 2549, Loss: 4.126672744750977\n",
            "Training Iteration 2550, Loss: 5.9131317138671875\n",
            "Training Iteration 2551, Loss: 2.0899763107299805\n",
            "Training Iteration 2552, Loss: 3.9399006366729736\n",
            "Training Iteration 2553, Loss: 4.558030605316162\n",
            "Training Iteration 2554, Loss: 6.547367572784424\n",
            "Training Iteration 2555, Loss: 5.16254186630249\n",
            "Training Iteration 2556, Loss: 4.035177230834961\n",
            "Training Iteration 2557, Loss: 2.8013739585876465\n",
            "Training Iteration 2558, Loss: 7.573760986328125\n",
            "Training Iteration 2559, Loss: 3.9882967472076416\n",
            "Training Iteration 2560, Loss: 2.1450424194335938\n",
            "Training Iteration 2561, Loss: 7.554127216339111\n",
            "Training Iteration 2562, Loss: 3.330742835998535\n",
            "Training Iteration 2563, Loss: 4.938132286071777\n",
            "Training Iteration 2564, Loss: 4.129389762878418\n",
            "Training Iteration 2565, Loss: 9.962808609008789\n",
            "Training Iteration 2566, Loss: 7.652924537658691\n",
            "Training Iteration 2567, Loss: 4.429285049438477\n",
            "Training Iteration 2568, Loss: 5.731783866882324\n",
            "Training Iteration 2569, Loss: 3.0263166427612305\n",
            "Training Iteration 2570, Loss: 4.701079845428467\n",
            "Training Iteration 2571, Loss: 3.311150550842285\n",
            "Training Iteration 2572, Loss: 2.7682831287384033\n",
            "Training Iteration 2573, Loss: 3.326719045639038\n",
            "Training Iteration 2574, Loss: 3.584360122680664\n",
            "Training Iteration 2575, Loss: 6.850796222686768\n",
            "Training Iteration 2576, Loss: 4.440135955810547\n",
            "Training Iteration 2577, Loss: 3.1663637161254883\n",
            "Training Iteration 2578, Loss: 2.9516077041625977\n",
            "Training Iteration 2579, Loss: 7.3526811599731445\n",
            "Training Iteration 2580, Loss: 2.3275270462036133\n",
            "Training Iteration 2581, Loss: 4.233030319213867\n",
            "Training Iteration 2582, Loss: 5.024193286895752\n",
            "Training Iteration 2583, Loss: 4.492912292480469\n",
            "Training Iteration 2584, Loss: 1.6277962923049927\n",
            "Training Iteration 2585, Loss: 6.086236000061035\n",
            "Training Iteration 2586, Loss: 5.523932933807373\n",
            "Training Iteration 2587, Loss: 2.911358118057251\n",
            "Training Iteration 2588, Loss: 6.407003402709961\n",
            "Training Iteration 2589, Loss: 4.233270645141602\n",
            "Training Iteration 2590, Loss: 2.6671552658081055\n",
            "Training Iteration 2591, Loss: 4.234058856964111\n",
            "Training Iteration 2592, Loss: 4.406626224517822\n",
            "Training Iteration 2593, Loss: 3.905658006668091\n",
            "Training Iteration 2594, Loss: 6.950619697570801\n",
            "Training Iteration 2595, Loss: 4.640406608581543\n",
            "Training Iteration 2596, Loss: 4.457023620605469\n",
            "Training Iteration 2597, Loss: 2.9807145595550537\n",
            "Training Iteration 2598, Loss: 1.8565478324890137\n",
            "Training Iteration 2599, Loss: 4.0975565910339355\n",
            "Training Iteration 2600, Loss: 3.571382522583008\n",
            "Training Iteration 2601, Loss: 4.1915717124938965\n",
            "Training Iteration 2602, Loss: 3.6521589756011963\n",
            "Training Iteration 2603, Loss: 4.922898292541504\n",
            "Training Iteration 2604, Loss: 3.220301628112793\n",
            "Training Iteration 2605, Loss: 2.513274669647217\n",
            "Training Iteration 2606, Loss: 4.31535005569458\n",
            "Training Iteration 2607, Loss: 6.689353942871094\n",
            "Training Iteration 2608, Loss: 5.6568474769592285\n",
            "Training Iteration 2609, Loss: 4.2355170249938965\n",
            "Training Iteration 2610, Loss: 2.547426462173462\n",
            "Training Iteration 2611, Loss: 1.7031883001327515\n",
            "Training Iteration 2612, Loss: 4.433833599090576\n",
            "Training Iteration 2613, Loss: 3.4811642169952393\n",
            "Training Iteration 2614, Loss: 5.994126796722412\n",
            "Training Iteration 2615, Loss: 2.924736499786377\n",
            "Training Iteration 2616, Loss: 3.6779606342315674\n",
            "Training Iteration 2617, Loss: 2.9801464080810547\n",
            "Training Iteration 2618, Loss: 5.955100059509277\n",
            "Training Iteration 2619, Loss: 4.3995490074157715\n",
            "Training Iteration 2620, Loss: 7.394596576690674\n",
            "Training Iteration 2621, Loss: 4.868790626525879\n",
            "Training Iteration 2622, Loss: 5.238681793212891\n",
            "Training Iteration 2623, Loss: 4.109042167663574\n",
            "Training Iteration 2624, Loss: 5.564349174499512\n",
            "Training Iteration 2625, Loss: 6.8499345779418945\n",
            "Training Iteration 2626, Loss: 7.557158470153809\n",
            "Training Iteration 2627, Loss: 5.815337181091309\n",
            "Training Iteration 2628, Loss: 3.8880319595336914\n",
            "Training Iteration 2629, Loss: 2.211678981781006\n",
            "Training Iteration 2630, Loss: 5.269125461578369\n",
            "Training Iteration 2631, Loss: 8.732636451721191\n",
            "Training Iteration 2632, Loss: 9.777022361755371\n",
            "Training Iteration 2633, Loss: 4.821131229400635\n",
            "Training Iteration 2634, Loss: 5.833548545837402\n",
            "Training Iteration 2635, Loss: 5.9962663650512695\n",
            "Training Iteration 2636, Loss: 3.1143970489501953\n",
            "Training Iteration 2637, Loss: 4.346035957336426\n",
            "Training Iteration 2638, Loss: 3.4048044681549072\n",
            "Training Iteration 2639, Loss: 5.102170944213867\n",
            "Training Iteration 2640, Loss: 2.1979568004608154\n",
            "Training Iteration 2641, Loss: 1.5083422660827637\n",
            "Training Iteration 2642, Loss: 5.767292499542236\n",
            "Training Iteration 2643, Loss: 9.106352806091309\n",
            "Training Iteration 2644, Loss: 4.241186141967773\n",
            "Training Iteration 2645, Loss: 4.231176376342773\n",
            "Training Iteration 2646, Loss: 4.821748733520508\n",
            "Training Iteration 2647, Loss: 5.05850887298584\n",
            "Training Iteration 2648, Loss: 8.653428077697754\n",
            "Training Iteration 2649, Loss: 5.700736045837402\n",
            "Training Iteration 2650, Loss: 5.509425640106201\n",
            "Training Iteration 2651, Loss: 9.668256759643555\n",
            "Training Iteration 2652, Loss: 5.920950889587402\n",
            "Training Iteration 2653, Loss: 3.914726972579956\n",
            "Training Iteration 2654, Loss: 4.892120838165283\n",
            "Training Iteration 2655, Loss: 4.756262302398682\n",
            "Training Iteration 2656, Loss: 3.4523470401763916\n",
            "Training Iteration 2657, Loss: 3.7611401081085205\n",
            "Training Iteration 2658, Loss: 7.205551624298096\n",
            "Training Iteration 2659, Loss: 4.457362651824951\n",
            "Training Iteration 2660, Loss: 3.0290603637695312\n",
            "Training Iteration 2661, Loss: 8.037629127502441\n",
            "Training Iteration 2662, Loss: 3.901895523071289\n",
            "Training Iteration 2663, Loss: 5.504491806030273\n",
            "Training Iteration 2664, Loss: 2.636265277862549\n",
            "Training Iteration 2665, Loss: 6.764009952545166\n",
            "Training Iteration 2666, Loss: 6.923577308654785\n",
            "Training Iteration 2667, Loss: 7.216932773590088\n",
            "Training Iteration 2668, Loss: 5.461055755615234\n",
            "Training Iteration 2669, Loss: 3.804203510284424\n",
            "Training Iteration 2670, Loss: 3.784686326980591\n",
            "Training Iteration 2671, Loss: 4.99376106262207\n",
            "Training Iteration 2672, Loss: 2.203606128692627\n",
            "Training Iteration 2673, Loss: 3.7602412700653076\n",
            "Training Iteration 2674, Loss: 5.788515090942383\n",
            "Training Iteration 2675, Loss: 3.6055164337158203\n",
            "Training Iteration 2676, Loss: 5.499571800231934\n",
            "Training Iteration 2677, Loss: 3.867478132247925\n",
            "Training Iteration 2678, Loss: 3.85178804397583\n",
            "Training Iteration 2679, Loss: 6.323859214782715\n",
            "Training Iteration 2680, Loss: 5.175601959228516\n",
            "Training Iteration 2681, Loss: 2.532780885696411\n",
            "Training Iteration 2682, Loss: 2.298597812652588\n",
            "Training Iteration 2683, Loss: 1.6322522163391113\n",
            "Training Iteration 2684, Loss: 4.8896284103393555\n",
            "Training Iteration 2685, Loss: 5.219112396240234\n",
            "Training Iteration 2686, Loss: 2.3649802207946777\n",
            "Training Iteration 2687, Loss: 5.26808500289917\n",
            "Training Iteration 2688, Loss: 7.56201171875\n",
            "Training Iteration 2689, Loss: 3.0358693599700928\n",
            "Training Iteration 2690, Loss: 2.9475927352905273\n",
            "Training Iteration 2691, Loss: 4.237652778625488\n",
            "Training Iteration 2692, Loss: 4.245110988616943\n",
            "Training Iteration 2693, Loss: 3.9404232501983643\n",
            "Training Iteration 2694, Loss: 6.193774223327637\n",
            "Training Iteration 2695, Loss: 3.4275593757629395\n",
            "Training Iteration 2696, Loss: 2.9001872539520264\n",
            "Training Iteration 2697, Loss: 4.2786946296691895\n",
            "Training Iteration 2698, Loss: 8.603327751159668\n",
            "Training Iteration 2699, Loss: 5.287155628204346\n",
            "Training Iteration 2700, Loss: 7.295137882232666\n",
            "Training Iteration 2701, Loss: 4.6729326248168945\n",
            "Training Iteration 2702, Loss: 3.5631840229034424\n",
            "Training Iteration 2703, Loss: 2.9045772552490234\n",
            "Training Iteration 2704, Loss: 5.127857208251953\n",
            "Training Iteration 2705, Loss: 3.561879873275757\n",
            "Training Iteration 2706, Loss: 1.5991486310958862\n",
            "Training Iteration 2707, Loss: 4.956389427185059\n",
            "Training Iteration 2708, Loss: 4.2122883796691895\n",
            "Training Iteration 2709, Loss: 4.34239387512207\n",
            "Training Iteration 2710, Loss: 2.6540510654449463\n",
            "Training Iteration 2711, Loss: 6.988380432128906\n",
            "Training Iteration 2712, Loss: 2.81197452545166\n",
            "Training Iteration 2713, Loss: 2.1115822792053223\n",
            "Training Iteration 2714, Loss: 3.899596691131592\n",
            "Training Iteration 2715, Loss: 2.4928526878356934\n",
            "Training Iteration 2716, Loss: 6.245598316192627\n",
            "Training Iteration 2717, Loss: 3.840947151184082\n",
            "Training Iteration 2718, Loss: 6.359281539916992\n",
            "Training Iteration 2719, Loss: 5.335247993469238\n",
            "Training Iteration 2720, Loss: 3.0193605422973633\n",
            "Training Iteration 2721, Loss: 3.543874502182007\n",
            "Training Iteration 2722, Loss: 4.718388080596924\n",
            "Training Iteration 2723, Loss: 5.6676177978515625\n",
            "Training Iteration 2724, Loss: 4.02224063873291\n",
            "Training Iteration 2725, Loss: 5.577792167663574\n",
            "Training Iteration 2726, Loss: 3.519346237182617\n",
            "Training Iteration 2727, Loss: 2.3272459506988525\n",
            "Training Iteration 2728, Loss: 4.950216293334961\n",
            "Training Iteration 2729, Loss: 8.009342193603516\n",
            "Training Iteration 2730, Loss: 3.430813789367676\n",
            "Training Iteration 2731, Loss: 8.679012298583984\n",
            "Training Iteration 2732, Loss: 2.8213677406311035\n",
            "Training Iteration 2733, Loss: 5.613795757293701\n",
            "Training Iteration 2734, Loss: 5.773281097412109\n",
            "Training Iteration 2735, Loss: 5.817130088806152\n",
            "Training Iteration 2736, Loss: 4.295045375823975\n",
            "Training Iteration 2737, Loss: 6.207418441772461\n",
            "Training Iteration 2738, Loss: 3.1182169914245605\n",
            "Training Iteration 2739, Loss: 4.7545166015625\n",
            "Training Iteration 2740, Loss: 4.068414211273193\n",
            "Training Iteration 2741, Loss: 5.7064208984375\n",
            "Training Iteration 2742, Loss: 6.684349060058594\n",
            "Training Iteration 2743, Loss: 8.50765323638916\n",
            "Training Iteration 2744, Loss: 5.127819538116455\n",
            "Training Iteration 2745, Loss: 5.781105995178223\n",
            "Training Iteration 2746, Loss: 6.796976089477539\n",
            "Training Iteration 2747, Loss: 2.2589499950408936\n",
            "Training Iteration 2748, Loss: 3.071742057800293\n",
            "Training Iteration 2749, Loss: 5.300745010375977\n",
            "Training Iteration 2750, Loss: 3.3079416751861572\n",
            "Training Iteration 2751, Loss: 4.063532829284668\n",
            "Training Iteration 2752, Loss: 2.6747541427612305\n",
            "Training Iteration 2753, Loss: 5.308144569396973\n",
            "Training Iteration 2754, Loss: 3.8158342838287354\n",
            "Training Iteration 2755, Loss: 1.7380566596984863\n",
            "Training Iteration 2756, Loss: 8.079740524291992\n",
            "Training Iteration 2757, Loss: 6.0067830085754395\n",
            "Training Iteration 2758, Loss: 3.266972303390503\n",
            "Training Iteration 2759, Loss: 4.503249645233154\n",
            "Training Iteration 2760, Loss: 3.445117712020874\n",
            "Training Iteration 2761, Loss: 2.2178802490234375\n",
            "Training Iteration 2762, Loss: 3.432803153991699\n",
            "Training Iteration 2763, Loss: 4.5577874183654785\n",
            "Training Iteration 2764, Loss: 2.454577922821045\n",
            "Training Iteration 2765, Loss: 2.6924314498901367\n",
            "Training Iteration 2766, Loss: 3.7446274757385254\n",
            "Training Iteration 2767, Loss: 4.18890905380249\n",
            "Training Iteration 2768, Loss: 4.881624221801758\n",
            "Training Iteration 2769, Loss: 3.8781418800354004\n",
            "Training Iteration 2770, Loss: 2.8214824199676514\n",
            "Training Iteration 2771, Loss: 5.164403438568115\n",
            "Training Iteration 2772, Loss: 5.043690204620361\n",
            "Training Iteration 2773, Loss: 5.449548721313477\n",
            "Training Iteration 2774, Loss: 3.8812270164489746\n",
            "Training Iteration 2775, Loss: 4.667244911193848\n",
            "Training Iteration 2776, Loss: 4.5266289710998535\n",
            "Training Iteration 2777, Loss: 6.239187240600586\n",
            "Training Iteration 2778, Loss: 5.072308540344238\n",
            "Training Iteration 2779, Loss: 7.2648606300354\n",
            "Training Iteration 2780, Loss: 4.176608085632324\n",
            "Training Iteration 2781, Loss: 2.217376708984375\n",
            "Training Iteration 2782, Loss: 4.099771499633789\n",
            "Training Iteration 2783, Loss: 1.2824835777282715\n",
            "Training Iteration 2784, Loss: 2.4907338619232178\n",
            "Training Iteration 2785, Loss: 6.199965000152588\n",
            "Training Iteration 2786, Loss: 6.68065881729126\n",
            "Training Iteration 2787, Loss: 5.487922668457031\n",
            "Training Iteration 2788, Loss: 5.898619651794434\n",
            "Training Iteration 2789, Loss: 5.087909698486328\n",
            "Training Iteration 2790, Loss: 4.661849498748779\n",
            "Training Iteration 2791, Loss: 6.334677696228027\n",
            "Training Iteration 2792, Loss: 2.9495763778686523\n",
            "Training Iteration 2793, Loss: 7.409346580505371\n",
            "Training Iteration 2794, Loss: 6.6300506591796875\n",
            "Training Iteration 2795, Loss: 6.128580570220947\n",
            "Training Iteration 2796, Loss: 4.851978778839111\n",
            "Training Iteration 2797, Loss: 4.136756896972656\n",
            "Training Iteration 2798, Loss: 7.660052299499512\n",
            "Training Iteration 2799, Loss: 2.7174108028411865\n",
            "Training Iteration 2800, Loss: 6.51641321182251\n",
            "Training Iteration 2801, Loss: 5.407750606536865\n",
            "Training Iteration 2802, Loss: 5.720077991485596\n",
            "Training Iteration 2803, Loss: 4.7868194580078125\n",
            "Training Iteration 2804, Loss: 6.844058990478516\n",
            "Training Iteration 2805, Loss: 3.495534896850586\n",
            "Training Iteration 2806, Loss: 5.696195125579834\n",
            "Training Iteration 2807, Loss: 2.6476478576660156\n",
            "Training Iteration 2808, Loss: 4.61209774017334\n",
            "Training Iteration 2809, Loss: 5.367519855499268\n",
            "Training Iteration 2810, Loss: 4.731082439422607\n",
            "Training Iteration 2811, Loss: 4.667923927307129\n",
            "Training Iteration 2812, Loss: 3.879392147064209\n",
            "Training Iteration 2813, Loss: 6.196522235870361\n",
            "Training Iteration 2814, Loss: 5.984221458435059\n",
            "Training Iteration 2815, Loss: 4.099340438842773\n",
            "Training Iteration 2816, Loss: 5.22556734085083\n",
            "Training Iteration 2817, Loss: 3.9706642627716064\n",
            "Training Iteration 2818, Loss: 3.7449870109558105\n",
            "Training Iteration 2819, Loss: 5.650292873382568\n",
            "Training Iteration 2820, Loss: 5.824833393096924\n",
            "Training Iteration 2821, Loss: 3.6978726387023926\n",
            "Training Iteration 2822, Loss: 2.4745121002197266\n",
            "Training Iteration 2823, Loss: 3.7498412132263184\n",
            "Training Iteration 2824, Loss: 4.935089111328125\n",
            "Training Iteration 2825, Loss: 1.1457644701004028\n",
            "Training Iteration 2826, Loss: 4.810446262359619\n",
            "Training Iteration 2827, Loss: 3.8611063957214355\n",
            "Training Iteration 2828, Loss: 3.712080478668213\n",
            "Training Iteration 2829, Loss: 5.079545974731445\n",
            "Training Iteration 2830, Loss: 5.701781749725342\n",
            "Training Iteration 2831, Loss: 6.264984130859375\n",
            "Training Iteration 2832, Loss: 2.84294056892395\n",
            "Training Iteration 2833, Loss: 4.0674848556518555\n",
            "Training Iteration 2834, Loss: 4.213098049163818\n",
            "Training Iteration 2835, Loss: 5.000980377197266\n",
            "Training Iteration 2836, Loss: 3.169788360595703\n",
            "Training Iteration 2837, Loss: 7.702035903930664\n",
            "Training Iteration 2838, Loss: 5.09801721572876\n",
            "Training Iteration 2839, Loss: 3.1270792484283447\n",
            "Training Iteration 2840, Loss: 3.5951497554779053\n",
            "Training Iteration 2841, Loss: 8.330052375793457\n",
            "Training Iteration 2842, Loss: 3.4125421047210693\n",
            "Training Iteration 2843, Loss: 5.1596150398254395\n",
            "Training Iteration 2844, Loss: 5.175450801849365\n",
            "Training Iteration 2845, Loss: 4.863880157470703\n",
            "Training Iteration 2846, Loss: 3.0910398960113525\n",
            "Training Iteration 2847, Loss: 4.908135890960693\n",
            "Training Iteration 2848, Loss: 7.192929744720459\n",
            "Training Iteration 2849, Loss: 5.309885025024414\n",
            "Training Iteration 2850, Loss: 4.628498077392578\n",
            "Training Iteration 2851, Loss: 3.4144339561462402\n",
            "Training Iteration 2852, Loss: 4.275664329528809\n",
            "Training Iteration 2853, Loss: 2.8714733123779297\n",
            "Training Iteration 2854, Loss: 6.346231460571289\n",
            "Training Iteration 2855, Loss: 2.707740545272827\n",
            "Training Iteration 2856, Loss: 7.557576656341553\n",
            "Training Iteration 2857, Loss: 5.424153804779053\n",
            "Training Iteration 2858, Loss: 4.411154747009277\n",
            "Training Iteration 2859, Loss: 7.982061862945557\n",
            "Training Iteration 2860, Loss: 7.731703758239746\n",
            "Training Iteration 2861, Loss: 7.660396575927734\n",
            "Training Iteration 2862, Loss: 5.259164810180664\n",
            "Training Iteration 2863, Loss: 5.614545822143555\n",
            "Training Iteration 2864, Loss: 5.335632801055908\n",
            "Training Iteration 2865, Loss: 6.820197105407715\n",
            "Training Iteration 2866, Loss: 4.931209564208984\n",
            "Training Iteration 2867, Loss: 4.745777130126953\n",
            "Training Iteration 2868, Loss: 8.083596229553223\n",
            "Training Iteration 2869, Loss: 6.609138011932373\n",
            "Training Iteration 2870, Loss: 3.175793170928955\n",
            "Training Iteration 2871, Loss: 6.501028537750244\n",
            "Training Iteration 2872, Loss: 2.374758243560791\n",
            "Training Iteration 2873, Loss: 4.359988212585449\n",
            "Training Iteration 2874, Loss: 4.086949825286865\n",
            "Training Iteration 2875, Loss: 6.967978477478027\n",
            "Training Iteration 2876, Loss: 5.087489604949951\n",
            "Training Iteration 2877, Loss: 3.821272611618042\n",
            "Training Iteration 2878, Loss: 7.0235748291015625\n",
            "Training Iteration 2879, Loss: 3.267033576965332\n",
            "Training Iteration 2880, Loss: 5.590273857116699\n",
            "Training Iteration 2881, Loss: 3.1058101654052734\n",
            "Training Iteration 2882, Loss: 1.7071400880813599\n",
            "Training Iteration 2883, Loss: 2.8536460399627686\n",
            "Training Iteration 2884, Loss: 4.841650009155273\n",
            "Training Iteration 2885, Loss: 4.747008323669434\n",
            "Training Iteration 2886, Loss: 3.7214064598083496\n",
            "Training Iteration 2887, Loss: 4.690424919128418\n",
            "Training Iteration 2888, Loss: 4.423178195953369\n",
            "Training Iteration 2889, Loss: 3.357478380203247\n",
            "Training Iteration 2890, Loss: 4.7583231925964355\n",
            "Training Iteration 2891, Loss: 4.620574951171875\n",
            "Training Iteration 2892, Loss: 3.71891188621521\n",
            "Training Iteration 2893, Loss: 3.7875452041625977\n",
            "Training Iteration 2894, Loss: 6.470931053161621\n",
            "Training Iteration 2895, Loss: 2.5481438636779785\n",
            "Training Iteration 2896, Loss: 3.1726338863372803\n",
            "Training Iteration 2897, Loss: 3.6429100036621094\n",
            "Training Iteration 2898, Loss: 4.5659589767456055\n",
            "Training Iteration 2899, Loss: 4.37850284576416\n",
            "Training Iteration 2900, Loss: 3.3869502544403076\n",
            "Training Iteration 2901, Loss: 5.367570877075195\n",
            "Training Iteration 2902, Loss: 6.019569396972656\n",
            "Training Iteration 2903, Loss: 3.753333568572998\n",
            "Training Iteration 2904, Loss: 3.4444825649261475\n",
            "Training Iteration 2905, Loss: 3.6981072425842285\n",
            "Training Iteration 2906, Loss: 2.752128839492798\n",
            "Training Iteration 2907, Loss: 4.950558185577393\n",
            "Training Iteration 2908, Loss: 5.103966236114502\n",
            "Training Iteration 2909, Loss: 4.866535186767578\n",
            "Training Iteration 2910, Loss: 4.283226013183594\n",
            "Training Iteration 2911, Loss: 3.847062587738037\n",
            "Training Iteration 2912, Loss: 4.900338172912598\n",
            "Training Iteration 2913, Loss: 3.0492477416992188\n",
            "Training Iteration 2914, Loss: 5.759241104125977\n",
            "Training Iteration 2915, Loss: 5.357244491577148\n",
            "Training Iteration 2916, Loss: 4.898197650909424\n",
            "Training Iteration 2917, Loss: 7.518111705780029\n",
            "Training Iteration 2918, Loss: 6.538974285125732\n",
            "Training Iteration 2919, Loss: 5.4538116455078125\n",
            "Training Iteration 2920, Loss: 4.111705780029297\n",
            "Training Iteration 2921, Loss: 4.957715034484863\n",
            "Training Iteration 2922, Loss: 2.7425451278686523\n",
            "Training Iteration 2923, Loss: 4.653602123260498\n",
            "Training Iteration 2924, Loss: 3.237579345703125\n",
            "Training Iteration 2925, Loss: 5.327732563018799\n",
            "Training Iteration 2926, Loss: 4.434194564819336\n",
            "Training Iteration 2927, Loss: 1.8474059104919434\n",
            "Training Iteration 2928, Loss: 2.944932460784912\n",
            "Training Iteration 2929, Loss: 4.113255500793457\n",
            "Training Iteration 2930, Loss: 7.283910751342773\n",
            "Training Iteration 2931, Loss: 2.4079062938690186\n",
            "Training Iteration 2932, Loss: 4.390789985656738\n",
            "Training Iteration 2933, Loss: 3.1112165451049805\n",
            "Training Iteration 2934, Loss: 2.9820444583892822\n",
            "Training Iteration 2935, Loss: 5.105116367340088\n",
            "Training Iteration 2936, Loss: 6.931389331817627\n",
            "Training Iteration 2937, Loss: 5.017104148864746\n",
            "Training Iteration 2938, Loss: 6.160444736480713\n",
            "Training Iteration 2939, Loss: 6.254281997680664\n",
            "Training Iteration 2940, Loss: 5.045783042907715\n",
            "Training Iteration 2941, Loss: 4.672595024108887\n",
            "Training Iteration 2942, Loss: 8.014646530151367\n",
            "Training Iteration 2943, Loss: 4.9930243492126465\n",
            "Training Iteration 2944, Loss: 3.652033567428589\n",
            "Training Iteration 2945, Loss: 7.79868745803833\n",
            "Training Iteration 2946, Loss: 4.953069686889648\n",
            "Training Iteration 2947, Loss: 4.816908359527588\n",
            "Training Iteration 2948, Loss: 6.262028694152832\n",
            "Training Iteration 2949, Loss: 3.9938113689422607\n",
            "Training Iteration 2950, Loss: 2.6057393550872803\n",
            "Training Iteration 2951, Loss: 8.86567497253418\n",
            "Training Iteration 2952, Loss: 4.391227722167969\n",
            "Training Iteration 2953, Loss: 4.935601711273193\n",
            "Training Iteration 2954, Loss: 4.544896125793457\n",
            "Training Iteration 2955, Loss: 3.5455853939056396\n",
            "Training Iteration 2956, Loss: 2.3020966053009033\n",
            "Training Iteration 2957, Loss: 6.134507179260254\n",
            "Training Iteration 2958, Loss: 3.06674861907959\n",
            "Training Iteration 2959, Loss: 4.979076862335205\n",
            "Training Iteration 2960, Loss: 3.5257225036621094\n",
            "Training Iteration 2961, Loss: 5.37195348739624\n",
            "Training Iteration 2962, Loss: 3.4608981609344482\n",
            "Training Iteration 2963, Loss: 3.347604274749756\n",
            "Training Iteration 2964, Loss: 2.0417749881744385\n",
            "Training Iteration 2965, Loss: 2.9775328636169434\n",
            "Training Iteration 2966, Loss: 2.401480197906494\n",
            "Training Iteration 2967, Loss: 4.040956497192383\n",
            "Training Iteration 2968, Loss: 4.603792190551758\n",
            "Training Iteration 2969, Loss: 3.228081703186035\n",
            "Training Iteration 2970, Loss: 5.7193756103515625\n",
            "Training Iteration 2971, Loss: 5.104671001434326\n",
            "Training Iteration 2972, Loss: 5.579935550689697\n",
            "Training Iteration 2973, Loss: 8.050686836242676\n",
            "Training Iteration 2974, Loss: 4.418124198913574\n",
            "Training Iteration 2975, Loss: 8.708680152893066\n",
            "Training Iteration 2976, Loss: 8.649471282958984\n",
            "Training Iteration 2977, Loss: 8.323644638061523\n",
            "Training Iteration 2978, Loss: 7.357481002807617\n",
            "Training Iteration 2979, Loss: 4.022459983825684\n",
            "Training Iteration 2980, Loss: 4.649298667907715\n",
            "Training Iteration 2981, Loss: 7.217438697814941\n",
            "Training Iteration 2982, Loss: 4.99566125869751\n",
            "Training Iteration 2983, Loss: 8.68493366241455\n",
            "Training Iteration 2984, Loss: 4.494740962982178\n",
            "Training Iteration 2985, Loss: 7.864650726318359\n",
            "Training Iteration 2986, Loss: 6.958982944488525\n",
            "Training Iteration 2987, Loss: 4.691665172576904\n",
            "Training Iteration 2988, Loss: 4.413053512573242\n",
            "Training Iteration 2989, Loss: 6.229212284088135\n",
            "Training Iteration 2990, Loss: 6.495460510253906\n",
            "Training Iteration 2991, Loss: 2.972414970397949\n",
            "Training Iteration 2992, Loss: 4.632223129272461\n",
            "Training Iteration 2993, Loss: 3.561981201171875\n",
            "Training Iteration 2994, Loss: 3.029456853866577\n",
            "Training Iteration 2995, Loss: 4.501415252685547\n",
            "Training Iteration 2996, Loss: 4.380993843078613\n",
            "Training Iteration 2997, Loss: 4.762134075164795\n",
            "Training Iteration 2998, Loss: 4.849932670593262\n",
            "Training Iteration 2999, Loss: 9.123525619506836\n",
            "Training Iteration 3000, Loss: 6.386488914489746\n",
            "Training Iteration 3001, Loss: 3.938060998916626\n",
            "Training Iteration 3002, Loss: 1.9352283477783203\n",
            "Training Iteration 3003, Loss: 2.394167184829712\n",
            "Training Iteration 3004, Loss: 5.787154197692871\n",
            "Training Iteration 3005, Loss: 5.615216255187988\n",
            "Training Iteration 3006, Loss: 2.830998659133911\n",
            "Training Iteration 3007, Loss: 5.472679138183594\n",
            "Training Iteration 3008, Loss: 5.524542331695557\n",
            "Training Iteration 3009, Loss: 3.717799186706543\n",
            "Training Iteration 3010, Loss: 3.3249192237854004\n",
            "Training Iteration 3011, Loss: 3.4613022804260254\n",
            "Training Iteration 3012, Loss: 4.209046840667725\n",
            "Training Iteration 3013, Loss: 3.968287467956543\n",
            "Training Iteration 3014, Loss: 3.3802101612091064\n",
            "Training Iteration 3015, Loss: 4.758459091186523\n",
            "Training Iteration 3016, Loss: 3.2934036254882812\n",
            "Training Iteration 3017, Loss: 2.5540931224823\n",
            "Training Iteration 3018, Loss: 3.9587743282318115\n",
            "Training Iteration 3019, Loss: 4.089374542236328\n",
            "Training Iteration 3020, Loss: 3.1614837646484375\n",
            "Training Iteration 3021, Loss: 3.4362235069274902\n",
            "Training Iteration 3022, Loss: 4.558833599090576\n",
            "Training Iteration 3023, Loss: 3.7017953395843506\n",
            "Training Iteration 3024, Loss: 4.65341854095459\n",
            "Training Iteration 3025, Loss: 2.8337955474853516\n",
            "Training Iteration 3026, Loss: 7.196005821228027\n",
            "Training Iteration 3027, Loss: 1.7516947984695435\n",
            "Training Iteration 3028, Loss: 4.10367488861084\n",
            "Training Iteration 3029, Loss: 4.652644157409668\n",
            "Training Iteration 3030, Loss: 5.697669506072998\n",
            "Training Iteration 3031, Loss: 4.192818641662598\n",
            "Training Iteration 3032, Loss: 3.6096889972686768\n",
            "Training Iteration 3033, Loss: 4.314087867736816\n",
            "Training Iteration 3034, Loss: 1.6175577640533447\n",
            "Training Iteration 3035, Loss: 4.051063060760498\n",
            "Training Iteration 3036, Loss: 1.652235984802246\n",
            "Training Iteration 3037, Loss: 4.888934135437012\n",
            "Training Iteration 3038, Loss: 8.714119911193848\n",
            "Training Iteration 3039, Loss: 6.828708648681641\n",
            "Training Iteration 3040, Loss: 7.29085636138916\n",
            "Training Iteration 3041, Loss: 5.519179344177246\n",
            "Training Iteration 3042, Loss: 2.960414409637451\n",
            "Training Iteration 3043, Loss: 2.582853317260742\n",
            "Training Iteration 3044, Loss: 3.685713768005371\n",
            "Training Iteration 3045, Loss: 6.187832832336426\n",
            "Training Iteration 3046, Loss: 3.822254180908203\n",
            "Training Iteration 3047, Loss: 5.041037082672119\n",
            "Training Iteration 3048, Loss: 4.596429347991943\n",
            "Training Iteration 3049, Loss: 4.704044818878174\n",
            "Training Iteration 3050, Loss: 5.77808141708374\n",
            "Training Iteration 3051, Loss: 3.7255783081054688\n",
            "Training Iteration 3052, Loss: 4.460792064666748\n",
            "Training Iteration 3053, Loss: 3.7052788734436035\n",
            "Training Iteration 3054, Loss: 3.3976378440856934\n",
            "Training Iteration 3055, Loss: 5.69459342956543\n",
            "Training Iteration 3056, Loss: 4.106784820556641\n",
            "Training Iteration 3057, Loss: 7.459648609161377\n",
            "Training Iteration 3058, Loss: 7.812381744384766\n",
            "Training Iteration 3059, Loss: 6.1755852699279785\n",
            "Training Iteration 3060, Loss: 2.7774744033813477\n",
            "Training Iteration 3061, Loss: 4.049307346343994\n",
            "Training Iteration 3062, Loss: 5.200158596038818\n",
            "Training Iteration 3063, Loss: 6.562198638916016\n",
            "Training Iteration 3064, Loss: 7.342728137969971\n",
            "Training Iteration 3065, Loss: 7.661157608032227\n",
            "Training Iteration 3066, Loss: 7.020555019378662\n",
            "Training Iteration 3067, Loss: 2.2779412269592285\n",
            "Training Iteration 3068, Loss: 1.7680158615112305\n",
            "Training Iteration 3069, Loss: 4.5277533531188965\n",
            "Training Iteration 3070, Loss: 7.077948570251465\n",
            "Training Iteration 3071, Loss: 8.677722930908203\n",
            "Training Iteration 3072, Loss: 9.621743202209473\n",
            "Training Iteration 3073, Loss: 7.450016975402832\n",
            "Training Iteration 3074, Loss: 6.483222961425781\n",
            "Training Iteration 3075, Loss: 9.498395919799805\n",
            "Training Iteration 3076, Loss: 7.180682182312012\n",
            "Training Iteration 3077, Loss: 4.390267848968506\n",
            "Training Iteration 3078, Loss: 4.190113067626953\n",
            "Training Iteration 3079, Loss: 3.5961050987243652\n",
            "Training Iteration 3080, Loss: 3.528048276901245\n",
            "Training Iteration 3081, Loss: 5.245339870452881\n",
            "Training Iteration 3082, Loss: 10.883953094482422\n",
            "Training Iteration 3083, Loss: 3.557739019393921\n",
            "Training Iteration 3084, Loss: 8.13388729095459\n",
            "Training Iteration 3085, Loss: 6.144604682922363\n",
            "Training Iteration 3086, Loss: 3.973735809326172\n",
            "Training Iteration 3087, Loss: 5.893945693969727\n",
            "Training Iteration 3088, Loss: 4.288241863250732\n",
            "Training Iteration 3089, Loss: 2.9616689682006836\n",
            "Training Iteration 3090, Loss: 4.759783744812012\n",
            "Training Iteration 3091, Loss: 3.9943089485168457\n",
            "Training Iteration 3092, Loss: 6.6492533683776855\n",
            "Training Iteration 3093, Loss: 6.250156402587891\n",
            "Training Iteration 3094, Loss: 4.140209674835205\n",
            "Training Iteration 3095, Loss: 4.2703118324279785\n",
            "Training Iteration 3096, Loss: 4.410767555236816\n",
            "Training Iteration 3097, Loss: 1.5890142917633057\n",
            "Training Iteration 3098, Loss: 6.824251651763916\n",
            "Training Iteration 3099, Loss: 7.364591598510742\n",
            "Training Iteration 3100, Loss: 6.854846954345703\n",
            "Training Iteration 3101, Loss: 2.806892156600952\n",
            "Training Iteration 3102, Loss: 3.3564372062683105\n",
            "Training Iteration 3103, Loss: 5.7825798988342285\n",
            "Training Iteration 3104, Loss: 4.219757080078125\n",
            "Training Iteration 3105, Loss: 5.444891452789307\n",
            "Training Iteration 3106, Loss: 5.628189563751221\n",
            "Training Iteration 3107, Loss: 6.227211952209473\n",
            "Training Iteration 3108, Loss: 3.5650200843811035\n",
            "Training Iteration 3109, Loss: 4.41151237487793\n",
            "Training Iteration 3110, Loss: 4.695530414581299\n",
            "Training Iteration 3111, Loss: 3.2418370246887207\n",
            "Training Iteration 3112, Loss: 3.8629581928253174\n",
            "Training Iteration 3113, Loss: 4.978995323181152\n",
            "Training Iteration 3114, Loss: 3.28056001663208\n",
            "Training Iteration 3115, Loss: 1.5935341119766235\n",
            "Training Iteration 3116, Loss: 4.369767189025879\n",
            "Training Iteration 3117, Loss: 5.236433029174805\n",
            "Training Iteration 3118, Loss: 3.600908041000366\n",
            "Training Iteration 3119, Loss: 2.3585197925567627\n",
            "Training Iteration 3120, Loss: 3.5222227573394775\n",
            "Training Iteration 3121, Loss: 4.6919050216674805\n",
            "Training Iteration 3122, Loss: 6.36681604385376\n",
            "Training Iteration 3123, Loss: 2.575993299484253\n",
            "Training Iteration 3124, Loss: 2.7405755519866943\n",
            "Training Iteration 3125, Loss: 3.90356707572937\n",
            "Training Iteration 3126, Loss: 2.333110809326172\n",
            "Training Iteration 3127, Loss: 5.831016540527344\n",
            "Training Iteration 3128, Loss: 2.8256418704986572\n",
            "Training Iteration 3129, Loss: 0.24791793525218964\n",
            "Training Iteration 3130, Loss: 4.944613456726074\n",
            "Training Iteration 3131, Loss: 7.476357460021973\n",
            "Training Iteration 3132, Loss: 9.533687591552734\n",
            "Training Iteration 3133, Loss: 6.829308032989502\n",
            "Training Iteration 3134, Loss: 6.849750995635986\n",
            "Training Iteration 3135, Loss: 6.0700764656066895\n",
            "Training Iteration 3136, Loss: 1.4361108541488647\n",
            "Training Iteration 3137, Loss: 6.0805864334106445\n",
            "Training Iteration 3138, Loss: 3.2764205932617188\n",
            "Training Iteration 3139, Loss: 6.9644455909729\n",
            "Training Iteration 3140, Loss: 7.597765922546387\n",
            "Training Iteration 3141, Loss: 8.256000518798828\n",
            "Training Iteration 3142, Loss: 7.9039082527160645\n",
            "Training Iteration 3143, Loss: 11.059253692626953\n",
            "Training Iteration 3144, Loss: 6.289477825164795\n",
            "Training Iteration 3145, Loss: 1.704153299331665\n",
            "Training Iteration 3146, Loss: 4.379940986633301\n",
            "Training Iteration 3147, Loss: 3.6165804862976074\n",
            "Training Iteration 3148, Loss: 2.242140769958496\n",
            "Training Iteration 3149, Loss: 4.471837043762207\n",
            "Training Iteration 3150, Loss: 3.8944687843322754\n",
            "Training Iteration 3151, Loss: 6.092855930328369\n",
            "Training Iteration 3152, Loss: 4.278436660766602\n",
            "Training Iteration 3153, Loss: 6.668202877044678\n",
            "Training Iteration 3154, Loss: 7.870585918426514\n",
            "Training Iteration 3155, Loss: 5.177008628845215\n",
            "Training Iteration 3156, Loss: 2.091215133666992\n",
            "Training Iteration 3157, Loss: 5.712416648864746\n",
            "Training Iteration 3158, Loss: 3.4647629261016846\n",
            "Training Iteration 3159, Loss: 5.859859466552734\n",
            "Training Iteration 3160, Loss: 5.236485004425049\n",
            "Training Iteration 3161, Loss: 5.869102478027344\n",
            "Training Iteration 3162, Loss: 4.611551761627197\n",
            "Training Iteration 3163, Loss: 5.278732776641846\n",
            "Training Iteration 3164, Loss: 4.141348361968994\n",
            "Training Iteration 3165, Loss: 3.4490723609924316\n",
            "Training Iteration 3166, Loss: 4.284531593322754\n",
            "Training Iteration 3167, Loss: 2.0882723331451416\n",
            "Training Iteration 3168, Loss: 3.2239906787872314\n",
            "Training Iteration 3169, Loss: 3.4364664554595947\n",
            "Training Iteration 3170, Loss: 3.9535276889801025\n",
            "Training Iteration 3171, Loss: 4.079524517059326\n",
            "Training Iteration 3172, Loss: 2.6813316345214844\n",
            "Training Iteration 3173, Loss: 2.9390861988067627\n",
            "Training Iteration 3174, Loss: 2.3336853981018066\n",
            "Training Iteration 3175, Loss: 3.4309475421905518\n",
            "Training Iteration 3176, Loss: 6.898323059082031\n",
            "Training Iteration 3177, Loss: 4.080031394958496\n",
            "Training Iteration 3178, Loss: 5.778774261474609\n",
            "Training Iteration 3179, Loss: 2.887639045715332\n",
            "Training Iteration 3180, Loss: 1.78811776638031\n",
            "Training Iteration 3181, Loss: 1.0031275749206543\n",
            "Training Iteration 3182, Loss: 3.9823176860809326\n",
            "Training Iteration 3183, Loss: 3.4016523361206055\n",
            "Training Iteration 3184, Loss: 3.1100387573242188\n",
            "Training Iteration 3185, Loss: 4.347977161407471\n",
            "Training Iteration 3186, Loss: 2.7554636001586914\n",
            "Training Iteration 3187, Loss: 2.4135003089904785\n",
            "Training Iteration 3188, Loss: 2.6496646404266357\n",
            "Training Iteration 3189, Loss: 8.414527893066406\n",
            "Training Iteration 3190, Loss: 5.639412879943848\n",
            "Training Iteration 3191, Loss: 5.657368183135986\n",
            "Training Iteration 3192, Loss: 1.9291491508483887\n",
            "Training Iteration 3193, Loss: 3.0851593017578125\n",
            "Training Iteration 3194, Loss: 5.474808692932129\n",
            "Training Iteration 3195, Loss: 3.8634698390960693\n",
            "Training Iteration 3196, Loss: 3.9653420448303223\n",
            "Training Iteration 3197, Loss: 5.155261993408203\n",
            "Training Iteration 3198, Loss: 5.32926607131958\n",
            "Training Iteration 3199, Loss: 4.263143062591553\n",
            "Training Iteration 3200, Loss: 5.164008617401123\n",
            "Training Iteration 3201, Loss: 4.282386302947998\n",
            "Training Iteration 3202, Loss: 3.8349180221557617\n",
            "Training Iteration 3203, Loss: 4.945459365844727\n",
            "Training Iteration 3204, Loss: 4.568027496337891\n",
            "Training Iteration 3205, Loss: 3.0529325008392334\n",
            "Training Iteration 3206, Loss: 2.586137294769287\n",
            "Training Iteration 3207, Loss: 3.908360481262207\n",
            "Training Iteration 3208, Loss: 4.447788238525391\n",
            "Training Iteration 3209, Loss: 6.174272060394287\n",
            "Training Iteration 3210, Loss: 3.5397984981536865\n",
            "Training Iteration 3211, Loss: 2.852073907852173\n",
            "Training Iteration 3212, Loss: 2.256094455718994\n",
            "Training Iteration 3213, Loss: 4.251568794250488\n",
            "Training Iteration 3214, Loss: 6.376349449157715\n",
            "Training Iteration 3215, Loss: 3.873445987701416\n",
            "Training Iteration 3216, Loss: 3.994581937789917\n",
            "Training Iteration 3217, Loss: 3.6090810298919678\n",
            "Training Iteration 3218, Loss: 4.158761501312256\n",
            "Training Iteration 3219, Loss: 6.064489364624023\n",
            "Training Iteration 3220, Loss: 6.172955513000488\n",
            "Training Iteration 3221, Loss: 3.683565139770508\n",
            "Training Iteration 3222, Loss: 8.336359024047852\n",
            "Training Iteration 3223, Loss: 2.740290641784668\n",
            "Training Iteration 3224, Loss: 2.6091127395629883\n",
            "Training Iteration 3225, Loss: 5.539510726928711\n",
            "Training Iteration 3226, Loss: 3.5392708778381348\n",
            "Training Iteration 3227, Loss: 4.47774600982666\n",
            "Training Iteration 3228, Loss: 3.198603868484497\n",
            "Training Iteration 3229, Loss: 5.003742694854736\n",
            "Training Iteration 3230, Loss: 9.595407485961914\n",
            "Training Iteration 3231, Loss: 5.7366509437561035\n",
            "Training Iteration 3232, Loss: 2.2703213691711426\n",
            "Training Iteration 3233, Loss: 4.9830169677734375\n",
            "Training Iteration 3234, Loss: 2.6208577156066895\n",
            "Training Iteration 3235, Loss: 5.193549156188965\n",
            "Training Iteration 3236, Loss: 4.834222793579102\n",
            "Training Iteration 3237, Loss: 6.079000949859619\n",
            "Training Iteration 3238, Loss: 5.168063163757324\n",
            "Training Iteration 3239, Loss: 3.2285120487213135\n",
            "Training Iteration 3240, Loss: 4.4268927574157715\n",
            "Training Iteration 3241, Loss: 3.1982436180114746\n",
            "Training Iteration 3242, Loss: 8.579071044921875\n",
            "Training Iteration 3243, Loss: 4.38902473449707\n",
            "Training Iteration 3244, Loss: 6.419550895690918\n",
            "Training Iteration 3245, Loss: 7.740304470062256\n",
            "Training Iteration 3246, Loss: 3.3545641899108887\n",
            "Training Iteration 3247, Loss: 2.597846746444702\n",
            "Training Iteration 3248, Loss: 5.118381977081299\n",
            "Training Iteration 3249, Loss: 2.538881778717041\n",
            "Training Iteration 3250, Loss: 3.8456404209136963\n",
            "Training Iteration 3251, Loss: 5.466071605682373\n",
            "Training Iteration 3252, Loss: 4.136561393737793\n",
            "Training Iteration 3253, Loss: 7.913112640380859\n",
            "Training Iteration 3254, Loss: 4.794123649597168\n",
            "Training Iteration 3255, Loss: 3.95902419090271\n",
            "Training Iteration 3256, Loss: 5.4247260093688965\n",
            "Training Iteration 3257, Loss: 4.659360408782959\n",
            "Training Iteration 3258, Loss: 5.245152950286865\n",
            "Training Iteration 3259, Loss: 7.206278324127197\n",
            "Training Iteration 3260, Loss: 6.831846237182617\n",
            "Training Iteration 3261, Loss: 2.772071361541748\n",
            "Training Iteration 3262, Loss: 8.317864418029785\n",
            "Training Iteration 3263, Loss: 8.546546936035156\n",
            "Training Iteration 3264, Loss: 3.9845986366271973\n",
            "Training Iteration 3265, Loss: 7.576744079589844\n",
            "Training Iteration 3266, Loss: 2.2365143299102783\n",
            "Training Iteration 3267, Loss: 4.9003682136535645\n",
            "Training Iteration 3268, Loss: 6.511686325073242\n",
            "Training Iteration 3269, Loss: 4.995853900909424\n",
            "Training Iteration 3270, Loss: 3.23460054397583\n",
            "Training Iteration 3271, Loss: 3.897157907485962\n",
            "Training Iteration 3272, Loss: 4.601275444030762\n",
            "Training Iteration 3273, Loss: 3.384516477584839\n",
            "Training Iteration 3274, Loss: 3.547384262084961\n",
            "Training Iteration 3275, Loss: 4.124113082885742\n",
            "Training Iteration 3276, Loss: 3.0094220638275146\n",
            "Training Iteration 3277, Loss: 3.3017897605895996\n",
            "Training Iteration 3278, Loss: 4.326949119567871\n",
            "Training Iteration 3279, Loss: 3.204657554626465\n",
            "Training Iteration 3280, Loss: 7.767536640167236\n",
            "Training Iteration 3281, Loss: 4.320454120635986\n",
            "Training Iteration 3282, Loss: 8.089864730834961\n",
            "Training Iteration 3283, Loss: 2.286992311477661\n",
            "Training Iteration 3284, Loss: 5.130100250244141\n",
            "Training Iteration 3285, Loss: 3.3886120319366455\n",
            "Training Iteration 3286, Loss: 3.8206405639648438\n",
            "Training Iteration 3287, Loss: 2.192279577255249\n",
            "Training Iteration 3288, Loss: 2.673954486846924\n",
            "Training Iteration 3289, Loss: 5.197941780090332\n",
            "Training Iteration 3290, Loss: 3.4433484077453613\n",
            "Training Iteration 3291, Loss: 2.2618443965911865\n",
            "Training Iteration 3292, Loss: 4.212148666381836\n",
            "Training Iteration 3293, Loss: 3.3960938453674316\n",
            "Training Iteration 3294, Loss: 8.24691390991211\n",
            "Training Iteration 3295, Loss: 5.628416538238525\n",
            "Training Iteration 3296, Loss: 4.762753963470459\n",
            "Training Iteration 3297, Loss: 6.081211090087891\n",
            "Training Iteration 3298, Loss: 3.639469623565674\n",
            "Training Iteration 3299, Loss: 3.2218730449676514\n",
            "Training Iteration 3300, Loss: 3.750986337661743\n",
            "Training Iteration 3301, Loss: 6.845572471618652\n",
            "Training Iteration 3302, Loss: 4.768481731414795\n",
            "Training Iteration 3303, Loss: 6.462497711181641\n",
            "Training Iteration 3304, Loss: 6.304191589355469\n",
            "Training Iteration 3305, Loss: 7.454049110412598\n",
            "Training Iteration 3306, Loss: 3.8855419158935547\n",
            "Training Iteration 3307, Loss: 3.1328799724578857\n",
            "Training Iteration 3308, Loss: 8.003998756408691\n",
            "Training Iteration 3309, Loss: 7.772761344909668\n",
            "Training Iteration 3310, Loss: 7.0290045738220215\n",
            "Training Iteration 3311, Loss: 5.06510066986084\n",
            "Training Iteration 3312, Loss: 5.030745983123779\n",
            "Training Iteration 3313, Loss: 4.295146942138672\n",
            "Training Iteration 3314, Loss: 6.686931133270264\n",
            "Training Iteration 3315, Loss: 9.29423713684082\n",
            "Training Iteration 3316, Loss: 5.5147271156311035\n",
            "Training Iteration 3317, Loss: 3.517940044403076\n",
            "Training Iteration 3318, Loss: 7.1907958984375\n",
            "Training Iteration 3319, Loss: 3.975454568862915\n",
            "Training Iteration 3320, Loss: 4.344263553619385\n",
            "Training Iteration 3321, Loss: 6.816577911376953\n",
            "Training Iteration 3322, Loss: 5.9106669425964355\n",
            "Training Iteration 3323, Loss: 4.3854570388793945\n",
            "Training Iteration 3324, Loss: 10.86246109008789\n",
            "Training Iteration 3325, Loss: 3.5247700214385986\n",
            "Training Iteration 3326, Loss: 6.297267913818359\n",
            "Training Iteration 3327, Loss: 2.6443088054656982\n",
            "Training Iteration 3328, Loss: 3.906865119934082\n",
            "Training Iteration 3329, Loss: 5.278145790100098\n",
            "Training Iteration 3330, Loss: 4.779324054718018\n",
            "Training Iteration 3331, Loss: 6.227730751037598\n",
            "Training Iteration 3332, Loss: 3.83174467086792\n",
            "Training Iteration 3333, Loss: 5.128042697906494\n",
            "Training Iteration 3334, Loss: 5.841860771179199\n",
            "Training Iteration 3335, Loss: 5.496638298034668\n",
            "Training Iteration 3336, Loss: 4.6164398193359375\n",
            "Training Iteration 3337, Loss: 4.384618282318115\n",
            "Training Iteration 3338, Loss: 4.726100921630859\n",
            "Training Iteration 3339, Loss: 7.6091814041137695\n",
            "Training Iteration 3340, Loss: 4.520934104919434\n",
            "Training Iteration 3341, Loss: 3.000387191772461\n",
            "Training Iteration 3342, Loss: 6.665285587310791\n",
            "Training Iteration 3343, Loss: 4.891074180603027\n",
            "Training Iteration 3344, Loss: 3.86903715133667\n",
            "Training Iteration 3345, Loss: 5.709572792053223\n",
            "Training Iteration 3346, Loss: 4.7884392738342285\n",
            "Training Iteration 3347, Loss: 6.148159503936768\n",
            "Training Iteration 3348, Loss: 5.773865699768066\n",
            "Training Iteration 3349, Loss: 2.827087640762329\n",
            "Training Iteration 3350, Loss: 2.3560256958007812\n",
            "Training Iteration 3351, Loss: 5.970530986785889\n",
            "Training Iteration 3352, Loss: 3.591407537460327\n",
            "Training Iteration 3353, Loss: 8.290898323059082\n",
            "Training Iteration 3354, Loss: 5.195492744445801\n",
            "Training Iteration 3355, Loss: 4.075003623962402\n",
            "Training Iteration 3356, Loss: 2.483930826187134\n",
            "Training Iteration 3357, Loss: 4.989433288574219\n",
            "Training Iteration 3358, Loss: 5.333540439605713\n",
            "Training Iteration 3359, Loss: 2.295180082321167\n",
            "Training Iteration 3360, Loss: 3.2899396419525146\n",
            "Training Iteration 3361, Loss: 3.894730567932129\n",
            "Training Iteration 3362, Loss: 3.8102822303771973\n",
            "Training Iteration 3363, Loss: 4.163976192474365\n",
            "Training Iteration 3364, Loss: 3.417268753051758\n",
            "Training Iteration 3365, Loss: 4.3778533935546875\n",
            "Training Iteration 3366, Loss: 5.742610454559326\n",
            "Training Iteration 3367, Loss: 4.739620208740234\n",
            "Training Iteration 3368, Loss: 7.146923542022705\n",
            "Training Iteration 3369, Loss: 3.2543559074401855\n",
            "Training Iteration 3370, Loss: 4.735304832458496\n",
            "Training Iteration 3371, Loss: 5.148671627044678\n",
            "Training Iteration 3372, Loss: 6.208059310913086\n",
            "Training Iteration 3373, Loss: 6.657399654388428\n",
            "Training Iteration 3374, Loss: 2.4414188861846924\n",
            "Training Iteration 3375, Loss: 3.4057741165161133\n",
            "Training Iteration 3376, Loss: 1.5372776985168457\n",
            "Training Iteration 3377, Loss: 6.323390960693359\n",
            "Training Iteration 3378, Loss: 5.290637016296387\n",
            "Training Iteration 3379, Loss: 6.278226375579834\n",
            "Training Iteration 3380, Loss: 6.20036506652832\n",
            "Training Iteration 3381, Loss: 9.193185806274414\n",
            "Training Iteration 3382, Loss: 4.837850093841553\n",
            "Training Iteration 3383, Loss: 3.429811954498291\n",
            "Training Iteration 3384, Loss: 4.854207992553711\n",
            "Training Iteration 3385, Loss: 5.207467079162598\n",
            "Training Iteration 3386, Loss: 4.817271709442139\n",
            "Training Iteration 3387, Loss: 4.293593406677246\n",
            "Training Iteration 3388, Loss: 9.940309524536133\n",
            "Training Iteration 3389, Loss: 9.366045951843262\n",
            "Training Iteration 3390, Loss: 4.841386795043945\n",
            "Training Iteration 3391, Loss: 6.479865074157715\n",
            "Training Iteration 3392, Loss: 6.351022243499756\n",
            "Training Iteration 3393, Loss: 2.8839943408966064\n",
            "Training Iteration 3394, Loss: 2.1693387031555176\n",
            "Training Iteration 3395, Loss: 5.427569389343262\n",
            "Training Iteration 3396, Loss: 6.553508758544922\n",
            "Training Iteration 3397, Loss: 3.0804812908172607\n",
            "Training Iteration 3398, Loss: 2.6754705905914307\n",
            "Training Iteration 3399, Loss: 6.026944637298584\n",
            "Training Iteration 3400, Loss: 3.9395110607147217\n",
            "Training Iteration 3401, Loss: 3.0879323482513428\n",
            "Training Iteration 3402, Loss: 4.722629547119141\n",
            "Training Iteration 3403, Loss: 2.7793221473693848\n",
            "Training Iteration 3404, Loss: 3.863598346710205\n",
            "Training Iteration 3405, Loss: 1.4195605516433716\n",
            "Training Iteration 3406, Loss: 6.206228733062744\n",
            "Training Iteration 3407, Loss: 2.5054144859313965\n",
            "Training Iteration 3408, Loss: 4.24317741394043\n",
            "Training Iteration 3409, Loss: 4.328038692474365\n",
            "Training Iteration 3410, Loss: 7.272339344024658\n",
            "Training Iteration 3411, Loss: 1.8546491861343384\n",
            "Training Iteration 3412, Loss: 2.9989006519317627\n",
            "Training Iteration 3413, Loss: 3.4895472526550293\n",
            "Training Iteration 3414, Loss: 3.766739845275879\n",
            "Training Iteration 3415, Loss: 1.4165394306182861\n",
            "Training Iteration 3416, Loss: 4.8053693771362305\n",
            "Training Iteration 3417, Loss: 3.3333990573883057\n",
            "Training Iteration 3418, Loss: 3.8943874835968018\n",
            "Training Iteration 3419, Loss: 5.9778218269348145\n",
            "Training Iteration 3420, Loss: 2.625502824783325\n",
            "Training Iteration 3421, Loss: 5.041590690612793\n",
            "Training Iteration 3422, Loss: 7.854959487915039\n",
            "Training Iteration 3423, Loss: 3.118272542953491\n",
            "Training Iteration 3424, Loss: 8.146267890930176\n",
            "Training Iteration 3425, Loss: 8.966950416564941\n",
            "Training Iteration 3426, Loss: 8.564184188842773\n",
            "Training Iteration 3427, Loss: 1.523224115371704\n",
            "Training Iteration 3428, Loss: 3.8830959796905518\n",
            "Training Iteration 3429, Loss: 5.726914882659912\n",
            "Training Iteration 3430, Loss: 3.3082053661346436\n",
            "Training Iteration 3431, Loss: 8.066064834594727\n",
            "Training Iteration 3432, Loss: 4.427338600158691\n",
            "Training Iteration 3433, Loss: 3.203538417816162\n",
            "Training Iteration 3434, Loss: 4.675882339477539\n",
            "Training Iteration 3435, Loss: 5.891663074493408\n",
            "Training Iteration 3436, Loss: 6.306484222412109\n",
            "Training Iteration 3437, Loss: 3.726994276046753\n",
            "Training Iteration 3438, Loss: 2.987408399581909\n",
            "Training Iteration 3439, Loss: 4.2938761711120605\n",
            "Training Iteration 3440, Loss: 4.9358229637146\n",
            "Training Iteration 3441, Loss: 3.121464967727661\n",
            "Training Iteration 3442, Loss: 6.325543403625488\n",
            "Training Iteration 3443, Loss: 7.822904109954834\n",
            "Training Iteration 3444, Loss: 3.41770076751709\n",
            "Training Iteration 3445, Loss: 8.46651840209961\n",
            "Training Iteration 3446, Loss: 5.097646713256836\n",
            "Training Iteration 3447, Loss: 5.016499042510986\n",
            "Training Iteration 3448, Loss: 4.754920959472656\n",
            "Training Iteration 3449, Loss: 6.417627334594727\n",
            "Training Iteration 3450, Loss: 4.930391311645508\n",
            "Training Iteration 3451, Loss: 3.059108257293701\n",
            "Training Iteration 3452, Loss: 2.968088150024414\n",
            "Training Iteration 3453, Loss: 3.7240450382232666\n",
            "Training Iteration 3454, Loss: 4.92930269241333\n",
            "Training Iteration 3455, Loss: 6.304667949676514\n",
            "Training Iteration 3456, Loss: 3.1027638912200928\n",
            "Training Iteration 3457, Loss: 6.9404296875\n",
            "Training Iteration 3458, Loss: 7.009685516357422\n",
            "Training Iteration 3459, Loss: 7.9303436279296875\n",
            "Training Iteration 3460, Loss: 4.393977165222168\n",
            "Training Iteration 3461, Loss: 3.372318983078003\n",
            "Training Iteration 3462, Loss: 4.350339889526367\n",
            "Training Iteration 3463, Loss: 3.8495254516601562\n",
            "Training Iteration 3464, Loss: 3.794005870819092\n",
            "Training Iteration 3465, Loss: 4.775242805480957\n",
            "Training Iteration 3466, Loss: 7.879350662231445\n",
            "Training Iteration 3467, Loss: 3.6722664833068848\n",
            "Training Iteration 3468, Loss: 4.5821638107299805\n",
            "Training Iteration 3469, Loss: 5.870973110198975\n",
            "Training Iteration 3470, Loss: 2.277864456176758\n",
            "Training Iteration 3471, Loss: 4.115405559539795\n",
            "Training Iteration 3472, Loss: 2.4322547912597656\n",
            "Training Iteration 3473, Loss: 8.798510551452637\n",
            "Training Iteration 3474, Loss: 8.349047660827637\n",
            "Training Iteration 3475, Loss: 5.898386478424072\n",
            "Training Iteration 3476, Loss: 3.2508578300476074\n",
            "Training Iteration 3477, Loss: 4.240094184875488\n",
            "Training Iteration 3478, Loss: 4.715632438659668\n",
            "Training Iteration 3479, Loss: 4.870174884796143\n",
            "Training Iteration 3480, Loss: 3.420758008956909\n",
            "Training Iteration 3481, Loss: 2.184680223464966\n",
            "Training Iteration 3482, Loss: 6.143687725067139\n",
            "Training Iteration 3483, Loss: 8.833436012268066\n",
            "Training Iteration 3484, Loss: 4.5358662605285645\n",
            "Training Iteration 3485, Loss: 4.9397735595703125\n",
            "Training Iteration 3486, Loss: 7.674879550933838\n",
            "Training Iteration 3487, Loss: 5.0118913650512695\n",
            "Training Iteration 3488, Loss: 3.3567392826080322\n",
            "Training Iteration 3489, Loss: 3.0002286434173584\n",
            "Training Iteration 3490, Loss: 5.91792106628418\n",
            "Training Iteration 3491, Loss: 6.205263614654541\n",
            "Training Iteration 3492, Loss: 4.01059627532959\n",
            "Training Iteration 3493, Loss: 3.023477792739868\n",
            "Training Iteration 3494, Loss: 8.800126075744629\n",
            "Training Iteration 3495, Loss: 1.755875825881958\n",
            "Training Iteration 3496, Loss: 5.828225135803223\n",
            "Training Iteration 3497, Loss: 2.5761780738830566\n",
            "Training Iteration 3498, Loss: 6.973224639892578\n",
            "Training Iteration 3499, Loss: 5.4575395584106445\n",
            "Training Iteration 3500, Loss: 2.370086908340454\n",
            "Training Iteration 3501, Loss: 5.261564254760742\n",
            "Training Iteration 3502, Loss: 5.357076644897461\n",
            "Training Iteration 3503, Loss: 6.621430397033691\n",
            "Training Iteration 3504, Loss: 5.958929061889648\n",
            "Training Iteration 3505, Loss: 2.5682873725891113\n",
            "Training Iteration 3506, Loss: 2.72835111618042\n",
            "Training Iteration 3507, Loss: 7.802626132965088\n",
            "Training Iteration 3508, Loss: 4.299829483032227\n",
            "Training Iteration 3509, Loss: 6.500222206115723\n",
            "Training Iteration 3510, Loss: 5.706934452056885\n",
            "Training Iteration 3511, Loss: 5.829067230224609\n",
            "Training Iteration 3512, Loss: 9.305484771728516\n",
            "Training Iteration 3513, Loss: 4.955739498138428\n",
            "Training Iteration 3514, Loss: 5.900120258331299\n",
            "Training Iteration 3515, Loss: 4.012131690979004\n",
            "Training Iteration 3516, Loss: 6.860133171081543\n",
            "Training Iteration 3517, Loss: 5.885546684265137\n",
            "Training Iteration 3518, Loss: 7.489566802978516\n",
            "Training Iteration 3519, Loss: 6.729555606842041\n",
            "Training Iteration 3520, Loss: 3.8233752250671387\n",
            "Training Iteration 3521, Loss: 2.8979361057281494\n",
            "Training Iteration 3522, Loss: 5.5727105140686035\n",
            "Training Iteration 3523, Loss: 6.071659564971924\n",
            "Training Iteration 3524, Loss: 2.800419807434082\n",
            "Training Iteration 3525, Loss: 3.4548068046569824\n",
            "Training Iteration 3526, Loss: 6.643265247344971\n",
            "Training Iteration 3527, Loss: 7.393365859985352\n",
            "Training Iteration 3528, Loss: 3.1778340339660645\n",
            "Training Iteration 3529, Loss: 4.718027114868164\n",
            "Training Iteration 3530, Loss: 2.3226633071899414\n",
            "Training Iteration 3531, Loss: 6.035058975219727\n",
            "Training Iteration 3532, Loss: 3.301018238067627\n",
            "Training Iteration 3533, Loss: 8.422195434570312\n",
            "Training Iteration 3534, Loss: 2.40057373046875\n",
            "Training Iteration 3535, Loss: 4.190892219543457\n",
            "Training Iteration 3536, Loss: 3.382157325744629\n",
            "Training Iteration 3537, Loss: 6.062861442565918\n",
            "Training Iteration 3538, Loss: 3.57991886138916\n",
            "Training Iteration 3539, Loss: 4.078769207000732\n",
            "Training Iteration 3540, Loss: 4.170793533325195\n",
            "Training Iteration 3541, Loss: 5.412276268005371\n",
            "Training Iteration 3542, Loss: 4.689351558685303\n",
            "Training Iteration 3543, Loss: 5.866302967071533\n",
            "Training Iteration 3544, Loss: 2.6842875480651855\n",
            "Training Iteration 3545, Loss: 4.67364501953125\n",
            "Training Iteration 3546, Loss: 4.428671836853027\n",
            "Training Iteration 3547, Loss: 4.185689449310303\n",
            "Training Iteration 3548, Loss: 4.552876949310303\n",
            "Training Iteration 3549, Loss: 4.478263854980469\n",
            "Training Iteration 3550, Loss: 7.491276741027832\n",
            "Training Iteration 3551, Loss: 3.735546827316284\n",
            "Training Iteration 3552, Loss: 5.099759101867676\n",
            "Training Iteration 3553, Loss: 5.096614360809326\n",
            "Training Iteration 3554, Loss: 3.514634847640991\n",
            "Training Iteration 3555, Loss: 4.296684741973877\n",
            "Training Iteration 3556, Loss: 4.335910320281982\n",
            "Training Iteration 3557, Loss: 4.333744049072266\n",
            "Training Iteration 3558, Loss: 4.776471138000488\n",
            "Training Iteration 3559, Loss: 6.276210308074951\n",
            "Training Iteration 3560, Loss: 5.215910911560059\n",
            "Training Iteration 3561, Loss: 4.441995620727539\n",
            "Training Iteration 3562, Loss: 8.212221145629883\n",
            "Training Iteration 3563, Loss: 5.452414035797119\n",
            "Training Iteration 3564, Loss: 5.029328346252441\n",
            "Training Iteration 3565, Loss: 4.98671293258667\n",
            "Training Iteration 3566, Loss: 3.464585304260254\n",
            "Training Iteration 3567, Loss: 3.6012256145477295\n",
            "Training Iteration 3568, Loss: 5.896522521972656\n",
            "Training Iteration 3569, Loss: 4.513088226318359\n",
            "Training Iteration 3570, Loss: 4.573884963989258\n",
            "Training Iteration 3571, Loss: 6.520881652832031\n",
            "Training Iteration 3572, Loss: 4.797487735748291\n",
            "Training Iteration 3573, Loss: 3.94377064704895\n",
            "Training Iteration 3574, Loss: 4.36415433883667\n",
            "Training Iteration 3575, Loss: 3.8738303184509277\n",
            "Training Iteration 3576, Loss: 5.703047752380371\n",
            "Training Iteration 3577, Loss: 6.772068023681641\n",
            "Training Iteration 3578, Loss: 8.734460830688477\n",
            "Training Iteration 3579, Loss: 4.494002342224121\n",
            "Training Iteration 3580, Loss: 5.693447113037109\n",
            "Training Iteration 3581, Loss: 3.8552446365356445\n",
            "Training Iteration 3582, Loss: 2.1613516807556152\n",
            "Training Iteration 3583, Loss: 2.892956018447876\n",
            "Training Iteration 3584, Loss: 3.266418933868408\n",
            "Training Iteration 3585, Loss: 4.545941352844238\n",
            "Training Iteration 3586, Loss: 3.2896664142608643\n",
            "Training Iteration 3587, Loss: 1.3806110620498657\n",
            "Training Iteration 3588, Loss: 3.380687952041626\n",
            "Training Iteration 3589, Loss: 5.745153903961182\n",
            "Training Iteration 3590, Loss: 2.576730251312256\n",
            "Training Iteration 3591, Loss: 2.1656782627105713\n",
            "Training Iteration 3592, Loss: 5.000677585601807\n",
            "Training Iteration 3593, Loss: 4.1621174812316895\n",
            "Training Iteration 3594, Loss: 7.0514326095581055\n",
            "Training Iteration 3595, Loss: 4.031885147094727\n",
            "Training Iteration 3596, Loss: 4.8985772132873535\n",
            "Training Iteration 3597, Loss: 4.231988430023193\n",
            "Training Iteration 3598, Loss: 4.764469146728516\n",
            "Training Iteration 3599, Loss: 5.578875541687012\n",
            "Training Iteration 3600, Loss: 6.076814651489258\n",
            "Training Iteration 3601, Loss: 2.8715832233428955\n",
            "Training Iteration 3602, Loss: 3.652897596359253\n",
            "Training Iteration 3603, Loss: 3.2073099613189697\n",
            "Training Iteration 3604, Loss: 2.5159339904785156\n",
            "Training Iteration 3605, Loss: 3.866328001022339\n",
            "Training Iteration 3606, Loss: 5.756717681884766\n",
            "Training Iteration 3607, Loss: 3.4183287620544434\n",
            "Training Iteration 3608, Loss: 6.831545829772949\n",
            "Training Iteration 3609, Loss: 1.817659616470337\n",
            "Training Iteration 3610, Loss: 2.228982448577881\n",
            "Training Iteration 3611, Loss: 3.0930845737457275\n",
            "Training Iteration 3612, Loss: 3.6091275215148926\n",
            "Training Iteration 3613, Loss: 4.856354713439941\n",
            "Training Iteration 3614, Loss: 3.14827299118042\n",
            "Training Iteration 3615, Loss: 5.747930526733398\n",
            "Training Iteration 3616, Loss: 3.5769762992858887\n",
            "Training Iteration 3617, Loss: 5.050676345825195\n",
            "Training Iteration 3618, Loss: 2.9989025592803955\n",
            "Training Iteration 3619, Loss: 4.4276556968688965\n",
            "Training Iteration 3620, Loss: 2.23667573928833\n",
            "Training Iteration 3621, Loss: 4.587475776672363\n",
            "Training Iteration 3622, Loss: 4.449444770812988\n",
            "Training Iteration 3623, Loss: 3.1395423412323\n",
            "Training Iteration 3624, Loss: 8.109689712524414\n",
            "Training Iteration 3625, Loss: 5.811360836029053\n",
            "Training Iteration 3626, Loss: 5.17724609375\n",
            "Training Iteration 3627, Loss: 6.820721626281738\n",
            "Training Iteration 3628, Loss: 3.123819589614868\n",
            "Training Iteration 3629, Loss: 4.449761867523193\n",
            "Training Iteration 3630, Loss: 10.1456937789917\n",
            "Training Iteration 3631, Loss: 4.276024341583252\n",
            "Training Iteration 3632, Loss: 4.576038360595703\n",
            "Training Iteration 3633, Loss: 3.2676172256469727\n",
            "Training Iteration 3634, Loss: 4.1856369972229\n",
            "Training Iteration 3635, Loss: 5.822904586791992\n",
            "Training Iteration 3636, Loss: 4.8361711502075195\n",
            "Training Iteration 3637, Loss: 5.103349208831787\n",
            "Training Iteration 3638, Loss: 2.589156150817871\n",
            "Training Iteration 3639, Loss: 5.810481071472168\n",
            "Training Iteration 3640, Loss: 5.552468776702881\n",
            "Training Iteration 3641, Loss: 6.054099082946777\n",
            "Training Iteration 3642, Loss: 3.190920829772949\n",
            "Training Iteration 3643, Loss: 4.139654636383057\n",
            "Training Iteration 3644, Loss: 6.728982925415039\n",
            "Training Iteration 3645, Loss: 8.063872337341309\n",
            "Training Iteration 3646, Loss: 9.118293762207031\n",
            "Training Iteration 3647, Loss: 5.244226455688477\n",
            "Training Iteration 3648, Loss: 7.355097770690918\n",
            "Training Iteration 3649, Loss: 5.016726493835449\n",
            "Training Iteration 3650, Loss: 2.6588430404663086\n",
            "Training Iteration 3651, Loss: 3.8945159912109375\n",
            "Training Iteration 3652, Loss: 2.436178684234619\n",
            "Training Iteration 3653, Loss: 2.417088270187378\n",
            "Training Iteration 3654, Loss: 4.8823041915893555\n",
            "Training Iteration 3655, Loss: 4.360284328460693\n",
            "Training Iteration 3656, Loss: 4.784024238586426\n",
            "Training Iteration 3657, Loss: 2.1196179389953613\n",
            "Training Iteration 3658, Loss: 5.6293182373046875\n",
            "Training Iteration 3659, Loss: 2.328922986984253\n",
            "Training Iteration 3660, Loss: 5.916647434234619\n",
            "Training Iteration 3661, Loss: 3.475700855255127\n",
            "Training Iteration 3662, Loss: 5.528408050537109\n",
            "Training Iteration 3663, Loss: 9.966509819030762\n",
            "Training Iteration 3664, Loss: 4.833896160125732\n",
            "Training Iteration 3665, Loss: 7.140880107879639\n",
            "Training Iteration 3666, Loss: 7.989218711853027\n",
            "Training Iteration 3667, Loss: 4.504926681518555\n",
            "Training Iteration 3668, Loss: 3.1236391067504883\n",
            "Training Iteration 3669, Loss: 2.572183847427368\n",
            "Training Iteration 3670, Loss: 6.398317337036133\n",
            "Training Iteration 3671, Loss: 6.114376068115234\n",
            "Training Iteration 3672, Loss: 3.9236156940460205\n",
            "Training Iteration 3673, Loss: 3.600381374359131\n",
            "Training Iteration 3674, Loss: 3.516336441040039\n",
            "Training Iteration 3675, Loss: 2.373511552810669\n",
            "Training Iteration 3676, Loss: 3.603210687637329\n",
            "Training Iteration 3677, Loss: 4.927684307098389\n",
            "Training Iteration 3678, Loss: 4.3570556640625\n",
            "Training Iteration 3679, Loss: 7.508408546447754\n",
            "Training Iteration 3680, Loss: 6.078952312469482\n",
            "Training Iteration 3681, Loss: 2.701964855194092\n",
            "Training Iteration 3682, Loss: 3.314854383468628\n",
            "Training Iteration 3683, Loss: 1.4220154285430908\n",
            "Training Iteration 3684, Loss: 2.7078909873962402\n",
            "Training Iteration 3685, Loss: 6.756503582000732\n",
            "Training Iteration 3686, Loss: 3.613741397857666\n",
            "Training Iteration 3687, Loss: 7.00822639465332\n",
            "Training Iteration 3688, Loss: 7.440793514251709\n",
            "Training Iteration 3689, Loss: 4.059747695922852\n",
            "Training Iteration 3690, Loss: 3.1251163482666016\n",
            "Training Iteration 3691, Loss: 2.973888397216797\n",
            "Training Iteration 3692, Loss: 1.7961091995239258\n",
            "Training Iteration 3693, Loss: 6.127801895141602\n",
            "Training Iteration 3694, Loss: 5.568971157073975\n",
            "Training Iteration 3695, Loss: 4.55380392074585\n",
            "Training Iteration 3696, Loss: 2.2780284881591797\n",
            "Training Iteration 3697, Loss: 3.963397979736328\n",
            "Training Iteration 3698, Loss: 4.554793834686279\n",
            "Training Iteration 3699, Loss: 4.703192710876465\n",
            "Training Iteration 3700, Loss: 2.4824328422546387\n",
            "Training Iteration 3701, Loss: 3.0735907554626465\n",
            "Training Iteration 3702, Loss: 3.134401798248291\n",
            "Training Iteration 3703, Loss: 5.467507839202881\n",
            "Training Iteration 3704, Loss: 4.530277252197266\n",
            "Training Iteration 3705, Loss: 5.234847068786621\n",
            "Training Iteration 3706, Loss: 5.715404987335205\n",
            "Training Iteration 3707, Loss: 6.273508548736572\n",
            "Training Iteration 3708, Loss: 4.079732418060303\n",
            "Training Iteration 3709, Loss: 4.496069431304932\n",
            "Training Iteration 3710, Loss: 3.548328161239624\n",
            "Training Iteration 3711, Loss: 5.757455825805664\n",
            "Training Iteration 3712, Loss: 4.00968074798584\n",
            "Training Iteration 3713, Loss: 3.989217758178711\n",
            "Training Iteration 3714, Loss: 3.4504175186157227\n",
            "Training Iteration 3715, Loss: 5.84600830078125\n",
            "Training Iteration 3716, Loss: 5.654501914978027\n",
            "Training Iteration 3717, Loss: 3.631302833557129\n",
            "Training Iteration 3718, Loss: 5.417508125305176\n",
            "Training Iteration 3719, Loss: 3.6808419227600098\n",
            "Training Iteration 3720, Loss: 3.2997865676879883\n",
            "Training Iteration 3721, Loss: 2.35396671295166\n",
            "Training Iteration 3722, Loss: 3.821174383163452\n",
            "Training Iteration 3723, Loss: 5.483753681182861\n",
            "Training Iteration 3724, Loss: 3.517975330352783\n",
            "Training Iteration 3725, Loss: 3.0113658905029297\n",
            "Training Iteration 3726, Loss: 1.7092607021331787\n",
            "Training Iteration 3727, Loss: 4.739098072052002\n",
            "Training Iteration 3728, Loss: 4.09403657913208\n",
            "Training Iteration 3729, Loss: 2.441028356552124\n",
            "Training Iteration 3730, Loss: 3.696756362915039\n",
            "Training Iteration 3731, Loss: 3.629833698272705\n",
            "Training Iteration 3732, Loss: 6.478151321411133\n",
            "Training Iteration 3733, Loss: 4.477811336517334\n",
            "Training Iteration 3734, Loss: 2.7033681869506836\n",
            "Training Iteration 3735, Loss: 4.332981586456299\n",
            "Training Iteration 3736, Loss: 7.067537784576416\n",
            "Training Iteration 3737, Loss: 6.297487735748291\n",
            "Training Iteration 3738, Loss: 7.765150547027588\n",
            "Training Iteration 3739, Loss: 6.986804485321045\n",
            "Training Iteration 3740, Loss: 3.063180923461914\n",
            "Training Iteration 3741, Loss: 2.1402552127838135\n",
            "Training Iteration 3742, Loss: 3.278705596923828\n",
            "Training Iteration 3743, Loss: 5.376025199890137\n",
            "Training Iteration 3744, Loss: 2.7251265048980713\n",
            "Training Iteration 3745, Loss: 5.433197975158691\n",
            "Training Iteration 3746, Loss: 2.7525320053100586\n",
            "Training Iteration 3747, Loss: 3.2778093814849854\n",
            "Training Iteration 3748, Loss: 5.699422836303711\n",
            "Training Iteration 3749, Loss: 5.147393226623535\n",
            "Training Iteration 3750, Loss: 4.348799228668213\n",
            "Training Iteration 3751, Loss: 4.905454158782959\n",
            "Training Iteration 3752, Loss: 5.747437000274658\n",
            "Training Iteration 3753, Loss: 5.844614505767822\n",
            "Training Iteration 3754, Loss: 4.484019756317139\n",
            "Training Iteration 3755, Loss: 2.8613362312316895\n",
            "Training Iteration 3756, Loss: 1.7883820533752441\n",
            "Training Iteration 3757, Loss: 5.295887470245361\n",
            "Training Iteration 3758, Loss: 3.972613573074341\n",
            "Training Iteration 3759, Loss: 2.675804376602173\n",
            "Training Iteration 3760, Loss: 3.4730474948883057\n",
            "Training Iteration 3761, Loss: 3.758225440979004\n",
            "Training Iteration 3762, Loss: 5.299003601074219\n",
            "Training Iteration 3763, Loss: 5.2786970138549805\n",
            "Training Iteration 3764, Loss: 1.7491999864578247\n",
            "Training Iteration 3765, Loss: 3.810382843017578\n",
            "Training Iteration 3766, Loss: 2.9778757095336914\n",
            "Training Iteration 3767, Loss: 3.9403698444366455\n",
            "Training Iteration 3768, Loss: 1.4593114852905273\n",
            "Training Iteration 3769, Loss: 3.9261724948883057\n",
            "Training Iteration 3770, Loss: 1.8151044845581055\n",
            "Training Iteration 3771, Loss: 3.423551082611084\n",
            "Training Iteration 3772, Loss: 4.510210990905762\n",
            "Training Iteration 3773, Loss: 3.530531406402588\n",
            "Training Iteration 3774, Loss: 1.7964028120040894\n",
            "Training Iteration 3775, Loss: 6.91835880279541\n",
            "Training Iteration 3776, Loss: 3.893576145172119\n",
            "Training Iteration 3777, Loss: 4.372767448425293\n",
            "Training Iteration 3778, Loss: 3.3814847469329834\n",
            "Training Iteration 3779, Loss: 3.581023931503296\n",
            "Training Iteration 3780, Loss: 5.0916314125061035\n",
            "Training Iteration 3781, Loss: 3.869582176208496\n",
            "Training Iteration 3782, Loss: 5.761709690093994\n",
            "Training Iteration 3783, Loss: 3.2641761302948\n",
            "Training Iteration 3784, Loss: 3.434674024581909\n",
            "Training Iteration 3785, Loss: 6.256565570831299\n",
            "Training Iteration 3786, Loss: 6.329816818237305\n",
            "Training Iteration 3787, Loss: 4.3570556640625\n",
            "Training Iteration 3788, Loss: 3.7338171005249023\n",
            "Training Iteration 3789, Loss: 5.205204963684082\n",
            "Training Iteration 3790, Loss: 3.283207416534424\n",
            "Training Iteration 3791, Loss: 4.8220295906066895\n",
            "Training Iteration 3792, Loss: 6.471199035644531\n",
            "Training Iteration 3793, Loss: 3.2724671363830566\n",
            "Training Iteration 3794, Loss: 4.965574741363525\n",
            "Training Iteration 3795, Loss: 4.570432662963867\n",
            "Training Iteration 3796, Loss: 2.8363757133483887\n",
            "Training Iteration 3797, Loss: 2.332563877105713\n",
            "Training Iteration 3798, Loss: 7.343406677246094\n",
            "Training Iteration 3799, Loss: 3.4795944690704346\n",
            "Training Iteration 3800, Loss: 6.446672439575195\n",
            "Training Iteration 3801, Loss: 3.9636144638061523\n",
            "Training Iteration 3802, Loss: 4.811270713806152\n",
            "Training Iteration 3803, Loss: 3.855682611465454\n",
            "Training Iteration 3804, Loss: 2.8899924755096436\n",
            "Training Iteration 3805, Loss: 2.268108606338501\n",
            "Training Iteration 3806, Loss: 3.417900562286377\n",
            "Training Iteration 3807, Loss: 6.328886985778809\n",
            "Training Iteration 3808, Loss: 5.704272270202637\n",
            "Training Iteration 3809, Loss: 3.250091791152954\n",
            "Training Iteration 3810, Loss: 4.384942531585693\n",
            "Training Iteration 3811, Loss: 4.673701763153076\n",
            "Training Iteration 3812, Loss: 3.878309726715088\n",
            "Training Iteration 3813, Loss: 2.7721943855285645\n",
            "Training Iteration 3814, Loss: 5.089414596557617\n",
            "Training Iteration 3815, Loss: 4.843488693237305\n",
            "Training Iteration 3816, Loss: 6.431576728820801\n",
            "Training Iteration 3817, Loss: 8.003710746765137\n",
            "Training Iteration 3818, Loss: 6.8389410972595215\n",
            "Training Iteration 3819, Loss: 5.438733100891113\n",
            "Training Iteration 3820, Loss: 5.281559944152832\n",
            "Training Iteration 3821, Loss: 3.7376866340637207\n",
            "Training Iteration 3822, Loss: 5.899046897888184\n",
            "Training Iteration 3823, Loss: 4.665766716003418\n",
            "Training Iteration 3824, Loss: 6.710020542144775\n",
            "Training Iteration 3825, Loss: 4.554624080657959\n",
            "Training Iteration 3826, Loss: 3.8335976600646973\n",
            "Training Iteration 3827, Loss: 6.245151519775391\n",
            "Training Iteration 3828, Loss: 4.673032283782959\n",
            "Training Iteration 3829, Loss: 5.441390037536621\n",
            "Training Iteration 3830, Loss: 3.3522753715515137\n",
            "Training Iteration 3831, Loss: 4.346189022064209\n",
            "Training Iteration 3832, Loss: 2.3594577312469482\n",
            "Training Iteration 3833, Loss: 3.721431255340576\n",
            "Training Iteration 3834, Loss: 8.439956665039062\n",
            "Training Iteration 3835, Loss: 7.194107532501221\n",
            "Training Iteration 3836, Loss: 2.749696731567383\n",
            "Training Iteration 3837, Loss: 4.1260809898376465\n",
            "Training Iteration 3838, Loss: 3.310972213745117\n",
            "Training Iteration 3839, Loss: 8.843221664428711\n",
            "Training Iteration 3840, Loss: 6.504688262939453\n",
            "Training Iteration 3841, Loss: 5.378801345825195\n",
            "Training Iteration 3842, Loss: 4.514401435852051\n",
            "Training Iteration 3843, Loss: 8.542569160461426\n",
            "Training Iteration 3844, Loss: 8.895063400268555\n",
            "Training Iteration 3845, Loss: 1.73744797706604\n",
            "Training Iteration 3846, Loss: 3.924018621444702\n",
            "Training Iteration 3847, Loss: 4.115580081939697\n",
            "Training Iteration 3848, Loss: 4.55135440826416\n",
            "Training Iteration 3849, Loss: 6.2289628982543945\n",
            "Training Iteration 3850, Loss: 5.272660255432129\n",
            "Training Iteration 3851, Loss: 4.764401435852051\n",
            "Training Iteration 3852, Loss: 3.3205206394195557\n",
            "Training Iteration 3853, Loss: 1.744912028312683\n",
            "Training Iteration 3854, Loss: 3.91542649269104\n",
            "Training Iteration 3855, Loss: 4.021557807922363\n",
            "Training Iteration 3856, Loss: 3.696697235107422\n",
            "Training Iteration 3857, Loss: 10.063902854919434\n",
            "Training Iteration 3858, Loss: 4.822561740875244\n",
            "Training Iteration 3859, Loss: 0.7895711660385132\n",
            "Training Iteration 3860, Loss: 6.963569641113281\n",
            "Training Iteration 3861, Loss: 7.499454975128174\n",
            "Training Iteration 3862, Loss: 6.618283271789551\n",
            "Training Iteration 3863, Loss: 2.6832003593444824\n",
            "Training Iteration 3864, Loss: 5.080599784851074\n",
            "Training Iteration 3865, Loss: 5.074128150939941\n",
            "Training Iteration 3866, Loss: 5.538543701171875\n",
            "Training Iteration 3867, Loss: 4.316856384277344\n",
            "Training Iteration 3868, Loss: 3.624572992324829\n",
            "Training Iteration 3869, Loss: 4.117974281311035\n",
            "Training Iteration 3870, Loss: 2.718755006790161\n",
            "Training Iteration 3871, Loss: 3.7778611183166504\n",
            "Training Iteration 3872, Loss: 6.344557285308838\n",
            "Training Iteration 3873, Loss: 4.505324840545654\n",
            "Training Iteration 3874, Loss: 6.021482944488525\n",
            "Training Iteration 3875, Loss: 4.852701663970947\n",
            "Training Iteration 3876, Loss: 2.7433505058288574\n",
            "Training Iteration 3877, Loss: 7.4797587394714355\n",
            "Training Iteration 3878, Loss: 3.8645124435424805\n",
            "Training Iteration 3879, Loss: 4.068466663360596\n",
            "Training Iteration 3880, Loss: 3.372640609741211\n",
            "Training Iteration 3881, Loss: 2.713427782058716\n",
            "Training Iteration 3882, Loss: 1.892153024673462\n",
            "Training Iteration 3883, Loss: 4.703621864318848\n",
            "Training Iteration 3884, Loss: 7.59937858581543\n",
            "Training Iteration 3885, Loss: 5.1579790115356445\n",
            "Training Iteration 3886, Loss: 3.2779905796051025\n",
            "Training Iteration 3887, Loss: 4.912414073944092\n",
            "Training Iteration 3888, Loss: 5.006891250610352\n",
            "Training Iteration 3889, Loss: 5.610599994659424\n",
            "Training Iteration 3890, Loss: 9.33391284942627\n",
            "Training Iteration 3891, Loss: 5.98502779006958\n",
            "Training Iteration 3892, Loss: 3.552041530609131\n",
            "Training Iteration 3893, Loss: 4.988995552062988\n",
            "Training Iteration 3894, Loss: 6.7844953536987305\n",
            "Training Iteration 3895, Loss: 2.2384634017944336\n",
            "Training Iteration 3896, Loss: 2.0078654289245605\n",
            "Training Iteration 3897, Loss: 5.7137932777404785\n",
            "Training Iteration 3898, Loss: 2.891650438308716\n",
            "Training Iteration 3899, Loss: 4.08896017074585\n",
            "Training Iteration 3900, Loss: 5.244868278503418\n",
            "Training Iteration 3901, Loss: 3.217327356338501\n",
            "Training Iteration 3902, Loss: 2.763284921646118\n",
            "Training Iteration 3903, Loss: 3.304764747619629\n",
            "Training Iteration 3904, Loss: 3.9775936603546143\n",
            "Training Iteration 3905, Loss: 2.7416653633117676\n",
            "Training Iteration 3906, Loss: 3.8832385540008545\n",
            "Training Iteration 3907, Loss: 3.875392436981201\n",
            "Training Iteration 3908, Loss: 5.724370002746582\n",
            "Training Iteration 3909, Loss: 3.2942464351654053\n",
            "Training Iteration 3910, Loss: 4.395514011383057\n",
            "Training Iteration 3911, Loss: 4.683257579803467\n",
            "Training Iteration 3912, Loss: 5.092838764190674\n",
            "Training Iteration 3913, Loss: 3.8725967407226562\n",
            "Training Iteration 3914, Loss: 5.6485137939453125\n",
            "Training Iteration 3915, Loss: 2.0389389991760254\n",
            "Training Iteration 3916, Loss: 4.80951452255249\n",
            "Training Iteration 3917, Loss: 6.277681827545166\n",
            "Training Iteration 3918, Loss: 5.309816360473633\n",
            "Training Iteration 3919, Loss: 2.9022715091705322\n",
            "Training Iteration 3920, Loss: 4.998648166656494\n",
            "Training Iteration 3921, Loss: 4.9110307693481445\n",
            "Training Iteration 3922, Loss: 3.146049976348877\n",
            "Training Iteration 3923, Loss: 5.678352355957031\n",
            "Training Iteration 3924, Loss: 5.460616111755371\n",
            "Training Iteration 3925, Loss: 3.2506489753723145\n",
            "Training Iteration 3926, Loss: 7.135690689086914\n",
            "Training Iteration 3927, Loss: 6.722716808319092\n",
            "Training Iteration 3928, Loss: 6.523040771484375\n",
            "Training Iteration 3929, Loss: 3.6770975589752197\n",
            "Training Iteration 3930, Loss: 5.003320217132568\n",
            "Training Iteration 3931, Loss: 5.333498477935791\n",
            "Training Iteration 3932, Loss: 1.805952548980713\n",
            "Training Iteration 3933, Loss: 4.3557448387146\n",
            "Training Iteration 3934, Loss: 4.586757183074951\n",
            "Training Iteration 3935, Loss: 3.5815248489379883\n",
            "Training Iteration 3936, Loss: 3.5844900608062744\n",
            "Training Iteration 3937, Loss: 3.083543062210083\n",
            "Training Iteration 3938, Loss: 4.318358421325684\n",
            "Training Iteration 3939, Loss: 3.1492908000946045\n",
            "Training Iteration 3940, Loss: 2.7890427112579346\n",
            "Training Iteration 3941, Loss: 4.72831916809082\n",
            "Training Iteration 3942, Loss: 3.2083096504211426\n",
            "Training Iteration 3943, Loss: 4.972043037414551\n",
            "Training Iteration 3944, Loss: 1.677720546722412\n",
            "Training Iteration 3945, Loss: 3.3014540672302246\n",
            "Training Iteration 3946, Loss: 4.047550201416016\n",
            "Training Iteration 3947, Loss: 5.517562389373779\n",
            "Training Iteration 3948, Loss: 2.4223012924194336\n",
            "Training Iteration 3949, Loss: 4.9412150382995605\n",
            "Training Iteration 3950, Loss: 4.464051246643066\n",
            "Training Iteration 3951, Loss: 4.25777530670166\n",
            "Training Iteration 3952, Loss: 3.6227736473083496\n",
            "Training Iteration 3953, Loss: 4.016790390014648\n",
            "Training Iteration 3954, Loss: 4.271554470062256\n",
            "Training Iteration 3955, Loss: 4.5533447265625\n",
            "Training Iteration 3956, Loss: 3.85331654548645\n",
            "Training Iteration 3957, Loss: 4.687438488006592\n",
            "Training Iteration 3958, Loss: 1.8174375295639038\n",
            "Training Iteration 3959, Loss: 4.874881744384766\n",
            "Training Iteration 3960, Loss: 4.331188678741455\n",
            "Training Iteration 3961, Loss: 3.9117345809936523\n",
            "Training Iteration 3962, Loss: 3.2453551292419434\n",
            "Training Iteration 3963, Loss: 3.9935781955718994\n",
            "Training Iteration 3964, Loss: 2.488698959350586\n",
            "Training Iteration 3965, Loss: 2.670151472091675\n",
            "Training Iteration 3966, Loss: 5.3041486740112305\n",
            "Training Iteration 3967, Loss: 3.6090450286865234\n",
            "Training Iteration 3968, Loss: 2.739187717437744\n",
            "Training Iteration 3969, Loss: 5.0944414138793945\n",
            "Training Iteration 3970, Loss: 2.2556545734405518\n",
            "Training Iteration 3971, Loss: 5.930793285369873\n",
            "Training Iteration 3972, Loss: 4.840467929840088\n",
            "Training Iteration 3973, Loss: 5.370536804199219\n",
            "Training Iteration 3974, Loss: 8.058075904846191\n",
            "Training Iteration 3975, Loss: 3.096381425857544\n",
            "Training Iteration 3976, Loss: 3.515070676803589\n",
            "Training Iteration 3977, Loss: 3.8030052185058594\n",
            "Training Iteration 3978, Loss: 4.4341559410095215\n",
            "Training Iteration 3979, Loss: 5.418667793273926\n",
            "Training Iteration 3980, Loss: 5.537456035614014\n",
            "Training Iteration 3981, Loss: 2.849358081817627\n",
            "Training Iteration 3982, Loss: 3.151482343673706\n",
            "Training Iteration 3983, Loss: 4.826454162597656\n",
            "Training Iteration 3984, Loss: 4.347673416137695\n",
            "Training Iteration 3985, Loss: 0.682075560092926\n",
            "Training Iteration 3986, Loss: 3.05246901512146\n",
            "Training Iteration 3987, Loss: 5.138735771179199\n",
            "Training Iteration 3988, Loss: 4.906903266906738\n",
            "Training Iteration 3989, Loss: 8.803353309631348\n",
            "Training Iteration 3990, Loss: 4.300840854644775\n",
            "Training Iteration 3991, Loss: 3.968691110610962\n",
            "Training Iteration 3992, Loss: 1.9803560972213745\n",
            "Training Iteration 3993, Loss: 4.972335338592529\n",
            "Training Iteration 3994, Loss: 3.9804630279541016\n",
            "Training Iteration 3995, Loss: 5.9857916831970215\n",
            "Training Iteration 3996, Loss: 6.362490177154541\n",
            "Training Iteration 3997, Loss: 2.3379316329956055\n",
            "Training Iteration 3998, Loss: 4.649622917175293\n",
            "Training Iteration 3999, Loss: 2.0358963012695312\n",
            "Training Iteration 4000, Loss: 8.525374412536621\n",
            "Training Iteration 4001, Loss: 7.244874477386475\n",
            "Training Iteration 4002, Loss: 4.622894763946533\n",
            "Training Iteration 4003, Loss: 4.339047431945801\n",
            "Training Iteration 4004, Loss: 6.068638324737549\n",
            "Training Iteration 4005, Loss: 4.126927852630615\n",
            "Training Iteration 4006, Loss: 2.689311981201172\n",
            "Training Iteration 4007, Loss: 5.099510192871094\n",
            "Training Iteration 4008, Loss: 4.393970012664795\n",
            "Training Iteration 4009, Loss: 5.832884788513184\n",
            "Training Iteration 4010, Loss: 8.285584449768066\n",
            "Training Iteration 4011, Loss: 6.5910325050354\n",
            "Training Iteration 4012, Loss: 2.537015438079834\n",
            "Training Iteration 4013, Loss: 7.550235748291016\n",
            "Training Iteration 4014, Loss: 6.185165882110596\n",
            "Training Iteration 4015, Loss: 4.0372395515441895\n",
            "Training Iteration 4016, Loss: 7.353180885314941\n",
            "Training Iteration 4017, Loss: 8.81897258758545\n",
            "Training Iteration 4018, Loss: 3.9444937705993652\n",
            "Training Iteration 4019, Loss: 6.773792743682861\n",
            "Training Iteration 4020, Loss: 1.564316749572754\n",
            "Training Iteration 4021, Loss: 6.059110164642334\n",
            "Training Iteration 4022, Loss: 3.3503029346466064\n",
            "Training Iteration 4023, Loss: 4.932219505310059\n",
            "Training Iteration 4024, Loss: 4.128580570220947\n",
            "Training Iteration 4025, Loss: 3.2979702949523926\n",
            "Training Iteration 4026, Loss: 6.005995273590088\n",
            "Training Iteration 4027, Loss: 5.035862445831299\n",
            "Training Iteration 4028, Loss: 4.796181678771973\n",
            "Training Iteration 4029, Loss: 3.3481667041778564\n",
            "Training Iteration 4030, Loss: 3.983873128890991\n",
            "Training Iteration 4031, Loss: 1.785435676574707\n",
            "Training Iteration 4032, Loss: 6.516449451446533\n",
            "Training Iteration 4033, Loss: 4.922328472137451\n",
            "Training Iteration 4034, Loss: 4.056999683380127\n",
            "Training Iteration 4035, Loss: 3.176117420196533\n",
            "Training Iteration 4036, Loss: 5.8480682373046875\n",
            "Training Iteration 4037, Loss: 5.027887344360352\n",
            "Training Iteration 4038, Loss: 3.725369691848755\n",
            "Training Iteration 4039, Loss: 4.092129230499268\n",
            "Training Iteration 4040, Loss: 6.151431083679199\n",
            "Training Iteration 4041, Loss: 5.436270713806152\n",
            "Training Iteration 4042, Loss: 5.46906852722168\n",
            "Training Iteration 4043, Loss: 3.3455419540405273\n",
            "Training Iteration 4044, Loss: 5.0450944900512695\n",
            "Training Iteration 4045, Loss: 3.285801410675049\n",
            "Training Iteration 4046, Loss: 5.7010955810546875\n",
            "Training Iteration 4047, Loss: 5.466808795928955\n",
            "Training Iteration 4048, Loss: 4.493781566619873\n",
            "Training Iteration 4049, Loss: 4.068866729736328\n",
            "Training Iteration 4050, Loss: 6.924894332885742\n",
            "Training Iteration 4051, Loss: 2.6766717433929443\n",
            "Training Iteration 4052, Loss: 6.437053203582764\n",
            "Training Iteration 4053, Loss: 3.107664108276367\n",
            "Training Iteration 4054, Loss: 7.818564414978027\n",
            "Training Iteration 4055, Loss: 3.66471791267395\n",
            "Training Iteration 4056, Loss: 6.140439987182617\n",
            "Training Iteration 4057, Loss: 8.413917541503906\n",
            "Training Iteration 4058, Loss: 5.158896446228027\n",
            "Training Iteration 4059, Loss: 5.008070945739746\n",
            "Training Iteration 4060, Loss: 5.426346778869629\n",
            "Training Iteration 4061, Loss: 3.494913101196289\n",
            "Training Iteration 4062, Loss: 5.9124860763549805\n",
            "Training Iteration 4063, Loss: 4.305957794189453\n",
            "Training Iteration 4064, Loss: 5.372735023498535\n",
            "Training Iteration 4065, Loss: 8.094444274902344\n",
            "Training Iteration 4066, Loss: 5.824109077453613\n",
            "Training Iteration 4067, Loss: 3.01643443107605\n",
            "Training Iteration 4068, Loss: 5.282924175262451\n",
            "Training Iteration 4069, Loss: 6.427426338195801\n",
            "Training Iteration 4070, Loss: 2.602402925491333\n",
            "Training Iteration 4071, Loss: 1.3171859979629517\n",
            "Training Iteration 4072, Loss: 2.6697254180908203\n",
            "Training Iteration 4073, Loss: 4.710209846496582\n",
            "Training Iteration 4074, Loss: 3.3939056396484375\n",
            "Training Iteration 4075, Loss: 4.6767897605896\n",
            "Training Iteration 4076, Loss: 3.9141712188720703\n",
            "Training Iteration 4077, Loss: 6.111231327056885\n",
            "Training Iteration 4078, Loss: 7.684607982635498\n",
            "Training Iteration 4079, Loss: 6.146484375\n",
            "Training Iteration 4080, Loss: 4.893652439117432\n",
            "Training Iteration 4081, Loss: 2.0906991958618164\n",
            "Training Iteration 4082, Loss: 5.655333995819092\n",
            "Training Iteration 4083, Loss: 4.1064677238464355\n",
            "Training Iteration 4084, Loss: 4.00601863861084\n",
            "Training Iteration 4085, Loss: 2.9369406700134277\n",
            "Training Iteration 4086, Loss: 6.822212219238281\n",
            "Training Iteration 4087, Loss: 5.2237396240234375\n",
            "Training Iteration 4088, Loss: 3.188988208770752\n",
            "Training Iteration 4089, Loss: 2.8866560459136963\n",
            "Training Iteration 4090, Loss: 0.4676705598831177\n",
            "Training Iteration 4091, Loss: 3.5033459663391113\n",
            "Training Iteration 4092, Loss: 8.855913162231445\n",
            "Training Iteration 4093, Loss: 4.488941192626953\n",
            "Training Iteration 4094, Loss: 4.921842098236084\n",
            "Training Iteration 4095, Loss: 1.7893260717391968\n",
            "Training Iteration 4096, Loss: 2.895487070083618\n",
            "Training Iteration 4097, Loss: 3.0573227405548096\n",
            "Training Iteration 4098, Loss: 1.8672219514846802\n",
            "Training Iteration 4099, Loss: 6.552578926086426\n",
            "Training Iteration 4100, Loss: 2.0256834030151367\n",
            "Training Iteration 4101, Loss: 3.83774995803833\n",
            "Training Iteration 4102, Loss: 2.992277145385742\n",
            "Training Iteration 4103, Loss: 3.8699443340301514\n",
            "Training Iteration 4104, Loss: 5.349255561828613\n",
            "Training Iteration 4105, Loss: 3.196864604949951\n",
            "Training Iteration 4106, Loss: 2.0637941360473633\n",
            "Training Iteration 4107, Loss: 2.804640531539917\n",
            "Training Iteration 4108, Loss: 3.0984668731689453\n",
            "Training Iteration 4109, Loss: 3.5787737369537354\n",
            "Training Iteration 4110, Loss: 3.906123638153076\n",
            "Training Iteration 4111, Loss: 3.164705753326416\n",
            "Training Iteration 4112, Loss: 3.9027926921844482\n",
            "Training Iteration 4113, Loss: 6.923740863800049\n",
            "Training Iteration 4114, Loss: 3.5736894607543945\n",
            "Training Iteration 4115, Loss: 7.3159565925598145\n",
            "Training Iteration 4116, Loss: 4.26731014251709\n",
            "Training Iteration 4117, Loss: 5.4579877853393555\n",
            "Training Iteration 4118, Loss: 3.6195430755615234\n",
            "Training Iteration 4119, Loss: 4.546123504638672\n",
            "Training Iteration 4120, Loss: 4.39091682434082\n",
            "Training Iteration 4121, Loss: 5.671595573425293\n",
            "Training Iteration 4122, Loss: 3.9900999069213867\n",
            "Training Iteration 4123, Loss: 3.0324766635894775\n",
            "Training Iteration 4124, Loss: 3.9116530418395996\n",
            "Training Iteration 4125, Loss: 3.8684885501861572\n",
            "Training Iteration 4126, Loss: 5.772947311401367\n",
            "Training Iteration 4127, Loss: 1.7269046306610107\n",
            "Training Iteration 4128, Loss: 6.004353046417236\n",
            "Training Iteration 4129, Loss: 4.260054111480713\n",
            "Training Iteration 4130, Loss: 5.372467041015625\n",
            "Training Iteration 4131, Loss: 5.473871231079102\n",
            "Training Iteration 4132, Loss: 6.682424545288086\n",
            "Training Iteration 4133, Loss: 4.627877712249756\n",
            "Training Iteration 4134, Loss: 3.1364190578460693\n",
            "Training Iteration 4135, Loss: 6.78313684463501\n",
            "Training Iteration 4136, Loss: 3.250725746154785\n",
            "Training Iteration 4137, Loss: 3.745307445526123\n",
            "Training Iteration 4138, Loss: 5.707324981689453\n",
            "Training Iteration 4139, Loss: 5.332071781158447\n",
            "Training Iteration 4140, Loss: 3.1783971786499023\n",
            "Training Iteration 4141, Loss: 3.805696487426758\n",
            "Training Iteration 4142, Loss: 5.436845779418945\n",
            "Training Iteration 4143, Loss: 4.666138648986816\n",
            "Training Iteration 4144, Loss: 6.6641387939453125\n",
            "Training Iteration 4145, Loss: 4.819308280944824\n",
            "Training Iteration 4146, Loss: 3.5584559440612793\n",
            "Training Iteration 4147, Loss: 4.7219109535217285\n",
            "Training Iteration 4148, Loss: 4.6479926109313965\n",
            "Training Iteration 4149, Loss: 5.349492073059082\n",
            "Training Iteration 4150, Loss: 2.783663749694824\n",
            "Training Iteration 4151, Loss: 5.730045318603516\n",
            "Training Iteration 4152, Loss: 4.68493127822876\n",
            "Training Iteration 4153, Loss: 3.178745985031128\n",
            "Training Iteration 4154, Loss: 4.189306259155273\n",
            "Training Iteration 4155, Loss: 6.530036449432373\n",
            "Training Iteration 4156, Loss: 4.544493198394775\n",
            "Training Iteration 4157, Loss: 5.619825839996338\n",
            "Training Iteration 4158, Loss: 3.478518486022949\n",
            "Training Iteration 4159, Loss: 4.50248384475708\n",
            "Training Iteration 4160, Loss: 4.677238464355469\n",
            "Training Iteration 4161, Loss: 5.240452289581299\n",
            "Training Iteration 4162, Loss: 4.299412727355957\n",
            "Training Iteration 4163, Loss: 5.860231399536133\n",
            "Training Iteration 4164, Loss: 5.801596164703369\n",
            "Training Iteration 4165, Loss: 4.714965343475342\n",
            "Training Iteration 4166, Loss: 4.674774169921875\n",
            "Training Iteration 4167, Loss: 2.1324901580810547\n",
            "Training Iteration 4168, Loss: 7.452078342437744\n",
            "Training Iteration 4169, Loss: 4.515373229980469\n",
            "Training Iteration 4170, Loss: 5.1265764236450195\n",
            "Training Iteration 4171, Loss: 4.5306196212768555\n",
            "Training Iteration 4172, Loss: 3.3772003650665283\n",
            "Training Iteration 4173, Loss: 5.596956253051758\n",
            "Training Iteration 4174, Loss: 3.545670747756958\n",
            "Training Iteration 4175, Loss: 4.582198143005371\n",
            "Training Iteration 4176, Loss: 5.687250137329102\n",
            "Training Iteration 4177, Loss: 4.402630805969238\n",
            "Training Iteration 4178, Loss: 2.7839605808258057\n",
            "Training Iteration 4179, Loss: 2.6612796783447266\n",
            "Training Iteration 4180, Loss: 7.285072326660156\n",
            "Training Iteration 4181, Loss: 2.885798215866089\n",
            "Training Iteration 4182, Loss: 8.665157318115234\n",
            "Training Iteration 4183, Loss: 4.427971839904785\n",
            "Training Iteration 4184, Loss: 5.705905914306641\n",
            "Training Iteration 4185, Loss: 3.5368661880493164\n",
            "Training Iteration 4186, Loss: 4.389132976531982\n",
            "Training Iteration 4187, Loss: 6.747093677520752\n",
            "Training Iteration 4188, Loss: 5.402381420135498\n",
            "Training Iteration 4189, Loss: 6.137038707733154\n",
            "Training Iteration 4190, Loss: 6.132011890411377\n",
            "Training Iteration 4191, Loss: 4.560482025146484\n",
            "Training Iteration 4192, Loss: 2.9905507564544678\n",
            "Training Iteration 4193, Loss: 4.0212082862854\n",
            "Training Iteration 4194, Loss: 3.715365409851074\n",
            "Training Iteration 4195, Loss: 5.749475955963135\n",
            "Training Iteration 4196, Loss: 4.217817783355713\n",
            "Training Iteration 4197, Loss: 4.648846626281738\n",
            "Training Iteration 4198, Loss: 4.934873580932617\n",
            "Training Iteration 4199, Loss: 2.504453659057617\n",
            "Training Iteration 4200, Loss: 1.2463014125823975\n",
            "Training Iteration 4201, Loss: 2.4505012035369873\n",
            "Training Iteration 4202, Loss: 4.795177936553955\n",
            "Training Iteration 4203, Loss: 3.4303863048553467\n",
            "Training Iteration 4204, Loss: 3.2518224716186523\n",
            "Training Iteration 4205, Loss: 3.96258807182312\n",
            "Training Iteration 4206, Loss: 7.256499290466309\n",
            "Training Iteration 4207, Loss: 5.041598796844482\n",
            "Training Iteration 4208, Loss: 3.4543612003326416\n",
            "Training Iteration 4209, Loss: 2.4259870052337646\n",
            "Training Iteration 4210, Loss: 3.0005345344543457\n",
            "Training Iteration 4211, Loss: 7.80893611907959\n",
            "Training Iteration 4212, Loss: 5.41730260848999\n",
            "Training Iteration 4213, Loss: 2.660973310470581\n",
            "Training Iteration 4214, Loss: 4.700129985809326\n",
            "Training Iteration 4215, Loss: 4.564684867858887\n",
            "Training Iteration 4216, Loss: 7.323155879974365\n",
            "Training Iteration 4217, Loss: 4.3838090896606445\n",
            "Training Iteration 4218, Loss: 9.72877311706543\n",
            "Training Iteration 4219, Loss: 5.426707744598389\n",
            "Training Iteration 4220, Loss: 3.7134931087493896\n",
            "Training Iteration 4221, Loss: 4.964721202850342\n",
            "Training Iteration 4222, Loss: 2.843158483505249\n",
            "Training Iteration 4223, Loss: 6.554734230041504\n",
            "Training Iteration 4224, Loss: 7.426610946655273\n",
            "Training Iteration 4225, Loss: 3.289371967315674\n",
            "Training Iteration 4226, Loss: 4.254140377044678\n",
            "Training Iteration 4227, Loss: 5.629491329193115\n",
            "Training Iteration 4228, Loss: 2.7356207370758057\n",
            "Training Iteration 4229, Loss: 3.5468883514404297\n",
            "Training Iteration 4230, Loss: 3.065638303756714\n",
            "Training Iteration 4231, Loss: 2.118114709854126\n",
            "Training Iteration 4232, Loss: 4.927272796630859\n",
            "Training Iteration 4233, Loss: 3.501230239868164\n",
            "Training Iteration 4234, Loss: 4.208559036254883\n",
            "Training Iteration 4235, Loss: 6.069551467895508\n",
            "Training Iteration 4236, Loss: 5.030248641967773\n",
            "Training Iteration 4237, Loss: 3.4118435382843018\n",
            "Training Iteration 4238, Loss: 6.002031326293945\n",
            "Training Iteration 4239, Loss: 4.033606052398682\n",
            "Training Iteration 4240, Loss: 4.612722873687744\n",
            "Training Iteration 4241, Loss: 2.0232810974121094\n",
            "Training Iteration 4242, Loss: 1.2057209014892578\n",
            "Training Iteration 4243, Loss: 2.8222711086273193\n",
            "Training Iteration 4244, Loss: 3.732616424560547\n",
            "Training Iteration 4245, Loss: 3.61153507232666\n",
            "Training Iteration 4246, Loss: 2.470215320587158\n",
            "Training Iteration 4247, Loss: 3.5681302547454834\n",
            "Training Iteration 4248, Loss: 2.0509283542633057\n",
            "Training Iteration 4249, Loss: 3.6192924976348877\n",
            "Training Iteration 4250, Loss: 3.009634017944336\n",
            "Training Iteration 4251, Loss: 3.919245719909668\n",
            "Training Iteration 4252, Loss: 2.5943517684936523\n",
            "Training Iteration 4253, Loss: 4.361572265625\n",
            "Training Iteration 4254, Loss: 2.8303062915802\n",
            "Training Iteration 4255, Loss: 5.419663429260254\n",
            "Training Iteration 4256, Loss: 3.290283679962158\n",
            "Training Iteration 4257, Loss: 4.482468605041504\n",
            "Training Iteration 4258, Loss: 5.449061393737793\n",
            "Training Iteration 4259, Loss: 2.9824438095092773\n",
            "Training Iteration 4260, Loss: 3.3594515323638916\n",
            "Training Iteration 4261, Loss: 2.6015262603759766\n",
            "Training Iteration 4262, Loss: 1.9499249458312988\n",
            "Training Iteration 4263, Loss: 6.124567985534668\n",
            "Training Iteration 4264, Loss: 4.3123860359191895\n",
            "Training Iteration 4265, Loss: 4.557188510894775\n",
            "Training Iteration 4266, Loss: 4.00734281539917\n",
            "Training Iteration 4267, Loss: 4.740686416625977\n",
            "Training Iteration 4268, Loss: 2.7305123805999756\n",
            "Training Iteration 4269, Loss: 3.6170144081115723\n",
            "Training Iteration 4270, Loss: 3.329710006713867\n",
            "Training Iteration 4271, Loss: 4.211944580078125\n",
            "Training Iteration 4272, Loss: 3.1332223415374756\n",
            "Training Iteration 4273, Loss: 2.4772560596466064\n",
            "Training Iteration 4274, Loss: 2.413691759109497\n",
            "Training Iteration 4275, Loss: 6.822075843811035\n",
            "Training Iteration 4276, Loss: 5.434981822967529\n",
            "Training Iteration 4277, Loss: 3.057720184326172\n",
            "Training Iteration 4278, Loss: 2.925555467605591\n",
            "Training Iteration 4279, Loss: 6.417417526245117\n",
            "Training Iteration 4280, Loss: 2.2933835983276367\n",
            "Training Iteration 4281, Loss: 4.253522872924805\n",
            "Training Iteration 4282, Loss: 5.137969493865967\n",
            "Training Iteration 4283, Loss: 1.9363672733306885\n",
            "Training Iteration 4284, Loss: 3.3524832725524902\n",
            "Training Iteration 4285, Loss: 2.109011173248291\n",
            "Training Iteration 4286, Loss: 3.242896318435669\n",
            "Training Iteration 4287, Loss: 3.7963199615478516\n",
            "Training Iteration 4288, Loss: 2.196974277496338\n",
            "Training Iteration 4289, Loss: 8.079941749572754\n",
            "Training Iteration 4290, Loss: 4.8401031494140625\n",
            "Training Iteration 4291, Loss: 3.2876415252685547\n",
            "Training Iteration 4292, Loss: 4.327538967132568\n",
            "Training Iteration 4293, Loss: 3.355128526687622\n",
            "Training Iteration 4294, Loss: 4.900479316711426\n",
            "Training Iteration 4295, Loss: 4.290163993835449\n",
            "Training Iteration 4296, Loss: 7.065540313720703\n",
            "Training Iteration 4297, Loss: 3.6279408931732178\n",
            "Training Iteration 4298, Loss: 4.932371616363525\n",
            "Training Iteration 4299, Loss: 3.8818812370300293\n",
            "Training Iteration 4300, Loss: 4.25723123550415\n",
            "Training Iteration 4301, Loss: 4.620175361633301\n",
            "Training Iteration 4302, Loss: 5.302155494689941\n",
            "Training Iteration 4303, Loss: 4.9432549476623535\n",
            "Training Iteration 4304, Loss: 2.978945732116699\n",
            "Training Iteration 4305, Loss: 4.449847221374512\n",
            "Training Iteration 4306, Loss: 5.35818338394165\n",
            "Training Iteration 4307, Loss: 3.9117438793182373\n",
            "Training Iteration 4308, Loss: 1.4835968017578125\n",
            "Training Iteration 4309, Loss: 3.030259609222412\n",
            "Training Iteration 4310, Loss: 4.041431427001953\n",
            "Training Iteration 4311, Loss: 2.3078460693359375\n",
            "Training Iteration 4312, Loss: 3.315227746963501\n",
            "Training Iteration 4313, Loss: 4.731005668640137\n",
            "Training Iteration 4314, Loss: 3.1094961166381836\n",
            "Training Iteration 4315, Loss: 5.5677714347839355\n",
            "Training Iteration 4316, Loss: 3.564849615097046\n",
            "Training Iteration 4317, Loss: 3.3441851139068604\n",
            "Training Iteration 4318, Loss: 2.6788034439086914\n",
            "Training Iteration 4319, Loss: 3.9962430000305176\n",
            "Training Iteration 4320, Loss: 3.9654784202575684\n",
            "Training Iteration 4321, Loss: 3.9826443195343018\n",
            "Training Iteration 4322, Loss: 2.8373355865478516\n",
            "Training Iteration 4323, Loss: 4.94639778137207\n",
            "Training Iteration 4324, Loss: 6.069863796234131\n",
            "Training Iteration 4325, Loss: 3.8825457096099854\n",
            "Training Iteration 4326, Loss: 5.106931209564209\n",
            "Training Iteration 4327, Loss: 3.9599497318267822\n",
            "Training Iteration 4328, Loss: 3.8712563514709473\n",
            "Training Iteration 4329, Loss: 4.491871356964111\n",
            "Training Iteration 4330, Loss: 2.797295093536377\n",
            "Training Iteration 4331, Loss: 3.979227066040039\n",
            "Training Iteration 4332, Loss: 2.108746290206909\n",
            "Training Iteration 4333, Loss: 3.9777884483337402\n",
            "Training Iteration 4334, Loss: 2.2276015281677246\n",
            "Training Iteration 4335, Loss: 2.2625041007995605\n",
            "Training Iteration 4336, Loss: 3.5087759494781494\n",
            "Training Iteration 4337, Loss: 5.887243270874023\n",
            "Training Iteration 4338, Loss: 2.07662296295166\n",
            "Training Iteration 4339, Loss: 3.118882179260254\n",
            "Training Iteration 4340, Loss: 3.213900327682495\n",
            "Training Iteration 4341, Loss: 2.8946046829223633\n",
            "Training Iteration 4342, Loss: 3.426018238067627\n",
            "Training Iteration 4343, Loss: 2.148797035217285\n",
            "Training Iteration 4344, Loss: 4.529619216918945\n",
            "Training Iteration 4345, Loss: 8.091184616088867\n",
            "Training Iteration 4346, Loss: 8.28314208984375\n",
            "Training Iteration 4347, Loss: 3.720336437225342\n",
            "Training Iteration 4348, Loss: 3.0936455726623535\n",
            "Training Iteration 4349, Loss: 2.4868814945220947\n",
            "Training Iteration 4350, Loss: 3.9872398376464844\n",
            "Training Iteration 4351, Loss: 3.230638027191162\n",
            "Training Iteration 4352, Loss: 3.8357441425323486\n",
            "Training Iteration 4353, Loss: 5.124421119689941\n",
            "Training Iteration 4354, Loss: 2.7291715145111084\n",
            "Training Iteration 4355, Loss: 3.7848997116088867\n",
            "Training Iteration 4356, Loss: 4.134111404418945\n",
            "Training Iteration 4357, Loss: 5.242797374725342\n",
            "Training Iteration 4358, Loss: 7.336520195007324\n",
            "Training Iteration 4359, Loss: 4.2562737464904785\n",
            "Training Iteration 4360, Loss: 3.507390260696411\n",
            "Training Iteration 4361, Loss: 2.884101629257202\n",
            "Training Iteration 4362, Loss: 3.7059905529022217\n",
            "Training Iteration 4363, Loss: 9.921683311462402\n",
            "Training Iteration 4364, Loss: 10.503016471862793\n",
            "Training Iteration 4365, Loss: 11.356790542602539\n",
            "Training Iteration 4366, Loss: 8.006895065307617\n",
            "Training Iteration 4367, Loss: 6.880451679229736\n",
            "Training Iteration 4368, Loss: 2.5885891914367676\n",
            "Training Iteration 4369, Loss: 4.901642322540283\n",
            "Training Iteration 4370, Loss: 7.986658573150635\n",
            "Training Iteration 4371, Loss: 6.942783355712891\n",
            "Training Iteration 4372, Loss: 6.465010643005371\n",
            "Training Iteration 4373, Loss: 4.791736602783203\n",
            "Training Iteration 4374, Loss: 4.949864387512207\n",
            "Training Iteration 4375, Loss: 3.018414258956909\n",
            "Training Iteration 4376, Loss: 7.84981632232666\n",
            "Training Iteration 4377, Loss: 4.184513568878174\n",
            "Training Iteration 4378, Loss: 6.43407678604126\n",
            "Training Iteration 4379, Loss: 6.50300931930542\n",
            "Training Iteration 4380, Loss: 2.48299241065979\n",
            "Training Iteration 4381, Loss: 5.9486308097839355\n",
            "Training Iteration 4382, Loss: 7.701776027679443\n",
            "Training Iteration 4383, Loss: 5.417559623718262\n",
            "Training Iteration 4384, Loss: 4.296552658081055\n",
            "Training Iteration 4385, Loss: 2.6358561515808105\n",
            "Training Iteration 4386, Loss: 4.448731422424316\n",
            "Training Iteration 4387, Loss: 5.267843723297119\n",
            "Training Iteration 4388, Loss: 4.652125358581543\n",
            "Training Iteration 4389, Loss: 5.7585272789001465\n",
            "Training Iteration 4390, Loss: 6.898316860198975\n",
            "Training Iteration 4391, Loss: 4.672607421875\n",
            "Training Iteration 4392, Loss: 4.522059440612793\n",
            "Training Iteration 4393, Loss: 3.605339765548706\n",
            "Training Iteration 4394, Loss: 5.3047194480896\n",
            "Training Iteration 4395, Loss: 4.711213111877441\n",
            "Training Iteration 4396, Loss: 5.09185791015625\n",
            "Training Iteration 4397, Loss: 6.964504241943359\n",
            "Training Iteration 4398, Loss: 4.937763214111328\n",
            "Training Iteration 4399, Loss: 4.488523960113525\n",
            "Training Iteration 4400, Loss: 3.4259815216064453\n",
            "Training Iteration 4401, Loss: 3.131618022918701\n",
            "Training Iteration 4402, Loss: 7.119688987731934\n",
            "Training Iteration 4403, Loss: 3.7644741535186768\n",
            "Training Iteration 4404, Loss: 6.221902370452881\n",
            "Training Iteration 4405, Loss: 5.3258161544799805\n",
            "Training Iteration 4406, Loss: 4.448291778564453\n",
            "Training Iteration 4407, Loss: 4.576135158538818\n",
            "Training Iteration 4408, Loss: 6.800074100494385\n",
            "Training Iteration 4409, Loss: 5.237401008605957\n",
            "Training Iteration 4410, Loss: 6.161181449890137\n",
            "Training Iteration 4411, Loss: 5.364055156707764\n",
            "Training Iteration 4412, Loss: 4.026880741119385\n",
            "Training Iteration 4413, Loss: 4.138077735900879\n",
            "Training Iteration 4414, Loss: 5.84332799911499\n",
            "Training Iteration 4415, Loss: 6.418522357940674\n",
            "Training Iteration 4416, Loss: 6.040888786315918\n",
            "Training Iteration 4417, Loss: 5.821163177490234\n",
            "Training Iteration 4418, Loss: 3.9332354068756104\n",
            "Training Iteration 4419, Loss: 5.673095703125\n",
            "Training Iteration 4420, Loss: 4.578949451446533\n",
            "Training Iteration 4421, Loss: 4.844402313232422\n",
            "Training Iteration 4422, Loss: 3.8710451126098633\n",
            "Training Iteration 4423, Loss: 3.9609599113464355\n",
            "Training Iteration 4424, Loss: 6.423860549926758\n",
            "Training Iteration 4425, Loss: 2.8029510974884033\n",
            "Training Iteration 4426, Loss: 3.687272310256958\n",
            "Training Iteration 4427, Loss: 2.5490097999572754\n",
            "Training Iteration 4428, Loss: 5.173595428466797\n",
            "Training Iteration 4429, Loss: 3.1632354259490967\n",
            "Training Iteration 4430, Loss: 2.6085195541381836\n",
            "Training Iteration 4431, Loss: 1.7021557092666626\n",
            "Training Iteration 4432, Loss: 1.1451928615570068\n",
            "Training Iteration 4433, Loss: 3.7541046142578125\n",
            "Training Iteration 4434, Loss: 1.8382248878479004\n",
            "Training Iteration 4435, Loss: 9.930184364318848\n",
            "Training Iteration 4436, Loss: 6.112470626831055\n",
            "Training Iteration 4437, Loss: 10.182449340820312\n",
            "Training Iteration 4438, Loss: 3.418775796890259\n",
            "Training Iteration 4439, Loss: 6.417280673980713\n",
            "Training Iteration 4440, Loss: 3.588238000869751\n",
            "Training Iteration 4441, Loss: 7.7162251472473145\n",
            "Training Iteration 4442, Loss: 3.99601674079895\n",
            "Training Iteration 4443, Loss: 4.28193998336792\n",
            "Training Iteration 4444, Loss: 5.815959453582764\n",
            "Training Iteration 4445, Loss: 4.140373706817627\n",
            "Training Iteration 4446, Loss: 4.1021294593811035\n",
            "Training Iteration 4447, Loss: 2.7505905628204346\n",
            "Training Iteration 4448, Loss: 6.11424446105957\n",
            "Training Iteration 4449, Loss: 3.2295737266540527\n",
            "Training Iteration 4450, Loss: 4.851420879364014\n",
            "Training Iteration 4451, Loss: 7.7037882804870605\n",
            "Training Iteration 4452, Loss: 4.902275085449219\n",
            "Training Iteration 4453, Loss: 5.800560474395752\n",
            "Training Iteration 4454, Loss: 5.222614288330078\n",
            "Training Iteration 4455, Loss: 5.251333236694336\n",
            "Training Iteration 4456, Loss: 3.5224273204803467\n",
            "Training Iteration 4457, Loss: 4.85270881652832\n",
            "Training Iteration 4458, Loss: 3.551292896270752\n",
            "Training Iteration 4459, Loss: 5.730863571166992\n",
            "Training Iteration 4460, Loss: 3.882521152496338\n",
            "Training Iteration 4461, Loss: 2.9814369678497314\n",
            "Training Iteration 4462, Loss: 2.897451162338257\n",
            "Training Iteration 4463, Loss: 5.10383415222168\n",
            "Training Iteration 4464, Loss: 7.9829511642456055\n",
            "Training Iteration 4465, Loss: 8.994701385498047\n",
            "Training Iteration 4466, Loss: 8.038447380065918\n",
            "Training Iteration 4467, Loss: 3.018611431121826\n",
            "Training Iteration 4468, Loss: 5.708462238311768\n",
            "Training Iteration 4469, Loss: 3.189546823501587\n",
            "Training Iteration 4470, Loss: 4.8434271812438965\n",
            "Training Iteration 4471, Loss: 2.9444775581359863\n",
            "Training Iteration 4472, Loss: 6.470541000366211\n",
            "Training Iteration 4473, Loss: 6.137204170227051\n",
            "Training Iteration 4474, Loss: 3.5799355506896973\n",
            "Training Iteration 4475, Loss: 5.593836784362793\n",
            "Training Iteration 4476, Loss: 6.021271228790283\n",
            "Training Iteration 4477, Loss: 4.102413177490234\n",
            "Training Iteration 4478, Loss: 5.295316696166992\n",
            "Training Iteration 4479, Loss: 8.589110374450684\n",
            "Training Iteration 4480, Loss: 4.960628986358643\n",
            "Training Iteration 4481, Loss: 7.169546127319336\n",
            "Training Iteration 4482, Loss: 5.618081092834473\n",
            "Training Iteration 4483, Loss: 5.309168338775635\n",
            "Training Iteration 4484, Loss: 4.985866546630859\n",
            "Training Iteration 4485, Loss: 7.540570259094238\n",
            "Training Iteration 4486, Loss: 4.275629043579102\n",
            "Training Iteration 4487, Loss: 7.566802024841309\n",
            "Training Iteration 4488, Loss: 5.030227184295654\n",
            "Training Iteration 4489, Loss: 4.492922782897949\n",
            "Training Iteration 4490, Loss: 4.167178153991699\n",
            "Training Iteration 4491, Loss: 6.543680191040039\n",
            "Training Iteration 4492, Loss: 5.230417728424072\n",
            "Training Iteration 4493, Loss: 3.0859527587890625\n",
            "Training Iteration 4494, Loss: 2.4403772354125977\n",
            "Training Iteration 4495, Loss: 2.0192885398864746\n",
            "Training Iteration 4496, Loss: 6.182646751403809\n",
            "Training Iteration 4497, Loss: 5.29030704498291\n",
            "Training Iteration 4498, Loss: 3.075477361679077\n",
            "Training Iteration 4499, Loss: 4.20065450668335\n",
            "Training Iteration 4500, Loss: 3.9741642475128174\n",
            "Training Iteration 4501, Loss: 8.069131851196289\n",
            "Training Iteration 4502, Loss: 5.059316635131836\n",
            "Training Iteration 4503, Loss: 6.101299285888672\n",
            "Training Iteration 4504, Loss: 4.363251686096191\n",
            "Training Iteration 4505, Loss: 6.783250331878662\n",
            "Training Iteration 4506, Loss: 1.8373600244522095\n",
            "Training Iteration 4507, Loss: 4.357625961303711\n",
            "Training Iteration 4508, Loss: 4.748056888580322\n",
            "Training Iteration 4509, Loss: 4.85078239440918\n",
            "Training Iteration 4510, Loss: 2.4430010318756104\n",
            "Training Iteration 4511, Loss: 5.517702579498291\n",
            "Training Iteration 4512, Loss: 4.361311435699463\n",
            "Training Iteration 4513, Loss: 4.233997344970703\n",
            "Training Iteration 4514, Loss: 2.925851583480835\n",
            "Training Iteration 4515, Loss: 7.539402961730957\n",
            "Training Iteration 4516, Loss: 6.978907585144043\n",
            "Training Iteration 4517, Loss: 8.033416748046875\n",
            "Training Iteration 4518, Loss: 3.1170425415039062\n",
            "Training Iteration 4519, Loss: 7.2604827880859375\n",
            "Training Iteration 4520, Loss: 2.8698482513427734\n",
            "Training Iteration 4521, Loss: 4.883041858673096\n",
            "Training Iteration 4522, Loss: 2.483964681625366\n",
            "Training Iteration 4523, Loss: 5.491145133972168\n",
            "Training Iteration 4524, Loss: 5.571502685546875\n",
            "Training Iteration 4525, Loss: 5.694821357727051\n",
            "Training Iteration 4526, Loss: 3.0661509037017822\n",
            "Training Iteration 4527, Loss: 3.3021240234375\n",
            "Training Iteration 4528, Loss: 3.939267158508301\n",
            "Training Iteration 4529, Loss: 4.074865341186523\n",
            "Training Iteration 4530, Loss: 2.283289670944214\n",
            "Training Iteration 4531, Loss: 2.3154783248901367\n",
            "Training Iteration 4532, Loss: 2.8392820358276367\n",
            "Training Iteration 4533, Loss: 3.4266505241394043\n",
            "Training Iteration 4534, Loss: 3.6722846031188965\n",
            "Training Iteration 4535, Loss: 6.911104202270508\n",
            "Training Iteration 4536, Loss: 2.426642894744873\n",
            "Training Iteration 4537, Loss: 3.1847879886627197\n",
            "Training Iteration 4538, Loss: 3.967397451400757\n",
            "Training Iteration 4539, Loss: 2.873185873031616\n",
            "Training Iteration 4540, Loss: 4.701947212219238\n",
            "Training Iteration 4541, Loss: 7.371895790100098\n",
            "Training Iteration 4542, Loss: 5.669956684112549\n",
            "Training Iteration 4543, Loss: 3.8291287422180176\n",
            "Training Iteration 4544, Loss: 5.851390838623047\n",
            "Training Iteration 4545, Loss: 3.865417957305908\n",
            "Training Iteration 4546, Loss: 2.116818428039551\n",
            "Training Iteration 4547, Loss: 5.420734405517578\n",
            "Training Iteration 4548, Loss: 4.854475975036621\n",
            "Training Iteration 4549, Loss: 4.082388877868652\n",
            "Training Iteration 4550, Loss: 3.3217639923095703\n",
            "Training Iteration 4551, Loss: 3.9916415214538574\n",
            "Training Iteration 4552, Loss: 3.443925619125366\n",
            "Training Iteration 4553, Loss: 3.2524354457855225\n",
            "Training Iteration 4554, Loss: 3.0931990146636963\n",
            "Training Iteration 4555, Loss: 7.771102428436279\n",
            "Training Iteration 4556, Loss: 3.5380685329437256\n",
            "Training Iteration 4557, Loss: 4.255860328674316\n",
            "Training Iteration 4558, Loss: 6.776220321655273\n",
            "Training Iteration 4559, Loss: 5.363699913024902\n",
            "Training Iteration 4560, Loss: 4.964434623718262\n",
            "Training Iteration 4561, Loss: 4.2506184577941895\n",
            "Training Iteration 4562, Loss: 4.05565881729126\n",
            "Training Iteration 4563, Loss: 2.3590738773345947\n",
            "Training Iteration 4564, Loss: 3.511425256729126\n",
            "Training Iteration 4565, Loss: 1.1646288633346558\n",
            "Training Iteration 4566, Loss: 4.438042163848877\n",
            "Training Iteration 4567, Loss: 4.464767932891846\n",
            "Training Iteration 4568, Loss: 5.100207328796387\n",
            "Training Iteration 4569, Loss: 4.490311622619629\n",
            "Training Iteration 4570, Loss: 5.029884338378906\n",
            "Training Iteration 4571, Loss: 2.9467244148254395\n",
            "Training Iteration 4572, Loss: 3.8129220008850098\n",
            "Training Iteration 4573, Loss: 4.6006293296813965\n",
            "Training Iteration 4574, Loss: 5.104955196380615\n",
            "Training Iteration 4575, Loss: 2.168492078781128\n",
            "Training Iteration 4576, Loss: 5.135477542877197\n",
            "Training Iteration 4577, Loss: 2.5200278759002686\n",
            "Training Iteration 4578, Loss: 3.1574654579162598\n",
            "Training Iteration 4579, Loss: 2.5329816341400146\n",
            "Training Iteration 4580, Loss: 3.599221706390381\n",
            "Training Iteration 4581, Loss: 4.385194778442383\n",
            "Training Iteration 4582, Loss: 5.050738334655762\n",
            "Training Iteration 4583, Loss: 6.917741775512695\n",
            "Training Iteration 4584, Loss: 1.9534882307052612\n",
            "Training Iteration 4585, Loss: 4.101225852966309\n",
            "Training Iteration 4586, Loss: 6.023880958557129\n",
            "Training Iteration 4587, Loss: 3.1215133666992188\n",
            "Training Iteration 4588, Loss: 3.7984275817871094\n",
            "Training Iteration 4589, Loss: 4.774008750915527\n",
            "Training Iteration 4590, Loss: 6.572037696838379\n",
            "Training Iteration 4591, Loss: 5.179919242858887\n",
            "Training Iteration 4592, Loss: 3.942960500717163\n",
            "Training Iteration 4593, Loss: 3.8969407081604004\n",
            "Training Iteration 4594, Loss: 2.940171480178833\n",
            "Training Iteration 4595, Loss: 4.2706193923950195\n",
            "Training Iteration 4596, Loss: 6.197850227355957\n",
            "Training Iteration 4597, Loss: 4.205242156982422\n",
            "Training Iteration 4598, Loss: 4.231191158294678\n",
            "Training Iteration 4599, Loss: 7.124914646148682\n",
            "Training Iteration 4600, Loss: 5.764003276824951\n",
            "Training Iteration 4601, Loss: 8.050433158874512\n",
            "Training Iteration 4602, Loss: 2.358454465866089\n",
            "Training Iteration 4603, Loss: 7.409758567810059\n",
            "Training Iteration 4604, Loss: 6.832137584686279\n",
            "Training Iteration 4605, Loss: 5.199708461761475\n",
            "Training Iteration 4606, Loss: 4.89396333694458\n",
            "Training Iteration 4607, Loss: 10.01197624206543\n",
            "Training Iteration 4608, Loss: 6.637767314910889\n",
            "Training Iteration 4609, Loss: 2.347306489944458\n",
            "Training Iteration 4610, Loss: 6.18021297454834\n",
            "Training Iteration 4611, Loss: 5.262689590454102\n",
            "Training Iteration 4612, Loss: 4.310807704925537\n",
            "Training Iteration 4613, Loss: 3.5738327503204346\n",
            "Training Iteration 4614, Loss: 2.2012996673583984\n",
            "Training Iteration 4615, Loss: 4.1273908615112305\n",
            "Training Iteration 4616, Loss: 7.942954063415527\n",
            "Training Iteration 4617, Loss: 4.813869476318359\n",
            "Training Iteration 4618, Loss: 3.284423351287842\n",
            "Training Iteration 4619, Loss: 4.943199634552002\n",
            "Training Iteration 4620, Loss: 3.072315216064453\n",
            "Training Iteration 4621, Loss: 5.880617618560791\n",
            "Training Iteration 4622, Loss: 7.835465908050537\n",
            "Training Iteration 4623, Loss: 4.699658393859863\n",
            "Training Iteration 4624, Loss: 3.179798126220703\n",
            "Training Iteration 4625, Loss: 2.6613430976867676\n",
            "Training Iteration 4626, Loss: 3.199728012084961\n",
            "Training Iteration 4627, Loss: 2.759983777999878\n",
            "Training Iteration 4628, Loss: 10.328141212463379\n",
            "Training Iteration 4629, Loss: 1.875042200088501\n",
            "Training Iteration 4630, Loss: 5.9665398597717285\n",
            "Training Iteration 4631, Loss: 5.873414516448975\n",
            "Training Iteration 4632, Loss: 7.640285968780518\n",
            "Training Iteration 4633, Loss: 9.123738288879395\n",
            "Training Iteration 4634, Loss: 8.003948211669922\n",
            "Training Iteration 4635, Loss: 4.308587074279785\n",
            "Training Iteration 4636, Loss: 4.521930694580078\n",
            "Training Iteration 4637, Loss: 2.869061231613159\n",
            "Training Iteration 4638, Loss: 3.1820435523986816\n",
            "Training Iteration 4639, Loss: 5.828709602355957\n",
            "Training Iteration 4640, Loss: 2.753702402114868\n",
            "Training Iteration 4641, Loss: 6.9683518409729\n",
            "Training Iteration 4642, Loss: 2.802297592163086\n",
            "Training Iteration 4643, Loss: 3.7486891746520996\n",
            "Training Iteration 4644, Loss: 5.887358665466309\n",
            "Training Iteration 4645, Loss: 4.590775966644287\n",
            "Training Iteration 4646, Loss: 3.4572389125823975\n",
            "Training Iteration 4647, Loss: 5.420784950256348\n",
            "Training Iteration 4648, Loss: 2.4967617988586426\n",
            "Training Iteration 4649, Loss: 5.326949596405029\n",
            "Training Iteration 4650, Loss: 4.231698989868164\n",
            "Training Iteration 4651, Loss: 6.269308090209961\n",
            "Training Iteration 4652, Loss: 2.5074315071105957\n",
            "Training Iteration 4653, Loss: 4.906737327575684\n",
            "Training Iteration 4654, Loss: 3.7948105335235596\n",
            "Training Iteration 4655, Loss: 5.058059215545654\n",
            "Training Iteration 4656, Loss: 3.8414645195007324\n",
            "Training Iteration 4657, Loss: 3.827462911605835\n",
            "Training Iteration 4658, Loss: 2.2595717906951904\n",
            "Training Iteration 4659, Loss: 1.9168124198913574\n",
            "Training Iteration 4660, Loss: 3.6859068870544434\n",
            "Training Iteration 4661, Loss: 3.6953954696655273\n",
            "Training Iteration 4662, Loss: 3.7494401931762695\n",
            "Training Iteration 4663, Loss: 2.964355945587158\n",
            "Training Iteration 4664, Loss: 3.8637514114379883\n",
            "Training Iteration 4665, Loss: 6.5452880859375\n",
            "Training Iteration 4666, Loss: 5.9634552001953125\n",
            "Training Iteration 4667, Loss: 5.97980260848999\n",
            "Training Iteration 4668, Loss: 7.493932247161865\n",
            "Training Iteration 4669, Loss: 2.715073347091675\n",
            "Training Iteration 4670, Loss: 5.391763210296631\n",
            "Training Iteration 4671, Loss: 2.6071925163269043\n",
            "Training Iteration 4672, Loss: 5.835171699523926\n",
            "Training Iteration 4673, Loss: 5.4022908210754395\n",
            "Training Iteration 4674, Loss: 7.316626071929932\n",
            "Training Iteration 4675, Loss: 3.7099032402038574\n",
            "Training Iteration 4676, Loss: 4.983030319213867\n",
            "Training Iteration 4677, Loss: 1.7624900341033936\n",
            "Training Iteration 4678, Loss: 4.126696586608887\n",
            "Training Iteration 4679, Loss: 11.252991676330566\n",
            "Training Iteration 4680, Loss: 8.643117904663086\n",
            "Training Iteration 4681, Loss: 5.022343635559082\n",
            "Training Iteration 4682, Loss: 5.369348049163818\n",
            "Training Iteration 4683, Loss: 3.6147103309631348\n",
            "Training Iteration 4684, Loss: 4.308010101318359\n",
            "Training Iteration 4685, Loss: 5.090816497802734\n",
            "Training Iteration 4686, Loss: 5.574192523956299\n",
            "Training Iteration 4687, Loss: 6.329932689666748\n",
            "Training Iteration 4688, Loss: 10.436564445495605\n",
            "Training Iteration 4689, Loss: 5.976778984069824\n",
            "Training Iteration 4690, Loss: 3.2557432651519775\n",
            "Training Iteration 4691, Loss: 3.050900459289551\n",
            "Training Iteration 4692, Loss: 6.807674884796143\n",
            "Training Iteration 4693, Loss: 5.7282915115356445\n",
            "Training Iteration 4694, Loss: 5.160606384277344\n",
            "Training Iteration 4695, Loss: 6.80348014831543\n",
            "Training Iteration 4696, Loss: 9.605040550231934\n",
            "Training Iteration 4697, Loss: 4.555388450622559\n",
            "Training Iteration 4698, Loss: 5.843120098114014\n",
            "Training Iteration 4699, Loss: 7.005302906036377\n",
            "Training Iteration 4700, Loss: 2.1597940921783447\n",
            "Training Iteration 4701, Loss: 4.354936122894287\n",
            "Training Iteration 4702, Loss: 7.318971157073975\n",
            "Training Iteration 4703, Loss: 8.517715454101562\n",
            "Training Iteration 4704, Loss: 5.466783046722412\n",
            "Training Iteration 4705, Loss: 1.759417176246643\n",
            "Training Iteration 4706, Loss: 4.547602653503418\n",
            "Training Iteration 4707, Loss: 4.487966537475586\n",
            "Training Iteration 4708, Loss: 7.1664652824401855\n",
            "Training Iteration 4709, Loss: 1.8789803981781006\n",
            "Training Iteration 4710, Loss: 5.2870869636535645\n",
            "Training Iteration 4711, Loss: 6.209627628326416\n",
            "Training Iteration 4712, Loss: 1.3020603656768799\n",
            "Training Iteration 4713, Loss: 5.102293014526367\n",
            "Training Iteration 4714, Loss: 3.0448689460754395\n",
            "Training Iteration 4715, Loss: 5.4935760498046875\n",
            "Training Iteration 4716, Loss: 0.832669734954834\n",
            "Training Iteration 4717, Loss: 6.688118934631348\n",
            "Training Iteration 4718, Loss: 5.8950300216674805\n",
            "Training Iteration 4719, Loss: 5.918097496032715\n",
            "Training Iteration 4720, Loss: 3.2897305488586426\n",
            "Training Iteration 4721, Loss: 3.8326539993286133\n",
            "Training Iteration 4722, Loss: 10.897537231445312\n",
            "Training Iteration 4723, Loss: 2.9179577827453613\n",
            "Training Iteration 4724, Loss: 2.3511950969696045\n",
            "Training Iteration 4725, Loss: 4.737157821655273\n",
            "Training Iteration 4726, Loss: 2.8688063621520996\n",
            "Training Iteration 4727, Loss: 7.151028633117676\n",
            "Training Iteration 4728, Loss: 5.183760643005371\n",
            "Training Iteration 4729, Loss: 5.075869560241699\n",
            "Training Iteration 4730, Loss: 4.277270317077637\n",
            "Training Iteration 4731, Loss: 3.0413575172424316\n",
            "Training Iteration 4732, Loss: 4.897616386413574\n",
            "Training Iteration 4733, Loss: 4.122922897338867\n",
            "Training Iteration 4734, Loss: 4.7653398513793945\n",
            "Training Iteration 4735, Loss: 3.100982666015625\n",
            "Training Iteration 4736, Loss: 4.347858428955078\n",
            "Training Iteration 4737, Loss: 3.161925792694092\n",
            "Training Iteration 4738, Loss: 4.455931663513184\n",
            "Training Iteration 4739, Loss: 2.5325093269348145\n",
            "Training Iteration 4740, Loss: 3.4810357093811035\n",
            "Training Iteration 4741, Loss: 4.793580532073975\n",
            "Training Iteration 4742, Loss: 3.7504749298095703\n",
            "Training Iteration 4743, Loss: 4.361459732055664\n",
            "Training Iteration 4744, Loss: 5.86102819442749\n",
            "Training Iteration 4745, Loss: 0.9431568384170532\n",
            "Training Iteration 4746, Loss: 9.673078536987305\n",
            "Training Iteration 4747, Loss: 3.1354660987854004\n",
            "Training Iteration 4748, Loss: 8.8222017288208\n",
            "Training Iteration 4749, Loss: 5.982921123504639\n",
            "Training Iteration 4750, Loss: 7.620816230773926\n",
            "Training Iteration 4751, Loss: 3.10787296295166\n",
            "Training Iteration 4752, Loss: 5.946054458618164\n",
            "Training Iteration 4753, Loss: 4.742744445800781\n",
            "Training Iteration 4754, Loss: 7.47701358795166\n",
            "Training Iteration 4755, Loss: 3.4385881423950195\n",
            "Training Iteration 4756, Loss: 7.0194244384765625\n",
            "Training Iteration 4757, Loss: 4.886225700378418\n",
            "Training Iteration 4758, Loss: 3.578749656677246\n",
            "Training Iteration 4759, Loss: 3.629368543624878\n",
            "Training Iteration 4760, Loss: 4.675076007843018\n",
            "Training Iteration 4761, Loss: 5.967169761657715\n",
            "Training Iteration 4762, Loss: 3.8787918090820312\n",
            "Training Iteration 4763, Loss: 6.28096866607666\n",
            "Training Iteration 4764, Loss: 4.822976112365723\n",
            "Training Iteration 4765, Loss: 4.931640625\n",
            "Training Iteration 4766, Loss: 4.722383499145508\n",
            "Training Iteration 4767, Loss: 3.6066036224365234\n",
            "Training Iteration 4768, Loss: 5.760908603668213\n",
            "Training Iteration 4769, Loss: 7.3359150886535645\n",
            "Training Iteration 4770, Loss: 7.001197338104248\n",
            "Training Iteration 4771, Loss: 5.292633533477783\n",
            "Training Iteration 4772, Loss: 5.24502420425415\n",
            "Training Iteration 4773, Loss: 3.256319284439087\n",
            "Training Iteration 4774, Loss: 6.0082316398620605\n",
            "Training Iteration 4775, Loss: 3.95453143119812\n",
            "Training Iteration 4776, Loss: 4.523223400115967\n",
            "Training Iteration 4777, Loss: 4.485191345214844\n",
            "Training Iteration 4778, Loss: 5.589792728424072\n",
            "Training Iteration 4779, Loss: 5.237977027893066\n",
            "Training Iteration 4780, Loss: 4.846866130828857\n",
            "Training Iteration 4781, Loss: 7.516906261444092\n",
            "Training Iteration 4782, Loss: 4.156442165374756\n",
            "Training Iteration 4783, Loss: 5.279404163360596\n",
            "Training Iteration 4784, Loss: 9.845677375793457\n",
            "Training Iteration 4785, Loss: 1.3139301538467407\n",
            "Training Iteration 4786, Loss: 3.0688741207122803\n",
            "Training Iteration 4787, Loss: 4.323094367980957\n",
            "Training Iteration 4788, Loss: 4.390279769897461\n",
            "Training Iteration 4789, Loss: 4.129398822784424\n",
            "Training Iteration 4790, Loss: 3.2082090377807617\n",
            "Training Iteration 4791, Loss: 4.740024089813232\n",
            "Training Iteration 4792, Loss: 4.076832294464111\n",
            "Training Iteration 4793, Loss: 7.05417537689209\n",
            "Training Iteration 4794, Loss: 6.0633673667907715\n",
            "Training Iteration 4795, Loss: 3.2148756980895996\n",
            "Training Iteration 4796, Loss: 4.064194202423096\n",
            "Training Iteration 4797, Loss: 7.7131428718566895\n",
            "Training Iteration 4798, Loss: 2.4930074214935303\n",
            "Training Iteration 4799, Loss: 6.127240180969238\n",
            "Training Iteration 4800, Loss: 2.6032238006591797\n",
            "Training Iteration 4801, Loss: 4.053769588470459\n",
            "Training Iteration 4802, Loss: 4.428424835205078\n",
            "Training Iteration 4803, Loss: 4.625272274017334\n",
            "Training Iteration 4804, Loss: 5.785749912261963\n",
            "Training Iteration 4805, Loss: 6.363583564758301\n",
            "Training Iteration 4806, Loss: 2.620941400527954\n",
            "Training Iteration 4807, Loss: 4.483325958251953\n",
            "Training Iteration 4808, Loss: 3.6650137901306152\n",
            "Training Iteration 4809, Loss: 3.2342517375946045\n",
            "Training Iteration 4810, Loss: 5.8936262130737305\n",
            "Training Iteration 4811, Loss: 5.880675792694092\n",
            "Training Iteration 4812, Loss: 2.78196120262146\n",
            "Training Iteration 4813, Loss: 4.536112308502197\n",
            "Training Iteration 4814, Loss: 2.997359275817871\n",
            "Training Iteration 4815, Loss: 4.570137023925781\n",
            "Training Iteration 4816, Loss: 4.204968452453613\n",
            "Training Iteration 4817, Loss: 3.0014705657958984\n",
            "Training Iteration 4818, Loss: 5.237936973571777\n",
            "Training Iteration 4819, Loss: 4.892308235168457\n",
            "Training Iteration 4820, Loss: 4.93848180770874\n",
            "Training Iteration 4821, Loss: 5.0265913009643555\n",
            "Training Iteration 4822, Loss: 5.197474002838135\n",
            "Training Iteration 4823, Loss: 6.587941646575928\n",
            "Training Iteration 4824, Loss: 7.869377136230469\n",
            "Training Iteration 4825, Loss: 4.5409111976623535\n",
            "Training Iteration 4826, Loss: 4.347531795501709\n",
            "Training Iteration 4827, Loss: 3.062811851501465\n",
            "Training Iteration 4828, Loss: 4.09128475189209\n",
            "Training Iteration 4829, Loss: 4.730580806732178\n",
            "Training Iteration 4830, Loss: 4.588596343994141\n",
            "Training Iteration 4831, Loss: 7.033761978149414\n",
            "Training Iteration 4832, Loss: 9.056499481201172\n",
            "Training Iteration 4833, Loss: 6.430782794952393\n",
            "Training Iteration 4834, Loss: 2.428222417831421\n",
            "Training Iteration 4835, Loss: 6.030238628387451\n",
            "Training Iteration 4836, Loss: 1.9908130168914795\n",
            "Training Iteration 4837, Loss: 5.673586845397949\n",
            "Training Iteration 4838, Loss: 2.987088680267334\n",
            "Training Iteration 4839, Loss: 6.458847522735596\n",
            "Training Iteration 4840, Loss: 3.644857406616211\n",
            "Training Iteration 4841, Loss: 2.1271421909332275\n",
            "Training Iteration 4842, Loss: 2.4362101554870605\n",
            "Training Iteration 4843, Loss: 3.1406795978546143\n",
            "Training Iteration 4844, Loss: 3.9884729385375977\n",
            "Training Iteration 4845, Loss: 4.427109241485596\n",
            "Training Iteration 4846, Loss: 2.010148286819458\n",
            "Training Iteration 4847, Loss: 7.12949800491333\n",
            "Training Iteration 4848, Loss: 2.6044979095458984\n",
            "Training Iteration 4849, Loss: 5.868566989898682\n",
            "Training Iteration 4850, Loss: 3.5131654739379883\n",
            "Training Iteration 4851, Loss: 6.972332000732422\n",
            "Training Iteration 4852, Loss: 1.6058655977249146\n",
            "Training Iteration 4853, Loss: 5.3674726486206055\n",
            "Training Iteration 4854, Loss: 7.582082748413086\n",
            "Training Iteration 4855, Loss: 8.394289016723633\n",
            "Training Iteration 4856, Loss: 5.855136871337891\n",
            "Training Iteration 4857, Loss: 6.047309875488281\n",
            "Training Iteration 4858, Loss: 5.206356048583984\n",
            "Training Iteration 4859, Loss: 5.5725016593933105\n",
            "Training Iteration 4860, Loss: 5.400960445404053\n",
            "Training Iteration 4861, Loss: 2.788698673248291\n",
            "Training Iteration 4862, Loss: 6.208688735961914\n",
            "Training Iteration 4863, Loss: 7.135673999786377\n",
            "Training Iteration 4864, Loss: 5.814760208129883\n",
            "Training Iteration 4865, Loss: 5.313196182250977\n",
            "Training Iteration 4866, Loss: 5.288949966430664\n",
            "Training Iteration 4867, Loss: 5.431463241577148\n",
            "Training Iteration 4868, Loss: 6.265741348266602\n",
            "Training Iteration 4869, Loss: 5.220877170562744\n",
            "Training Iteration 4870, Loss: 3.1017439365386963\n",
            "Training Iteration 4871, Loss: 7.452398300170898\n",
            "Training Iteration 4872, Loss: 4.668174743652344\n",
            "Training Iteration 4873, Loss: 8.167932510375977\n",
            "Training Iteration 4874, Loss: 6.2963175773620605\n",
            "Training Iteration 4875, Loss: 2.506031036376953\n",
            "Training Iteration 4876, Loss: 2.4479706287384033\n",
            "Training Iteration 4877, Loss: 3.759538173675537\n",
            "Training Iteration 4878, Loss: 6.979514122009277\n",
            "Training Iteration 4879, Loss: 9.365629196166992\n",
            "Training Iteration 4880, Loss: 8.038111686706543\n",
            "Training Iteration 4881, Loss: 4.165923118591309\n",
            "Training Iteration 4882, Loss: 6.997228145599365\n",
            "Training Iteration 4883, Loss: 3.655513286590576\n",
            "Training Iteration 4884, Loss: 5.215616703033447\n",
            "Training Iteration 4885, Loss: 3.6723432540893555\n",
            "Training Iteration 4886, Loss: 2.054516077041626\n",
            "Training Iteration 4887, Loss: 4.441212177276611\n",
            "Training Iteration 4888, Loss: 8.012578010559082\n",
            "Training Iteration 4889, Loss: 4.038896560668945\n",
            "Training Iteration 4890, Loss: 4.2185282707214355\n",
            "Training Iteration 4891, Loss: 4.317447662353516\n",
            "Training Iteration 4892, Loss: 4.530297756195068\n",
            "Training Iteration 4893, Loss: 3.456678628921509\n",
            "Training Iteration 4894, Loss: 2.616682529449463\n",
            "Training Iteration 4895, Loss: 5.278371810913086\n",
            "Training Iteration 4896, Loss: 4.159998416900635\n",
            "Training Iteration 4897, Loss: 3.668557643890381\n",
            "Training Iteration 4898, Loss: 6.140438556671143\n",
            "Training Iteration 4899, Loss: 2.63033390045166\n",
            "Training Iteration 4900, Loss: 4.433973789215088\n",
            "Training Iteration 4901, Loss: 6.228695869445801\n",
            "Training Iteration 4902, Loss: 4.878007888793945\n",
            "Training Iteration 4903, Loss: 3.039555549621582\n",
            "Training Iteration 4904, Loss: 4.578719615936279\n",
            "Training Iteration 4905, Loss: 2.708181858062744\n",
            "Training Iteration 4906, Loss: 3.6166110038757324\n",
            "Training Iteration 4907, Loss: 3.6859323978424072\n",
            "Training Iteration 4908, Loss: 3.0771946907043457\n",
            "Training Iteration 4909, Loss: 3.9856483936309814\n",
            "Training Iteration 4910, Loss: 2.473579168319702\n",
            "Training Iteration 4911, Loss: 3.4818403720855713\n",
            "Training Iteration 4912, Loss: 3.540815591812134\n",
            "Training Iteration 4913, Loss: 2.8112947940826416\n",
            "Training Iteration 4914, Loss: 2.21384334564209\n",
            "Training Iteration 4915, Loss: 4.6289849281311035\n",
            "Training Iteration 4916, Loss: 5.596678733825684\n",
            "Training Iteration 4917, Loss: 2.9738962650299072\n",
            "Training Iteration 4918, Loss: 3.6155753135681152\n",
            "Training Iteration 4919, Loss: 4.407817840576172\n",
            "Training Iteration 4920, Loss: 4.109415054321289\n",
            "Training Iteration 4921, Loss: 6.551260471343994\n",
            "Training Iteration 4922, Loss: 3.661146640777588\n",
            "Training Iteration 4923, Loss: 2.9020512104034424\n",
            "Training Iteration 4924, Loss: 2.9063522815704346\n",
            "Training Iteration 4925, Loss: 2.2190866470336914\n",
            "Training Iteration 4926, Loss: 4.689358234405518\n",
            "Training Iteration 4927, Loss: 5.214374542236328\n",
            "Training Iteration 4928, Loss: 5.5907511711120605\n",
            "Training Iteration 4929, Loss: 4.3785834312438965\n",
            "Training Iteration 4930, Loss: 7.546151638031006\n",
            "Training Iteration 4931, Loss: 7.179487228393555\n",
            "Training Iteration 4932, Loss: 4.392236709594727\n",
            "Training Iteration 4933, Loss: 5.3113813400268555\n",
            "Training Iteration 4934, Loss: 4.03854513168335\n",
            "Training Iteration 4935, Loss: 2.0214450359344482\n",
            "Training Iteration 4936, Loss: 3.7506039142608643\n",
            "Training Iteration 4937, Loss: 7.539381504058838\n",
            "Training Iteration 4938, Loss: 5.390204906463623\n",
            "Training Iteration 4939, Loss: 3.3170325756073\n",
            "Training Iteration 4940, Loss: 9.482032775878906\n",
            "Training Iteration 4941, Loss: 4.709847450256348\n",
            "Training Iteration 4942, Loss: 3.563640832901001\n",
            "Training Iteration 4943, Loss: 8.656660079956055\n",
            "Training Iteration 4944, Loss: 5.431336402893066\n",
            "Training Iteration 4945, Loss: 4.553215026855469\n",
            "Training Iteration 4946, Loss: 5.003568649291992\n",
            "Training Iteration 4947, Loss: 2.840359687805176\n",
            "Training Iteration 4948, Loss: 7.462252616882324\n",
            "Training Iteration 4949, Loss: 7.214665412902832\n",
            "Training Iteration 4950, Loss: 5.215794563293457\n",
            "Training Iteration 4951, Loss: 3.8935346603393555\n",
            "Training Iteration 4952, Loss: 4.311825752258301\n",
            "Training Iteration 4953, Loss: 11.003958702087402\n",
            "Training Iteration 4954, Loss: 5.777570724487305\n",
            "Training Iteration 4955, Loss: 4.049312114715576\n",
            "Training Iteration 4956, Loss: 10.169073104858398\n",
            "Training Iteration 4957, Loss: 5.64758825302124\n",
            "Training Iteration 4958, Loss: 7.958043575286865\n",
            "Training Iteration 4959, Loss: 3.373563766479492\n",
            "Training Iteration 4960, Loss: 4.321604251861572\n",
            "Training Iteration 4961, Loss: 5.295020580291748\n",
            "Training Iteration 4962, Loss: 10.716766357421875\n",
            "Training Iteration 4963, Loss: 7.940776348114014\n",
            "Training Iteration 4964, Loss: 8.477527618408203\n",
            "Training Iteration 4965, Loss: 3.145905017852783\n",
            "Training Iteration 4966, Loss: 5.2236328125\n",
            "Training Iteration 4967, Loss: 5.364527225494385\n",
            "Training Iteration 4968, Loss: 7.4654059410095215\n",
            "Training Iteration 4969, Loss: 3.04781436920166\n",
            "Training Iteration 4970, Loss: 4.918767929077148\n",
            "Training Iteration 4971, Loss: 3.1988472938537598\n",
            "Training Iteration 4972, Loss: 4.6262969970703125\n",
            "Training Iteration 4973, Loss: 4.505781173706055\n",
            "Training Iteration 4974, Loss: 7.8177103996276855\n",
            "Training Iteration 4975, Loss: 10.805879592895508\n",
            "Training Iteration 4976, Loss: 10.013503074645996\n",
            "Training Iteration 4977, Loss: 5.262752532958984\n",
            "Training Iteration 4978, Loss: 2.644273281097412\n",
            "Training Iteration 4979, Loss: 2.2250540256500244\n",
            "Training Iteration 4980, Loss: 4.124409198760986\n",
            "Training Iteration 4981, Loss: 6.071100234985352\n",
            "Training Iteration 4982, Loss: 5.9348015785217285\n",
            "Training Iteration 4983, Loss: 5.2572455406188965\n",
            "Training Iteration 4984, Loss: 7.764845371246338\n",
            "Training Iteration 4985, Loss: 5.1947407722473145\n",
            "Training Iteration 4986, Loss: 4.5240960121154785\n",
            "Training Iteration 4987, Loss: 3.2347335815429688\n",
            "Training Iteration 4988, Loss: 1.214967966079712\n",
            "Training Iteration 4989, Loss: 4.039005756378174\n",
            "Training Iteration 4990, Loss: 5.013893127441406\n",
            "Training Iteration 4991, Loss: 2.666329860687256\n",
            "Training Iteration 4992, Loss: 1.9963939189910889\n",
            "Training Iteration 4993, Loss: 5.315183162689209\n",
            "Training Iteration 4994, Loss: 5.039118766784668\n",
            "Training Iteration 4995, Loss: 1.5844964981079102\n",
            "Training Iteration 4996, Loss: 3.945322275161743\n",
            "Training Iteration 4997, Loss: 4.082453727722168\n",
            "Training Iteration 4998, Loss: 4.664679050445557\n",
            "Training Iteration 4999, Loss: 4.84303617477417\n",
            "Training Iteration 5000, Loss: 4.913236618041992\n",
            "Training Iteration 5001, Loss: 2.4487555027008057\n",
            "Training Iteration 5002, Loss: 5.997868537902832\n",
            "Training Iteration 5003, Loss: 5.619700908660889\n",
            "Training Iteration 5004, Loss: 4.821193218231201\n",
            "Training Iteration 5005, Loss: 5.209843635559082\n",
            "Training Iteration 5006, Loss: 4.790350914001465\n",
            "Training Iteration 5007, Loss: 5.088308811187744\n",
            "Training Iteration 5008, Loss: 6.001327991485596\n",
            "Training Iteration 5009, Loss: 7.325321197509766\n",
            "Training Iteration 5010, Loss: 4.774071216583252\n",
            "Training Iteration 5011, Loss: 3.0798158645629883\n",
            "Training Iteration 5012, Loss: 3.970745325088501\n",
            "Training Iteration 5013, Loss: 3.6227779388427734\n",
            "Training Iteration 5014, Loss: 4.278495788574219\n",
            "Training Iteration 5015, Loss: 2.6294238567352295\n",
            "Training Iteration 5016, Loss: 4.636977672576904\n",
            "Training Iteration 5017, Loss: 4.167346477508545\n",
            "Training Iteration 5018, Loss: 3.393205404281616\n",
            "Training Iteration 5019, Loss: 4.732980728149414\n",
            "Training Iteration 5020, Loss: 6.910333156585693\n",
            "Training Iteration 5021, Loss: 6.001254081726074\n",
            "Training Iteration 5022, Loss: 5.2133283615112305\n",
            "Training Iteration 5023, Loss: 2.168788433074951\n",
            "Training Iteration 5024, Loss: 4.869074821472168\n",
            "Training Iteration 5025, Loss: 3.997865676879883\n",
            "Training Iteration 5026, Loss: 7.9548115730285645\n",
            "Training Iteration 5027, Loss: 5.595476150512695\n",
            "Training Iteration 5028, Loss: 5.285457611083984\n",
            "Training Iteration 5029, Loss: 3.3980584144592285\n",
            "Training Iteration 5030, Loss: 5.895791053771973\n",
            "Training Iteration 5031, Loss: 5.03457498550415\n",
            "Training Iteration 5032, Loss: 6.632211685180664\n",
            "Training Iteration 5033, Loss: 3.172246217727661\n",
            "Training Iteration 5034, Loss: 3.152772903442383\n",
            "Training Iteration 5035, Loss: 7.613043308258057\n",
            "Training Iteration 5036, Loss: 2.353813648223877\n",
            "Training Iteration 5037, Loss: 2.692762613296509\n",
            "Training Iteration 5038, Loss: 3.699958086013794\n",
            "Training Iteration 5039, Loss: 5.533990859985352\n",
            "Training Iteration 5040, Loss: 4.4859490394592285\n",
            "Training Iteration 5041, Loss: 3.4816548824310303\n",
            "Training Iteration 5042, Loss: 2.6447553634643555\n",
            "Training Iteration 5043, Loss: 5.917080402374268\n",
            "Training Iteration 5044, Loss: 4.516448020935059\n",
            "Training Iteration 5045, Loss: 6.779980659484863\n",
            "Training Iteration 5046, Loss: 4.968337535858154\n",
            "Training Iteration 5047, Loss: 5.464994430541992\n",
            "Training Iteration 5048, Loss: 3.25596284866333\n",
            "Training Iteration 5049, Loss: 9.784896850585938\n",
            "Training Iteration 5050, Loss: 4.366174221038818\n",
            "Training Iteration 5051, Loss: 4.515042304992676\n",
            "Training Iteration 5052, Loss: 7.12229061126709\n",
            "Training Iteration 5053, Loss: 4.3574323654174805\n",
            "Training Iteration 5054, Loss: 4.534964084625244\n",
            "Training Iteration 5055, Loss: 16.458951950073242\n",
            "Training Iteration 5056, Loss: 4.188069820404053\n",
            "Training Iteration 5057, Loss: 5.078366756439209\n",
            "Training Iteration 5058, Loss: 3.536618947982788\n",
            "Training Iteration 5059, Loss: 7.267205238342285\n",
            "Training Iteration 5060, Loss: 3.935117483139038\n",
            "Training Iteration 5061, Loss: 3.0710628032684326\n",
            "Training Iteration 5062, Loss: 2.7653396129608154\n",
            "Training Iteration 5063, Loss: 4.242303848266602\n",
            "Training Iteration 5064, Loss: 3.9804725646972656\n",
            "Training Iteration 5065, Loss: 3.398801803588867\n",
            "Training Iteration 5066, Loss: 2.3708159923553467\n",
            "Training Iteration 5067, Loss: 11.273904800415039\n",
            "Training Iteration 5068, Loss: 0.5830408930778503\n",
            "Training Iteration 5069, Loss: 4.186129570007324\n",
            "Training Iteration 5070, Loss: 3.6486716270446777\n",
            "Training Iteration 5071, Loss: 2.243187427520752\n",
            "Training Iteration 5072, Loss: 3.5770974159240723\n",
            "Training Iteration 5073, Loss: 5.342731475830078\n",
            "Training Iteration 5074, Loss: 2.701897382736206\n",
            "Training Iteration 5075, Loss: 2.2376623153686523\n",
            "Training Iteration 5076, Loss: 5.922817230224609\n",
            "Training Iteration 5077, Loss: 7.772404193878174\n",
            "Training Iteration 5078, Loss: 6.372254848480225\n",
            "Training Iteration 5079, Loss: 3.5245561599731445\n",
            "Training Iteration 5080, Loss: 3.678250312805176\n",
            "Training Iteration 5081, Loss: 2.7874553203582764\n",
            "Training Iteration 5082, Loss: 4.009685039520264\n",
            "Training Iteration 5083, Loss: 2.5420820713043213\n",
            "Training Iteration 5084, Loss: 2.9985527992248535\n",
            "Training Iteration 5085, Loss: 3.6019325256347656\n",
            "Training Iteration 5086, Loss: 6.393941879272461\n",
            "Training Iteration 5087, Loss: 4.451446533203125\n",
            "Training Iteration 5088, Loss: 2.2777490615844727\n",
            "Training Iteration 5089, Loss: 5.256840705871582\n",
            "Training Iteration 5090, Loss: 6.289065361022949\n",
            "Training Iteration 5091, Loss: 6.350464820861816\n",
            "Training Iteration 5092, Loss: 4.283755779266357\n",
            "Training Iteration 5093, Loss: 2.717466115951538\n",
            "Training Iteration 5094, Loss: 6.663569450378418\n",
            "Training Iteration 5095, Loss: 4.891323566436768\n",
            "Training Iteration 5096, Loss: 4.0832624435424805\n",
            "Training Iteration 5097, Loss: 6.162184715270996\n",
            "Training Iteration 5098, Loss: 4.761768341064453\n",
            "Training Iteration 5099, Loss: 5.92356014251709\n",
            "Training Iteration 5100, Loss: 3.773371934890747\n",
            "Training Iteration 5101, Loss: 5.9921698570251465\n",
            "Training Iteration 5102, Loss: 1.706958293914795\n",
            "Training Iteration 5103, Loss: 6.662481307983398\n",
            "Training Iteration 5104, Loss: 6.343567848205566\n",
            "Training Iteration 5105, Loss: 4.664866924285889\n",
            "Training Iteration 5106, Loss: 5.599172115325928\n",
            "Training Iteration 5107, Loss: 4.266294479370117\n",
            "Training Iteration 5108, Loss: 4.881659030914307\n",
            "Training Iteration 5109, Loss: 2.5592353343963623\n",
            "Training Iteration 5110, Loss: 3.4909615516662598\n",
            "Training Iteration 5111, Loss: 2.63092303276062\n",
            "Training Iteration 5112, Loss: 4.080808162689209\n",
            "Training Iteration 5113, Loss: 7.784335136413574\n",
            "Training Iteration 5114, Loss: 6.047972679138184\n",
            "Training Iteration 5115, Loss: 3.206212282180786\n",
            "Training Iteration 5116, Loss: 5.315312385559082\n",
            "Training Iteration 5117, Loss: 3.4377830028533936\n",
            "Training Iteration 5118, Loss: 5.051797866821289\n",
            "Training Iteration 5119, Loss: 3.1674861907958984\n",
            "Training Iteration 5120, Loss: 5.486666679382324\n",
            "Training Iteration 5121, Loss: 6.845003604888916\n",
            "Training Iteration 5122, Loss: 4.776901721954346\n",
            "Training Iteration 5123, Loss: 4.75723934173584\n",
            "Training Iteration 5124, Loss: 3.11643648147583\n",
            "Training Iteration 5125, Loss: 5.509744644165039\n",
            "Training Iteration 5126, Loss: 4.040700435638428\n",
            "Training Iteration 5127, Loss: 2.7058937549591064\n",
            "Training Iteration 5128, Loss: 1.9562747478485107\n",
            "Training Iteration 5129, Loss: 1.9165666103363037\n",
            "Training Iteration 5130, Loss: 5.2016825675964355\n",
            "Training Iteration 5131, Loss: 3.8602681159973145\n",
            "Training Iteration 5132, Loss: 2.9825191497802734\n",
            "Training Iteration 5133, Loss: 3.1238512992858887\n",
            "Training Iteration 5134, Loss: 3.688889741897583\n",
            "Training Iteration 5135, Loss: 2.797433853149414\n",
            "Training Iteration 5136, Loss: 4.179771900177002\n",
            "Training Iteration 5137, Loss: 10.582479476928711\n",
            "Training Iteration 5138, Loss: 6.272006988525391\n",
            "Training Iteration 5139, Loss: 5.13375997543335\n",
            "Training Iteration 5140, Loss: 4.23025369644165\n",
            "Training Iteration 5141, Loss: 5.321364402770996\n",
            "Training Iteration 5142, Loss: 4.082372188568115\n",
            "Training Iteration 5143, Loss: 1.7800748348236084\n",
            "Training Iteration 5144, Loss: 3.288632869720459\n",
            "Training Iteration 5145, Loss: 6.033060073852539\n",
            "Training Iteration 5146, Loss: 2.5805420875549316\n",
            "Training Iteration 5147, Loss: 2.4167943000793457\n",
            "Training Iteration 5148, Loss: 3.8179378509521484\n",
            "Training Iteration 5149, Loss: 3.400043249130249\n",
            "Training Iteration 5150, Loss: 3.7116010189056396\n",
            "Training Iteration 5151, Loss: 6.185530662536621\n",
            "Training Iteration 5152, Loss: 6.341181755065918\n",
            "Training Iteration 5153, Loss: 4.5932464599609375\n",
            "Training Iteration 5154, Loss: 3.9649481773376465\n",
            "Training Iteration 5155, Loss: 5.627364635467529\n",
            "Training Iteration 5156, Loss: 3.770012378692627\n",
            "Training Iteration 5157, Loss: 3.2759323120117188\n",
            "Training Iteration 5158, Loss: 3.8662474155426025\n",
            "Training Iteration 5159, Loss: 3.769712448120117\n",
            "Training Iteration 5160, Loss: 3.984619140625\n",
            "Training Iteration 5161, Loss: 5.336169719696045\n",
            "Training Iteration 5162, Loss: 2.9187350273132324\n",
            "Training Iteration 5163, Loss: 1.7069461345672607\n",
            "Training Iteration 5164, Loss: 3.7213220596313477\n",
            "Training Iteration 5165, Loss: 3.647467613220215\n",
            "Training Iteration 5166, Loss: 6.248381614685059\n",
            "Training Iteration 5167, Loss: 3.3994460105895996\n",
            "Training Iteration 5168, Loss: 4.550605773925781\n",
            "Training Iteration 5169, Loss: 5.830094814300537\n",
            "Training Iteration 5170, Loss: 2.5166103839874268\n",
            "Training Iteration 5171, Loss: 4.576986312866211\n",
            "Training Iteration 5172, Loss: 2.210024356842041\n",
            "Training Iteration 5173, Loss: 3.6145405769348145\n",
            "Training Iteration 5174, Loss: 2.2417550086975098\n",
            "Training Iteration 5175, Loss: 3.1073105335235596\n",
            "Training Iteration 5176, Loss: 3.526968240737915\n",
            "Training Iteration 5177, Loss: 5.9020538330078125\n",
            "Training Iteration 5178, Loss: 2.8524398803710938\n",
            "Training Iteration 5179, Loss: 4.3337321281433105\n",
            "Training Iteration 5180, Loss: 8.896242141723633\n",
            "Training Iteration 5181, Loss: 5.655282974243164\n",
            "Training Iteration 5182, Loss: 4.295764923095703\n",
            "Training Iteration 5183, Loss: 3.089353084564209\n",
            "Training Iteration 5184, Loss: 3.3574085235595703\n",
            "Training Iteration 5185, Loss: 8.229312896728516\n",
            "Training Iteration 5186, Loss: 5.774667263031006\n",
            "Training Iteration 5187, Loss: 5.39340353012085\n",
            "Training Iteration 5188, Loss: 8.69821834564209\n",
            "Training Iteration 5189, Loss: 7.129900932312012\n",
            "Training Iteration 5190, Loss: 6.0218071937561035\n",
            "Training Iteration 5191, Loss: 4.349001884460449\n",
            "Training Iteration 5192, Loss: 5.230325698852539\n",
            "Training Iteration 5193, Loss: 3.074030876159668\n",
            "Training Iteration 5194, Loss: 3.2365288734436035\n",
            "Training Iteration 5195, Loss: 7.58491849899292\n",
            "Training Iteration 5196, Loss: 3.699737071990967\n",
            "Training Iteration 5197, Loss: 3.224848747253418\n",
            "Training Iteration 5198, Loss: 7.12465763092041\n",
            "Training Iteration 5199, Loss: 6.267388343811035\n",
            "Training Iteration 5200, Loss: 3.8219692707061768\n",
            "Training Iteration 5201, Loss: 8.584287643432617\n",
            "Training Iteration 5202, Loss: 4.027061939239502\n",
            "Training Iteration 5203, Loss: 4.416711807250977\n",
            "Training Iteration 5204, Loss: 5.117714881896973\n",
            "Training Iteration 5205, Loss: 3.241055488586426\n",
            "Training Iteration 5206, Loss: 1.9257538318634033\n",
            "Training Iteration 5207, Loss: 5.357605934143066\n",
            "Training Iteration 5208, Loss: 3.2361059188842773\n",
            "Training Iteration 5209, Loss: 4.159312725067139\n",
            "Training Iteration 5210, Loss: 4.698056221008301\n",
            "Training Iteration 5211, Loss: 7.522192001342773\n",
            "Training Iteration 5212, Loss: 5.144216537475586\n",
            "Training Iteration 5213, Loss: 4.971128940582275\n",
            "Training Iteration 5214, Loss: 2.2651827335357666\n",
            "Training Iteration 5215, Loss: 4.416309356689453\n",
            "Training Iteration 5216, Loss: 4.790979862213135\n",
            "Training Iteration 5217, Loss: 3.5223464965820312\n",
            "Training Iteration 5218, Loss: 8.088537216186523\n",
            "Training Iteration 5219, Loss: 6.781869888305664\n",
            "Training Iteration 5220, Loss: 6.2486138343811035\n",
            "Training Iteration 5221, Loss: 4.559824466705322\n",
            "Training Iteration 5222, Loss: 3.251708745956421\n",
            "Training Iteration 5223, Loss: 4.338583469390869\n",
            "Training Iteration 5224, Loss: 7.511441230773926\n",
            "Training Iteration 5225, Loss: 4.681588172912598\n",
            "Training Iteration 5226, Loss: 2.8497977256774902\n",
            "Training Iteration 5227, Loss: 5.153749942779541\n",
            "Training Iteration 5228, Loss: 5.365016937255859\n",
            "Training Iteration 5229, Loss: 6.066036224365234\n",
            "Training Iteration 5230, Loss: 6.383523941040039\n",
            "Training Iteration 5231, Loss: 5.724048137664795\n",
            "Training Iteration 5232, Loss: 4.165942668914795\n",
            "Training Iteration 5233, Loss: 3.6381800174713135\n",
            "Training Iteration 5234, Loss: 5.089890480041504\n",
            "Training Iteration 5235, Loss: 5.410270690917969\n",
            "Training Iteration 5236, Loss: 2.190866231918335\n",
            "Training Iteration 5237, Loss: 3.403308153152466\n",
            "Training Iteration 5238, Loss: 3.17557954788208\n",
            "Training Iteration 5239, Loss: 4.758671283721924\n",
            "Training Iteration 5240, Loss: 5.3296895027160645\n",
            "Training Iteration 5241, Loss: 5.694540500640869\n",
            "Training Iteration 5242, Loss: 6.685786247253418\n",
            "Training Iteration 5243, Loss: 2.956631660461426\n",
            "Training Iteration 5244, Loss: 3.1791930198669434\n",
            "Training Iteration 5245, Loss: 3.784736156463623\n",
            "Training Iteration 5246, Loss: 3.938459873199463\n",
            "Training Iteration 5247, Loss: 6.819401264190674\n",
            "Training Iteration 5248, Loss: 8.319478988647461\n",
            "Training Iteration 5249, Loss: 4.6292643547058105\n",
            "Training Iteration 5250, Loss: 3.121704339981079\n",
            "Training Iteration 5251, Loss: 4.256161212921143\n",
            "Training Iteration 5252, Loss: 4.045979022979736\n",
            "Training Iteration 5253, Loss: 2.7755935192108154\n",
            "Training Iteration 5254, Loss: 4.278695106506348\n",
            "Training Iteration 5255, Loss: 2.8050365447998047\n",
            "Training Iteration 5256, Loss: 7.053333759307861\n",
            "Training Iteration 5257, Loss: 6.109282493591309\n",
            "Training Iteration 5258, Loss: 6.689515113830566\n",
            "Training Iteration 5259, Loss: 4.634616374969482\n",
            "Training Iteration 5260, Loss: 3.610598564147949\n",
            "Training Iteration 5261, Loss: 4.012084484100342\n",
            "Training Iteration 5262, Loss: 7.8451128005981445\n",
            "Training Iteration 5263, Loss: 4.974918842315674\n",
            "Training Iteration 5264, Loss: 5.210679054260254\n",
            "Training Iteration 5265, Loss: 6.0088982582092285\n",
            "Training Iteration 5266, Loss: 2.4714815616607666\n",
            "Training Iteration 5267, Loss: 3.917423725128174\n",
            "Training Iteration 5268, Loss: 7.865168571472168\n",
            "Training Iteration 5269, Loss: 4.223733901977539\n",
            "Training Iteration 5270, Loss: 4.1339826583862305\n",
            "Training Iteration 5271, Loss: 3.145719528198242\n",
            "Training Iteration 5272, Loss: 2.3366522789001465\n",
            "Training Iteration 5273, Loss: 2.3638620376586914\n",
            "Training Iteration 5274, Loss: 5.12376070022583\n",
            "Training Iteration 5275, Loss: 4.92449426651001\n",
            "Training Iteration 5276, Loss: 5.620573043823242\n",
            "Training Iteration 5277, Loss: 6.772047996520996\n",
            "Training Iteration 5278, Loss: 2.9485018253326416\n",
            "Training Iteration 5279, Loss: 3.2251570224761963\n",
            "Training Iteration 5280, Loss: 3.7610726356506348\n",
            "Training Iteration 5281, Loss: 3.15460205078125\n",
            "Training Iteration 5282, Loss: 5.004538059234619\n",
            "Training Iteration 5283, Loss: 2.0006446838378906\n",
            "Training Iteration 5284, Loss: 6.515498161315918\n",
            "Training Iteration 5285, Loss: 7.373507022857666\n",
            "Training Iteration 5286, Loss: 5.530170440673828\n",
            "Training Iteration 5287, Loss: 5.446969509124756\n",
            "Training Iteration 5288, Loss: 7.087531089782715\n",
            "Training Iteration 5289, Loss: 3.7962570190429688\n",
            "Training Iteration 5290, Loss: 6.408982276916504\n",
            "Training Iteration 5291, Loss: 9.520831108093262\n",
            "Training Iteration 5292, Loss: 4.333940029144287\n",
            "Training Iteration 5293, Loss: 5.6710429191589355\n",
            "Training Iteration 5294, Loss: 2.7395544052124023\n",
            "Training Iteration 5295, Loss: 4.958455562591553\n",
            "Training Iteration 5296, Loss: 5.063769817352295\n",
            "Training Iteration 5297, Loss: 7.075506210327148\n",
            "Training Iteration 5298, Loss: 10.719837188720703\n",
            "Training Iteration 5299, Loss: 7.159264087677002\n",
            "Training Iteration 5300, Loss: 5.464504241943359\n",
            "Training Iteration 5301, Loss: 5.634544372558594\n",
            "Training Iteration 5302, Loss: 3.2435412406921387\n",
            "Training Iteration 5303, Loss: 3.52339243888855\n",
            "Training Iteration 5304, Loss: 4.544117450714111\n",
            "Training Iteration 5305, Loss: 5.458388328552246\n",
            "Training Iteration 5306, Loss: 4.804616451263428\n",
            "Training Iteration 5307, Loss: 5.79093074798584\n",
            "Training Iteration 5308, Loss: 4.764801502227783\n",
            "Training Iteration 5309, Loss: 3.278160810470581\n",
            "Training Iteration 5310, Loss: 9.684374809265137\n",
            "Training Iteration 5311, Loss: 4.178971290588379\n",
            "Training Iteration 5312, Loss: 5.329761028289795\n",
            "Training Iteration 5313, Loss: 8.0777006149292\n",
            "Training Iteration 5314, Loss: 4.0053324699401855\n",
            "Training Iteration 5315, Loss: 3.048058032989502\n",
            "Training Iteration 5316, Loss: 5.550730228424072\n",
            "Training Iteration 5317, Loss: 6.6407036781311035\n",
            "Training Iteration 5318, Loss: 4.672275543212891\n",
            "Training Iteration 5319, Loss: 4.014775276184082\n",
            "Training Iteration 5320, Loss: 2.1724255084991455\n",
            "Training Iteration 5321, Loss: 2.7011404037475586\n",
            "Training Iteration 5322, Loss: 3.7167110443115234\n",
            "Training Iteration 5323, Loss: 3.7972536087036133\n",
            "Training Iteration 5324, Loss: 3.1645748615264893\n",
            "Training Iteration 5325, Loss: 1.8007240295410156\n",
            "Training Iteration 5326, Loss: 2.44378399848938\n",
            "Training Iteration 5327, Loss: 7.5558648109436035\n",
            "Training Iteration 5328, Loss: 1.66452157497406\n",
            "Training Iteration 5329, Loss: 4.388021469116211\n",
            "Training Iteration 5330, Loss: 2.114633083343506\n",
            "Training Iteration 5331, Loss: 4.5818562507629395\n",
            "Training Iteration 5332, Loss: 3.1605823040008545\n",
            "Training Iteration 5333, Loss: 2.5485639572143555\n",
            "Training Iteration 5334, Loss: 3.4161629676818848\n",
            "Training Iteration 5335, Loss: 3.755659580230713\n",
            "Training Iteration 5336, Loss: 6.358125686645508\n",
            "Training Iteration 5337, Loss: 2.8298583030700684\n",
            "Training Iteration 5338, Loss: 4.141921043395996\n",
            "Training Iteration 5339, Loss: 4.570388317108154\n",
            "Training Iteration 5340, Loss: 3.6260151863098145\n",
            "Training Iteration 5341, Loss: 4.053462028503418\n",
            "Training Iteration 5342, Loss: 2.7340164184570312\n",
            "Training Iteration 5343, Loss: 2.8636536598205566\n",
            "Training Iteration 5344, Loss: 5.6457672119140625\n",
            "Training Iteration 5345, Loss: 4.984640598297119\n",
            "Training Iteration 5346, Loss: 3.4372963905334473\n",
            "Training Iteration 5347, Loss: 3.6903326511383057\n",
            "Training Iteration 5348, Loss: 3.4003286361694336\n",
            "Training Iteration 5349, Loss: 5.294333457946777\n",
            "Training Iteration 5350, Loss: 3.157956123352051\n",
            "Training Iteration 5351, Loss: 8.7227201461792\n",
            "Training Iteration 5352, Loss: 4.668844223022461\n",
            "Training Iteration 5353, Loss: 6.521824359893799\n",
            "Training Iteration 5354, Loss: 4.008127212524414\n",
            "Training Iteration 5355, Loss: 2.7826106548309326\n",
            "Training Iteration 5356, Loss: 4.559308052062988\n",
            "Training Iteration 5357, Loss: 3.4519526958465576\n",
            "Training Iteration 5358, Loss: 5.9520039558410645\n",
            "Training Iteration 5359, Loss: 7.507610321044922\n",
            "Training Iteration 5360, Loss: 6.686795711517334\n",
            "Training Iteration 5361, Loss: 6.660647392272949\n",
            "Training Iteration 5362, Loss: 6.16486120223999\n",
            "Training Iteration 5363, Loss: 3.8926663398742676\n",
            "Training Iteration 5364, Loss: 4.208227634429932\n",
            "Training Iteration 5365, Loss: 4.2874860763549805\n",
            "Training Iteration 5366, Loss: 2.8881468772888184\n",
            "Training Iteration 5367, Loss: 5.232685089111328\n",
            "Training Iteration 5368, Loss: 3.2060916423797607\n",
            "Training Iteration 5369, Loss: 3.266258716583252\n",
            "Training Iteration 5370, Loss: 4.190958499908447\n",
            "Training Iteration 5371, Loss: 4.009858131408691\n",
            "Training Iteration 5372, Loss: 2.5686028003692627\n",
            "Training Iteration 5373, Loss: 3.826394557952881\n",
            "Training Iteration 5374, Loss: 5.7691569328308105\n",
            "Training Iteration 5375, Loss: 3.719694137573242\n",
            "Training Iteration 5376, Loss: 6.74276876449585\n",
            "Training Iteration 5377, Loss: 3.456944465637207\n",
            "Training Iteration 5378, Loss: 3.3053762912750244\n",
            "Training Iteration 5379, Loss: 6.323609352111816\n",
            "Training Iteration 5380, Loss: 2.177020788192749\n",
            "Training Iteration 5381, Loss: 2.3046693801879883\n",
            "Training Iteration 5382, Loss: 1.81270432472229\n",
            "Training Iteration 5383, Loss: 3.9166088104248047\n",
            "Training Iteration 5384, Loss: 4.221590995788574\n",
            "Training Iteration 5385, Loss: 6.1273956298828125\n",
            "Training Iteration 5386, Loss: 6.980493068695068\n",
            "Training Iteration 5387, Loss: 4.264463424682617\n",
            "Training Iteration 5388, Loss: 2.9328718185424805\n",
            "Training Iteration 5389, Loss: 8.152688980102539\n",
            "Training Iteration 5390, Loss: 7.102682113647461\n",
            "Training Iteration 5391, Loss: 6.677077293395996\n",
            "Training Iteration 5392, Loss: 7.130121231079102\n",
            "Training Iteration 5393, Loss: 6.8016252517700195\n",
            "Training Iteration 5394, Loss: 6.36118745803833\n",
            "Training Iteration 5395, Loss: 2.5080342292785645\n",
            "Training Iteration 5396, Loss: 6.984267234802246\n",
            "Training Iteration 5397, Loss: 6.845800876617432\n",
            "Training Iteration 5398, Loss: 5.505730628967285\n",
            "Training Iteration 5399, Loss: 3.0251758098602295\n",
            "Training Iteration 5400, Loss: 2.1758313179016113\n",
            "Training Iteration 5401, Loss: 4.465927600860596\n",
            "Training Iteration 5402, Loss: 5.708725929260254\n",
            "Training Iteration 5403, Loss: 3.9676671028137207\n",
            "Training Iteration 5404, Loss: 1.708155870437622\n",
            "Training Iteration 5405, Loss: 4.386228084564209\n",
            "Training Iteration 5406, Loss: 2.1596579551696777\n",
            "Training Iteration 5407, Loss: 2.8573150634765625\n",
            "Training Iteration 5408, Loss: 3.4064955711364746\n",
            "Training Iteration 5409, Loss: 3.3746654987335205\n",
            "Training Iteration 5410, Loss: 2.747512102127075\n",
            "Training Iteration 5411, Loss: 5.1866068840026855\n",
            "Training Iteration 5412, Loss: 3.6346538066864014\n",
            "Training Iteration 5413, Loss: 5.213831901550293\n",
            "Training Iteration 5414, Loss: 4.888174057006836\n",
            "Training Iteration 5415, Loss: 2.284799098968506\n",
            "Training Iteration 5416, Loss: 4.401841640472412\n",
            "Training Iteration 5417, Loss: 2.2946789264678955\n",
            "Training Iteration 5418, Loss: 4.984218120574951\n",
            "Training Iteration 5419, Loss: 6.157957077026367\n",
            "Training Iteration 5420, Loss: 5.291892051696777\n",
            "Training Iteration 5421, Loss: 2.4020607471466064\n",
            "Training Iteration 5422, Loss: 5.562790393829346\n",
            "Training Iteration 5423, Loss: 3.8775877952575684\n",
            "Training Iteration 5424, Loss: 5.965904235839844\n",
            "Training Iteration 5425, Loss: 2.6959176063537598\n",
            "Training Iteration 5426, Loss: 2.5803866386413574\n",
            "Training Iteration 5427, Loss: 1.8069034814834595\n",
            "Training Iteration 5428, Loss: 5.439979553222656\n",
            "Training Iteration 5429, Loss: 4.6258087158203125\n",
            "Training Iteration 5430, Loss: 2.545891046524048\n",
            "Training Iteration 5431, Loss: 6.39946174621582\n",
            "Training Iteration 5432, Loss: 7.053238391876221\n",
            "Training Iteration 5433, Loss: 4.35451078414917\n",
            "Training Iteration 5434, Loss: 4.696099281311035\n",
            "Training Iteration 5435, Loss: 4.016549110412598\n",
            "Training Iteration 5436, Loss: 6.798459529876709\n",
            "Training Iteration 5437, Loss: 5.241028785705566\n",
            "Training Iteration 5438, Loss: 3.718661308288574\n",
            "Training Iteration 5439, Loss: 6.549773216247559\n",
            "Training Iteration 5440, Loss: 1.511184573173523\n",
            "Training Iteration 5441, Loss: 6.065277576446533\n",
            "Training Iteration 5442, Loss: 2.311771869659424\n",
            "Training Iteration 5443, Loss: 2.7583980560302734\n",
            "Training Iteration 5444, Loss: 6.545572757720947\n",
            "Training Iteration 5445, Loss: 4.0748515129089355\n",
            "Training Iteration 5446, Loss: 2.2392325401306152\n",
            "Training Iteration 5447, Loss: 5.537642478942871\n",
            "Training Iteration 5448, Loss: 6.209444999694824\n",
            "Training Iteration 5449, Loss: 3.2737739086151123\n",
            "Training Iteration 5450, Loss: 1.6958292722702026\n",
            "Training Iteration 5451, Loss: 4.836877346038818\n",
            "Training Iteration 5452, Loss: 4.278693675994873\n",
            "Training Iteration 5453, Loss: 5.191498279571533\n",
            "Training Iteration 5454, Loss: 2.7772350311279297\n",
            "Training Iteration 5455, Loss: 4.476707935333252\n",
            "Training Iteration 5456, Loss: 3.3944344520568848\n",
            "Training Iteration 5457, Loss: 5.102084159851074\n",
            "Training Iteration 5458, Loss: 5.599440574645996\n",
            "Training Iteration 5459, Loss: 3.909850597381592\n",
            "Training Iteration 5460, Loss: 2.386118173599243\n",
            "Training Iteration 5461, Loss: 2.9998157024383545\n",
            "Training Iteration 5462, Loss: 4.718123435974121\n",
            "Training Iteration 5463, Loss: 3.567380905151367\n",
            "Training Iteration 5464, Loss: 4.878086090087891\n",
            "Training Iteration 5465, Loss: 5.0218353271484375\n",
            "Training Iteration 5466, Loss: 2.4596240520477295\n",
            "Training Iteration 5467, Loss: 5.638045310974121\n",
            "Training Iteration 5468, Loss: 4.404938220977783\n",
            "Training Iteration 5469, Loss: 5.896048545837402\n",
            "Training Iteration 5470, Loss: 3.5829520225524902\n",
            "Training Iteration 5471, Loss: 3.1853137016296387\n",
            "Training Iteration 5472, Loss: 5.678379535675049\n",
            "Training Iteration 5473, Loss: 3.3818957805633545\n",
            "Training Iteration 5474, Loss: 4.113370895385742\n",
            "Training Iteration 5475, Loss: 4.843544960021973\n",
            "Training Iteration 5476, Loss: 2.762810468673706\n",
            "Training Iteration 5477, Loss: 2.5326521396636963\n",
            "Training Iteration 5478, Loss: 3.8554232120513916\n",
            "Training Iteration 5479, Loss: 3.4259703159332275\n",
            "Training Iteration 5480, Loss: 4.552742958068848\n",
            "Training Iteration 5481, Loss: 7.618030071258545\n",
            "Training Iteration 5482, Loss: 6.489439010620117\n",
            "Training Iteration 5483, Loss: 2.2090282440185547\n",
            "Training Iteration 5484, Loss: 6.833170413970947\n",
            "Training Iteration 5485, Loss: 2.7214670181274414\n",
            "Training Iteration 5486, Loss: 2.599327564239502\n",
            "Training Iteration 5487, Loss: 2.355323314666748\n",
            "Training Iteration 5488, Loss: 2.743756055831909\n",
            "Training Iteration 5489, Loss: 5.0209126472473145\n",
            "Training Iteration 5490, Loss: 4.1508283615112305\n",
            "Training Iteration 5491, Loss: 2.599212408065796\n",
            "Training Iteration 5492, Loss: 3.441627025604248\n",
            "Training Iteration 5493, Loss: 3.0315041542053223\n",
            "Training Iteration 5494, Loss: 4.2519025802612305\n",
            "Training Iteration 5495, Loss: 2.8561267852783203\n",
            "Training Iteration 5496, Loss: 2.0954771041870117\n",
            "Training Iteration 5497, Loss: 2.4566946029663086\n",
            "Training Iteration 5498, Loss: 4.3411455154418945\n",
            "Training Iteration 5499, Loss: 5.245057582855225\n",
            "Training Iteration 5500, Loss: 3.9475133419036865\n",
            "Training Iteration 5501, Loss: 2.972994804382324\n",
            "Training Iteration 5502, Loss: 3.3392088413238525\n",
            "Training Iteration 5503, Loss: 4.754175662994385\n",
            "Training Iteration 5504, Loss: 4.190269947052002\n",
            "Training Iteration 5505, Loss: 3.7807722091674805\n",
            "Training Iteration 5506, Loss: 3.0457603931427\n",
            "Training Iteration 5507, Loss: 3.1771085262298584\n",
            "Training Iteration 5508, Loss: 3.2429802417755127\n",
            "Training Iteration 5509, Loss: 2.8932459354400635\n",
            "Training Iteration 5510, Loss: 5.012271404266357\n",
            "Training Iteration 5511, Loss: 3.9575448036193848\n",
            "Training Iteration 5512, Loss: 4.048031806945801\n",
            "Training Iteration 5513, Loss: 2.126131057739258\n",
            "Training Iteration 5514, Loss: 4.210486888885498\n",
            "Training Iteration 5515, Loss: 3.801833391189575\n",
            "Training Iteration 5516, Loss: 5.854614734649658\n",
            "Training Iteration 5517, Loss: 5.253965854644775\n",
            "Training Iteration 5518, Loss: 2.209038496017456\n",
            "Training Iteration 5519, Loss: 2.3395841121673584\n",
            "Training Iteration 5520, Loss: 3.127990245819092\n",
            "Training Iteration 5521, Loss: 3.568186044692993\n",
            "Training Iteration 5522, Loss: 3.2980473041534424\n",
            "Training Iteration 5523, Loss: 5.225907802581787\n",
            "Training Iteration 5524, Loss: 3.4288527965545654\n",
            "Training Iteration 5525, Loss: 4.894640922546387\n",
            "Training Iteration 5526, Loss: 7.340595245361328\n",
            "Training Iteration 5527, Loss: 4.351581573486328\n",
            "Training Iteration 5528, Loss: 3.879157543182373\n",
            "Training Iteration 5529, Loss: 4.131681442260742\n",
            "Training Iteration 5530, Loss: 8.49994945526123\n",
            "Training Iteration 5531, Loss: 5.214469909667969\n",
            "Training Iteration 5532, Loss: 3.3999228477478027\n",
            "Training Iteration 5533, Loss: 5.9557576179504395\n",
            "Training Iteration 5534, Loss: 11.156793594360352\n",
            "Training Iteration 5535, Loss: 6.439908981323242\n",
            "Training Iteration 5536, Loss: 4.2118120193481445\n",
            "Training Iteration 5537, Loss: 2.894829750061035\n",
            "Training Iteration 5538, Loss: 2.3901424407958984\n",
            "Training Iteration 5539, Loss: 4.532955169677734\n",
            "Training Iteration 5540, Loss: 2.9133400917053223\n",
            "Training Iteration 5541, Loss: 3.4124677181243896\n",
            "Training Iteration 5542, Loss: 5.891002655029297\n",
            "Training Iteration 5543, Loss: 4.431838035583496\n",
            "Training Iteration 5544, Loss: 3.988389492034912\n",
            "Training Iteration 5545, Loss: 3.043278932571411\n",
            "Training Iteration 5546, Loss: 2.4106929302215576\n",
            "Training Iteration 5547, Loss: 5.246053218841553\n",
            "Training Iteration 5548, Loss: 4.571282863616943\n",
            "Training Iteration 5549, Loss: 2.396343946456909\n",
            "Training Iteration 5550, Loss: 3.542401075363159\n",
            "Training Iteration 5551, Loss: 4.488393783569336\n",
            "Training Iteration 5552, Loss: 2.449331283569336\n",
            "Training Iteration 5553, Loss: 5.1584553718566895\n",
            "Training Iteration 5554, Loss: 4.779936790466309\n",
            "Training Iteration 5555, Loss: 4.023702621459961\n",
            "Training Iteration 5556, Loss: 2.406832695007324\n",
            "Training Iteration 5557, Loss: 4.206009864807129\n",
            "Training Iteration 5558, Loss: 4.918792724609375\n",
            "Training Iteration 5559, Loss: 3.2049942016601562\n",
            "Training Iteration 5560, Loss: 6.439750671386719\n",
            "Training Iteration 5561, Loss: 4.848233222961426\n",
            "Training Iteration 5562, Loss: 4.856569290161133\n",
            "Training Iteration 5563, Loss: 2.7950916290283203\n",
            "Training Iteration 5564, Loss: 4.678624153137207\n",
            "Training Iteration 5565, Loss: 4.67851448059082\n",
            "Training Iteration 5566, Loss: 3.502896308898926\n",
            "Training Iteration 5567, Loss: 5.022068023681641\n",
            "Training Iteration 5568, Loss: 6.055706977844238\n",
            "Training Iteration 5569, Loss: 4.303617000579834\n",
            "Training Iteration 5570, Loss: 5.079909324645996\n",
            "Training Iteration 5571, Loss: 4.471223831176758\n",
            "Training Iteration 5572, Loss: 4.528722763061523\n",
            "Training Iteration 5573, Loss: 4.440749168395996\n",
            "Training Iteration 5574, Loss: 2.968538284301758\n",
            "Training Iteration 5575, Loss: 6.3736677169799805\n",
            "Training Iteration 5576, Loss: 4.442852020263672\n",
            "Training Iteration 5577, Loss: 2.6293396949768066\n",
            "Training Iteration 5578, Loss: 3.6321463584899902\n",
            "Training Iteration 5579, Loss: 4.195648193359375\n",
            "Training Iteration 5580, Loss: 3.6121485233306885\n",
            "Training Iteration 5581, Loss: 4.445457458496094\n",
            "Training Iteration 5582, Loss: 2.868865489959717\n",
            "Training Iteration 5583, Loss: 5.2607550621032715\n",
            "Training Iteration 5584, Loss: 5.6554646492004395\n",
            "Training Iteration 5585, Loss: 4.692830562591553\n",
            "Training Iteration 5586, Loss: 4.165408134460449\n",
            "Training Iteration 5587, Loss: 3.31876540184021\n",
            "Training Iteration 5588, Loss: 4.9670515060424805\n",
            "Training Iteration 5589, Loss: 4.976940155029297\n",
            "Training Iteration 5590, Loss: 3.982663154602051\n",
            "Training Iteration 5591, Loss: 6.026284694671631\n",
            "Training Iteration 5592, Loss: 2.5932610034942627\n",
            "Training Iteration 5593, Loss: 4.9264912605285645\n",
            "Training Iteration 5594, Loss: 5.676040172576904\n",
            "Training Iteration 5595, Loss: 3.994934558868408\n",
            "Training Iteration 5596, Loss: 3.211688756942749\n",
            "Training Iteration 5597, Loss: 6.052901268005371\n",
            "Training Iteration 5598, Loss: 2.6690611839294434\n",
            "Training Iteration 5599, Loss: 9.216670036315918\n",
            "Training Iteration 5600, Loss: 7.013118267059326\n",
            "Training Iteration 5601, Loss: 7.313821792602539\n",
            "Training Iteration 5602, Loss: 6.053461074829102\n",
            "Training Iteration 5603, Loss: 3.409447193145752\n",
            "Training Iteration 5604, Loss: 6.782736301422119\n",
            "Training Iteration 5605, Loss: 4.038353443145752\n",
            "Training Iteration 5606, Loss: 3.5302011966705322\n",
            "Training Iteration 5607, Loss: 4.734864234924316\n",
            "Training Iteration 5608, Loss: 3.5115530490875244\n",
            "Training Iteration 5609, Loss: 7.321134567260742\n",
            "Training Iteration 5610, Loss: 5.888172149658203\n",
            "Training Iteration 5611, Loss: 1.156745195388794\n",
            "Training Iteration 5612, Loss: 4.329975128173828\n",
            "Training Iteration 5613, Loss: 3.7382185459136963\n",
            "Training Iteration 5614, Loss: 3.7707431316375732\n",
            "Training Iteration 5615, Loss: 3.8638405799865723\n",
            "Training Iteration 5616, Loss: 7.110002517700195\n",
            "Training Iteration 5617, Loss: 8.18112564086914\n",
            "Training Iteration 5618, Loss: 4.652347564697266\n",
            "Training Iteration 5619, Loss: 2.854337692260742\n",
            "Training Iteration 5620, Loss: 3.9301109313964844\n",
            "Training Iteration 5621, Loss: 6.872152805328369\n",
            "Training Iteration 5622, Loss: 5.4113006591796875\n",
            "Training Iteration 5623, Loss: 3.2153759002685547\n",
            "Training Iteration 5624, Loss: 4.966302394866943\n",
            "Training Iteration 5625, Loss: 2.7934131622314453\n",
            "Training Iteration 5626, Loss: 3.3715872764587402\n",
            "Training Iteration 5627, Loss: 6.289391994476318\n",
            "Training Iteration 5628, Loss: 5.0227952003479\n",
            "Training Iteration 5629, Loss: 2.744880437850952\n",
            "Training Iteration 5630, Loss: 7.807705879211426\n",
            "Training Iteration 5631, Loss: 4.13221549987793\n",
            "Training Iteration 5632, Loss: 8.054862976074219\n",
            "Training Iteration 5633, Loss: 5.152295112609863\n",
            "Training Iteration 5634, Loss: 2.6147408485412598\n",
            "Training Iteration 5635, Loss: 4.457579612731934\n",
            "Training Iteration 5636, Loss: 2.332789421081543\n",
            "Training Iteration 5637, Loss: 2.108937978744507\n",
            "Training Iteration 5638, Loss: 4.050965309143066\n",
            "Training Iteration 5639, Loss: 5.173142910003662\n",
            "Training Iteration 5640, Loss: 4.018589973449707\n",
            "Training Iteration 5641, Loss: 3.2533063888549805\n",
            "Training Iteration 5642, Loss: 3.7041308879852295\n",
            "Training Iteration 5643, Loss: 4.210418701171875\n",
            "Training Iteration 5644, Loss: 3.2210190296173096\n",
            "Training Iteration 5645, Loss: 1.9648720026016235\n",
            "Training Iteration 5646, Loss: 2.299914598464966\n",
            "Training Iteration 5647, Loss: 3.79144024848938\n",
            "Training Iteration 5648, Loss: 6.315145015716553\n",
            "Training Iteration 5649, Loss: 6.831937789916992\n",
            "Training Iteration 5650, Loss: 2.9121596813201904\n",
            "Training Iteration 5651, Loss: 3.843146324157715\n",
            "Training Iteration 5652, Loss: 3.092745304107666\n",
            "Training Iteration 5653, Loss: 4.892856121063232\n",
            "Training Iteration 5654, Loss: 6.992864608764648\n",
            "Training Iteration 5655, Loss: 5.618239402770996\n",
            "Training Iteration 5656, Loss: 5.777145862579346\n",
            "Training Iteration 5657, Loss: 3.545687198638916\n",
            "Training Iteration 5658, Loss: 4.902519702911377\n",
            "Training Iteration 5659, Loss: 3.8249621391296387\n",
            "Training Iteration 5660, Loss: 3.806286573410034\n",
            "Training Iteration 5661, Loss: 5.6070027351379395\n",
            "Training Iteration 5662, Loss: 7.651747703552246\n",
            "Training Iteration 5663, Loss: 3.793517827987671\n",
            "Training Iteration 5664, Loss: 2.980393409729004\n",
            "Training Iteration 5665, Loss: 5.797877788543701\n",
            "Training Iteration 5666, Loss: 5.601437568664551\n",
            "Training Iteration 5667, Loss: 6.005259990692139\n",
            "Training Iteration 5668, Loss: 1.9751383066177368\n",
            "Training Iteration 5669, Loss: 3.575648069381714\n",
            "Training Iteration 5670, Loss: 4.891613960266113\n",
            "Training Iteration 5671, Loss: 4.463863849639893\n",
            "Training Iteration 5672, Loss: 2.0163509845733643\n",
            "Training Iteration 5673, Loss: 4.65686559677124\n",
            "Training Iteration 5674, Loss: 4.645588397979736\n",
            "Training Iteration 5675, Loss: 3.080599069595337\n",
            "Training Iteration 5676, Loss: 5.0573201179504395\n",
            "Training Iteration 5677, Loss: 4.767935276031494\n",
            "Training Iteration 5678, Loss: 5.865029811859131\n",
            "Training Iteration 5679, Loss: 5.227159023284912\n",
            "Training Iteration 5680, Loss: 4.631628513336182\n",
            "Training Iteration 5681, Loss: 3.814228057861328\n",
            "Training Iteration 5682, Loss: 4.534197807312012\n",
            "Training Iteration 5683, Loss: 5.209825038909912\n",
            "Training Iteration 5684, Loss: 3.3573451042175293\n",
            "Training Iteration 5685, Loss: 6.698195457458496\n",
            "Training Iteration 5686, Loss: 2.7307662963867188\n",
            "Training Iteration 5687, Loss: 4.486161231994629\n",
            "Training Iteration 5688, Loss: 5.40496301651001\n",
            "Training Iteration 5689, Loss: 3.853499412536621\n",
            "Training Iteration 5690, Loss: 6.893442630767822\n",
            "Training Iteration 5691, Loss: 5.830596923828125\n",
            "Training Iteration 5692, Loss: 4.57282018661499\n",
            "Training Iteration 5693, Loss: 5.2641167640686035\n",
            "Training Iteration 5694, Loss: 2.33828067779541\n",
            "Training Iteration 5695, Loss: 2.9841814041137695\n",
            "Training Iteration 5696, Loss: 4.969966411590576\n",
            "Training Iteration 5697, Loss: 4.226916313171387\n",
            "Training Iteration 5698, Loss: 4.225616455078125\n",
            "Training Iteration 5699, Loss: 2.9127390384674072\n",
            "Training Iteration 5700, Loss: 4.980049133300781\n",
            "Training Iteration 5701, Loss: 3.4949886798858643\n",
            "Training Iteration 5702, Loss: 5.104568004608154\n",
            "Training Iteration 5703, Loss: 3.849241256713867\n",
            "Training Iteration 5704, Loss: 5.453143119812012\n",
            "Training Iteration 5705, Loss: 4.480526447296143\n",
            "Training Iteration 5706, Loss: 6.469282150268555\n",
            "Training Iteration 5707, Loss: 5.00205135345459\n",
            "Training Iteration 5708, Loss: 0.7804402709007263\n",
            "Training Iteration 5709, Loss: 3.681025266647339\n",
            "Training Iteration 5710, Loss: 3.308770179748535\n",
            "Training Iteration 5711, Loss: 5.343659400939941\n",
            "Training Iteration 5712, Loss: 4.240596294403076\n",
            "Training Iteration 5713, Loss: 5.475407600402832\n",
            "Training Iteration 5714, Loss: 2.102238655090332\n",
            "Training Iteration 5715, Loss: 3.872606039047241\n",
            "Training Iteration 5716, Loss: 2.4438464641571045\n",
            "Training Iteration 5717, Loss: 1.5472809076309204\n",
            "Training Iteration 5718, Loss: 6.375739097595215\n",
            "Training Iteration 5719, Loss: 3.458951711654663\n",
            "Training Iteration 5720, Loss: 4.286945343017578\n",
            "Training Iteration 5721, Loss: 5.205463886260986\n",
            "Training Iteration 5722, Loss: 4.064869403839111\n",
            "Training Iteration 5723, Loss: 1.604745864868164\n",
            "Training Iteration 5724, Loss: 4.570991039276123\n",
            "Training Iteration 5725, Loss: 3.609858512878418\n",
            "Training Iteration 5726, Loss: 3.8380467891693115\n",
            "Training Iteration 5727, Loss: 3.830488920211792\n",
            "Training Iteration 5728, Loss: 3.959960699081421\n",
            "Training Iteration 5729, Loss: 4.786022186279297\n",
            "Training Iteration 5730, Loss: 3.3412423133850098\n",
            "Training Iteration 5731, Loss: 3.6248416900634766\n",
            "Training Iteration 5732, Loss: 8.679891586303711\n",
            "Training Iteration 5733, Loss: 3.8475606441497803\n",
            "Training Iteration 5734, Loss: 7.239606857299805\n",
            "Training Iteration 5735, Loss: 3.662271738052368\n",
            "Training Iteration 5736, Loss: 5.868411540985107\n",
            "Training Iteration 5737, Loss: 5.401983737945557\n",
            "Training Iteration 5738, Loss: 3.786560535430908\n",
            "Training Iteration 5739, Loss: 3.3530609607696533\n",
            "Training Iteration 5740, Loss: 4.736907005310059\n",
            "Training Iteration 5741, Loss: 4.571597099304199\n",
            "Training Iteration 5742, Loss: 4.342408657073975\n",
            "Training Iteration 5743, Loss: 4.590728282928467\n",
            "Training Iteration 5744, Loss: 9.602800369262695\n",
            "Training Iteration 5745, Loss: 1.924547791481018\n",
            "Training Iteration 5746, Loss: 3.182959794998169\n",
            "Training Iteration 5747, Loss: 5.154494762420654\n",
            "Training Iteration 5748, Loss: 1.9065282344818115\n",
            "Training Iteration 5749, Loss: 4.179377555847168\n",
            "Training Iteration 5750, Loss: 2.147179365158081\n",
            "Training Iteration 5751, Loss: 5.6264328956604\n",
            "Training Iteration 5752, Loss: 5.9583048820495605\n",
            "Training Iteration 5753, Loss: 5.408116817474365\n",
            "Training Iteration 5754, Loss: 7.157877445220947\n",
            "Training Iteration 5755, Loss: 4.367245674133301\n",
            "Training Iteration 5756, Loss: 3.4465363025665283\n",
            "Training Iteration 5757, Loss: 3.1260740756988525\n",
            "Training Iteration 5758, Loss: 5.388722896575928\n",
            "Training Iteration 5759, Loss: 4.843548774719238\n",
            "Training Iteration 5760, Loss: 3.4667465686798096\n",
            "Training Iteration 5761, Loss: 3.5094802379608154\n",
            "Training Iteration 5762, Loss: 4.396780490875244\n",
            "Training Iteration 5763, Loss: 4.94367790222168\n",
            "Training Iteration 5764, Loss: 2.2439966201782227\n",
            "Training Iteration 5765, Loss: 2.938494920730591\n",
            "Training Iteration 5766, Loss: 3.8565680980682373\n",
            "Training Iteration 5767, Loss: 3.6399261951446533\n",
            "Training Iteration 5768, Loss: 4.8128275871276855\n",
            "Training Iteration 5769, Loss: 2.5213539600372314\n",
            "Training Iteration 5770, Loss: 3.3973147869110107\n",
            "Training Iteration 5771, Loss: 5.699050426483154\n",
            "Training Iteration 5772, Loss: 6.284174919128418\n",
            "Training Iteration 5773, Loss: 1.7118732929229736\n",
            "Training Iteration 5774, Loss: 1.9631786346435547\n",
            "Training Iteration 5775, Loss: 3.044158697128296\n",
            "Training Iteration 5776, Loss: 3.947518825531006\n",
            "Training Iteration 5777, Loss: 6.732654094696045\n",
            "Training Iteration 5778, Loss: 5.106805324554443\n",
            "Training Iteration 5779, Loss: 5.578533172607422\n",
            "Training Iteration 5780, Loss: 2.401528835296631\n",
            "Training Iteration 5781, Loss: 3.843522548675537\n",
            "Training Iteration 5782, Loss: 5.00898551940918\n",
            "Training Iteration 5783, Loss: 3.9517836570739746\n",
            "Training Iteration 5784, Loss: 3.9927406311035156\n",
            "Training Iteration 5785, Loss: 3.8528196811676025\n",
            "Training Iteration 5786, Loss: 5.548914909362793\n",
            "Training Iteration 5787, Loss: 4.515305042266846\n",
            "Training Iteration 5788, Loss: 4.757714748382568\n",
            "Training Iteration 5789, Loss: 3.2482094764709473\n",
            "Training Iteration 5790, Loss: 4.407510280609131\n",
            "Training Iteration 5791, Loss: 4.613352298736572\n",
            "Training Iteration 5792, Loss: 3.2698874473571777\n",
            "Training Iteration 5793, Loss: 3.741523504257202\n",
            "Training Iteration 5794, Loss: 5.008201599121094\n",
            "Training Iteration 5795, Loss: 5.580132484436035\n",
            "Training Iteration 5796, Loss: 3.2102811336517334\n",
            "Training Iteration 5797, Loss: 1.827025055885315\n",
            "Training Iteration 5798, Loss: 3.82037353515625\n",
            "Training Iteration 5799, Loss: 4.699690341949463\n",
            "Training Iteration 5800, Loss: 5.402608871459961\n",
            "Training Iteration 5801, Loss: 6.483863830566406\n",
            "Training Iteration 5802, Loss: 3.5010077953338623\n",
            "Training Iteration 5803, Loss: 4.460886001586914\n",
            "Training Iteration 5804, Loss: 7.415882110595703\n",
            "Training Iteration 5805, Loss: 4.323250770568848\n",
            "Training Iteration 5806, Loss: 4.820457458496094\n",
            "Training Iteration 5807, Loss: 3.6200733184814453\n",
            "Training Iteration 5808, Loss: 2.8208093643188477\n",
            "Training Iteration 5809, Loss: 5.1244988441467285\n",
            "Training Iteration 5810, Loss: 4.2926344871521\n",
            "Training Iteration 5811, Loss: 3.7076492309570312\n",
            "Training Iteration 5812, Loss: 2.4126899242401123\n",
            "Training Iteration 5813, Loss: 3.206146478652954\n",
            "Training Iteration 5814, Loss: 2.505413293838501\n",
            "Training Iteration 5815, Loss: 4.284707069396973\n",
            "Training Iteration 5816, Loss: 4.512003421783447\n",
            "Training Iteration 5817, Loss: 5.560986518859863\n",
            "Training Iteration 5818, Loss: 2.522552490234375\n",
            "Training Iteration 5819, Loss: 3.6666696071624756\n",
            "Training Iteration 5820, Loss: 4.247100353240967\n",
            "Training Iteration 5821, Loss: 1.5075578689575195\n",
            "Training Iteration 5822, Loss: 3.3782923221588135\n",
            "Training Iteration 5823, Loss: 5.745420932769775\n",
            "Training Iteration 5824, Loss: 2.8505489826202393\n",
            "Training Iteration 5825, Loss: 4.629281997680664\n",
            "Training Iteration 5826, Loss: 2.906926155090332\n",
            "Training Iteration 5827, Loss: 4.8556036949157715\n",
            "Training Iteration 5828, Loss: 3.814142942428589\n",
            "Training Iteration 5829, Loss: 7.336143493652344\n",
            "Training Iteration 5830, Loss: 3.3970887660980225\n",
            "Training Iteration 5831, Loss: 5.474446773529053\n",
            "Training Iteration 5832, Loss: 6.456751346588135\n",
            "Training Iteration 5833, Loss: 3.9418888092041016\n",
            "Training Iteration 5834, Loss: 3.934541702270508\n",
            "Training Iteration 5835, Loss: 4.844718933105469\n",
            "Training Iteration 5836, Loss: 6.175232887268066\n",
            "Training Iteration 5837, Loss: 5.307524681091309\n",
            "Training Iteration 5838, Loss: 4.493406295776367\n",
            "Training Iteration 5839, Loss: 3.4088871479034424\n",
            "Training Iteration 5840, Loss: 6.361992835998535\n",
            "Training Iteration 5841, Loss: 5.548429489135742\n",
            "Training Iteration 5842, Loss: 3.9784703254699707\n",
            "Training Iteration 5843, Loss: 4.3264241218566895\n",
            "Training Iteration 5844, Loss: 3.34163236618042\n",
            "Training Iteration 5845, Loss: 3.553786039352417\n",
            "Training Iteration 5846, Loss: 2.228980541229248\n",
            "Training Iteration 5847, Loss: 9.939913749694824\n",
            "Training Iteration 5848, Loss: 7.571655750274658\n",
            "Training Iteration 5849, Loss: 5.212622165679932\n",
            "Training Iteration 5850, Loss: 6.163422584533691\n",
            "Training Iteration 5851, Loss: 4.198254108428955\n",
            "Training Iteration 5852, Loss: 4.2465596199035645\n",
            "Training Iteration 5853, Loss: 4.570154666900635\n",
            "Training Iteration 5854, Loss: 2.27647066116333\n",
            "Training Iteration 5855, Loss: 4.984221458435059\n",
            "Training Iteration 5856, Loss: 2.3919835090637207\n",
            "Training Iteration 5857, Loss: 3.7218518257141113\n",
            "Training Iteration 5858, Loss: 5.223957061767578\n",
            "Training Iteration 5859, Loss: 5.691854476928711\n",
            "Training Iteration 5860, Loss: 9.581894874572754\n",
            "Training Iteration 5861, Loss: 5.602788925170898\n",
            "Training Iteration 5862, Loss: 3.4214930534362793\n",
            "Training Iteration 5863, Loss: 4.512626647949219\n",
            "Training Iteration 5864, Loss: 3.5117838382720947\n",
            "Training Iteration 5865, Loss: 8.47912311553955\n",
            "Training Iteration 5866, Loss: 5.959392547607422\n",
            "Training Iteration 5867, Loss: 8.163420677185059\n",
            "Training Iteration 5868, Loss: 4.827139854431152\n",
            "Training Iteration 5869, Loss: 3.4030938148498535\n",
            "Training Iteration 5870, Loss: 4.050662994384766\n",
            "Training Iteration 5871, Loss: 5.119212627410889\n",
            "Training Iteration 5872, Loss: 4.5057268142700195\n",
            "Training Iteration 5873, Loss: 5.0448737144470215\n",
            "Training Iteration 5874, Loss: 4.265674114227295\n",
            "Training Iteration 5875, Loss: 4.181825160980225\n",
            "Training Iteration 5876, Loss: 2.0619242191314697\n",
            "Training Iteration 5877, Loss: 6.180007457733154\n",
            "Training Iteration 5878, Loss: 4.8420796394348145\n",
            "Training Iteration 5879, Loss: 7.640725612640381\n",
            "Training Iteration 5880, Loss: 6.0829267501831055\n",
            "Training Iteration 5881, Loss: 5.972717761993408\n",
            "Training Iteration 5882, Loss: 3.8275558948516846\n",
            "Training Iteration 5883, Loss: 3.939918041229248\n",
            "Training Iteration 5884, Loss: 7.122071266174316\n",
            "Training Iteration 5885, Loss: 4.931881427764893\n",
            "Training Iteration 5886, Loss: 3.9613213539123535\n",
            "Training Iteration 5887, Loss: 2.0546181201934814\n",
            "Training Iteration 5888, Loss: 3.939023494720459\n",
            "Training Iteration 5889, Loss: 4.068147659301758\n",
            "Training Iteration 5890, Loss: 6.545158863067627\n",
            "Training Iteration 5891, Loss: 1.6929913759231567\n",
            "Training Iteration 5892, Loss: 2.0456104278564453\n",
            "Training Iteration 5893, Loss: 5.233874320983887\n",
            "Training Iteration 5894, Loss: 2.4164235591888428\n",
            "Training Iteration 5895, Loss: 4.473559379577637\n",
            "Training Iteration 5896, Loss: 3.095348358154297\n",
            "Training Iteration 5897, Loss: 7.117721080780029\n",
            "Training Iteration 5898, Loss: 5.054731369018555\n",
            "Training Iteration 5899, Loss: 3.8268978595733643\n",
            "Training Iteration 5900, Loss: 4.261340141296387\n",
            "Training Iteration 5901, Loss: 4.897627830505371\n",
            "Training Iteration 5902, Loss: 4.014529705047607\n",
            "Training Iteration 5903, Loss: 3.0928218364715576\n",
            "Training Iteration 5904, Loss: 3.092020273208618\n",
            "Training Iteration 5905, Loss: 7.0555548667907715\n",
            "Training Iteration 5906, Loss: 4.808541297912598\n",
            "Training Iteration 5907, Loss: 4.459938049316406\n",
            "Training Iteration 5908, Loss: 4.075136184692383\n",
            "Training Iteration 5909, Loss: 3.7919201850891113\n",
            "Training Iteration 5910, Loss: 3.5962164402008057\n",
            "Training Iteration 5911, Loss: 3.29471492767334\n",
            "Training Iteration 5912, Loss: 4.545650005340576\n",
            "Training Iteration 5913, Loss: 6.607264041900635\n",
            "Training Iteration 5914, Loss: 5.941394805908203\n",
            "Training Iteration 5915, Loss: 4.7132978439331055\n",
            "Training Iteration 5916, Loss: 2.158599615097046\n",
            "Training Iteration 5917, Loss: 2.7836554050445557\n",
            "Training Iteration 5918, Loss: 3.96258282661438\n",
            "Training Iteration 5919, Loss: 1.4993683099746704\n",
            "Training Iteration 5920, Loss: 8.968137741088867\n",
            "Training Iteration 5921, Loss: 1.3156371116638184\n",
            "Training Iteration 5922, Loss: 2.954167366027832\n",
            "Training Iteration 5923, Loss: 2.678718090057373\n",
            "Training Iteration 5924, Loss: 10.496622085571289\n",
            "Training Iteration 5925, Loss: 4.2207417488098145\n",
            "Training Iteration 5926, Loss: 4.3685503005981445\n",
            "Training Iteration 5927, Loss: 2.525498390197754\n",
            "Training Iteration 5928, Loss: 3.5041027069091797\n",
            "Training Iteration 5929, Loss: 11.360766410827637\n",
            "Training Iteration 5930, Loss: 3.7898762226104736\n",
            "Training Iteration 5931, Loss: 6.428768157958984\n",
            "Training Iteration 5932, Loss: 7.771670341491699\n",
            "Training Iteration 5933, Loss: 2.9911868572235107\n",
            "Training Iteration 5934, Loss: 3.6311564445495605\n",
            "Training Iteration 5935, Loss: 1.3676602840423584\n",
            "Training Iteration 5936, Loss: 2.3580751419067383\n",
            "Training Iteration 5937, Loss: 3.1624317169189453\n",
            "Training Iteration 5938, Loss: 2.2949142456054688\n",
            "Training Iteration 5939, Loss: 3.666799545288086\n",
            "Training Iteration 5940, Loss: 1.9970405101776123\n",
            "Training Iteration 5941, Loss: 5.155265808105469\n",
            "Training Iteration 5942, Loss: 4.9130330085754395\n",
            "Training Iteration 5943, Loss: 4.8510260581970215\n",
            "Training Iteration 5944, Loss: 5.608813762664795\n",
            "Training Iteration 5945, Loss: 5.387598037719727\n",
            "Training Iteration 5946, Loss: 5.853677272796631\n",
            "Training Iteration 5947, Loss: 3.632479667663574\n",
            "Training Iteration 5948, Loss: 4.587169170379639\n",
            "Training Iteration 5949, Loss: 3.0751781463623047\n",
            "Training Iteration 5950, Loss: 5.64936637878418\n",
            "Training Iteration 5951, Loss: 4.695161819458008\n",
            "Training Iteration 5952, Loss: 4.119596481323242\n",
            "Training Iteration 5953, Loss: 6.237322807312012\n",
            "Training Iteration 5954, Loss: 4.522230625152588\n",
            "Training Iteration 5955, Loss: 9.804332733154297\n",
            "Training Iteration 5956, Loss: 4.977662086486816\n",
            "Training Iteration 5957, Loss: 3.0466206073760986\n",
            "Training Iteration 5958, Loss: 4.6838579177856445\n",
            "Training Iteration 5959, Loss: 4.025450706481934\n",
            "Training Iteration 5960, Loss: 8.90746021270752\n",
            "Training Iteration 5961, Loss: 3.158599615097046\n",
            "Training Iteration 5962, Loss: 2.78202748298645\n",
            "Training Iteration 5963, Loss: 6.720394134521484\n",
            "Training Iteration 5964, Loss: 2.7969770431518555\n",
            "Training Iteration 5965, Loss: 5.912998199462891\n",
            "Training Iteration 5966, Loss: 3.214141845703125\n",
            "Training Iteration 5967, Loss: 4.700796127319336\n",
            "Training Iteration 5968, Loss: 3.654684066772461\n",
            "Training Iteration 5969, Loss: 4.600574970245361\n",
            "Training Iteration 5970, Loss: 1.983305811882019\n",
            "Training Iteration 5971, Loss: 5.804343223571777\n",
            "Training Iteration 5972, Loss: 3.921590805053711\n",
            "Training Iteration 5973, Loss: 4.504217624664307\n",
            "Training Iteration 5974, Loss: 4.017050743103027\n",
            "Training Iteration 5975, Loss: 5.492990016937256\n",
            "Training Iteration 5976, Loss: 5.552554130554199\n",
            "Training Iteration 5977, Loss: 4.558201789855957\n",
            "Training Iteration 5978, Loss: 4.724681377410889\n",
            "Training Iteration 5979, Loss: 4.571092128753662\n",
            "Training Iteration 5980, Loss: 4.488733768463135\n",
            "Training Iteration 5981, Loss: 2.6808106899261475\n",
            "Training Iteration 5982, Loss: 2.1254963874816895\n",
            "Training Iteration 5983, Loss: 2.6288280487060547\n",
            "Training Iteration 5984, Loss: 2.758244037628174\n",
            "Training Iteration 5985, Loss: 9.475327491760254\n",
            "Training Iteration 5986, Loss: 6.008771896362305\n",
            "Training Iteration 5987, Loss: 8.03757381439209\n",
            "Training Iteration 5988, Loss: 1.712472677230835\n",
            "Training Iteration 5989, Loss: 4.22411584854126\n",
            "Training Iteration 5990, Loss: 4.049759387969971\n",
            "Training Iteration 5991, Loss: 5.244762420654297\n",
            "Training Iteration 5992, Loss: 6.297399997711182\n",
            "Training Iteration 5993, Loss: 4.450209140777588\n",
            "Training Iteration 5994, Loss: 3.5501456260681152\n",
            "Training Iteration 5995, Loss: 3.214789867401123\n",
            "Training Iteration 5996, Loss: 3.3334157466888428\n",
            "Training Iteration 5997, Loss: 5.88118314743042\n",
            "Training Iteration 5998, Loss: 2.763850212097168\n",
            "Training Iteration 5999, Loss: 3.5147528648376465\n",
            "Training Iteration 6000, Loss: 5.755236625671387\n",
            "Training Iteration 6001, Loss: 3.979973793029785\n",
            "Training Iteration 6002, Loss: 4.845909595489502\n",
            "Training Iteration 6003, Loss: 5.752167701721191\n",
            "Training Iteration 6004, Loss: 5.107293128967285\n",
            "Training Iteration 6005, Loss: 2.4182403087615967\n",
            "Training Iteration 6006, Loss: 8.02324104309082\n",
            "Training Iteration 6007, Loss: 3.0496816635131836\n",
            "Training Iteration 6008, Loss: 4.416759490966797\n",
            "Training Iteration 6009, Loss: 6.440586090087891\n",
            "Training Iteration 6010, Loss: 5.538202285766602\n",
            "Training Iteration 6011, Loss: 4.461872100830078\n",
            "Training Iteration 6012, Loss: 3.8475866317749023\n",
            "Training Iteration 6013, Loss: 2.8218491077423096\n",
            "Training Iteration 6014, Loss: 5.6732707023620605\n",
            "Training Iteration 6015, Loss: 9.519974708557129\n",
            "Training Iteration 6016, Loss: 4.202645778656006\n",
            "Training Iteration 6017, Loss: 5.569142818450928\n",
            "Training Iteration 6018, Loss: 5.407003879547119\n",
            "Training Iteration 6019, Loss: 4.276640892028809\n",
            "Training Iteration 6020, Loss: 3.4190361499786377\n",
            "Training Iteration 6021, Loss: 9.801058769226074\n",
            "Training Iteration 6022, Loss: 5.511447906494141\n",
            "Training Iteration 6023, Loss: 5.625511169433594\n",
            "Training Iteration 6024, Loss: 3.720094919204712\n",
            "Training Iteration 6025, Loss: 3.9704573154449463\n",
            "Training Iteration 6026, Loss: 4.4609479904174805\n",
            "Training Iteration 6027, Loss: 6.511466979980469\n",
            "Training Iteration 6028, Loss: 7.565998554229736\n",
            "Training Iteration 6029, Loss: 2.383416175842285\n",
            "Training Iteration 6030, Loss: 9.38662338256836\n",
            "Training Iteration 6031, Loss: 6.572160720825195\n",
            "Training Iteration 6032, Loss: 1.7416688203811646\n",
            "Training Iteration 6033, Loss: 6.282404899597168\n",
            "Training Iteration 6034, Loss: 4.666903018951416\n",
            "Training Iteration 6035, Loss: 5.475847244262695\n",
            "Training Iteration 6036, Loss: 3.534315347671509\n",
            "Training Iteration 6037, Loss: 4.478240013122559\n",
            "Training Iteration 6038, Loss: 3.60620379447937\n",
            "Training Iteration 6039, Loss: 5.805478096008301\n",
            "Training Iteration 6040, Loss: 5.141317367553711\n",
            "Training Iteration 6041, Loss: 5.532215118408203\n",
            "Training Iteration 6042, Loss: 6.236682891845703\n",
            "Training Iteration 6043, Loss: 6.497873783111572\n",
            "Training Iteration 6044, Loss: 5.637838363647461\n",
            "Training Iteration 6045, Loss: 3.292870283126831\n",
            "Training Iteration 6046, Loss: 2.5319859981536865\n",
            "Training Iteration 6047, Loss: 4.0065016746521\n",
            "Training Iteration 6048, Loss: 6.172154426574707\n",
            "Training Iteration 6049, Loss: 4.212404251098633\n",
            "Training Iteration 6050, Loss: 2.3797249794006348\n",
            "Training Iteration 6051, Loss: 3.7107622623443604\n",
            "Training Iteration 6052, Loss: 6.678042411804199\n",
            "Training Iteration 6053, Loss: 3.0166780948638916\n",
            "Training Iteration 6054, Loss: 3.2109148502349854\n",
            "Training Iteration 6055, Loss: 3.138446569442749\n",
            "Training Iteration 6056, Loss: 4.311099052429199\n",
            "Training Iteration 6057, Loss: 2.97292160987854\n",
            "Training Iteration 6058, Loss: 7.415775775909424\n",
            "Training Iteration 6059, Loss: 2.0590903759002686\n",
            "Training Iteration 6060, Loss: 4.83999490737915\n",
            "Training Iteration 6061, Loss: 5.398322582244873\n",
            "Training Iteration 6062, Loss: 2.654451847076416\n",
            "Training Iteration 6063, Loss: 4.4062299728393555\n",
            "Training Iteration 6064, Loss: 3.1514315605163574\n",
            "Training Iteration 6065, Loss: 3.4563591480255127\n",
            "Training Iteration 6066, Loss: 3.795832633972168\n",
            "Training Iteration 6067, Loss: 2.91180157661438\n",
            "Training Iteration 6068, Loss: 5.713309288024902\n",
            "Training Iteration 6069, Loss: 2.7527129650115967\n",
            "Training Iteration 6070, Loss: 3.2824549674987793\n",
            "Training Iteration 6071, Loss: 4.3732829093933105\n",
            "Training Iteration 6072, Loss: 3.6000094413757324\n",
            "Training Iteration 6073, Loss: 3.5355420112609863\n",
            "Training Iteration 6074, Loss: 1.8693305253982544\n",
            "Training Iteration 6075, Loss: 3.6472692489624023\n",
            "Training Iteration 6076, Loss: 4.281555652618408\n",
            "Training Iteration 6077, Loss: 5.485435485839844\n",
            "Training Iteration 6078, Loss: 3.89365553855896\n",
            "Training Iteration 6079, Loss: 1.421433448791504\n",
            "Training Iteration 6080, Loss: 6.561638832092285\n",
            "Training Iteration 6081, Loss: 5.608876705169678\n",
            "Training Iteration 6082, Loss: 1.8468017578125\n",
            "Training Iteration 6083, Loss: 6.751272201538086\n",
            "Training Iteration 6084, Loss: 3.8555006980895996\n",
            "Training Iteration 6085, Loss: 8.372846603393555\n",
            "Training Iteration 6086, Loss: 2.5432846546173096\n",
            "Training Iteration 6087, Loss: 3.4846277236938477\n",
            "Training Iteration 6088, Loss: 5.998385906219482\n",
            "Training Iteration 6089, Loss: 6.219065189361572\n",
            "Training Iteration 6090, Loss: 5.393494606018066\n",
            "Training Iteration 6091, Loss: 4.660154819488525\n",
            "Training Iteration 6092, Loss: 3.9340949058532715\n",
            "Training Iteration 6093, Loss: 4.085372447967529\n",
            "Training Iteration 6094, Loss: 3.518876075744629\n",
            "Training Iteration 6095, Loss: 3.424419641494751\n",
            "Training Iteration 6096, Loss: 9.318279266357422\n",
            "Training Iteration 6097, Loss: 8.62896728515625\n",
            "Training Iteration 6098, Loss: 7.5916361808776855\n",
            "Training Iteration 6099, Loss: 5.575041770935059\n",
            "Training Iteration 6100, Loss: 8.495665550231934\n",
            "Training Iteration 6101, Loss: 5.825852870941162\n",
            "Training Iteration 6102, Loss: 5.524201393127441\n",
            "Training Iteration 6103, Loss: 4.666865348815918\n",
            "Training Iteration 6104, Loss: 5.198896884918213\n",
            "Training Iteration 6105, Loss: 4.437747478485107\n",
            "Training Iteration 6106, Loss: 6.319772720336914\n",
            "Training Iteration 6107, Loss: 6.711984634399414\n",
            "Training Iteration 6108, Loss: 4.898552894592285\n",
            "Training Iteration 6109, Loss: 6.343021392822266\n",
            "Training Iteration 6110, Loss: 3.494480609893799\n",
            "Training Iteration 6111, Loss: 4.068945407867432\n",
            "Training Iteration 6112, Loss: 3.929182529449463\n",
            "Training Iteration 6113, Loss: 3.5245883464813232\n",
            "Training Iteration 6114, Loss: 8.440709114074707\n",
            "Training Iteration 6115, Loss: 4.998090744018555\n",
            "Training Iteration 6116, Loss: 4.0300116539001465\n",
            "Training Iteration 6117, Loss: 2.442708969116211\n",
            "Training Iteration 6118, Loss: 4.234025001525879\n",
            "Training Iteration 6119, Loss: 2.7668020725250244\n",
            "Training Iteration 6120, Loss: 2.8038763999938965\n",
            "Training Iteration 6121, Loss: 5.998598575592041\n",
            "Training Iteration 6122, Loss: 6.861105442047119\n",
            "Training Iteration 6123, Loss: 2.221857786178589\n",
            "Training Iteration 6124, Loss: 6.367599964141846\n",
            "Training Iteration 6125, Loss: 5.680100440979004\n",
            "Training Iteration 6126, Loss: 3.1123814582824707\n",
            "Training Iteration 6127, Loss: 3.3573248386383057\n",
            "Training Iteration 6128, Loss: 2.2992353439331055\n",
            "Training Iteration 6129, Loss: 3.442798614501953\n",
            "Training Iteration 6130, Loss: 3.506300926208496\n",
            "Training Iteration 6131, Loss: 3.667860507965088\n",
            "Training Iteration 6132, Loss: 5.304368019104004\n",
            "Training Iteration 6133, Loss: 4.411131858825684\n",
            "Training Iteration 6134, Loss: 2.809832811355591\n",
            "Training Iteration 6135, Loss: 1.8834867477416992\n",
            "Training Iteration 6136, Loss: 3.0293185710906982\n",
            "Training Iteration 6137, Loss: 4.294604778289795\n",
            "Training Iteration 6138, Loss: 5.797018527984619\n",
            "Training Iteration 6139, Loss: 2.0967752933502197\n",
            "Training Iteration 6140, Loss: 4.283444404602051\n",
            "Training Iteration 6141, Loss: 2.374133348464966\n",
            "Training Iteration 6142, Loss: 4.048484802246094\n",
            "Training Iteration 6143, Loss: 3.2070984840393066\n",
            "Training Iteration 6144, Loss: 1.9851855039596558\n",
            "Training Iteration 6145, Loss: 3.4275615215301514\n",
            "Training Iteration 6146, Loss: 2.375092029571533\n",
            "Training Iteration 6147, Loss: 3.298383951187134\n",
            "Training Iteration 6148, Loss: 2.9201676845550537\n",
            "Training Iteration 6149, Loss: 4.152498722076416\n",
            "Training Iteration 6150, Loss: 5.2785725593566895\n",
            "Training Iteration 6151, Loss: 5.127851963043213\n",
            "Training Iteration 6152, Loss: 2.7859034538269043\n",
            "Training Iteration 6153, Loss: 6.376018047332764\n",
            "Training Iteration 6154, Loss: 2.496156692504883\n",
            "Training Iteration 6155, Loss: 4.060152053833008\n",
            "Training Iteration 6156, Loss: 4.988627910614014\n",
            "Training Iteration 6157, Loss: 3.3090481758117676\n",
            "Training Iteration 6158, Loss: 4.613473892211914\n",
            "Training Iteration 6159, Loss: 3.910036087036133\n",
            "Training Iteration 6160, Loss: 3.9840803146362305\n",
            "Training Iteration 6161, Loss: 2.681852102279663\n",
            "Training Iteration 6162, Loss: 2.084652900695801\n",
            "Training Iteration 6163, Loss: 5.7006940841674805\n",
            "Training Iteration 6164, Loss: 4.169376850128174\n",
            "Training Iteration 6165, Loss: 3.7920916080474854\n",
            "Training Iteration 6166, Loss: 5.497670650482178\n",
            "Training Iteration 6167, Loss: 6.606584072113037\n",
            "Training Iteration 6168, Loss: 4.452775478363037\n",
            "Training Iteration 6169, Loss: 3.2455496788024902\n",
            "Training Iteration 6170, Loss: 1.9086450338363647\n",
            "Training Iteration 6171, Loss: 3.4202451705932617\n",
            "Training Iteration 6172, Loss: 3.7003517150878906\n",
            "Training Iteration 6173, Loss: 2.844642162322998\n",
            "Training Iteration 6174, Loss: 6.143949508666992\n",
            "Training Iteration 6175, Loss: 6.773804664611816\n",
            "Training Iteration 6176, Loss: 1.1859378814697266\n",
            "Training Iteration 6177, Loss: 2.8363680839538574\n",
            "Training Iteration 6178, Loss: 2.9058284759521484\n",
            "Training Iteration 6179, Loss: 3.0400776863098145\n",
            "Training Iteration 6180, Loss: 2.790286064147949\n",
            "Training Iteration 6181, Loss: 4.780790328979492\n",
            "Training Iteration 6182, Loss: 1.3665179014205933\n",
            "Training Iteration 6183, Loss: 1.295611023902893\n",
            "Training Iteration 6184, Loss: 4.259317874908447\n",
            "Training Iteration 6185, Loss: 2.4186463356018066\n",
            "Training Iteration 6186, Loss: 5.267274379730225\n",
            "Training Iteration 6187, Loss: 5.241656303405762\n",
            "Training Iteration 6188, Loss: 3.9927477836608887\n",
            "Training Iteration 6189, Loss: 2.793009042739868\n",
            "Training Iteration 6190, Loss: 5.5755109786987305\n",
            "Training Iteration 6191, Loss: 5.070462703704834\n",
            "Training Iteration 6192, Loss: 3.6788523197174072\n",
            "Training Iteration 6193, Loss: 2.753333568572998\n",
            "Training Iteration 6194, Loss: 2.640805721282959\n",
            "Training Iteration 6195, Loss: 4.120722770690918\n",
            "Training Iteration 6196, Loss: 6.282364845275879\n",
            "Training Iteration 6197, Loss: 4.2304301261901855\n",
            "Training Iteration 6198, Loss: 9.965191841125488\n",
            "Training Iteration 6199, Loss: 2.4922196865081787\n",
            "Training Iteration 6200, Loss: 2.818056583404541\n",
            "Training Iteration 6201, Loss: 2.727644681930542\n",
            "Training Iteration 6202, Loss: 6.517019271850586\n",
            "Training Iteration 6203, Loss: 5.920021057128906\n",
            "Training Iteration 6204, Loss: 1.8738200664520264\n",
            "Training Iteration 6205, Loss: 1.8455469608306885\n",
            "Training Iteration 6206, Loss: 3.6242451667785645\n",
            "Training Iteration 6207, Loss: 3.4555256366729736\n",
            "Training Iteration 6208, Loss: 2.838818073272705\n",
            "Training Iteration 6209, Loss: 2.9855637550354004\n",
            "Training Iteration 6210, Loss: 4.018640995025635\n",
            "Training Iteration 6211, Loss: 3.496875524520874\n",
            "Training Iteration 6212, Loss: 2.8707151412963867\n",
            "Training Iteration 6213, Loss: 2.510571241378784\n",
            "Training Iteration 6214, Loss: 2.772508144378662\n",
            "Training Iteration 6215, Loss: 2.13387131690979\n",
            "Training Iteration 6216, Loss: 5.07621955871582\n",
            "Training Iteration 6217, Loss: 5.423708438873291\n",
            "Training Iteration 6218, Loss: 7.477347373962402\n",
            "Training Iteration 6219, Loss: 4.667972564697266\n",
            "Training Iteration 6220, Loss: 3.5140833854675293\n",
            "Training Iteration 6221, Loss: 2.8551042079925537\n",
            "Training Iteration 6222, Loss: 2.787252902984619\n",
            "Training Iteration 6223, Loss: 5.0352630615234375\n",
            "Training Iteration 6224, Loss: 8.819676399230957\n",
            "Training Iteration 6225, Loss: 3.9826996326446533\n",
            "Training Iteration 6226, Loss: 3.742863416671753\n",
            "Training Iteration 6227, Loss: 4.0551557540893555\n",
            "Training Iteration 6228, Loss: 5.002303600311279\n",
            "Training Iteration 6229, Loss: 4.3007636070251465\n",
            "Training Iteration 6230, Loss: 2.460944652557373\n",
            "Training Iteration 6231, Loss: 2.9477930068969727\n",
            "Training Iteration 6232, Loss: 4.914759159088135\n",
            "Training Iteration 6233, Loss: 4.053253650665283\n",
            "Training Iteration 6234, Loss: 3.391331434249878\n",
            "Training Iteration 6235, Loss: 4.396099090576172\n",
            "Training Iteration 6236, Loss: 3.440702199935913\n",
            "Training Iteration 6237, Loss: 2.3687591552734375\n",
            "Training Iteration 6238, Loss: 6.752374172210693\n",
            "Training Iteration 6239, Loss: 4.097314834594727\n",
            "Training Iteration 6240, Loss: 4.354585647583008\n",
            "Training Iteration 6241, Loss: 2.286813974380493\n",
            "Training Iteration 6242, Loss: 7.868744850158691\n",
            "Training Iteration 6243, Loss: 4.566594123840332\n",
            "Training Iteration 6244, Loss: 6.606835842132568\n",
            "Training Iteration 6245, Loss: 5.761321544647217\n",
            "Training Iteration 6246, Loss: 4.089548587799072\n",
            "Training Iteration 6247, Loss: 3.166689872741699\n",
            "Training Iteration 6248, Loss: 4.072625637054443\n",
            "Training Iteration 6249, Loss: 5.518929958343506\n",
            "Training Iteration 6250, Loss: 4.449304580688477\n",
            "Training Iteration 6251, Loss: 3.95916748046875\n",
            "Training Iteration 6252, Loss: 5.420905590057373\n",
            "Training Iteration 6253, Loss: 5.1085286140441895\n",
            "Training Iteration 6254, Loss: 4.967327117919922\n",
            "Training Iteration 6255, Loss: 4.447294235229492\n",
            "Training Iteration 6256, Loss: 3.709644317626953\n",
            "Training Iteration 6257, Loss: 3.670060634613037\n",
            "Training Iteration 6258, Loss: 5.218968868255615\n",
            "Training Iteration 6259, Loss: 4.16951847076416\n",
            "Training Iteration 6260, Loss: 3.7536745071411133\n",
            "Training Iteration 6261, Loss: 2.4181456565856934\n",
            "Training Iteration 6262, Loss: 3.0609169006347656\n",
            "Training Iteration 6263, Loss: 2.7814269065856934\n",
            "Training Iteration 6264, Loss: 6.243666648864746\n",
            "Training Iteration 6265, Loss: 6.045013427734375\n",
            "Training Iteration 6266, Loss: 5.912395000457764\n",
            "Training Iteration 6267, Loss: 7.212501049041748\n",
            "Training Iteration 6268, Loss: 6.785139083862305\n",
            "Training Iteration 6269, Loss: 1.3171077966690063\n",
            "Training Iteration 6270, Loss: 4.208214282989502\n",
            "Training Iteration 6271, Loss: 4.448491096496582\n",
            "Training Iteration 6272, Loss: 5.623404502868652\n",
            "Training Iteration 6273, Loss: 3.9825997352600098\n",
            "Training Iteration 6274, Loss: 5.674315452575684\n",
            "Training Iteration 6275, Loss: 4.838502407073975\n",
            "Training Iteration 6276, Loss: 4.401560306549072\n",
            "Training Iteration 6277, Loss: 4.694976329803467\n",
            "Training Iteration 6278, Loss: 6.009803771972656\n",
            "Training Iteration 6279, Loss: 6.7444844245910645\n",
            "Training Iteration 6280, Loss: 5.195476055145264\n",
            "Training Iteration 6281, Loss: 3.610914707183838\n",
            "Training Iteration 6282, Loss: 7.196889877319336\n",
            "Training Iteration 6283, Loss: 5.165917873382568\n",
            "Training Iteration 6284, Loss: 4.420058250427246\n",
            "Training Iteration 6285, Loss: 4.1651930809021\n",
            "Training Iteration 6286, Loss: 4.176475524902344\n",
            "Training Iteration 6287, Loss: 8.135269165039062\n",
            "Training Iteration 6288, Loss: 7.367500305175781\n",
            "Training Iteration 6289, Loss: 5.197177410125732\n",
            "Training Iteration 6290, Loss: 4.006486892700195\n",
            "Training Iteration 6291, Loss: 7.529571533203125\n",
            "Training Iteration 6292, Loss: 6.499384880065918\n",
            "Training Iteration 6293, Loss: 3.180901527404785\n",
            "Training Iteration 6294, Loss: 3.958930730819702\n",
            "Training Iteration 6295, Loss: 6.117903709411621\n",
            "Training Iteration 6296, Loss: 4.536460876464844\n",
            "Training Iteration 6297, Loss: 5.452431678771973\n",
            "Training Iteration 6298, Loss: 5.382966995239258\n",
            "Training Iteration 6299, Loss: 3.691504716873169\n",
            "Training Iteration 6300, Loss: 2.0262694358825684\n",
            "Training Iteration 6301, Loss: 4.787980556488037\n",
            "Training Iteration 6302, Loss: 4.608865261077881\n",
            "Training Iteration 6303, Loss: 3.3010101318359375\n",
            "Training Iteration 6304, Loss: 5.156249046325684\n",
            "Training Iteration 6305, Loss: 3.29022479057312\n",
            "Training Iteration 6306, Loss: 4.650250434875488\n",
            "Training Iteration 6307, Loss: 3.7820370197296143\n",
            "Training Iteration 6308, Loss: 5.564383506774902\n",
            "Training Iteration 6309, Loss: 3.172851085662842\n",
            "Training Iteration 6310, Loss: 5.108031749725342\n",
            "Training Iteration 6311, Loss: 2.0892696380615234\n",
            "Training Iteration 6312, Loss: 3.9681625366210938\n",
            "Training Iteration 6313, Loss: 2.6608974933624268\n",
            "Training Iteration 6314, Loss: 3.0798299312591553\n",
            "Training Iteration 6315, Loss: 4.920350551605225\n",
            "Training Iteration 6316, Loss: 6.176762104034424\n",
            "Training Iteration 6317, Loss: 3.8204174041748047\n",
            "Training Iteration 6318, Loss: 9.214274406433105\n",
            "Training Iteration 6319, Loss: 3.942263126373291\n",
            "Training Iteration 6320, Loss: 4.800202369689941\n",
            "Training Iteration 6321, Loss: 3.3159284591674805\n",
            "Training Iteration 6322, Loss: 4.023924350738525\n",
            "Training Iteration 6323, Loss: 3.9769744873046875\n",
            "Training Iteration 6324, Loss: 6.508411407470703\n",
            "Training Iteration 6325, Loss: 5.60146427154541\n",
            "Training Iteration 6326, Loss: 5.025979995727539\n",
            "Training Iteration 6327, Loss: 3.717331886291504\n",
            "Training Iteration 6328, Loss: 4.135891914367676\n",
            "Training Iteration 6329, Loss: 3.898594379425049\n",
            "Training Iteration 6330, Loss: 2.910151720046997\n",
            "Training Iteration 6331, Loss: 7.551108360290527\n",
            "Training Iteration 6332, Loss: 3.8884456157684326\n",
            "Training Iteration 6333, Loss: 6.078411102294922\n",
            "Training Iteration 6334, Loss: 3.156115770339966\n",
            "Training Iteration 6335, Loss: 4.0450568199157715\n",
            "Training Iteration 6336, Loss: 5.445147514343262\n",
            "Training Iteration 6337, Loss: 4.091981410980225\n",
            "Training Iteration 6338, Loss: 10.504243850708008\n",
            "Training Iteration 6339, Loss: 3.467705011367798\n",
            "Training Iteration 6340, Loss: 5.433868885040283\n",
            "Training Iteration 6341, Loss: 4.403827667236328\n",
            "Training Iteration 6342, Loss: 4.333942413330078\n",
            "Training Iteration 6343, Loss: 4.386467933654785\n",
            "Training Iteration 6344, Loss: 4.174391746520996\n",
            "Training Iteration 6345, Loss: 5.494061470031738\n",
            "Training Iteration 6346, Loss: 10.565607070922852\n",
            "Training Iteration 6347, Loss: 6.366811752319336\n",
            "Training Iteration 6348, Loss: 4.071846961975098\n",
            "Training Iteration 6349, Loss: 3.0680322647094727\n",
            "Training Iteration 6350, Loss: 5.969204425811768\n",
            "Training Iteration 6351, Loss: 4.698988914489746\n",
            "Training Iteration 6352, Loss: 8.427742958068848\n",
            "Training Iteration 6353, Loss: 8.057811737060547\n",
            "Training Iteration 6354, Loss: 5.530679702758789\n",
            "Training Iteration 6355, Loss: 3.3046135902404785\n",
            "Training Iteration 6356, Loss: 4.531882286071777\n",
            "Training Iteration 6357, Loss: 2.5481388568878174\n",
            "Training Iteration 6358, Loss: 2.3434247970581055\n",
            "Training Iteration 6359, Loss: 7.961255073547363\n",
            "Training Iteration 6360, Loss: 6.835233211517334\n",
            "Training Iteration 6361, Loss: 5.427613735198975\n",
            "Training Iteration 6362, Loss: 4.6285200119018555\n",
            "Training Iteration 6363, Loss: 4.610360622406006\n",
            "Training Iteration 6364, Loss: 11.103433609008789\n",
            "Training Iteration 6365, Loss: 5.654477596282959\n",
            "Training Iteration 6366, Loss: 4.764059066772461\n",
            "Training Iteration 6367, Loss: 4.611304759979248\n",
            "Training Iteration 6368, Loss: 7.12345027923584\n",
            "Training Iteration 6369, Loss: 5.9607038497924805\n",
            "Training Iteration 6370, Loss: 10.64441204071045\n",
            "Training Iteration 6371, Loss: 3.557203769683838\n",
            "Training Iteration 6372, Loss: 7.048918724060059\n",
            "Training Iteration 6373, Loss: 3.493257999420166\n",
            "Training Iteration 6374, Loss: 5.073245048522949\n",
            "Training Iteration 6375, Loss: 8.563965797424316\n",
            "Training Iteration 6376, Loss: 6.014074802398682\n",
            "Training Iteration 6377, Loss: 6.155421733856201\n",
            "Training Iteration 6378, Loss: 7.320189952850342\n",
            "Training Iteration 6379, Loss: 6.613487243652344\n",
            "Training Iteration 6380, Loss: 7.4959397315979\n",
            "Training Iteration 6381, Loss: 5.5911102294921875\n",
            "Training Iteration 6382, Loss: 7.44789457321167\n",
            "Training Iteration 6383, Loss: 3.9051921367645264\n",
            "Training Iteration 6384, Loss: 7.129048824310303\n",
            "Training Iteration 6385, Loss: 6.454549789428711\n",
            "Training Iteration 6386, Loss: 4.870415210723877\n",
            "Training Iteration 6387, Loss: 7.280014514923096\n",
            "Training Iteration 6388, Loss: 5.175602436065674\n",
            "Training Iteration 6389, Loss: 7.055682182312012\n",
            "Training Iteration 6390, Loss: 4.752935886383057\n",
            "Training Iteration 6391, Loss: 5.205240249633789\n",
            "Training Iteration 6392, Loss: 4.83988094329834\n",
            "Training Iteration 6393, Loss: 11.694610595703125\n",
            "Training Iteration 6394, Loss: 2.122466564178467\n",
            "Training Iteration 6395, Loss: 7.130378246307373\n",
            "Training Iteration 6396, Loss: 3.465329170227051\n",
            "Training Iteration 6397, Loss: 6.504500865936279\n",
            "Training Iteration 6398, Loss: 3.3630259037017822\n",
            "Training Iteration 6399, Loss: 3.603271245956421\n",
            "Training Iteration 6400, Loss: 3.6229114532470703\n",
            "Training Iteration 6401, Loss: 8.328865051269531\n",
            "Training Iteration 6402, Loss: 7.706570148468018\n",
            "Training Iteration 6403, Loss: 4.432989597320557\n",
            "Training Iteration 6404, Loss: 6.027174472808838\n",
            "Training Iteration 6405, Loss: 3.3629631996154785\n",
            "Training Iteration 6406, Loss: 0.6429493427276611\n",
            "Training Iteration 6407, Loss: 4.882287502288818\n",
            "Training Iteration 6408, Loss: 7.310878276824951\n",
            "Training Iteration 6409, Loss: 4.144618034362793\n",
            "Training Iteration 6410, Loss: 3.173210620880127\n",
            "Training Iteration 6411, Loss: 6.560890197753906\n",
            "Training Iteration 6412, Loss: 4.000707626342773\n",
            "Training Iteration 6413, Loss: 2.6032967567443848\n",
            "Training Iteration 6414, Loss: 2.300083875656128\n",
            "Training Iteration 6415, Loss: 9.971710205078125\n",
            "Training Iteration 6416, Loss: 4.712498664855957\n",
            "Training Iteration 6417, Loss: 5.428408622741699\n",
            "Training Iteration 6418, Loss: 4.854200839996338\n",
            "Training Iteration 6419, Loss: 2.090157985687256\n",
            "Training Iteration 6420, Loss: 2.6190311908721924\n",
            "Training Iteration 6421, Loss: 4.852077007293701\n",
            "Training Iteration 6422, Loss: 3.25065016746521\n",
            "Training Iteration 6423, Loss: 3.215864658355713\n",
            "Training Iteration 6424, Loss: 5.11959981918335\n",
            "Training Iteration 6425, Loss: 3.02909517288208\n",
            "Training Iteration 6426, Loss: 2.9689929485321045\n",
            "Training Iteration 6427, Loss: 7.308224678039551\n",
            "Training Iteration 6428, Loss: 2.6834418773651123\n",
            "Training Iteration 6429, Loss: 6.061707496643066\n",
            "Training Iteration 6430, Loss: 4.561479568481445\n",
            "Training Iteration 6431, Loss: 3.382462501525879\n",
            "Training Iteration 6432, Loss: 3.0107123851776123\n",
            "Training Iteration 6433, Loss: 4.333737373352051\n",
            "Training Iteration 6434, Loss: 3.5286967754364014\n",
            "Training Iteration 6435, Loss: 10.220417022705078\n",
            "Training Iteration 6436, Loss: 5.448821067810059\n",
            "Training Iteration 6437, Loss: 3.2428574562072754\n",
            "Training Iteration 6438, Loss: 2.567781925201416\n",
            "Training Iteration 6439, Loss: 4.759654521942139\n",
            "Training Iteration 6440, Loss: 2.0126712322235107\n",
            "Training Iteration 6441, Loss: 6.2423601150512695\n",
            "Training Iteration 6442, Loss: 6.517411231994629\n",
            "Training Iteration 6443, Loss: 6.063770294189453\n",
            "Training Iteration 6444, Loss: 4.0384368896484375\n",
            "Training Iteration 6445, Loss: 3.6464293003082275\n",
            "Training Iteration 6446, Loss: 4.985048294067383\n",
            "Training Iteration 6447, Loss: 6.773364067077637\n",
            "Training Iteration 6448, Loss: 5.4757513999938965\n",
            "Training Iteration 6449, Loss: 4.059351921081543\n",
            "Training Iteration 6450, Loss: 3.369969367980957\n",
            "Training Iteration 6451, Loss: 5.90229606628418\n",
            "Training Iteration 6452, Loss: 4.450010776519775\n",
            "Training Iteration 6453, Loss: 6.299480438232422\n",
            "Training Iteration 6454, Loss: 6.06789493560791\n",
            "Training Iteration 6455, Loss: 6.863860130310059\n",
            "Training Iteration 6456, Loss: 9.426877975463867\n",
            "Training Iteration 6457, Loss: 2.076566696166992\n",
            "Training Iteration 6458, Loss: 4.611876487731934\n",
            "Training Iteration 6459, Loss: 3.6978938579559326\n",
            "Training Iteration 6460, Loss: 1.8559887409210205\n",
            "Training Iteration 6461, Loss: 5.746739387512207\n",
            "Training Iteration 6462, Loss: 4.867673873901367\n",
            "Training Iteration 6463, Loss: 6.003535270690918\n",
            "Training Iteration 6464, Loss: 8.74361515045166\n",
            "Training Iteration 6465, Loss: 6.27333402633667\n",
            "Training Iteration 6466, Loss: 3.9594242572784424\n",
            "Training Iteration 6467, Loss: 3.8011701107025146\n",
            "Training Iteration 6468, Loss: 3.8499557971954346\n",
            "Training Iteration 6469, Loss: 5.512995719909668\n",
            "Training Iteration 6470, Loss: 3.151183843612671\n",
            "Training Iteration 6471, Loss: 4.981849670410156\n",
            "Training Iteration 6472, Loss: 7.047626972198486\n",
            "Training Iteration 6473, Loss: 2.77986478805542\n",
            "Training Iteration 6474, Loss: 2.8736085891723633\n",
            "Training Iteration 6475, Loss: 5.992404937744141\n",
            "Training Iteration 6476, Loss: 2.938465118408203\n",
            "Training Iteration 6477, Loss: 3.631831645965576\n",
            "Training Iteration 6478, Loss: 2.4624247550964355\n",
            "Training Iteration 6479, Loss: 1.3454444408416748\n",
            "Training Iteration 6480, Loss: 4.579442501068115\n",
            "Training Iteration 6481, Loss: 4.994588375091553\n",
            "Training Iteration 6482, Loss: 3.482530355453491\n",
            "Training Iteration 6483, Loss: 3.899996757507324\n",
            "Training Iteration 6484, Loss: 6.011466979980469\n",
            "Training Iteration 6485, Loss: 4.0366597175598145\n",
            "Training Iteration 6486, Loss: 4.4393815994262695\n",
            "Training Iteration 6487, Loss: 6.284521102905273\n",
            "Training Iteration 6488, Loss: 4.556765079498291\n",
            "Training Iteration 6489, Loss: 4.933091163635254\n",
            "Training Iteration 6490, Loss: 2.7828216552734375\n",
            "Training Iteration 6491, Loss: 3.1480937004089355\n",
            "Training Iteration 6492, Loss: 6.351229190826416\n",
            "Training Iteration 6493, Loss: 3.3442111015319824\n",
            "Training Iteration 6494, Loss: 5.768044471740723\n",
            "Training Iteration 6495, Loss: 6.2101521492004395\n",
            "Training Iteration 6496, Loss: 5.370040416717529\n",
            "Training Iteration 6497, Loss: 2.9759392738342285\n",
            "Training Iteration 6498, Loss: 3.9390032291412354\n",
            "Training Iteration 6499, Loss: 4.967629909515381\n",
            "Training Iteration 6500, Loss: 5.2273478507995605\n",
            "Training Iteration 6501, Loss: 2.583827257156372\n",
            "Training Iteration 6502, Loss: 5.700202941894531\n",
            "Training Iteration 6503, Loss: 7.775584697723389\n",
            "Training Iteration 6504, Loss: 3.952786445617676\n",
            "Training Iteration 6505, Loss: 5.782645225524902\n",
            "Training Iteration 6506, Loss: 3.9730114936828613\n",
            "Training Iteration 6507, Loss: 2.674105167388916\n",
            "Training Iteration 6508, Loss: 2.6480841636657715\n",
            "Training Iteration 6509, Loss: 6.224254608154297\n",
            "Training Iteration 6510, Loss: 5.40349817276001\n",
            "Training Iteration 6511, Loss: 4.350393772125244\n",
            "Training Iteration 6512, Loss: 3.042877197265625\n",
            "Training Iteration 6513, Loss: 5.438545227050781\n",
            "Training Iteration 6514, Loss: 3.729837656021118\n",
            "Training Iteration 6515, Loss: 3.073533535003662\n",
            "Training Iteration 6516, Loss: 1.561600923538208\n",
            "Training Iteration 6517, Loss: 2.673916816711426\n",
            "Training Iteration 6518, Loss: 4.860330581665039\n",
            "Training Iteration 6519, Loss: 7.011218547821045\n",
            "Training Iteration 6520, Loss: 2.803621292114258\n",
            "Training Iteration 6521, Loss: 2.3965957164764404\n",
            "Training Iteration 6522, Loss: 4.190980911254883\n",
            "Training Iteration 6523, Loss: 5.277475833892822\n",
            "Training Iteration 6524, Loss: 3.2195751667022705\n",
            "Training Iteration 6525, Loss: 5.186333656311035\n",
            "Training Iteration 6526, Loss: 2.9344844818115234\n",
            "Training Iteration 6527, Loss: 7.147078990936279\n",
            "Training Iteration 6528, Loss: 7.5032958984375\n",
            "Training Iteration 6529, Loss: 2.480232000350952\n",
            "Training Iteration 6530, Loss: 8.403520584106445\n",
            "Training Iteration 6531, Loss: 4.197264194488525\n",
            "Training Iteration 6532, Loss: 1.899398684501648\n",
            "Training Iteration 6533, Loss: 5.115549087524414\n",
            "Training Iteration 6534, Loss: 3.0417275428771973\n",
            "Training Iteration 6535, Loss: 3.34847354888916\n",
            "Training Iteration 6536, Loss: 5.6975202560424805\n",
            "Training Iteration 6537, Loss: 5.511510848999023\n",
            "Training Iteration 6538, Loss: 5.635921478271484\n",
            "Training Iteration 6539, Loss: 5.724656581878662\n",
            "Training Iteration 6540, Loss: 6.828230857849121\n",
            "Training Iteration 6541, Loss: 5.820946216583252\n",
            "Training Iteration 6542, Loss: 5.105785846710205\n",
            "Training Iteration 6543, Loss: 4.248965263366699\n",
            "Training Iteration 6544, Loss: 5.927175998687744\n",
            "Training Iteration 6545, Loss: 2.9643630981445312\n",
            "Training Iteration 6546, Loss: 3.531266212463379\n",
            "Training Iteration 6547, Loss: 5.449677467346191\n",
            "Training Iteration 6548, Loss: 4.240862846374512\n",
            "Training Iteration 6549, Loss: 5.543715953826904\n",
            "Training Iteration 6550, Loss: 5.238345623016357\n",
            "Training Iteration 6551, Loss: 2.765347719192505\n",
            "Training Iteration 6552, Loss: 4.654081344604492\n",
            "Training Iteration 6553, Loss: 6.759634017944336\n",
            "Training Iteration 6554, Loss: 2.0217599868774414\n",
            "tensor([[1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        ...,\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12]])\n",
            "Training loss for epcoh 10: 3.584658224625598\n",
            "Training accuracy for epoch 10: 0.2823789722656697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        ...,\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12],\n",
            "        [1.9933e-01, 7.7769e-01, 7.6592e-03, 1.5229e-02, 9.7458e-05, 2.1226e-12]])\n",
            "Validation loss for epcoh 10: 3.6397840332301237\n",
            "Test accuracy for epoch 10: 0.2780956740672923\n"
          ]
        }
      ],
      "source": [
        "for epoch in trange(EPOCHS, desc='Total Epochs: ', colour=\"green\"):\n",
        "    train_epoch_loss = train(epoch)\n",
        "    #print(f\"Test Average Recall: {test_average_recall:.4f}\")\n",
        "    outputs_train, targets_train, train_epoch_loss = validation(training_loader)\n",
        "\n",
        "    #  Testing started from here\n",
        "    #  evaluate on training data\n",
        "    train_accuracy = accuracy_score(targets_train, outputs_train)\n",
        "    print(f\"Training loss for epcoh {epoch+1}: {train_epoch_loss}\")\n",
        "    print(f\"Training accuracy for epoch {epoch+1}: {train_accuracy}\")\n",
        "\n",
        "\n",
        "    # evaluate on testing data\n",
        "    outputs_test, targets_test, valid_epoch_loss = validation(testing_loader)\n",
        "    test_accuracy = accuracy_score(targets_test, outputs_test)\n",
        "    print(f\"Validation loss for epcoh {epoch+1}: {valid_epoch_loss}\")\n",
        "    print(f\"Test accuracy for epoch {epoch+1}: {test_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94ac58e26f3c45469861564941ab3ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1f330798f4f4fbfb878fca6b7920704",
              "IPY_MODEL_aacabf4b1fa0410fa40f70eed86caf8e",
              "IPY_MODEL_6ff50e0be48742608a8a46bfcec18667"
            ],
            "layout": "IPY_MODEL_78d0c5887b794e8f978380b539e7c937"
          }
        },
        "f1f330798f4f4fbfb878fca6b7920704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cb9f41b732b4d02881d4707bc6880b5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_58c97a9ebf5c4ef290eaad5d23e82c6c",
            "value": "Downloading (â€¦)olve/main/vocab.json: 100%"
          }
        },
        "aacabf4b1fa0410fa40f70eed86caf8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ed075a965c94ee2a0e4756fce818ad9",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49b3253aa5e1496d8c7bc02326539d88",
            "value": 898823
          }
        },
        "6ff50e0be48742608a8a46bfcec18667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e553b1b972914ddb90d48581887b5a59",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4ebabbe3be224d79a77bc6847b5e8369",
            "value": " 899k/899k [00:01&lt;00:00, 670kB/s]"
          }
        },
        "78d0c5887b794e8f978380b539e7c937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb9f41b732b4d02881d4707bc6880b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c97a9ebf5c4ef290eaad5d23e82c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ed075a965c94ee2a0e4756fce818ad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49b3253aa5e1496d8c7bc02326539d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e553b1b972914ddb90d48581887b5a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ebabbe3be224d79a77bc6847b5e8369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bfae0298b49426c86e8f14c33922587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96876374d8cb4c369a29e132bff097d9",
              "IPY_MODEL_114cd8db7ea245469b8f2afd12638b8c",
              "IPY_MODEL_5465a932687548e0870232366ad24a20"
            ],
            "layout": "IPY_MODEL_db5d593c4c384b73928ff7feaaa0f34a"
          }
        },
        "96876374d8cb4c369a29e132bff097d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9047d84171f40a7a53d8591a4184a82",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_399d10b734ba40ce80999bf3caf446cb",
            "value": "Downloading (â€¦)olve/main/merges.txt: 100%"
          }
        },
        "114cd8db7ea245469b8f2afd12638b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7160bf4626054f458d2e3554e756f3cd",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f4b808068a54a53aca2d04ea757e05f",
            "value": 456318
          }
        },
        "5465a932687548e0870232366ad24a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38384a539f6447d0b9f03f366271ca9b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8f32d10e127e4593b7f3a901b77b1f8a",
            "value": " 456k/456k [00:01&lt;00:00, 412kB/s]"
          }
        },
        "db5d593c4c384b73928ff7feaaa0f34a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9047d84171f40a7a53d8591a4184a82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "399d10b734ba40ce80999bf3caf446cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7160bf4626054f458d2e3554e756f3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4b808068a54a53aca2d04ea757e05f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38384a539f6447d0b9f03f366271ca9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f32d10e127e4593b7f3a901b77b1f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "643318fbeae545a7818cd712a9aa1cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f04aa6254e94e6686821c1e95f6499d",
              "IPY_MODEL_e12ae132c89246e1bb7672339f0abf78",
              "IPY_MODEL_546ea448a1b34f8fb01b25b2cc31656c"
            ],
            "layout": "IPY_MODEL_f98fc9ca4ca849a2bbdcf1f49135171a"
          }
        },
        "0f04aa6254e94e6686821c1e95f6499d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ef73dfdf8f94bd9ba7c9a03646e06de",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7d657fdeceff45328412ddbfd1a686c7",
            "value": "Downloading (â€¦)lve/main/config.json: 100%"
          }
        },
        "e12ae132c89246e1bb7672339f0abf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecac57ecc40744c0a1f8091becf42337",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_446bc9f0fd38436092baefef8b1eaaca",
            "value": 481
          }
        },
        "546ea448a1b34f8fb01b25b2cc31656c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a28056c74d74bed9182d772cb91d6aa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3b7da2b038024762a629f371d84a2e7d",
            "value": " 481/481 [00:00&lt;00:00, 6.16kB/s]"
          }
        },
        "f98fc9ca4ca849a2bbdcf1f49135171a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef73dfdf8f94bd9ba7c9a03646e06de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d657fdeceff45328412ddbfd1a686c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecac57ecc40744c0a1f8091becf42337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "446bc9f0fd38436092baefef8b1eaaca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a28056c74d74bed9182d772cb91d6aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b7da2b038024762a629f371d84a2e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19da3bb79d3449998ad4b5d4cf975fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d2853dc0b6b4104aaec1fb4ad6bd0ac",
              "IPY_MODEL_47d651852b774ed3bb7fb95c837a6437",
              "IPY_MODEL_39ee35eb1aae496a8d7a7c5cbfa4177f"
            ],
            "layout": "IPY_MODEL_6899e52941e34d048e1d2a1a5385de36"
          }
        },
        "0d2853dc0b6b4104aaec1fb4ad6bd0ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4adcffc6336f4c718da0c421475c6451",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1dd6d8240b7243179af663c5d26c5c0a",
            "value": "Downloading (â€¦)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "47d651852b774ed3bb7fb95c837a6437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5738a914d1f14bf39bd52d5bb2681c23",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9eae00a936c4574a2e917bac9134b86",
            "value": 501200538
          }
        },
        "39ee35eb1aae496a8d7a7c5cbfa4177f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cca2c7926ef04129a8248ae734294245",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_54a342427c78479589f5323e31bf5735",
            "value": " 501M/501M [00:03&lt;00:00, 159MB/s]"
          }
        },
        "6899e52941e34d048e1d2a1a5385de36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4adcffc6336f4c718da0c421475c6451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dd6d8240b7243179af663c5d26c5c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5738a914d1f14bf39bd52d5bb2681c23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9eae00a936c4574a2e917bac9134b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cca2c7926ef04129a8248ae734294245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54a342427c78479589f5323e31bf5735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c492a7efa5c4e5cb69a0ed4155cb743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_043899dd1cbb4c65835350d26b89eb75",
              "IPY_MODEL_e4fc382a8a124ed28e60e551794c5747",
              "IPY_MODEL_8b2ba646164b43359def87283404b0ed"
            ],
            "layout": "IPY_MODEL_c33818f8ed2d46d2a393bf4e51e68731"
          }
        },
        "043899dd1cbb4c65835350d26b89eb75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83cec01e601b4b05a5d706eb7e6bdb05",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d54dc6674fb948ac8dfef7ccdc5e5c08",
            "value": "Total Epochs: 100%"
          }
        },
        "e4fc382a8a124ed28e60e551794c5747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55cf8dceb634436a9a2479631b89f8d0",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab051e9273ce41b19149fd6c9fc8841f",
            "value": 10
          }
        },
        "8b2ba646164b43359def87283404b0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_410452f0b7cd496da67a7149f26a865a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c87606c0f9e14ac9817ffac8318763de",
            "value": " 10/10 [5:06:37&lt;00:00, 1834.97s/it]"
          }
        },
        "c33818f8ed2d46d2a393bf4e51e68731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83cec01e601b4b05a5d706eb7e6bdb05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d54dc6674fb948ac8dfef7ccdc5e5c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55cf8dceb634436a9a2479631b89f8d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab051e9273ce41b19149fd6c9fc8841f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "green",
            "description_width": ""
          }
        },
        "410452f0b7cd496da67a7149f26a865a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c87606c0f9e14ac9817ffac8318763de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c95cc0977837400c914be2ae528906cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9bccb50996f4df2b273be44cefb92a6",
              "IPY_MODEL_245bd22a3d784361b8050ea21f41d683",
              "IPY_MODEL_855f0df68820494089dda5e1e6956c2a"
            ],
            "layout": "IPY_MODEL_f12f71553c6b4f32a8829c4c12030c47"
          }
        },
        "b9bccb50996f4df2b273be44cefb92a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d24e3f5b43994a73bdba36a44fef3c09",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5c219e5f32484099b5d14d4d6529c2a2",
            "value": "Epoch No: 1: "
          }
        },
        "245bd22a3d784361b8050ea21f41d683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_774231e7e54c4090a909ac363555ecaa",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb74b91809464bafa8ba440368f2f904",
            "value": 6553
          }
        },
        "855f0df68820494089dda5e1e6956c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e95adbcb6cd946f78376d07435ccd148",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a3b6e35965c34f8791b0a0f88223976c",
            "value": " 6554/? [23:32&lt;00:00,  5.50it/s]"
          }
        },
        "f12f71553c6b4f32a8829c4c12030c47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24e3f5b43994a73bdba36a44fef3c09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c219e5f32484099b5d14d4d6529c2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "774231e7e54c4090a909ac363555ecaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb74b91809464bafa8ba440368f2f904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "e95adbcb6cd946f78376d07435ccd148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3b6e35965c34f8791b0a0f88223976c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ee41e99bd9f4a6595fc86be0d574edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2eb998586bbd46218446687aa7dd2e1c",
              "IPY_MODEL_ae5237f9ca424de5a1d5e837377f0874",
              "IPY_MODEL_e03c8f57dcce414ca6bbce344e7ed3ea"
            ],
            "layout": "IPY_MODEL_67626ce0574b46038688ea0f68ae140d"
          }
        },
        "2eb998586bbd46218446687aa7dd2e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a89cb41b9704da19d50b9b9fc340676",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d79f4062763b442481e7ac77c8295a72",
            "value": "Epoch No: 2: "
          }
        },
        "ae5237f9ca424de5a1d5e837377f0874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_055569a19fda462791b88b4a3266903d",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84586c6072de4132b32d3f50c79940bc",
            "value": 6553
          }
        },
        "e03c8f57dcce414ca6bbce344e7ed3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acd6b76021f94316a6b913ee5b67e1a2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_03eef5aadf9742b2947ed05193d2e0c6",
            "value": " 6554/? [23:04&lt;00:00,  5.53it/s]"
          }
        },
        "67626ce0574b46038688ea0f68ae140d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a89cb41b9704da19d50b9b9fc340676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d79f4062763b442481e7ac77c8295a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "055569a19fda462791b88b4a3266903d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84586c6072de4132b32d3f50c79940bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "acd6b76021f94316a6b913ee5b67e1a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03eef5aadf9742b2947ed05193d2e0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddfadc0336c0498f8b3338fab7e72635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4760443aaf944d9ca5687ad408b29d4a",
              "IPY_MODEL_7fea5a03a1b44b1699c78ea383b0ebcf",
              "IPY_MODEL_b1b3559364d04c7094e9ad61255bd382"
            ],
            "layout": "IPY_MODEL_ea0fe9aee0d34809ae545514914490cc"
          }
        },
        "4760443aaf944d9ca5687ad408b29d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1671fa7e5fa0462eaca162524c797d4b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3ffc1b07f5da4ec3bcf51b569253b3f7",
            "value": "Epoch No: 3: "
          }
        },
        "7fea5a03a1b44b1699c78ea383b0ebcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff07b390084e4f7280aa06a3fb7f8730",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef75da2f5c6440c2b9b6fb55e545ae07",
            "value": 6553
          }
        },
        "b1b3559364d04c7094e9ad61255bd382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fe216d577af427cac7b0222e428b076",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1e05b8a091e14edc803444fa96e96e45",
            "value": " 6554/? [22:54&lt;00:00,  5.61it/s]"
          }
        },
        "ea0fe9aee0d34809ae545514914490cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1671fa7e5fa0462eaca162524c797d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ffc1b07f5da4ec3bcf51b569253b3f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff07b390084e4f7280aa06a3fb7f8730": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef75da2f5c6440c2b9b6fb55e545ae07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "4fe216d577af427cac7b0222e428b076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e05b8a091e14edc803444fa96e96e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53375d19fa8c428cb67647444fa062c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_125e52cf8c01460c89b9541db70dd8ab",
              "IPY_MODEL_f2b80777e75e4a85b68e3e9e844ab5bf",
              "IPY_MODEL_7b747f5a5eb84bc4beb8457be8092e80"
            ],
            "layout": "IPY_MODEL_7fa0e275f3bb47cb8463f0711b5a229a"
          }
        },
        "125e52cf8c01460c89b9541db70dd8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df2b9ef55ea4e23bd26af6292563cf6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_672494baee9542b4966aacca02e2e46c",
            "value": "Epoch No: 4: "
          }
        },
        "f2b80777e75e4a85b68e3e9e844ab5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e35d43220176412a923d3e0b3e883d39",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75cf99d85c3844fe96de0a3c0089f1ad",
            "value": 6553
          }
        },
        "7b747f5a5eb84bc4beb8457be8092e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171e9625db6d458cb906bbf4735ad53f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bdd86e71d3f14ca19d3fe24ed3992d8f",
            "value": " 6554/? [22:51&lt;00:00,  5.58it/s]"
          }
        },
        "7fa0e275f3bb47cb8463f0711b5a229a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df2b9ef55ea4e23bd26af6292563cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "672494baee9542b4966aacca02e2e46c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e35d43220176412a923d3e0b3e883d39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75cf99d85c3844fe96de0a3c0089f1ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "171e9625db6d458cb906bbf4735ad53f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd86e71d3f14ca19d3fe24ed3992d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05322f4214f2468e8dc65933269ad0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b77b8375db74274a09aceb36ef70e6a",
              "IPY_MODEL_ffcf50c3a6ea4a629769bc90f01b5034",
              "IPY_MODEL_0062909d53cf407a9aa0e1bf8d406671"
            ],
            "layout": "IPY_MODEL_398e0c638d1246e4886688cde8ab9d8d"
          }
        },
        "2b77b8375db74274a09aceb36ef70e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3b22310622e4da19a553f444b9aa8d6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0f2e1a63d2724b94854c8fa81c4fb98f",
            "value": "Epoch No: 5: "
          }
        },
        "ffcf50c3a6ea4a629769bc90f01b5034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a3090901faa40e1b742945c4b2440d5",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3150ea59357425c93edc3afbb653d41",
            "value": 6553
          }
        },
        "0062909d53cf407a9aa0e1bf8d406671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83ff390c2d9d49c3890f4e39c4f13940",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef4b90b1276744c9b7dbc1940d95a077",
            "value": " 6554/? [22:52&lt;00:00,  5.60it/s]"
          }
        },
        "398e0c638d1246e4886688cde8ab9d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3b22310622e4da19a553f444b9aa8d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f2e1a63d2724b94854c8fa81c4fb98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a3090901faa40e1b742945c4b2440d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3150ea59357425c93edc3afbb653d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "83ff390c2d9d49c3890f4e39c4f13940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef4b90b1276744c9b7dbc1940d95a077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42cf010108004f9e9a749882344f9856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f673ec4c2cd474b96c014660ff4eb65",
              "IPY_MODEL_2d5b2ff4547e41f2900e23e463f08a26",
              "IPY_MODEL_695ba4bb3b9b4c6696bee26f9fb47cfe"
            ],
            "layout": "IPY_MODEL_8e19023a118b428ab160431a079ae08d"
          }
        },
        "1f673ec4c2cd474b96c014660ff4eb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2914a2479fc04a469d01f75b640f1840",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6c189a2c732a4717bff3544fb13ac11f",
            "value": "Epoch No: 6: "
          }
        },
        "2d5b2ff4547e41f2900e23e463f08a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50f725f9bb4e4e06bdc1f7404f0f2d9c",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08c2158f853e4faf8ab75722cfcf436e",
            "value": 6553
          }
        },
        "695ba4bb3b9b4c6696bee26f9fb47cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_112b9b83b0f940efba6d4662eca645ee",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e1839709699e47d7a3a0969a33d60b19",
            "value": " 6554/? [22:50&lt;00:00,  5.64it/s]"
          }
        },
        "8e19023a118b428ab160431a079ae08d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2914a2479fc04a469d01f75b640f1840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c189a2c732a4717bff3544fb13ac11f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50f725f9bb4e4e06bdc1f7404f0f2d9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08c2158f853e4faf8ab75722cfcf436e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "112b9b83b0f940efba6d4662eca645ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1839709699e47d7a3a0969a33d60b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61e40208a5c04f5a971a69ce1606347e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e5c7bf577394a1d968cdae49d4ef405",
              "IPY_MODEL_09bd63e8559b44098b5a5b0311c42efb",
              "IPY_MODEL_fafea01ac2734bff8bf93f478d58d2cc"
            ],
            "layout": "IPY_MODEL_05f561b1924848a895b7483e98903e3a"
          }
        },
        "8e5c7bf577394a1d968cdae49d4ef405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a540a3e8c4ce43dc9140ecfad658481f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_43ece7139cd74179ab02ada4af26ff29",
            "value": "Epoch No: 7: "
          }
        },
        "09bd63e8559b44098b5a5b0311c42efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8ef03b203ab4bb9abee2833f1407d79",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_046bbbc662494a79aab47d8683732c80",
            "value": 6553
          }
        },
        "fafea01ac2734bff8bf93f478d58d2cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bbb626bd3ff41729f8b85ca75cb4d10",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8e590d20978c4a7693e7bfe01bd46678",
            "value": " 6554/? [22:52&lt;00:00,  5.61it/s]"
          }
        },
        "05f561b1924848a895b7483e98903e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a540a3e8c4ce43dc9140ecfad658481f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43ece7139cd74179ab02ada4af26ff29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8ef03b203ab4bb9abee2833f1407d79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "046bbbc662494a79aab47d8683732c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "6bbb626bd3ff41729f8b85ca75cb4d10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e590d20978c4a7693e7bfe01bd46678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f6cf0cf57d54d50a14a7abe44018bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e43a21f7496741deafb39e69dcfa64e3",
              "IPY_MODEL_f0114abc98e845ea92f42a8f25421406",
              "IPY_MODEL_d581384fe89a41399a56e5d97400afb9"
            ],
            "layout": "IPY_MODEL_a440e72af980444695fe60c3c971b35b"
          }
        },
        "e43a21f7496741deafb39e69dcfa64e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_384c1f876a0d46a498b4f7e0c7f79d06",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a8802f0cbac7401982cece00857ab985",
            "value": "Epoch No: 8: "
          }
        },
        "f0114abc98e845ea92f42a8f25421406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a2409d1bc584eedb76c0b2f11602e8c",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d88ad786b701466e80e42a24d336fefa",
            "value": 6553
          }
        },
        "d581384fe89a41399a56e5d97400afb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6039df90bbde45c08b5d374b54997a89",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c84d582a066c4d80bf44d9c36a05b380",
            "value": " 6554/? [22:51&lt;00:00,  5.65it/s]"
          }
        },
        "a440e72af980444695fe60c3c971b35b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384c1f876a0d46a498b4f7e0c7f79d06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8802f0cbac7401982cece00857ab985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a2409d1bc584eedb76c0b2f11602e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d88ad786b701466e80e42a24d336fefa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "6039df90bbde45c08b5d374b54997a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c84d582a066c4d80bf44d9c36a05b380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1a0588431ab41b48f2ddb645fce31f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a24e7cf74dbf4868a989e182f8fbd00b",
              "IPY_MODEL_f36270909b6441869e171541d9dc50ad",
              "IPY_MODEL_06acbcb485bc454ab34d083ef452ba82"
            ],
            "layout": "IPY_MODEL_bfcd3e6d4e804eae8ee7564110697aa5"
          }
        },
        "a24e7cf74dbf4868a989e182f8fbd00b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7fb56370d9a46f490aa4eb0b862c96b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f23a3e6eeee54473802ff48328c36e5a",
            "value": "Epoch No: 9: "
          }
        },
        "f36270909b6441869e171541d9dc50ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5cd072a7fd94dcea2cc2c1294289079",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ac8cbb4493947b3b14a5a7bd9850274",
            "value": 6553
          }
        },
        "06acbcb485bc454ab34d083ef452ba82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08a8d1a822bb4904acbaa0db28027f1e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e61c626a92284bce904c9f7107091a57",
            "value": " 6554/? [22:51&lt;00:00,  5.61it/s]"
          }
        },
        "bfcd3e6d4e804eae8ee7564110697aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7fb56370d9a46f490aa4eb0b862c96b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f23a3e6eeee54473802ff48328c36e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5cd072a7fd94dcea2cc2c1294289079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ac8cbb4493947b3b14a5a7bd9850274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "08a8d1a822bb4904acbaa0db28027f1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e61c626a92284bce904c9f7107091a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "460f0129a25c4af6a5aaed5b6c17d4a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9f47ba2157f48bea8b24427f61ad89f",
              "IPY_MODEL_12cb957de17a490891b2f4b221d3f350",
              "IPY_MODEL_af6e83a9b79a44668ccd6de451a18d09"
            ],
            "layout": "IPY_MODEL_e5f9e0c1a8ec4878a5c2a4ad554c6e8c"
          }
        },
        "d9f47ba2157f48bea8b24427f61ad89f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed4ace53b5394de191616b89bc3bc248",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b71bc68ce9624500aa3fa908e5126b6a",
            "value": "Epoch No: 10: "
          }
        },
        "12cb957de17a490891b2f4b221d3f350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1766ebfa683947068187c8a93b7122c5",
            "max": 6553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd060465c8b94305a220f23ecf79c70e",
            "value": 6553
          }
        },
        "af6e83a9b79a44668ccd6de451a18d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0fd1a3a25964d1f85d80948d6c9effc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_56d68c5bbade4f718fa51464113c2c04",
            "value": " 6554/? [22:51&lt;00:00,  5.64it/s]"
          }
        },
        "e5f9e0c1a8ec4878a5c2a4ad554c6e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed4ace53b5394de191616b89bc3bc248": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71bc68ce9624500aa3fa908e5126b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1766ebfa683947068187c8a93b7122c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd060465c8b94305a220f23ecf79c70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "blue",
            "description_width": ""
          }
        },
        "d0fd1a3a25964d1f85d80948d6c9effc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d68c5bbade4f718fa51464113c2c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}