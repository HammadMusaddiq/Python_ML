{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    'DebiasMulticlassWordEmbedding-master/Debiasing/output/intersection_evalset/poly/reddit_US_txt_tok_clean_cleanedforw2v_0_inter_biasedEmbeddingsOut.w2v',\n",
    "    'DebiasMulticlassWordEmbedding-master/Debiasing/output/intersection_evalset/poly/reddit_US_txt_tok_clean_cleanedforw2v_0_inter_hardDebiasedEmbeddingsOut.w2v'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 1\n",
    "# the maximum number of different words to keep in the original texts\n",
    "# 40_000 is a normal number\n",
    "# 100_000 seems good too\n",
    "MAX_FEATURES = 100000 \n",
    "BATCH_SIZE = 512\n",
    "\n",
    "#units parameters in Keras.layers.LSTM/cuDNNLSTM\n",
    "#it it the dimension of the output vector of each LSTM cell.\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 1\n",
    "\n",
    "#we will convert each word in a comment_text to a number.\n",
    "#So a comment_text is a list of number. How many numbers in this list?\n",
    "#we want the length of this list is a constant -> MAX_LEN\n",
    "MAX_LEN = 220\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    #each line in the file looks like \n",
    "    # apple 0.3 0.4 0.5 0.6 ...\n",
    "    # that is a word followed by 50 float numbers\n",
    "\n",
    "    with open(path) as f:\n",
    "        #return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "        return dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(f))\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    #path: a path that contains embedding matrix\n",
    "    #word_index is a dict of the form ('apple': 123, 'banana': 349, etc)\n",
    "    # that means word_index[word] gives the index of the word\n",
    "    # word_index was built from all commment_texts\n",
    "\n",
    "    #we will construct an embedding_matrix for the words in word_index\n",
    "    #using pre-trained embedding word vectors from 'path'\n",
    "\n",
    "    embedding_index = load_embeddings(path)\n",
    "\n",
    "    #embedding_matrix is a matrix of len(word_index)+1  x 50\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "\n",
    "    # word_index is a dict. Each element is (word:i) where i is the index\n",
    "    # of the word\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            #RHS is a vector of 300d\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "   # a simpler version can be found here\n",
    "   # https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
    "\n",
    "   # Trainable params of the model: 1,671,687\n",
    "   # Recall that the number of samples in train.csv is 1_804_874\n",
    "\n",
    "    #words is a vector of MAX_LEN dimension\n",
    "    words = Input(shape=(MAX_LEN,))\n",
    "\n",
    "    #Embedding is the keras layer. We use the pre-trained embbeding_matrix\n",
    "    # https://keras.io/layers/embeddings/\n",
    "    # have to say that parameters in this layer are not trainable\n",
    "    # x is a vector of 600 dimension\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "\n",
    "    #*embedding_matrix.shape is a short way for \n",
    "    #input_dim = embedding_matrix.shape[0], output_dim  = embedding_matrix.shape[1]\n",
    "\n",
    "    #here the author used pre-train embedding matrix.\n",
    "    #instead of train from begining like in tensorflow example\n",
    "\n",
    "    #https://stackoverflow.com/questions/50393666/how-to-understand-spatialdropout1d-and-when-to-use-it\n",
    "    x = SpatialDropout1D(0.25)(x)\n",
    "\n",
    "    # x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    # x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='tanh')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid', name = 'main_output')(hidden)\n",
    "\n",
    "    #num_aux_targets = 6 since y_aux_train has 6 columns\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid', name = 'aux_ouput')(hidden)\n",
    "\n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "\n",
    "    #model.summary() will gives a good view of the model structure\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(clipnorm=0.1),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "\n",
    "#\n",
    "#Take the columns 'comment_text' from train,\n",
    "# then fillall NaN values by emtpy string '' (redundant)\n",
    "x_train = train['comment_text'].fillna('').values\n",
    "\n",
    "#if true, y_train[i] =1, if false, it is = 0\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "\n",
    "#\n",
    "#Take the columns 'comment_text' from test,\n",
    "# then fillall NaN values by emtpy string '' (redundant)\n",
    "x_test = test['comment_text'].fillna('').values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 45)\n",
      "(97320, 2)\n",
      "(1804874,)\n",
      "(1804874,)\n",
      "(1804874, 6)\n",
      "(97320,)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_aux_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/preprocessing/text/\n",
    "# tokenizer is a class with some method\n",
    "tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "#we apply method fit_on_texts of tokenizer on x_train and x_test\n",
    "#it will initialize some parameters/attribute inside tokenizer\n",
    "#https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L139\n",
    "#https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L210\n",
    "\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "#for example, after fit_on_texts, we can type\n",
    "#tokenizer.word_counts #give a OderedDict\n",
    "#tokenizer.document_counts # an int\n",
    "#tokenizer.word_index is a dict of words with correponding indices\n",
    "#There are 410046 different words in all 'comment_text'\n",
    "#len(tokenizer.word_index) == 410_046\n",
    "\n",
    "\n",
    "#these words come from all 'comment_text' in training.csv and test.csv\n",
    "\n",
    "#tokenizer.index_word: the inverse of tokenizer.word_index\n",
    "\n",
    "\n",
    "#https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L267\n",
    "#we will convert each word in a comment_text to a number.\n",
    "#So a comment_text is a list of number.\n",
    "\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804874\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0    18     6    33  2257\n",
      "    62    50 49792    10   102    35  1153     2   192    18  1024   120\n",
      "   164   360    95   218]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0 2053  344    8   10\n",
      "  134   35 6728  106   18 1332    2  152   55  116]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(x_train[0])\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/preprocessing/sequence/\n",
    "# https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py\n",
    "#each comment_text is now a list of word\n",
    "# we want the length of this list is a constant -> MAX_LEN\n",
    "# if the list is longer, then we cut/trim it \n",
    "# if shorter, then we add/pad it with 0's at the beginning\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 220)\n",
      "(97320, 220)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44895it [00:00, 49505.38it/s]\n",
      "44895it [00:01, 33844.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409328, 50)\n"
     ]
    }
   ],
   "source": [
    "# create an embedding_matrix \n",
    "#after this, embedding_matrix is a matrix of size\n",
    "# len(tokenizer.word_index)+1   x 50\n",
    "# for bw, dbw in EMBEDDING_FILES:\n",
    "biased_embedding_matrix = build_matrix(tokenizer.word_index, EMBEDDING_FILES[0])\n",
    "debiased_embedding_matrix = build_matrix(tokenizer.word_index, EMBEDDING_FILES[1])\n",
    "\n",
    "print(biased_embedding_matrix.shape)\n",
    "#== (?, 50)\n",
    "\n",
    "#embedding_matrix[i] is a 600d vector representation of the word whose index is i\n",
    "#embedding_matrix[10]\n",
    "#tokenizer.index_word[10] == 'you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(biased_embedding_matrix, y_aux_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 220)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 220, 50)      20466400    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " spatial_dropout1d (SpatialDrop  (None, 220, 50)     0           ['embedding[0][0]']              \n",
      " out1D)                                                                                           \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 220, 256)     183296      ['spatial_dropout1d[0][0]']      \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 220, 256)    394240      ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 256)         0           ['bidirectional_1[0][0]']        \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 256)         0           ['bidirectional_1[0][0]']        \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 512)          0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          262656      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 512)          0           ['concatenate[0][0]',            \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 512)          0           ['add[0][0]',                    \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " main_output (Dense)            (None, 1)            513         ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " aux_ouput (Dense)              (None, 6)            3078        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,572,839\n",
      "Trainable params: 1,106,439\n",
      "Non-trainable params: 20,466,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 220)\n",
      "(1804874,)\n",
      "(1804874, 6)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_aux_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 158s 16s/step - loss: 0.3433 - main_output_loss: 0.2032 - aux_ouput_loss: 0.1402 - main_output_accuracy: 0.9546 - aux_ouput_accuracy: 0.6630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25fd8c71420>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train[:5000],\n",
    "    [y_train[:5000], y_aux_train[:5000]],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    # callbacks=[\n",
    "    #     LearningRateScheduler(lambda epochs: 1e-3 * (0.6 ** global_epoch), verbose = 1)\n",
    "    # ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 27s 8s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test[:5000], batch_size=2048)[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'][:5000],\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1.pth\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1.pth\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "from keras.models import load_model\n",
    "reconstructed_model = load_model(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_trans",
   "language": "python",
   "name": "bert_trans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
